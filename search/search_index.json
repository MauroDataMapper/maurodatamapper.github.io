{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"privacy-policy/","title":"Privacy policy","text":"<p>Our contact details:</p> <p>Name: The Meta Foundation</p> <p>E-mail: contact@maurodata.org</p>"},{"location":"privacy-policy/#the-type-of-personal-information-we-collect","title":"The type of personal information we collect","text":"<p>We use Google Analytics to collect  information about how you use this website and how the website performs during your visit.  This includes IP addresses. This information will be for internal purposes and not shared externally except in  aggregate form. </p> <p>Google Analytics processes anonymised information about:</p> <ul> <li>the pages you visit on this documentation site</li> <li>how long you spend on each Mauro Data Mapper Documentation page</li> <li>how you got to this site</li> <li>what you click on while you\u2019re visiting this site</li> </ul> <p>We do not store your personal information through Google Analytics (for example your name or address).</p> <p>We will not identify you through analytics information, and we will not combine analytics  information with other data sets in a way that would identify who you are.</p>"},{"location":"privacy-policy/#our-legal-basis-for-processing-your-data","title":"Our legal basis for processing your data","text":"<p>The legal basis for processing anonymised data for Google Analytics is your consent.  You may withdraw  your consent by updating your cookie settings using the button below and unticking the option  for Google Analytics cookies.</p>"},{"location":"privacy-policy/#how-we-store-your-information","title":"How we store your information","text":"<p>For more details, please see the  \"Data privacy and security\"  document for Google Analytics:</p> <p>This privacy statement was last updated on 12<sup>th</sup> October 2021.</p> <p>Change cookie settings</p>"},{"location":"about/development-roadmap/","title":"Development Roadmap","text":"<p>Development of the core Mauro platform, its interfaces and plugins, are largely driven by the community.  A number of features planned for  development are listed below, in no particular order.  Some of these features are funded and underway; others may be de-prioritised based on user  demand.  For more information about any of these features, please contact us.</p>"},{"location":"about/development-roadmap/#core","title":"Core","text":"<ul> <li>Improved support for federation between instances of Mauro</li> <li>More advanced, configurable publication workflows</li> <li>Support for publishing DOIs</li> <li>Visual editors for Data Models</li> <li>Customizable dashboards and metrics</li> <li>Refinement of the Dataflow functionality and visualization </li> </ul>"},{"location":"about/development-roadmap/#security","title":"Security","text":"<ul> <li>Improved integration with OAuth / OpenID Connect for authorization</li> <li>Refinement of the API key functionality to restrict the scope of client application interactions </li> </ul>"},{"location":"about/development-roadmap/#import-export","title":"Import / export","text":"<ul> <li>Terminology and Reference Data Model Import plugins for SKOS</li> <li>A wider range of configuration options for exporting XML Schema</li> <li>User templating for Word / PDF exports</li> <li>Wizards for advanced import / export of models, including preview and customisation options</li> </ul>"},{"location":"about/development-roadmap/#other","title":"Other","text":"<ul> <li>Support for a wider range of email configurations</li> <li>Python API client library</li> <li>Simplified cloud-based deployment</li> <li>Setup wizard for first-time installation.</li> <li>Support for DOI registration and maintenance.</li> </ul>"},{"location":"about/history/","title":"History","text":"<p>The team at Oxford have been engaged in research and development of Metadata Registries for more than a decade. The Mauro Data Mapper represents the third generation of this technology. </p> <p>In the first generation, we used the eXist database as a simple store of XML representations of metadata items as individual attributes with arbitrary XML document types. This version of the catalogue did not make use of or enforce the constraints of a data modelling language, and the approach to semantic linkage was through classification, rather than individual assertions of refinement. It was applied successfully in two large projects, but proved difficult to maintain.</p> <p>The second generation of the technology was developed for the National Institute for Health Research (NIHR) Health Informatics Collaborative: a national programme aimed at facilitating the re-use of routinely-collected hospital data for medical research. This version of the catalogue allowed arbitrary links and included some of the features of the data modelling language presented here, but without a consistent, formal interpretation.</p> <p>The design included our principle that models should be the units of context and versioning. However, links were not contained within attributes, classes, or even models and hence not subject to model finalisation, making it impossible to ensure consistency. Furthermore, it was possible to import arbitrary items across models, without any guarantee that the definition of these items was independent of context.</p> <p>This third generation of the technology was developed from scratch, based on the experience of the previous implementations and, crucially, was developed in parallel with the formal semantics presented above. The result was a catalogue in which it was possible to guarantee consistency of definitions and to achieve a greater degree of scalability through automation and federation.</p> <p>The team have been engaged with the standards community, particularly the development of the ISO/IEC 11179/3 standard for metadata registration and related specifications.  While the model  defined in this standard is more closely aligned with the original generation of the tool, all subsequent implementations of Mauro maintain  compliance with this standard for interoperability.    </p>"},{"location":"about/introduction/","title":"Introduction","text":""},{"location":"about/introduction/#what-is-mauro-data-mapper","title":"What is Mauro Data Mapper?","text":"<p>Mauro Data Mapper is a toolkit for the design and documentation of databases, data flows, and data standards, as well as related software artefacts such as data schemas and data forms.  It was originally developed for the description of data in clinical research, but it is equally applicable in other settings.</p> <p>Data and software artefacts can be described as linked, versioned Data Models.  The links let us re-use and relate data definitions, recording and reasoning about semantic interoperability.  The versioning lets us keep track of changes in design, in implementation, or in understanding.</p>"},{"location":"about/introduction/#why-is-metadata-important","title":"Why is metadata important?","text":"<p>To fully understand the meaning of data, first we need to know some further information about its context, known as metadata. </p> <p>For example, consider a blood pressure reading. Although this has standard units, the method and state of the patient at the time the measurement was taken will affect the recorded value. Therefore, by outlining this additional information, the reading can be understood and interpreted more accurately. In this way, metadata allows data to be more searchable, comparable and standardised enabling further interoperability.</p>"},{"location":"about/introduction/#how-does-mauro-data-mapper-work","title":"How does Mauro Data Mapper work?","text":"<p>The Mauro Data Mapper is a web based tool which stores and manages descriptions of data. These can be descriptions of data already collected, such as databases or csv files. Or these can be descriptions of data you wish to collect or transfer between organisations, such as a specification for a webform or an XML schema. </p> <p>Mauro Data Mapper represents both types of descriptions of data as Data Models. These are defined as a structured collection of metadata and effectively model the data that they describe.</p> <p></p> <p>Each Data Model consists of several Data Classes, which are groups of data that are related in some way. For example, a group of data that appears in the same table of a database or the same section of a form. Data Classes can sometimes also contain Nested Data Classes.</p> <p>Within each Data Class is then a number of Data Elements which are the descriptions of an individual field or variable.</p> <p>For example, a webform where patients enter their details would be a Data Model. This form could consist of two separate sections such as 'Personal details' and 'Contact details' which would each be a Data Class. The individual entries within each of these sections, such as 'First Name', 'Last Name', 'Date of Birth' etc, would each be a Data Element.</p> <p>However, there might be a section within another section on the webform, such as 'Correspondence Address' which lies within 'Contact details'. In this case, 'Correspondence Address' would become a Nested Data Class, where the 'Contact details' Data Class would be the parent. </p> <p></p> <p></p> <p>By organising metadata in this way, Mauro Data Mapper allows users to easily search data but also automatically import database schemas and export forms; helping to record data in standardised formats.</p>"},{"location":"about/introduction/#an-open-source-community","title":"An open-source community","text":"<p>The Mauro platform and plugins are distributed under an open source Apache 2.0 license. We are keen  to build an active community of users and developers, and encourage contributions to our code and documentation, and facilitate model sharing.  </p>"},{"location":"about/introduction/#support","title":"Support","text":"<p>The development of Mauro Data Mapper has been funded by the NIHR Oxford Biomedical Research Center as part of the NIHR Health Informatics Collaborative (HIC). </p> <p>The NIHR HIC is a partnership of 28 NHS trusts and health boards, including the 20 hosting NIHR Biomedical Research Centres (BRCs), working together to facilitate the equitable re-use of NHS data for translational research. </p> <p>The NIHR HIC has established cross-site data collaborations in areas such as cardiovascular medicine, critical care, renal disease, infectious diseases, and cancer. Mauro Data Mapper, and its previous incarnation, the Metadata Catalogue, has been used for collaboratively editing Data Models for research, and for generating software artefacts such as XML Schema.</p>"},{"location":"about/mauro/","title":"Mauro","text":""},{"location":"about/mauro/#who-is-mauro","title":"Who is Mauro?","text":"<p>Mauro Data Mapper is named after Fra Mauro who was a 15<sup>th</sup> century Venetian Monk and also a famous cartographer. A diligent scholar, and enthusiast of  ancient greek geographers such as Ptolemy, Mauro created the most detailed map of the world at the time, featuring  thousands of texts and illustrations.</p> <p>Unlike other explorers who would embark on expeditions to gather their own data about the world, Mauro adopted a different approach and based himself  at the port in Venice. He interviewed travellers who would give their detailed interpretations of the countries they had visited.  Mauro then merged this information together to generate maps which were rich with contextual data. Unusually for the time, these maps were based  on authoritative sources and proofs of existence, rather than religion and superstition.  </p> <p>This philosophy lies at the heart of Mauro Data Mapper, which not only aims to be a hub of detailed metadata, but also allows users to make  and share their own insights, understanding and experience.</p> <p></p> <p>The Fra Mauro map of the world, shown above (source: Wikipedia), was produced between 1448 and 1459 as a commission for King Alfonso V of Portugal. Containing thousands of texts and illustrations, the map would have measured over five square metres.  For a higher-resolution version,  click here. </p> <p>Fra Mauro's Mappa Mundi along with a copy dated 1804 are stored in the British Library. To find out more about these historic items, watch the video below, courtesy of the British Library.</p>"},{"location":"about/mauro/#further-reading","title":"Further reading","text":"<ul> <li> <p>Wikipedia</p> <p>Wikipedia entry on Fra Mauro</p> </li> <li> <p>A Mapmaker's Dream: The Meditations of Fra Mauro, Cartographer to the Court of Venice</p> <p>James Cowan, 1997</p> </li> <li> <p>Fra Mauro and the Modern Globe</p> <p>Klaus A. Vogel, No. 57/58, Papers, Read at the 11TH Coronelli Symposium, Venice 2007, and other papers (2011 (for 2009/2010)), pp. 81-92, JSTOR.org</p> </li> <li> <p>Fra Mauro's Mappa Mundi</p> <p>Google Arts &amp; Culture</p> </li> </ul>"},{"location":"about/release-notes/","title":"Release Notes","text":"<p>This page describes the changes for each component: 'core', 'ui', and the various centrally-maintained plugins and client libraries. For each, a table is presented with version number, notable new features, notable pull requests, and any dependencies.</p> <p>For more information about the structure and architecture of the code, please see our technical architecture pages.</p> <p>We use Semantic Versioning for all repositories, please note that anything labelled with <code>-SNAPSHOT</code> is under development and should be considered  potentially unstable.</p> <p>In our code repositories, we use Git Flow,  and so the 'main / master' branch may be considered stable,  but 'bleeding edge' features may be available within 'develop' or any feature branch.</p> <p>Please see our Installing Plugins pages for details about build artefacts and dependencies.</p> <p>The current full release is 2023.1 (a.k.a B5.3.0_F7.3.0).</p>"},{"location":"about/release-notes/#core-api","title":"Core API","text":"<p>GitHub</p> Version Release Date Major Changes 5.3.0(GitHub Link) 24th February 2023 <ul> <li>Updated to Grails 5.3.2 and Groovy 3.0.11</li> <li>Pagination, listing and folder tree fixes</li> <li>Federation support for subscribing to OAuth-secured feeds</li> <li>Initial support for running on Amazon Aurora RDS</li> <li>Other fixes and performance improvements</li> </ul> 5.2.0(GitHub Link) 18th August 2022 <ul> <li>Updated to Grails 5.1.9</li> <li>Federation support for subscribing to Atom feeds</li> <li>Permissions fixes and improvements</li> </ul> 5.1.0(GitHub Link) 29th April 2022 <ul> <li>Updated to Groovy 3.0.10</li> <li>Improvements to Reference Data Models</li> <li>Performance fixes and improvements</li> </ul> 5.0.0(GitHub Link) 28th January 2022 <ul> <li>Updated to Java 17 (Temurin)</li> <li>Updated to Grails 5.1.2</li> <li>Updated to Groovy 3.0.9</li> <li>Updated to Gradle 7.3.3</li> </ul> 4.11.0(GitHub Link) 3rd November 2021 <ul> <li>Updates and bug-fixes for Profiles, Subscribed Catalogues, Reference Data Models and more</li> </ul> 4.10.0(GitHub Link) 2nd September 2021 <ul> <li>Add API endpoints for editing Reference Data Models</li> <li>Configurable bootstrapping in production mode</li> <li>Many improvements and bug-fixes to Versioned Folders, merging and branching</li> </ul> 4.9.0(GitHub Link) 24th August 2021 <ul> <li>Allow profiles to be editable after finalisation</li> <li>Add derived field types for profiles</li> </ul> 4.8.0(GitHub Link) 7th August 2021 <ul> <li>Updates to branching and finalising for Versioned Folders</li> <li>Speed increase on finalising models</li> <li>Add rules to merge functionality</li> <li>Handle system configuration in multiple .yml files</li> <li>Various improvements to base import functionality</li> <li>Other bug fixes</li> </ul> 4.7.0(GitHub Link) 1st July 2021 <ul> <li>Bug fixes for dynamic profiles</li> <li>Further improvements for versioning, versioned folders, and merging model branches</li> <li>Other bug fixes</li> </ul> 4.6.0(GitHub Link) 18th June 2021 <ul> <li>Bug fixes for dynamic profiles</li> <li>More endpoints for versioned folders and versioning</li> <li>Improve performance of terminology importer</li> </ul> 4.5.0(GitHub Link) 18th May 2021 <ul> <li>Dynamic Profiles</li> <li>Initial endpoints for Versioned Folders</li> <li>Add Facets to all containers</li> </ul> 4.4.1(GitHub Link) 22nd April 2021 <ul> <li>Hidden / internal importer parameters</li> <li>ATOM feed for known models</li> <li>Bug fixes</li> </ul> 4.3.0(GitHub Link) 26th Mar 2021 <ul> <li>Multiple bug fixes</li> <li>More control over admin system properties</li> <li>Add change notes to the edit history</li> <li>Custom tag names on model branches</li> </ul> 4.2.0(GitHub Link) 4th Mar 2021 <ul> <li>DataModel component import feature</li> <li>Dataflow importer / exporter in XML / JSON </li> <li>Fix for Lucene indexing failure</li> <li>Fix for exception during model import</li> <li>Back-end performance improvements</li> </ul> 4.1.0(GitHub Link) 10th Feb 2021 <ul> <li>Fix for multi-model imports</li> </ul> 4.0.0(GitHub Link) 1st Feb 2021 Listing major changes since last release of Metadata Catalogue <ul> <li>Upgrade to Grails 4</li> <li>Complete refactoring of code and underlying database</li> <li>Modular code structure</li> <li>Reference Data Model</li> <li>Removed access controls for individuals, increased the options for groups</li> <li>Updated Security Module</li> <li>API Keys / access tokens</li> <li>Much improved support for branching and merging</li> </ul>"},{"location":"about/release-notes/#ui","title":"UI","text":"<p>GitHub</p> Version Release Date Major Changes 7.3.0(GitHub Link) 24th February 2023 <ul> <li>Updates to Profiles, Favourites, and Code Sets</li> <li>Wizards for creating Reference Data Types and Models</li> <li>Visual improvements</li> <li>Other improvements and fixes</li> </ul> 7.2.0(GitHub Link) 18th August 2022 <ul> <li>Updates to Search pages</li> <li>Updates to Bulk Editor</li> <li>Display of asynchronous jobs</li> </ul> 7.1.0(GitHub Link) 29th April 2022 <ul> <li>Updates and fixes to UI components including navigation tree, dialogs, and editors</li> </ul> 7.0.0(GitHub Link) 28th January 2022 <ul> <li>Updated to Angular 12</li> <li>Updated to Node 14.18.1</li> <li>Updated to NPM 8.3.0</li> </ul> 6.7.0(GitHub Link) 3rd November 2021 <ul> <li>Update admin session dashboard</li> <li>Features for importing / extending data model components</li> <li>Many updates and bug-fixes to a range of components (see full list on GitHub)</li> </ul> 6.6.1(GitHub Link) 22nd September 2021 <ul> <li>Open ID Connect redirect issue fixed</li> </ul> 6.6.0(GitHub Link) 2nd September 2021 <ul> <li>Term search within Terminology fixed</li> <li>Fix for advanced search within a folder</li> <li>Improvements to merge diff tool</li> </ul> 6.5.0(GitHub Link) 24th August 2021 <ul> <li>Improvements to profiles and other DOI profile functionality</li> <li>Fix to favourite highlighting</li> <li>DataFlow presentation no-longer using paginated call</li> <li>Open ID Connect conformance issue resolved</li> <li>Various other bug fixes and layout improvements</li> </ul> 6.4.0(GitHub Link) 7th August 2021 <ul> <li>Fix for making models publicly readable</li> <li>DOI functionality (requires plugin)</li> <li>Multiple updates to model merging tool</li> <li>Fix to merge graph for Versioned Folders</li> <li>Drag-and-drop ordering for Data Elements</li> <li>\"Select all\" option for adding Terms to a Codeset</li> <li>Tree icons for model items</li> <li>Various other bug fixes</li> </ul> 6.3.1(GitHub Link) 1st July 2021 <ul> <li>Basic support for external authentication</li> <li>Improved Export error handling</li> <li>Update merge tool</li> <li>Enable subscription functionality by default</li> </ul> 6.2.0(GitHub Link) 18th June 2021 <ul> <li>Context view on model tree</li> <li>Versioned folder features</li> </ul> 6.1.0(GitHub Link) 18th May 2021 <ul> <li>Properties, annotations and rules available on folders</li> <li>Bug fix for default datatypes</li> </ul> 6.0.0(GitHub Link) 22nd April 2021 <ul> <li>Refresh of most model view screens, maximizing screen usage</li> <li>Support for structured profiles</li> <li>Publish / subscribe functionality</li> <li>Support for user-provided themes</li> <li>Many other bug fixes</li> </ul> 5.2.0(GitHub Link) 26th Mar 2021 <ul> <li>New screens for administrator system properties</li> </ul> 5.1.0(GitHub Link) 4th Mar 2021 <ul> <li>Global progress indicator</li> <li>Fix for Markdown link editing</li> <li>Other fixes / performance improvements</li> </ul> 5.0.0(GitHub Link) 5th Feb 2021 Listing major changes since last release of Metadata Catalogue <ul> <li>Upgrade to Angular 9</li> <li>Modular code structure</li> <li>Refactored, simplified layout to all screens</li> <li>Reference Data Model</li> <li>API Keys / access tokens</li> <li>WYSIWYG HTML editor</li> <li>Much improved support for branching and merging</li> </ul>"},{"location":"about/release-notes/#plugins-client-libraries-others","title":"Plugins, client libraries, others","text":"<p>Release notes for plugins, client libraries, and other Mauro repositories are not formally listed here for ease of maintenance.  However, you can  view the latest versions of each plugin on the Plugins page.  More details about tagged releases and issues addressed  can be found on the individual GitHub repos (see the 'tags' section to view all releases).</p>"},{"location":"about/research/","title":"Research","text":"<p>The core of Mauro Data Mapper is based on sound theoretical principles developed by the Oxford team over the course of the last decade. </p> <p>Formal, rigorous definitions of semantic interoperability, have been peer-reviewed and presented at academic conferences and in journals. Some of these articles are listed below:</p> <ul> <li> <p>A formal, scalable approach to semantic interoperability Jim Davies, James Welch, David Milward, Steve Harris Science of Computer Programming 192, Elsevier. June 2020.</p> </li> <li> <p>Engineering Agile Big\u2212Data Systems Kevin Feeney\u201a Jim Davies\u201a James Welch\u201a Sebastian Hellmann\u201a Christian Dirschl\u201a Andreas Koller\u201a Pieter Francois and Arkadiusz Marciniak  River Publishers; Illustrated edition. December 2018.</p> </li> <li> <p>Domain\u2212Specific Modelling for Clinical Research Jim Davies\u201a Jeremy Gibbons\u201a Adam Milward\u201a David Milward\u201a Seyyed Shah\u201a Monika Solanki and James Welch In SPLASH Workshop on Domain\u2212Specific Modelling. October 2015.</p> </li> <li> <p>The CancerGrid Experience: Metadata\u2212Based Model\u2212Driven Engineering for Clinical Trials Jim Davies\u201a Jeremy Gibbons\u201a Steve Harris and Charles Crichton In Science of Computer Programming. Vol. 89B. Pages 126\u2212143. September 2014.</p> </li> <li> <p>Form Follows Function: Model\u2212Driven Engineering for Clinical Trials Jim Davies\u201a Jeremy Gibbons\u201a Radu Calinescu\u201a Charles Crichton\u201a Steve Harris and Andrew Tsui In International Symposium on Foundations of Health Information Engineering and Systems. Vol. 7151 of LNCS. Pages 21\u221238. Springer. August, 2011.</p> </li> <li> <p>Models for Forms Daniel Abler\u201a Charles Crichton\u201a Jim Davies\u201a Steve Harris and James Welch In Proceedings of the 11<sup>th</sup> Workshop on Domain-Specific Modeling, 2011.</p> </li> <li> <p>Semantic Interoperability in Practice Jim Davies\u201a Steve Harris and Aadya Shukla In HICSS (Electronic Government Track). 2010.</p> </li> <li> <p>Metadata\u2212Driven Software for Clinical Trials Charles Crichton\u201a Jim Davies\u201a Jeremy Gibbons\u201a Steve Harris\u201a Andrew Tsui and James Brenton In ICSE Workshop on Software Engineering and Health Care. May 2009.</p> </li> </ul>"},{"location":"community/contribute/","title":"Contribute","text":"<p>Mauro Data Mapper is an open source tool, supported by a small core team of developers who welcome contributions from the user community. </p> <p>There are many ways to contribute, but before doing so we recommend that you join our community through our Zulip organisation so that you can understand the priorities and see which tasks are already in progress. We welcome any contributions, however small, and will ensure any contribution is credited appropriately.</p> <p>Our immediate general priorities are in bug fixes, documentation, and the creation of plugins. In each case, we have tooling or templates to help get started and the core development team are happy to provide additional support for contributing activities.</p> <p>Before contributing to any of our repositories, please have a read of our Contributor License Agreement. Any submissions to our repositories will be under this agreement to ensure that our code stays free of any further restrictions.</p>"},{"location":"community/contribute/#documentation","title":"Documentation","text":"<p>New members of the community and those with little experience collaborating with an Open Source community may like to start here. Our documentation is under constant evaluation and improvement: screenshots and instructions need updating; API documentation needs further elaboration and examples while user guides to new features need writing and updating. We're particularly interested in real-world examples of usage - experience reports that can be turned into 'how-to' guides for others wanting to use Mauro in the future. Many of the examples we have are based on health data, so we're also keen to get examples from other domains too.</p> <p>The documentation can be found in the 'docs' repository in the Mauro Github organisation; clone the repository into a suitable location. The whole documentation website can be built using the MkDocs tool. Follow the instructions to install it locally and run <code>mkdocs serve</code> to build and host a version on your local machine. This will automatically update as you make changes to the source files.</p> <p>Please create a new branch with your changes and submit a pull request with your changes when you're ready to submit them. Before accepting, we will check for the following properties:</p> <ul> <li>That there are no conflicts with the existing 'develop' branch of the documentation</li> <li>That there are no spelling mistakes or grammatical errors</li> <li>That there are no broken or invalid links in the new text</li> </ul> <p>We may also ask some community members to review changes before they are published. Please add comments to your pull request to guide us on how best to accept your changes.</p> <p>Once your pull request has been accepted, the changes will be uploaded to the Github help pages as part of the next documentation 'release'.</p>"},{"location":"community/contribute/#bug-fixes","title":"Bug fixes","text":"<p>For developers getting started with our code base, fixing bugs is a great place to start. Lists of open issues are available on our Youtrack and GitHub sites, or you may have found your own issue you'd like to dive in and fix. All our repositories use the GitFlow model for managing code branches. New work should be created in a feature branch. Our core repositories act on a 'pull request' system and each repository will have criteria to meet before committing code. For the Core and UI repositories, more information is given below.</p> <p>With all contributions to the source code, we recommend you engage with the community first, to make sure no work is duplicated, and to understand the current priorities.</p>"},{"location":"community/contribute/#core","title":"Core","text":"<p>The Core code is built using Gradle and Grails. Instructions for getting the code running locally are provided in the repository's README file. Before accepting any pull requests, the core team will:</p> <ul> <li>Checkout the relevant branch and check that any new functionality works correctly</li> <li>Run all integration and unit tests (where this hasn't already been done by our build servers)</li> <li>Run <code>lint</code> tools over the code to ensure new code meets existing quality checks</li> </ul> <p>Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch.</p> <p>Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly.</p>"},{"location":"community/contribute/#ui","title":"UI","text":"<p>The UI project is build using <code>npm</code> and <code>ng</code> and instructions for getting the code running locally are provided in the repository's README file. In order to build against a working back-end server, you might like to first check-out the Core code and get that running, or use our Docker installation to bring up a back-end which your copy of the web interface can communicate with.</p> <p>Before approving a pull request in the UI repository, the core team will:</p> <ul> <li>Checkout the relevant branch and check that any new functionality works correctly</li> <li>run <code>ng test</code> to ensure that all tests pass correctly</li> <li>run <code>ng lint</code> and <code>npm run eslint</code> to check that any code changes are sensible</li> <li>check through the diff of the current branch to look for any unintended changes</li> </ul> <p>Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch.</p> <p>Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly.</p>"},{"location":"community/contribute/#plugin-development","title":"Plugin development","text":"<p>Back-end plugins can be written to provide functionality for specific use cases. They can make use of existing code 'hooks', such as importers, exporters, and profiles or they may simply provide additional API endpoints for particular purposes. </p> <p>The Mauro Data Mapper Plugins Github organisation provides template plugins for these use cases, which can be cloned and used as a starting point  for the development of a new plugin.  </p> <p>If your plugin has universal appeal, we'd be happy to host it within our organisation so that other users can find it.  Otherwise we can advertise  it through these pages, or keep a pinned list within Zulip.</p>"},{"location":"community/contribute/#share-your-models","title":"Share your models","text":"<p>The development team would be pleased to receive details of any models that we might be able to re-use - either for sharing with the community, or example models that can be used for demo purposes.</p> <p>For real-world models that will be useful to other members of the community, we intend to use these pages to advertise either downloadable files, or URLs that can be used to access publicly available models.</p> <p>Please pay particular care to any copyright or licensing information on the model itself before sharing.   </p>"},{"location":"community/support/","title":"Support","text":"<p>We have an active and ever growing community of Mauro Data Mapper users in a variety of different application domains. As an open source project, we encourage all our users to interact and help solve problems, share experience or offer advice. The main channel for this is our new Zulip organisation which we encourage all potential users to visit. </p> <p>In addition to active users and contributors, the core development team are also active on Zulip and able to help with any technical questions. You can log into our Zulip organisation using your GitHub credentials, or alternatively ask an existing member to send you an invite.</p> <p>Note that although our Slack channels are still open for the core development team, we will be migrating all previous channels / conversations to Zulip where appropriate.</p>"},{"location":"community/support/#reporting-issues","title":"Reporting Issues","text":"<p>The Mauro team are keen to hear of any bugs, performance problems or other issues that may be affecting your experience of using the tools.  </p> <p>The core development team run an instance of JetBrains YouTrack for managing issues. You can view current issues or submit new ones through hosted instances of the Web Interface. Click the 'Report Issue' link in the footer of the page. When an error is encountered through the web interface, there is also an option to submit a new issue directly. This will include any debug details necessary for the development team to investigate.</p> <p>Please first check to see if your issue has been previously reported. If in doubt, submit a new issue, but in many cases it may be more helpful to add additional context or examples to an existing issue rather than creating another.</p> <p>When submitting issues in this way, please record your name in the 'Reporters Name' field. This will allow the development team to follow up with further questions or reports on progress. Please also do include any additional information that will allow the development team to better understand your issue. There are many online guides to help you write good bug reports - one such example is here.</p> <p>As well as YouTrack, you can also submit issues to our GitHub organisations. Please try to report issues against the correct repository. Back-end issues should be submitted against mdm-core while user interface issues should be submitted against mdm-ui and so on.</p>"},{"location":"community/support/#requesting-features","title":"Requesting Features","text":"<p>The core team are very pleased to receive suggestions for new features or improvements to the tools - especially those that will benefit a wide audience or encourage adoption of the tools. </p> <p>Our Development Roadmap page shows a few items that are either in progress or planned for development in the near future. The team may also be engaged in other funded projects with other deliverables not reported  on that page, so do please discuss with the team if there are features that you would find useful.  In particular, early feedback can ensure we cater  for as many use-cases as possible when building new functionality and we can let you know directly once support is in progress, ready for testing, or available as part of a tagged code release.</p> <p>Visit our Zulip organisation to discuss new features and get in touch with the development team directly. </p>"},{"location":"developer/build_status/","title":"Current System Build Status","text":"<p>The following page uses Jenkins status badges to inform the current <code>main</code> and <code>develop</code> branches build status. It is primarily used by developers prior to release to check the status \"across the board\" before releasing</p>"},{"location":"developer/build_status/#mdm-core","title":"MDM Core","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-ui","title":"MDM UI","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugins","title":"MDM Plugins","text":""},{"location":"developer/build_status/#grails-plugins","title":"Grails Plugins","text":""},{"location":"developer/build_status/#mdm-plugin-artdecor","title":"mdm-plugin-artdecor","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-authentication-keycloak","title":"mdm-plugin-authentication-keycloak","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-authentication-openid-connect","title":"mdm-plugin-authentication-openid-connect","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-awsglue","title":"mdm-plugin-awsglue","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-csv","title":"mdm-plugin-csv","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-database-mysql","title":"mdm-plugin-database-mysql","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-database-postgresql","title":"mdm-plugin-database-postgresql","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-database-sqlserver","title":"mdm-plugin-database-sqlserver","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-digital-object-identifiers","title":"mdm-plugin-digital-object-identifiers","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-excel","title":"mdm-plugin-excel","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-fhir","title":"mdm-plugin-fhir","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-freemarker","title":"mdm-plugin-freemarker","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-profile-dcat","title":"mdm-plugin-profile-dcat","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-profile-dementia-platform","title":"mdm-plugin-profile-dementia-platform","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-profile-hdruk","title":"mdm-plugin-profile-hdruk","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-profile-schema-org","title":"mdm-plugin-profile-schema-org","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-sparql","title":"mdm-plugin-sparql","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-xsd","title":"mdm-plugin-xsd","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#gradle-plugins","title":"Gradle Plugins","text":""},{"location":"developer/build_status/#mdm-plugin-database","title":"mdm-plugin-database","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-database-oracle","title":"mdm-plugin-database-oracle","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#other-plugins","title":"Other Plugins","text":""},{"location":"developer/build_status/#mdm-plugin-testing-utils","title":"mdm-plugin-testing-utils","text":"Branch Build Status main develop"},{"location":"developer/build_status/#mdm-plugin-template","title":"mdm-plugin-template","text":"Branch Build Status grails N/A"},{"location":"developer/build_status/#mdm-plugin-template-gradle","title":"mdm-plugin-template-gradle","text":"Branch Build Status gradle N/A"},{"location":"developer/build_status/#under-development","title":"Under development","text":""},{"location":"developer/build_status/#mdm-plugin-opensafely","title":"mdm-plugin-opensafely","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-dita","title":"mdm-plugin-dita","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-azure-purview","title":"mdm-plugin-azure-purview","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-iso-11179","title":"mdm-plugin-iso-11179","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/build_status/#mdm-plugin-research","title":"mdm-plugin-research","text":"Branch Build Status Quality Gate main develop"},{"location":"developer/releasing/","title":"Releasing","text":"<p>The following process should be followed EXACTLY to release MauroDataMapper in its entirety. Please pay attention to versions and whats changed between each git commit.</p> <p>Warning</p> <p>The main release cycle should be performed using Jenkins, not done manually.  However plugins &amp; documentation do still need to be done manually</p> <p>Warning</p> <p>You will need git flow installed in the command line to be able to perform the release. Installation instructions are here. You will also need to make sure all your repositories are initialised as git-flow repositories. This can be done using <code>git flow init</code>,  however you MUST make sure you have checked out the <code>develop</code> and <code>main</code> branches before you initialise.</p> <p>Information</p> <p>Unless patching a release which has failed ALL releases should be the next MINOR release. If there is significant and appropriate change then a MAJOR release should be used. A PATCH release should only be done if the MINOR release failed and this is a release being done to fix that release failure.</p> <p>Information</p> <p>A unified view of the current build status of develop and main branches can be see here</p> <p>You should identify the following from the repositories before starting</p> <ul> <li>RELEASE_VERSION : The quarterly release version. e.g. 2022.2</li> <li>CORE_VERSION : The appropriate non-snapshot version from mdm-core/gradle.properties on the develop branch</li> <li>UI_VERSION : The appropriate non-snapshot version from mdm-ui/package.json on the develop branch</li> </ul>"},{"location":"developer/releasing/#jenkins-release","title":"Jenkins Release","text":"<p>Use the <code>mdm-release/release</code> job, filling in the requested parameters. Sit back and relax.</p>"},{"location":"developer/releasing/#mdm-plugins","title":"MDM plugins","text":"<p>Warning</p> <p>You will need to wait for the main branch of mdm-core to finish before proceeding on the latest plugins</p> <p>Information</p> <p>This repository provides useful scripts and a unified live view  of the state of each branch build.</p>"},{"location":"developer/releasing/#release-order","title":"Release Order","text":"<p>You will need to push some of the repositories in the correct order as they have dependencies on other plugins. The following plugins have a release order, if not listed then there is not required order.</p> <ol> <li>mdm-plugin-database</li> <li>mdm-plugin-testing-utils<ol> <li>mdm-plugin-database-oracle</li> </ol> </li> <li>mdm-plugin-database-mysql</li> <li>mdm-plugin-database-postgresql</li> <li>mdm-plugin-database-sqlserver</li> <li>mdm-plugin-profile-schema-org</li> <li>mdm-plugin-profile-hdruk</li> </ol>"},{"location":"developer/releasing/#no-changes-waiting-to-be-released","title":"No changes waiting to be released","text":"<ul> <li>You don't need to release these every time we release mdm-core</li> <li>However you should make sure all <code>develop</code> branches are updated to <code>mdmCoreVersion=${CORE_VERSION}</code> and then push the update.</li> <li>Any jobs which fail will need to have the code updated.</li> <li>If any of the code changes are in side the code base (not test changes) then you will need to release</li> <li>If only test code changes are needed or no changes are needed then don't do a release as we have proved it's still compatible</li> </ul>"},{"location":"developer/releasing/#changes-waiting-to-be-released","title":"Changes waiting to be released","text":"<ul> <li>You should make sure the <code>develop</code> branch is updated to <code>mdmCoreVersion=${CORE_VERSION}</code> and then push the update</li> <li>Any tests which fail will need to have the code updated</li> <li>If any of the code changes are in side the code base (not test changes) then you will need to release with an updated <code>mdmCoreVersion</code></li> <li>If only test code changes are needed or no changes are needed then release using the last <code>mdmCoreVersion</code> used as its still compatible</li> </ul> <pre><code>git checkout main &amp;&amp; git pull &amp;&amp; git checkout develop &amp;&amp; git pull\ngit flow release start ${PLUGIN_VERSION}\n</code></pre> <ul> <li>Update gradle.properties</li> <li>Update README.md \"How to apply\"</li> </ul> <pre><code>git commit -am \"Release ${PLUGIN_VERSION}\"\ngit flow release finish -m \"${PLUGIN_VERSION}\" ${PLUGIN_VERSION}\n</code></pre> <ul> <li>Update gradle.properties to next minor snapshot</li> <li>DO NOT change the README.md file</li> </ul> <pre><code>git commit -am 'Next snapshot'\ngit checkout main &amp;&amp; git push &amp;&amp; git checkout develop &amp;&amp; git push &amp;&amp; git push --tags\n</code></pre>"},{"location":"developer/releasing/#document-announce","title":"Document &amp; Announce","text":"<p>Information</p> <p>To be able to autogenerate the release documentation for plugins you will need to clone https://github.com/MauroDataMapper-Plugins/mdm-plugins. Then place all the plugins into this directory.</p> <p>Warning</p> <p>Remember to remove the following from the autogenerated plugins release output</p> <ul> <li>Private Repositories</li> <li>Unreleased plugins</li> </ul>"},{"location":"developer/releasing/#github","title":"Github","text":"<p>Each of the repositories requires the tag to be released and links to the issues fixed supplied.</p> <ul> <li>Navigate to the supplied tags page</li> <li>Select the latest tag and choose \"Create release\"</li> <li>Copy in the appropriate text from the below list, making sure to update<ul> <li>the stated YouTrack version to the tag being released</li> <li>the stated Github milestone</li> </ul> </li> <li>Click the \"Auto-generate release notes\" button</li> </ul> <code>mdm-core</code> Tags Page <pre><code>See [Issues fixed](https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23B4.8.0) and [Milestones](https://github.com/MauroDataMapper/mdm-core/issues?q=is%3Aissue+milestone%3A4.8.0)\n</code></pre> <code>mdm-ui</code> Tags Page <pre><code>See [Issues fixed](https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23F6.4.0) and [MileStones](https://github.com/MauroDataMapper/mdm-ui/issues?q=is%3Aissue+milestone%3A6.4.0)\n</code></pre> <code>mdm-application</code> Tags Page <pre><code>See [Issues fixed](https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23B4.8.0), [Core Milestones](https://github.com/MauroDataMapper/mdm-core/issues?q=is%3Aissue+milestone%3A4.8.0) and [Application Milestones](https://github.com/MauroDataMapper/mdm-application-build/issues?q=is%3Aissue+milestone%3A4.8.0)\n</code></pre> <code>mdm-docker</code> Tags Page <pre><code>See [Issues fixed](https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23B4.8.0%20%23F6.4.0)\nand [Core Milestones](https://github.com/MauroDataMapper/mdm-core/issues?q=is%3Aissue+milestone%3A4.8.0)\nand [Application Milestones](https://github.com/MauroDataMapper/mdm-application-build/issues?q=is%3Aissue+milestone%3A4.8.0)\nand [UI Milestones](https://github.com/MauroDataMapper/mdm-ui/issues?q=is%3Aissue+milestone%3A6.4.0)\n</code></pre>"},{"location":"developer/releasing/#documentation","title":"Documentation","text":"<p>Caution</p> <p>Make sure you've done all the github releases first otherwise the release notes won't contain anything useful</p> <p>This should be performed inside the docs repository.</p> <pre><code>git checkout main &amp;&amp; git pull &amp;&amp; git checkout develop &amp;&amp; git pull\ngit flow release start \"${RELEASE_VERSION}\"\n</code></pre> <ul> <li>Perform the updates as per the 2 sub-sections (Main Release, Plugins Release) below</li> </ul> <pre><code>git commit -am \"Release ${RELEASE_VERSION}\"\ngit flow release finish -m \"${RELEASE_VERSION}\" \"${RELEASE_VERSION}\"\ngit checkout main &amp;&amp; git push &amp;&amp; git checkout develop &amp;&amp; git push &amp;&amp; git push --tags\n</code></pre>"},{"location":"developer/releasing/#main-release","title":"Main Release","text":"<p>Update <code>docs/about/release-notes.md</code> file.</p> <p>The <code>td</code> sections, in order, are: Add  1. Release version &amp; link to the release in Github  2. Release date  3. Major Changes: This is an <code>ul</code> so each change should be wrapped in an <code>li</code> element</p> <ul> <li>Update the \"Current Full Release\" line:</li> </ul> <pre><code>The current full release is **${RELEASE_VERSION}** (a.k.a B${CORE_VERSION}_F${UI_VERSION}).\n</code></pre> <ul> <li>Add the following block with updated versions to the top of the <code>## Core API</code> section:</li> </ul> <pre><code>&lt;tr&gt;\n    &lt;td&gt;&lt;b&gt;4.10.0&lt;/b&gt;&lt;br/&gt;&lt;a href=\"https://github.com/MauroDataMapper/mdm-core/releases/tag/4.10.0\"&gt;(GitHub Link)&lt;/a&gt;&lt;/td&gt;\n    &lt;td&gt;2nd September 2021&lt;/td&gt;\n    &lt;td&gt;\n        &lt;ul&gt;\n            &lt;li&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/td&gt;\n&lt;/tr&gt;\n</code></pre> <ul> <li>Add the following block with updated versions to the top of the <code>## UI</code> section:</li> </ul> <pre><code>&lt;tr&gt;\n    &lt;td&gt;&lt;b&gt;6.6.0&lt;/b&gt;&lt;br/&gt;&lt;a href=\"https://github.com/MauroDataMapper/mdm-ui/releases/tag/6.6.0\"&gt;(GitHub Link)&lt;/a&gt;&lt;/td&gt;\n    &lt;td&gt;2nd September 2021&lt;/td&gt;\n    &lt;td&gt;\n        &lt;ul&gt;\n            &lt;li&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/td&gt;\n&lt;/tr&gt;\n</code></pre>"},{"location":"developer/releasing/#plugins-release","title":"Plugins Release","text":"<ul> <li>Run <code>./releases.sh</code> in mdm-plugins.</li> <li>Remove the private repositories from the HTML format and copy into the <code>docs/installing/plugins.md</code> file</li> </ul>"},{"location":"developer/releasing/#zulip-announce","title":"Zulip Announce","text":"<p>Caution</p> <p>Make sure you've done all the github releases first otherwise the release notes won't contain anything useful</p> <ul> <li>Run <code>./releases.sh</code> in mdm-plugins.</li> <li>Copy the below markdown block into the <code>announce</code> stream of Zulip</li> <li>Update the versions for the applications</li> <li>Update the tag version for the release notes</li> <li>Copy in the plain text format of the \"releases\" output underneath it</li> </ul> <pre><code># New Release\n\n| Application | Version | Release Notes |\n|----|-----|-----|\n| Docker | `2022.2` | https://github.com/MauroDataMapper/mdm-docker/releases/tag/2022.2 |\n| RESTful API | `4.9.0` | https://github.com/MauroDataMapper/mdm-application-build/releases/tag/4.9.0 |\n| UI | `6.5.0` | https://github.com/MauroDataMapper/mdm-ui/releases/tag/6.5.0 |\n</code></pre>"},{"location":"developer/releasing/#manual-release-process","title":"Manual Release Process","text":"<p>In the event Jenkins is unavailable follow the process below to release the systems Jenkins would release</p>"},{"location":"developer/releasing/#mdm-core","title":"mdm-core","text":"<pre><code>git checkout main &amp;&amp; git pull &amp;&amp; git checkout develop &amp;&amp; git pull\ngit flow release start ${CORE_VERSION}\n</code></pre> <p>Update gradle.properties</p> <pre><code>git commit -am \"Release ${CORE_VERSION}\"\ngit flow release finish -m \"${CORE_VERSION}\" ${CORE_VERSION}\n</code></pre> <p>Update gradle.properties to next minor snapshot</p> <pre><code>git commit -am 'Next snapshot'\ngit checkout main &amp;&amp; git push &amp;&amp; git checkout develop &amp;&amp; git push &amp;&amp; git push --tags\n</code></pre>"},{"location":"developer/releasing/#mdm-resources","title":"mdm-resources","text":"<pre><code>git checkout main &amp;&amp; git pull &amp;&amp; git checkout develop &amp;&amp; git pull\ngit flow release start ${CORE_VERSION}\n</code></pre> <p>Update package.json version to CORE_VERSION</p> <pre><code>npm install &amp;&amp; npm run build\ngit commit -am \"Release ${CORE_VERSION}\"\ngit flow release finish -m \"${CORE_VERSION}\" ${CORE_VERSION}\n</code></pre> <p>Update package.json version to next snapshot CORE_VERSION</p> <pre><code>npm install &amp;&amp; npm run build\ngit commit -am 'Next snapshot'\ngit checkout main &amp;&amp; git push &amp;&amp; git checkout develop &amp;&amp; git push &amp;&amp; git push --tags\n</code></pre>"},{"location":"developer/releasing/#mdm-ui","title":"mdm-ui","text":"<p>Warning</p> <p>You must release mdm-resources first, or be able to use an existing release of mdm-resources</p> <pre><code>git checkout main &amp;&amp; git pull &amp;&amp; git checkout develop &amp;&amp; git pull\ngit flow release start ${UI_VERSION}\n</code></pre> <p>Update package.json</p> <ul> <li><code>version</code> to UI_VERSION</li> <li><code>\"@maurodatamapper/mdm-resources\"</code> to CORE_VERSION</li> </ul> <pre><code>npm install\n</code></pre> <p>Caution</p> <p>MAKE SURE in <code>package.lock</code> that all the lines for mdm-resources that have a commit hash MATCH the hash for the release,  if NOT then delete all entries in the lock file for mdm-resources with a commit hash and run npm install again</p> <pre><code>git commit -am \"Release ${UI_VERSION}\"\ngit flow release finish -m \"${UI_VERSION}\" ${UI_VERSION}\n</code></pre> <p>Update package.json</p> <ul> <li><code>version</code> to next minor snapshot UI_VERSION</li> </ul> <pre><code>npm install\ngit commit -am 'Next snapshot'\ngit checkout main &amp;&amp; git push &amp;&amp; git checkout develop &amp;&amp; git push &amp;&amp; git push --tags\n</code></pre>"},{"location":"developer/releasing/#mdm-application-build","title":"mdm-application-build","text":"<p>Warning</p> <p>You will need to wait for the main branch of mdm-core to finish before proceeding on the latest mdm-application-build</p> <pre><code>git checkout main &amp;&amp; git pull &amp;&amp; git checkout develop &amp;&amp; git pull\ngit flow release start ${CORE_VERSION}\n</code></pre> <p>Update gradle.properties</p> <pre><code>git commit -am \"Release ${CORE_VERSION}\"\ngit flow release finish -m \"${CORE_VERSION}\" ${CORE_VERSION}\n</code></pre> <p>Update gradle.properties to next minor snapshot</p> <pre><code>git commit -am 'Next snapshot'\ngit checkout main &amp;&amp; git push &amp;&amp; git checkout develop &amp;&amp; git push &amp;&amp; git push --tags\n</code></pre>"},{"location":"developer/releasing/#mdm-docker","title":"mdm-docker","text":"<p>Warning</p> <p>You will need to wait for the main branch of mdm-application-build and mdm-ui to finish before proceeding</p> <pre><code>git checkout main &amp;&amp; git pull &amp;&amp; git checkout develop &amp;&amp; git pull\ngit flow release start \"${RELEASE_VERSION}\"\n</code></pre> <p>Update docker-compose.yml</p> <ul> <li>The 2 commit ARGS</li> <li>The image tag to <code>B${CORE_VERSION}_F${UI_VERSION}</code></li> </ul> <p>Dry run and check it comes up as expected</p> <pre><code>docker-compose build\ndocker-compose up\n</code></pre> <p>If it all comes up.</p> <pre><code>git commit -am \"Release ${RELEASE_VERSION}\"\ngit flow release finish -m \"${RELEASE_VERSION}\" \"${RELEASE_VERSION}\"\ngit checkout main \ngit tag \"B${CORE_VERSION}_F${UI_VERSION}\"\ngit push &amp;&amp; git checkout develop &amp;&amp; git push &amp;&amp; git push --tags\n</code></pre>"},{"location":"glossary/glossary/","title":"Glossary","text":""},{"location":"glossary/glossary/#a","title":"A","text":"<p>Aliases </p>"},{"location":"glossary/glossary/#b","title":"B","text":"<p>Branch</p>"},{"location":"glossary/glossary/#c","title":"C","text":"<p>Classification</p>"},{"location":"glossary/glossary/#d","title":"D","text":"<p>Data Asset </p> <p>Data Class </p> <p>Data Element </p> <p>Dataflow </p> <p>Data Model </p> <p>Data Standard </p> <p>Data Type </p>"},{"location":"glossary/glossary/#e","title":"E","text":"<p>Enumeration Data Type </p>"},{"location":"glossary/glossary/#f","title":"F","text":"<p>Finalise</p> <p>Folder</p> <p>Fork</p>"},{"location":"glossary/glossary/#l","title":"L","text":"<p>Label </p>"},{"location":"glossary/glossary/#m","title":"M","text":"<p>Merging</p> <p>Multi-facet aware</p> <p>Multiplicity </p>"},{"location":"glossary/glossary/#p","title":"P","text":"<p>Primitive Data Type </p> <p>Profile</p>"},{"location":"glossary/glossary/#r","title":"R","text":"<p>Reference Data Model </p> <p>Reference Data Type </p>"},{"location":"glossary/glossary/#s","title":"S","text":"<p>Semantic links </p>"},{"location":"glossary/glossary/#t","title":"T","text":"<p>Terminology Data Type </p>"},{"location":"glossary/glossary/#v","title":"V","text":"<p>Version</p> <p>Versioned Folder</p>"},{"location":"glossary/aliases/aliases/","title":"Aliases","text":""},{"location":"glossary/aliases/aliases/#what-is-an-alias","title":"What is an Alias?","text":"<p>An Alias is an alternative name for a catalogue item which helps to locate it when searched for. Data Models, Data Classes and Data Elements must have one primary Label, but can have many Aliases. </p>"},{"location":"glossary/aliases/aliases/#how-are-aliases-used","title":"How are Aliases used?","text":"<p>Aliases appear in the first row of the details panel when an item is selected in the Model Tree. An example of some suitable Aliases for the Data Model labelled \u2018Head and Neck Cancer Audit (HANA)\u2019 could be \u2018Neck Cancer\u2019, \u2018Head Cancer\u2019, \u2018Head and Neck Cancer\u2019 and \u2018HANA\u2019. Therefore, if one of these items is searched for, the \u2018Head and Neck Cancer Audit (HANA)\u2019 Data Model will appear in the search results. </p> <p>This helps users access the catalogue item they need, without having to know the exact Label.</p> <p></p>"},{"location":"glossary/aliases/aliases/#how-do-you-edit-an-alias","title":"How do you edit an Alias?","text":"<p>To add or remove Aliases click the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. This will allow you to edit the 'Aliases' row at the top of the details panel.  </p> <p>Type the name of the new Alias and click the green '+' sign on the right to add it. To delete an Alias, click the red 'x' sign to the right of the Alias you wish to remove. </p> <p></p> <p>Once you have finished editing, click 'Save changes' at the bottom right of the details panel. A green box should appear at the bottom right of the screen, confirming that the catalogue item has been successfully updated.</p>"},{"location":"glossary/branch/branch/","title":"Branch","text":""},{"location":"glossary/branch/branch/#what-is-a-branch","title":"What is a Branch?","text":"<p>A Branch is a method of diverging from the main line of a model to continue further changes without interferring with the rest of the model. The name branch can be likened to that of a tree structure; a tree has a main trunk, while branches extend from that trunk.</p> <p>The simplest way to understand Branches in Mauro is to view the Merge Graph of a model:</p> <p></p> <p>In this example, notice that Model Version Tree DataModel (1.0.0) has been branched into three:</p> <ul> <li>main - this is the name given to all default branches. Every new version of a model automatically has a main branch created, acting as the main line, or trunk, of the model's changes.</li> <li>Two separate branches have also been created. These are branched from the original model but may now include different changes compared to main (or each other).</li> </ul>"},{"location":"glossary/branch/branch/#why-are-branches-useful","title":"Why are Branches useful?","text":"<p>Branches allow for more complex model editing and versioning scenarios. If a single editor is working on updates to a data model, then simple versioning may be sufficient to increment each update/version of that data model. If, however, there are multiple editors working on the same model at the same time, this could cause conflicts; for example, one editor could accidentally lose changes made by another editor.</p> <p>The solution for this scenario is to create one (or more) Branches of a model, allowing multiple editors or teams to make their own changes to the model in question without directly interfering or conflicting with other editors. Once an editor is happy that their Branch is complete, they may then merge it with another branch to ensure that everyones changes are pulled together into one place.</p> <p>An alternative to branching is called Forking, whereby a model is cloned to make new changes.</p>"},{"location":"glossary/branch/branch/#how-to-create-a-branch","title":"How to create a Branch","text":"<p>To create one or more Branches of a model, please refer to the user guide Branching, versioning and forking Data Models.</p>"},{"location":"glossary/classification/classification/","title":"Classification","text":""},{"location":"glossary/classification/classification/#what-is-a-classification","title":"What is a Classification?","text":"<p>A Classification is a container which acts like a tag on a catalogue item, allowing you to categorise and group together related catalogue items, such as models. A Classification is a list-based method of collecting models when compared to Folders, allowing a catalogue item to be linked to more than one Classification if necessary.</p> <p></p>"},{"location":"glossary/classification/classification/#why-use-classifications","title":"Why use Classifications?","text":"<p>Classifications are a useful method of organising your catalogue items in a \"many-to-many\" relationship - a catalogue item may related to many Classifications, and a Classification may relate to many catalogue items.</p>"},{"location":"glossary/data-asset/data-asset/","title":"Data Asset","text":""},{"location":"glossary/data-asset/data-asset/#what-is-a-data-asset","title":"What is a Data Asset?","text":"<p>There are two types of Data Models within Mauro Data Mapper:</p> <ul> <li>Data Asset</li> <li>Data Standard</li> </ul> <p>A Data Asset contains existing data. This can be in the form of a database, dataset or a number of completed forms. </p>"},{"location":"glossary/data-asset/data-asset/#how-are-data-assets-used","title":"How are Data Assets used?","text":"<p>A Data Model which is a Data Asset is represented by a database icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree. </p> <p></p> <p>Data Assets may also include summary metadata within its properties and its data can also be populated from other Data Assets via a Dataflow. </p>"},{"location":"glossary/data-asset/data-asset/#selecting-a-data-model-type","title":"Selecting a Data Model type","text":"<p>You will need to assign a Data Model type whenever you are adding or importing a Data Model. </p> <p>When adding a Data Model, select the type from the dropdown menu on the 'Data Model Details form'. For further information on this, visit our 'Create a Data Model user guide'. </p> <p></p> <p>When importing a Data Model using Excel, you will need to specify the type in the relevant column of the Data Model listing sheet. For further information on this, please see our 'Import a Data Model from Excel user guide'. </p>"},{"location":"glossary/data-class/data-class/","title":"Data Class","text":""},{"location":"glossary/data-class/data-class/#what-is-a-data-class","title":"What is a Data Class?","text":"<p>A Data Class is a collection of data, also known as Data Elements, that are related to each other in some way. For example, each Data Element could appear in the same table of a database, or the same section within a form.</p>"},{"location":"glossary/data-class/data-class/#how-are-data-classes-used","title":"How are Data Classes used?","text":"<p>Data Classes are the building blocks of a Data Model. Within each Data Class lies several Data Elements and these are the descriptions of an individual field, variable, column or property. </p> <p>You can also have a Data Class within a Data Class, known as a Nested Data Class, which can be a useful way of managing complex sets of data. There is no limit on the number of Nested Data Classes you can include. </p> <p>For example, in a webform, there may be a section called 'Contact details', which would be one Data Class. Within that section however, there may be another Data Class labelled 'Correspondence Address'. This would be a Nested Data Class.  </p> <p></p> <p>Each Data Class has a:</p> <ul> <li> <p>Label     This is the name of the Data Class which has to be unique within the Data Model or parent Data Class.</p> </li> <li> <p>Aliases     Alternative names that can help locate the Data Class when searched for.</p> </li> <li> <p>Description     A definition either written in HTML, Markdown, or plain text which explains the types of data items that are grouped together within the Data    Class, as well as any contextual details.</p> </li> <li> <p>Parent Hierarchy     The parent of a Data Class can either be the Data Model itself, in which case it is described as a \u2018top level data class\u2019. Or, if it is a Nested Data Class, its parent Data Class.</p> </li> <li> <p>Multiplicity     This specifies the minimum and maximum number of times the Data Class appears within its parent.      Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data    which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally).</p> </li> <li> <p>Classifications     These are effectively tags that you can apply to the Data Class. </p> </li> </ul> <p>The above are all shown within the details panel, when the Data Class is selected in the Model Tree.</p> <p></p> <p>Other characteristics are displayed in the tabs underneath the details panel, when the Data Class is selected in the Model Tree.</p> <ul> <li> <p>Content     This refers to the various Data Elements and Nested Data Classes within the selected Data Class.</p> </li> <li> <p>Properties     Arbitrary additional metadata about this Data Class.</p> </li> <li> <p>Comments     Any relevant comments or notes. </p> </li> <li> <p>Links Semantic links between relevant Data Classes.</p> </li> <li> <p>Summary     Further metadata information on the nature of the Data Elements within the Data Class. This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. </p> </li> <li> <p>Attachments     Files can be added to provide additional information and context. </p> </li> </ul>"},{"location":"glossary/data-element/data-element/","title":"Data Element","text":""},{"location":"glossary/data-element/data-element/#what-is-a-data-element","title":"What is a Data Element?","text":"<p>A Data Element is a description of an individual field, variable, column or property of a data item. Each Data Element has a name and a Data Type. </p>"},{"location":"glossary/data-element/data-element/#how-are-data-elements-used","title":"How are Data Elements used?","text":"<p>Data Elements that are related to each other in some way are grouped together in Data Classes. These Data Classes are the building blocks of Data Models. For example, a Data Element could be an individual field such as \u2018Postcode\u2019 within a webform. </p> <p></p> <p>Each Data Element has a:</p> <ul> <li> <p>Label     This is the name of the Data Element which has to be unique within its parent Data Class.</p> </li> <li> <p>Aliases     Alternative names that can help locate the Data Element when searched for.</p> </li> <li> <p>Description     A definition either written in HTML, Markdown, or plain text which explains any contextual details relating to the Data Element.</p> </li> <li> <p>Data Type     The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: </p> <ul> <li> <p>Enumeration Data Type This is a constrained set of possible Enumeration values, which are typically used to describe lists of data.  For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description.</p> </li> <li> <p>Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019, \u2018Integer\u2019 or \u2018Date\u2019.</p> </li> <li> <p>Reference Data Type Data which refers to another Data Class within the same Data Model. </p> </li> <li> <p>Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms.</p> </li> </ul> </li> <li> <p>Parent Hierarchy     Details the parent Data Class and Data Model of the Data Element.</p> </li> <li> <p>Multiplicity     This specifies the minimum and maximum number of times the Data Element appears within its parent.       Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally).</p> </li> <li> <p>Classifications     These are effectively tags that you can apply to the Data Element. </p> </li> </ul> <p>The above are all shown within the details panel, when the Data Element is selected in the Model Tree.</p> <p></p> <p>Other characteristics are displayed in the tabs underneath the details panel, when the Data Element is selected in the Model Tree.</p> <ul> <li> <p>Properties     Additional metadata about this Data Element. This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility.</p> </li> <li> <p>Comments     Any relevant comments or notes. </p> </li> <li> <p>Links Semantic links between relevant Data Elements.</p> </li> <li> <p>Summary     Further metadata information on the nature of the Data Elements. This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. </p> </li> <li> <p>Attachments     Files can be added to provide additional information and context. </p> </li> </ul>"},{"location":"glossary/data-model/data-model/","title":"Data Model","text":""},{"location":"glossary/data-model/data-model/#what-is-a-data-model","title":"What is a Data Model?","text":"<p>A Data Model is a description of an existing collection of metadata, or a specification of data that is to be collected. Data Models which contain existing data are known as a Data Asset, while models which contain templates for data collection are known as a Data Standard. Both are called models as they are effectively representations of the data that they describe. </p>"},{"location":"glossary/data-model/data-model/#how-are-data-models-used","title":"How are Data Models used?","text":"<p>Data Models make the connection between the names of columns, fields or variables and our understanding of how the corresponding data is acquired, managed and interpreted. Mauro Data Mapper acts as a directory for these Data Models and allows us to create, search and share these data descriptions.</p> <p>Within each Data Model lies several Data Classes which are groups of data that are related in some way. Data Classes contain Data Elements which are the descriptions of an individual field or variable. </p> <p>For example, a webform where patients enter their details would be a Data Model. The 'Personal details' and 'Contact details' sections within the webform would each be a Data Class. While the individual entries such as 'First Name', 'Last Name', 'Date of Birth' etc, would each be a Data Element. </p> <p>The 'Correspondence Address' section is within the 'Contact details' section and would therefore be a Nested Data Class.</p> <p></p> <p></p> <p>Each Data Model has a:</p> <ul> <li> <p>Label     This is the unique name of the Data Model.</p> </li> <li> <p>Aliases     Alternative names that can help locate the Data Model when searched for.</p> </li> <li> <p>Organisation     Details of who is responsible for creating the Data Model. </p> </li> <li> <p>Description     A definition either written in html or plain text which explains the types of data items that are grouped together within the Data Model, as well as any contextual details.</p> </li> <li> <p>Type     This defines whether the Data Model is a Data Asset, which contains existing data, or a Data Standard, which contains templates for data collection. </p> </li> <li> <p>Classifications     These are effectively tags that you can apply to the Data Class. </p> </li> </ul> <p>The above are all shown within the details panel, when the Data Model is selected in the Model Tree.</p> <p></p> <p>Other characteristics are displayed in the tabs underneath the details panel, when the Data Model is selected in the Model Tree.</p> <ul> <li> <p>Data Classes      This is a list of all the Data Classes within the Data Model.</p> </li> <li> <p>Types     The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: </p> <ul> <li> <p>Enumeration Data Type This is a constrained set of possible Enumeration values, which are typically used to describe lists of data.  For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description.</p> </li> <li> <p>Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019, \u2018Integer\u2019 or \u2018Date\u2019.</p> </li> <li> <p>Reference Data Type Data which refers to another Data Class within the same Data Model. </p> </li> <li> <p>Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms.</p> </li> </ul> </li> <li> <p>Properties     Additional metadata about this Data Model. This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility.</p> </li> <li> <p>Summary     Further metadata information on the nature of the Data Classes within the Data Model. This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. </p> </li> <li> <p>Comments     Any relevant comments or notes. </p> </li> <li> <p>History     A detailed record of user, date, time and description of all the changes made to the Data Model. </p> </li> <li> <p>Diagram     A UML diagram which is a graphical way of summarising the Data Classes and Data Elements within the Data Model. </p> </li> <li> <p>Links Semantic links between relevant Data Models.</p> </li> <li> <p>Attachments     Files can be added to provide additional information and context. </p> </li> <li> <p>Dataflow     A diagram illustrating where the data within the Data Model has come from and how it has moved across different databases and organisations. This gives users valuable information on the history of each data point and how it has been manipulated.  </p> </li> </ul>"},{"location":"glossary/data-standard/data-standard/","title":"Data Standard","text":""},{"location":"glossary/data-standard/data-standard/#what-is-a-data-standard","title":"What is a Data Standard?","text":"<p>There are two types of Data Models within Mauro Data Mapper: </p> <ul> <li>Data Standard</li> <li>Data Asset</li> </ul> <p>A Data Standard is essentially a template for collecting new data. This can be a form, schema or a specification for distributed data collection.</p>"},{"location":"glossary/data-standard/data-standard/#how-are-data-standards-used","title":"How are Data Standards used?","text":"<p>A Data Model which is a Data Standard is represented by a document icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree. </p> <p></p> <p>As a Data Standard does not contain any collected data, there will be no summary metadata properties or Dataflows associated with this type of model. </p>"},{"location":"glossary/data-standard/data-standard/#selecting-a-data-model-type","title":"Selecting a Data Model type","text":"<p>You will need to assign a Data Model type whenever you are adding or importing a Data Model. </p> <p>When adding a Data Model, select the type from the dropdown menu on the 'Data Model Details form'. For further information on how to do this, visit our 'Create a Data Model user guide'. </p> <p></p> <p>When importing a Data Model using Excel, you will need to specify the type in the relevant column on the Data Model listing sheet. For further information on this, please see our 'Import a Data Model from Excel user guide'. </p>"},{"location":"glossary/data-type/data-type/","title":"Data Type","text":""},{"location":"glossary/data-type/data-type/#what-is-a-data-type","title":"What is a Data Type?","text":"<p>A Data Type describes the range of possible values that each Data Element may take. </p>"},{"location":"glossary/data-type/data-type/#how-are-data-types-used","title":"How are Data Types used?","text":"<p>There are four different Data Types stored within Data Models of Mauro Data Mapper:</p> <ul> <li> <p>Enumeration Data Type     This is a constrained set of possible Enumeration values, which are typically used to describe lists of data.     For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description.</p> </li> <li> <p>Primitive Data Type     Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019, \u2018Integer\u2019 or \u2018Date\u2019.</p> </li> <li> <p>Reference Data Type     Data which refers to another Data Class within the same Data Model. </p> </li> <li> <p>Terminology Data Type     A structured collection of Enumerated Values which have relationships between different data terms.</p> </li> </ul> <p>When adding a new Data Type to a Data Element, you will need to select the relevant Data Type from the dropdown menu on the 'Data Element Details' form. For more information on how to do this, go to step '5.1 Add Data Elements' on our Document a Health Dataset user guide'</p> <p> </p> <p>Each Data Type has a:</p> <ul> <li> <p>Label     This is the unique name of the Data Type.</p> </li> <li> <p>Aliases     Alternative names that can help locate the Data Type when searched for.</p> </li> <li> <p>Description     A definition written in either html or plain text which explains any contextual details relating to the Data Type.</p> </li> <li> <p>DataModel     The Data Model that the Data Type belongs to.</p> </li> <li> <p>Type     The Data Type (Enumeration, Primitive, Reference or Terminology).</p> </li> <li> <p>Classifications     These are effectively tags that you can apply to the Data Type.</p> </li> </ul> <p> </p> <p>Other characteristics are displayed in the tabs underneath the details panel, when the Data Type is selected in the Model Tree.</p> <ul> <li> <p>Properties     Arbitrary additional metadata about this Data Type.</p> </li> <li> <p>Data Elements     The Data Elements that use the selected Data Type.</p> </li> <li> <p>Comments     Any relevant comments or notes.</p> </li> <li> <p>Links Semantic links between relevant Data Types.</p> </li> <li> <p>Attachments     Files can be added to provide additional information and context.</p> </li> </ul>"},{"location":"glossary/dataflow/dataflow/","title":"Dataflow","text":""},{"location":"glossary/dataflow/dataflow/#what-is-a-dataflow","title":"What is a Dataflow?","text":"<p>A Dataflow is a way of describing the meaning of data. It is a visual representation of how data flows from one Data Model, typically a Data Asset, to another. This helps users to understand where data has been extracted from, how it has been extracted and the series of transformations it has gone through.  </p> <p>Each Dataflow has a source Data Model, which is where the data originated from, and a target Data Model, which is where the final data will be located. The transformations in between define how the data from one or more Data Elements within the source Data Model flow into one or more Data Elements within the target Data Model.  </p>"},{"location":"glossary/dataflow/dataflow/#how-are-dataflows-used","title":"How are Dataflows used?","text":"<p>The Dataflows relating to each Data Asset can be found in the Dataflows tab at the bottom of the details panel, when the relevant Data Model is selected in the Model Tree. </p> <p>The Dataflows tab displays a diagram which consists of a series of annotated grey blocks connected by annotated arrows. This tab allows you to view the Dataflows at a Data Model level, a Data Class level and a Data Element level. Therefore, the grey blocks can represent Data Assets, Data Classes or Data Elements, depending on the selected view. The arrows represent the relevant transformations that have occurred and are annotated with human readable descriptions.</p> <p></p>"},{"location":"glossary/dataflow/dataflow/#data-model-view","title":"Data Model view","text":"<p>When you first click the Dataflows tab, the Data Model view will initially be displayed. This gives a high level overview of how data flows from one Data Asset to another. The grey blocks represent Data Assets which can be databases, lab systems or modules of data.</p>"},{"location":"glossary/dataflow/dataflow/#data-class-view","title":"Data Class view","text":"<p>To find out information on the different components of a particular Dataflow and how tables of data flow from one place to another, you can access the Data Class view. </p> <p>To do this, hover over one of the transformation arrows until the hand icon appears and double click. The Data Class view will then be displayed with each Data Class represented by a grey rectangle.</p> <p></p>"},{"location":"glossary/dataflow/dataflow/#data-element-view","title":"Data Element view","text":"<p>To find out more details on the specific fields of a data product and how they have been derived, you can access the Data Element view. </p> <p>Again, hover over a transformation arrow until the hand icon appears and double click. Here, you will find the various fields grouped together and the relationships between them. This is particularly useful for end users as it explains how data has been manipulated. </p> <p></p>"},{"location":"glossary/dataflow/dataflow/#zoom","title":"Zoom","text":"<p>In all three views, you can zoom in and out by scrolling on your mouse, or by clicking the \u2018+ Zoom in' and \u2018- Zoom out' buttons at the top left of the Dataflows view and in the header bar. </p>"},{"location":"glossary/dataflow/dataflow/#reset","title":"Reset","text":"<p>To reset the view, select \u2018RESET\u2019 at the top left of the Dataflows view or click the circular arrow \u2018Reset zoom and canvas\u2019 button in the header bar. </p>"},{"location":"glossary/dataflow/dataflow/#fullscreen","title":"Fullscreen","text":"<p>You can also view the Dataflow tab in fullscreen mode by clicking the 'Popup in full screen' icon on the top right of the header bar.</p>"},{"location":"glossary/dataflow/dataflow/#download-image","title":"Download image","text":"<p>To download an image of the Dataflow select the 'Download as image' icon at the top right of the header bar. This will download a png image of the current view displayed in the Dataflow tab to your downloads folder.</p> <p></p>"},{"location":"glossary/dataflow/dataflow/#navigating-through-dataflows","title":"Navigating through Dataflows","text":"<p>To navigate between the three different views, hover and double click over the transformation arrows in each view to access the next level. For example, to move from the Data Model view to the Data Class view. </p> <p>To navigate up a level of hierarchy, select the upwards arrow \u2018Move up a level of hierarchy\u2019 icon in the top left of the header bar. For example, to move from the Data Class view to the Data Model view.</p> <p>In all three views you can move the blocks and transformation arrows by clicking and holding the left mouse button which will allow you to drag them to the desired position. This can be helpful to visualise the Dataflows in a clearer manner.  </p>"},{"location":"glossary/enumeration-data-type/enumeration-data-type/","title":"Enumeration Data Type","text":""},{"location":"glossary/enumeration-data-type/enumeration-data-type/#what-is-an-enumeration-data-type","title":"What is an Enumeration Data Type?","text":"<p>An Enumeration Data Type is one of the four possible Data Types within Mauro Data Mapper. Each Enumeration Data Type describes a constrained set of possible Enumeration values. Enumerations are typically used for describing simple lists of data.</p>"},{"location":"glossary/enumeration-data-type/enumeration-data-type/#how-are-enumeration-data-types-used","title":"How are Enumeration Data Types used?","text":"<p>Each Enumeration value has an associated label, or a coded key, and a textual description, or human-readable value. These key-value pairs can be used to describe a list of data. For example, one Enumeration Data Type could be 'Ethnic category', where each Enumeration value describes a specific ethnicity. </p> <p>Further details of the particular Enumeration Data Type can be found in its details panel.</p> <p> </p> <p>The individual Enumeration values, along with their corresponding 'Key' and 'Value' are displayed in an \u2018Enumerations\u2019 table below the details panel. These can be edited by clicking the pencil icon to the right of each row. Enumerations can also be removed by clicking the red bin icon to the right of each row. </p> <p>To add an Enumeration, click \u2018+Add Enumeration\u2019 at the top right of the 'Enumerations' table. This will add a row to the bottom of the table, where you can then populate the \u2018Group\u2019, \u2018Key\u2019 and \u2018Value\u2019 fields. Once completed, click the green tick to the right of the row, and the new Enumeration will be added.</p> <p> </p> <p>You can also view all the Data Elements that use that Enumeration Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity, 'Name' and 'Description' of each Data Element. </p> <p> </p>"},{"location":"glossary/finalise/finalise/","title":"Finalise","text":""},{"location":"glossary/finalise/finalise/#what-is-a-finalised-model","title":"What is a Finalised model?","text":"<p>When a model is Finalised, it may not be edited any further, locking it into a Version number and making it read only. The act of Finalising makes it clear that a model is ready leave its Draft state and be considered finished, or ready for use.</p> <p>When working with models, they can be in one of two states:</p> <ul> <li>Draft state, meaning an editor may make changes to the model.</li> </ul> <p></p> <ul> <li>Finalised state, meaning the model is finished and ready for consumption.</li> </ul> <p></p> <p>Only the main branch may be finalised; if multiple branches exists for a model, each with a set of changes, these must be merged together back into the main branch.</p>"},{"location":"glossary/finalise/finalise/#why-finalise-a-model","title":"Why Finalise a model?","text":"<p>Finalisation is tied to Versions and version control - once a model is Finalised, it is locked into a particular Version number. The only way to make further changes to a Finalised model is to start a new version of the model, once again entering it into a draft state.</p> <p>By locking a version of a model, this prevents further unintended changes being made and definitively states that this model contains a set contents.</p>"},{"location":"glossary/finalise/finalise/#how-to-finalise-models","title":"How to Finalise models","text":"<p>To Finalise a model, please refer to the user guide How to Finalise a Data Model.</p>"},{"location":"glossary/folder/folder/","title":"Folder","text":""},{"location":"glossary/folder/folder/#what-is-a-folder","title":"What is a Folder?","text":"<p>A Folder is a container for organising your data models, functioning very similar to a folder stored on a computer. A Folder may contain data models and other sub-folders to manage and organise your catalogue.</p> <p>Catalogue items may only be stored in one folder at a time, because Folders use a hierarchical-structure. If a catalogue item should be linked to more than one container, consider using Classifications instead.</p> <p></p>"},{"location":"glossary/folder/folder/#why-use-folders","title":"Why use Folders?","text":"<p>Folders are a useful method of organising your catalogue, especially as it gets larger in scope. Folders are also subject to the same access rights as any other catalogue item, such as making them:</p> <ul> <li>Publicly readable, </li> <li>Readable by authenticated users, or</li> <li>Restricted to certain user groups.</li> </ul> <p>Folders are simply containers though; for more complex workflow and organisational scenarios, consider Versioned Folders</p>"},{"location":"glossary/fork/fork/","title":"Fork","text":""},{"location":"glossary/fork/fork/#what-is-a-fork","title":"What is a Fork?","text":"<p>A Fork is a copy of an existing Version of a model, with the intention of freely making many alterations to that model without affecting the original.</p> <p>The simplest way to understand Forks in Mauro is to view the Merge Graph of a model:</p> <p></p> <p>In this example, Model Version Tree DataModel (1.0.0) has progressed to version 2.0.0. However, a separate Fork has also been created, with its own main branch, which will not follow the same versioning history as the original. This allows the Fork to make many changes and not refer back to the original version history.</p>"},{"location":"glossary/fork/fork/#why-are-forks-useful","title":"Why are Forks useful?","text":"<p>Forks are useful for two reasons:</p> <ol> <li>They allow a simple starting point for creating a new model which may start similar but will deviate greatly once finished.</li> <li>Forks allow an existing model to be directed under a new Authority.</li> </ol>"},{"location":"glossary/fork/fork/#differences-with-branches","title":"Differences with Branches","text":"<p>At first glance, a Fork and a Branch may seem to serve the same purpose, but the key difference between them are:</p> <ul> <li>A Branch is designed to be a temporary deviation from the main line of a model, with the intention of eventaully merging it back into the main line later.</li> <li>A Fork is designed to be a permanent deviation from the main line of a model, acting as the starting point for a new version history. Forks are not intended to be merged back into their original starting point.</li> </ul>"},{"location":"glossary/fork/fork/#how-to-create-a-fork","title":"How to create a Fork","text":"<p>To create a Fork of a model, please refer to the user guide Branching, versioning and forking Data Models.</p>"},{"location":"glossary/label/label/","title":"Label","text":""},{"location":"glossary/label/label/#what-is-a-label","title":"What is a Label?","text":"<p>A Label is a name that describes and uniquely identifies each item within Mauro Data Mapper. This Label will appear in the Model Tree on the left hand side of the catalogue and at the top of the page when the item is selected. The Label is also used to identify the item when searched for. </p> <p></p>"},{"location":"glossary/label/label/#how-are-labels-used","title":"How are Labels used?","text":"<p>The Label of each item must be unique within its parent group so that no two items share the same Label. Therefore, each Data Model must have a unique Label. Each Data Class must have a unique Label within its parent Data Model. Each Data Element must have a unique Label within its parent Data Class. </p> <p>For example, there can only be one Data Class called \u2018Personal details\u2019 within a particular Data Model. Therefore, if you need to add a similar Data Class, include version information within the Label such as \u2018Personal details 2.0\u2019 to uniquely identify it. </p> <p></p> <p>In some cases, two different Data Models could consist of a Data Class with the same Label, such as 'Personal details'. However, because these two Data Classes are each associated with their own unique parent Data Model, then this is acceptable. Only when two items are within the same parent must they each have a unique Label.  </p> <p>Warning</p> <p>The following special characters are not permitted in labels, and will produce an error message:</p> <ul> <li><code>@</code></li> <li><code>$</code></li> <li><code>|</code></li> </ul>"},{"location":"glossary/label/label/#how-do-you-edit-a-label","title":"How do you edit a Label?","text":"<p>You can edit the Label of any item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the Label at the top of the details panel. </p> <p></p>"},{"location":"glossary/merging/merging/","title":"Merging","text":""},{"location":"glossary/merging/merging/#what-is-merging","title":"What is Merging?","text":"<p>To Merge a model means to combine the contents of one Branch into another. Merging branches can be simple when there are few edits, but in complex scenarios with multiple edits made, merging may require manual intervention to determine what conflicted changes must be resolved, and how.</p> <p>Merging is usually the final set of actions to perform when creating Branch to work on changes in isolation. It is usual to merge the changes from one branch back into the main branch.</p>"},{"location":"glossary/merging/merging/#how-to-merge","title":"How to Merge","text":"<p>To Merge branches, please refer to the user guide How to Merge Branches.</p>"},{"location":"glossary/multi-facet-aware/multi-facet-aware/","title":"Multi facet aware","text":""},{"location":"glossary/multi-facet-aware/multi-facet-aware/#what-does-multi-facet-aware-mean","title":"What does \"multi-facet\" aware mean?","text":"<p>The \"multi-facet aware\" trait can be applied to any Mauro catalogue item that has at least these traits:</p> <ol> <li>Is metadata aware - that is, may contain additional metadata beyond its core properties.</li> <li>Is annotation aware - that is, may contain annotations or comments.</li> <li>Is semantic link aware - that is, may contain semantic links to other catalogue items.</li> <li>Is reference file aware - that is, may contain files or attachments for further reference material.</li> <li>Is rule aware - that is, may contain rule information.</li> </ol>"},{"location":"glossary/multi-facet-aware/multi-facet-aware/#what-items-are-multi-facet-aware","title":"What items are \"multi-facet\" aware?","text":"<p>The following Mauro catalogue items are multi-facet aware by the definition listed above:</p> <ol> <li>Classifiers</li> <li>Code sets</li> <li>Data classes</li> <li>Data class components</li> <li>Data elements</li> <li>Data element components</li> <li>Data flows</li> <li>Data models</li> <li>Data types</li> <li>Enumeration types</li> <li>Enumeration values</li> <li>Folders</li> <li>Model data types</li> <li>Primitive types</li> <li>Reference data elements</li> <li>Reference data models</li> <li>Reference data types</li> <li>Reference enumeration types</li> <li>Reference enumeration values</li> <li>Reference primitive types</li> <li>Reference types</li> <li>Terms</li> <li>Term relationships</li> <li>Term relationship types</li> <li>Terminologies</li> <li>Versioned folders</li> </ol>"},{"location":"glossary/multi-facet-aware/multi-facet-aware/#usage-in-rest-api","title":"Usage in REST API","text":"<p>The Mauro REST API has many operations that are replicated across multiple catalogue item types, such as:</p> <ul> <li>Getting information on a catalogue item</li> <li>Saving data to a catalogue item</li> <li>Removing a catalogue item</li> <li>And so on...</li> </ul> <p>Many of these endpoints require a {multiFacetAwareDomainType} parameter in their URLs. One generalised example is fetching all metadata for a catalogue item:</p> <p>/api/{multiFacetAwareDomainType}/{id}/metadata</p> <p>Replace the {multiFacetAwareDomainType} parameter in these types of URL with a name listed above, remembering to:</p> <ol> <li>Remove spaces and whitespace in the domain names.</li> <li>Use <code>camelCase</code> for the domain name.</li> <li>Pluralise the domain name.</li> </ol> <p>Some examples would be:</p> <ul> <li>Folder becomes <code>folders</code>.</li> <li>Data Model becomes <code>dataModels</code>.</li> <li>Data Class becomes <code>dataClasses</code>.</li> <li>Terminology becomes <code>terminologies</code>.</li> <li>And so on...</li> </ul>"},{"location":"glossary/multiplicity/multiplicity/","title":"Multiplicity","text":""},{"location":"glossary/multiplicity/multiplicity/#what-is-multiplicity","title":"What is Multiplicity?","text":"<p>An item's Multiplicity refers to the minimum and maximum number of times it can appear within its parent in Mauro Data Mapper.  For example, the number of instances a particular Data Element appears within a Data Class. </p>"},{"location":"glossary/multiplicity/multiplicity/#how-is-multiplicity-used","title":"How is Multiplicity used?","text":"<p>Every Data Class and Data Element is assigned a Multiplicity. Typically, the Multiplicity is written in the form \u2018minmax\u2019, where min and max are integers representing the minimum and maximum number of times an item will appear within its parent. The symbol * represents an unbounded maximum.</p> <ul> <li>Using this notation:<ul> <li>Mandatory data has a minimum Multiplicity of 1 and a maximum Multiplicity of a specific integer or if there is no upper bound, then '*' (which is represented by '-1' internally).  </li> <li>Optional data has a minimum Multiplicity of 0 and a maximum Multiplicity of 1 or if there is no upper bound, *.</li> </ul> </li> </ul> <p>For example, each person in a Data Model only has one date of birth and if you want to record this, then the corresponding Multiplicity will be 1..1. However, each person may have many prescription records, which may or may not be relevant. In this case, the Multiplicity would be 0..*. </p> <p>Furthermore, each person will only have one date of death, which again you may or may not want to record, so the Multiplicity for this would be 0..1.</p>"},{"location":"glossary/multiplicity/multiplicity/#how-do-you-edit-an-items-multiplicity","title":"How do you edit an item's Multiplicity?","text":"<p>The Multiplicity can be found in the details panel of Data Classes and Data Elements. It is a mandatory field when adding or importing a Data Class or Data Elements, as explained in the 'Document a Health Datatset user guide'  and 'Import a Data Model from Excel user guide'. </p> <p>You can edit the Multiplicity of an item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the min and max values of that item's Multiplicity.</p> <p></p>"},{"location":"glossary/primitive-data-type/primitive-data-type/","title":"Primitive Data Type","text":""},{"location":"glossary/primitive-data-type/primitive-data-type/#what-is-a-primitive-data-type","title":"What is a Primitive Data Type?","text":"<p>A Primitive Data Type is one of the four possible Data Types within Mauro Data Mapper. It describes data which has no further details about structure or referencing. </p> <p>For example, if a Data Element is a name, then it\u2019s Primitive Data Type would be \u2018String\u2019. Whereas, if a Data Element is a number, then it\u2019s Primitive Data Type would be \u2018Integer\u2019.</p>"},{"location":"glossary/primitive-data-type/primitive-data-type/#how-are-primitive-data-types-used","title":"How are Primitive Data Types used?","text":"<p>Primitive Data Types are typically used for data which are strings or integers such as names, dates or times. </p> <p>Further details of the particular Primitive Data Type can be found in its details panel.</p> <p> </p> <p>You can also view all the Data Elements that use that Primitive Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity, Name and Description of each Data Element. </p> <p> </p>"},{"location":"glossary/profile/profile/","title":"Profile","text":""},{"location":"glossary/profile/profile/#what-is-a-profile","title":"What is a Profile?","text":"<p>A profile is a group of related metadata properties, typically sharing the same namespace. A profile allows these metadata properties to be grouped into sections and for each defined key may add a description, a default value and a validation constraint. For more details, see the Properties and Profiles tutorial.</p> <p></p>"},{"location":"glossary/profile/profile/#why-use-profiles","title":"Why use Profiles?","text":"<p>Grouping related properties together into a profile is useful for cleanly representing data entry, otherwise manually editing metadata becomes more verbose and laborious. The Mauro user interface is able to present profiles in a clean data entry view and uses profile information to allow for:</p> <ul> <li>Type-specific data entry controls - such as text fields, data pickers, checkboxes, and so on.</li> <li>Validation of fields.</li> </ul> <p>Profiles can be statically defined via Mauro plugins in code, or dynamically defined using the same Mauro Data Models, Classes and Elements as any other model. The Dynamic Profiles user guide explains how this can be achieved.</p>"},{"location":"glossary/reference-data-model/reference-data-model/","title":"Reference Data Model","text":""},{"location":"glossary/reference-data-model/reference-data-model/#what-is-a-reference-data-model","title":"What is a Reference Data Model?","text":"<p>A Reference Data Model is a collection of tabular data together with a description of that data; it can be thought of as both the data and metadata of information strutured as rows and columns, for example Comma-Separated Values (CSV).</p>"},{"location":"glossary/reference-data-model/reference-data-model/#how-are-reference-data-models-used","title":"How are Reference Data Models used?","text":"<p>Reference Data Models are a useful way to represent lookup data which are less structured than a Terminology but with more information (rows or columns) than can be easily represented in an Enumeration Data Type. They make the connection between the names of columns, the values stored, and our understanding of how the corresponding data is acquired, managed and interpreted. </p> <p>Mauro Data Mapper acts as a directory for these Reference Data Models and allows us to create, search and share these data descriptions and data.</p> <p>A Reference Data Model is structured as Reference Data Elements, Reference Data Types and Reference Data Values.</p> <p>For example, the CSV file etr.csv contains a list of NHS Trusts. </p> <p></p> <p>In this example, the column headings (Organisation Code, Name etc....) are each stored in Mauro Data Mapper as a Reference Data Element. </p> <p>Each Reference Data Element is typed, with that type stored as a Reference Data Type. (Disambiguation: note that Reference Data Type described here is different to the Reference Data Type which links similar Data Classes in a Data Model ).</p> <p>Every value is stored as a Reference Data Value.</p> <p>When this file is imported into Mauro Data Mapper, the Reference Data Elements are shown as follows:</p> <p></p> <p>And the Reference Data Values thus:</p> <p></p> <p>Note that when a CSV file is imported as a Reference Data Model all Reference Data Elements are given a Reference Data Type of the primitive 'string'.</p> <p>Each Reference Data Model has a:</p> <ul> <li> <p>Label     This is the unique name of the Reference Data Model.</p> </li> <li> <p>Aliases     Alternative names that can help locate the Reference Data Model when searched for.</p> </li> <li> <p>Organisation     Details of who is responsible for creating the Reference Data Model. </p> </li> <li> <p>Description     A definition either written in html or plain text which explains the types of data items that are grouped together within the Reference Data Model, as well as any contextual details.</p> </li> <li> <p>Classifications     These are effectively tags that you can apply to the Reference Data Model. </p> </li> </ul> <p>The above are all shown within the description panel, when the Reference Data Model is selected in the Model Tree.</p>"},{"location":"glossary/reference-data-type/reference-data-type/","title":"Reference Data Type","text":""},{"location":"glossary/reference-data-type/reference-data-type/#what-is-a-reference-data-type","title":"What is a Reference Data Type?","text":"<p>A Reference Data Type is one of the four possible Data Types within Mauro Data Mapper. It is used to describe relationships between different Data Classes within the same Data Model. Typically, Reference Data Types \u2018refer\u2019 and consequently link to another specified Data Class.  </p>"},{"location":"glossary/reference-data-type/reference-data-type/#how-are-reference-data-types-used","title":"How are Reference Data Types used?","text":"<p>By using Reference Data Types to link similar Data Classes together, users don\u2019t have to repeatedly add in new Data Classes, if a similar version already exists. </p> <p>For example, consider a Data Model where you have a \u2018Patient\u2019 Data Class and a \u2018GP\u2019 Data Class. Within the \u2018Patient\u2019 Data Class, there may be a Data Element called \u2018registeredGP\u2019. This Data Element refers to the \u2018GP\u2019 Data Class, and therefore it is a Reference Data Type. </p> <p>In the details panel of a Reference Data Type, a link to the Data Class it refers to is displayed in the \u2018Type\u2019 field. Clicking this link will navigate you to the details panel of that particular Data Class, where you can view all its associated Data Elements in the \u2018Content\u2019 tab below the details panel.  </p> <p> </p> <p>You can also view all the Data Elements that use that Reference Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity, Name and Description of each Data Element. </p> <p> </p>"},{"location":"glossary/semantic-links/semantic-links/","title":"Semantic links","text":""},{"location":"glossary/semantic-links/semantic-links/#what-is-a-semantic-link","title":"What is a Semantic link?","text":"<p>The term semantics refers to the meaning of data. It is essential that whenever data is produced in one system and used in another, it\u2019s meaning remains consistent between the two systems. This is known as semantic interoperability. </p> <p>Therefore, a Semantic link indicates that there is some sort of relationship between two descriptions of data. </p>"},{"location":"glossary/semantic-links/semantic-links/#how-are-semantic-links-used","title":"How are Semantic links used?","text":"<p>There are two types of Semantic links used within Mauro Data Mapper. </p>"},{"location":"glossary/semantic-links/semantic-links/#refines","title":"Refines","text":"<p>The most common type of semantic relationship is a \u2018Refines\u2019 link. Refine means to improve and in this context it signifies improving the quality and amount of information provided. Consequently, when one description refines another, it means that everything that is true about one description is also true about the other description, whilst often adding more information or context. </p> <p>For example, if a description of a column in a database specification Refines an item in a Data Standard, then everything the Data Standard says about that data item applies to the column. The rest of the database specification may contain more information about that column, such as specific conditions or constraints, but the description in the Data Standard still applies.</p> <p>This type of Semantic link can be created between Data Models, Data Classes and Data Elements.</p>"},{"location":"glossary/semantic-links/semantic-links/#does-not-refine","title":"Does not refine","text":"<p>Similarly, you can also create a \u2018does not refine\u2019 link which is used to indicate that the present definition is not intended as a refinement of another. </p> <p>If a Data Model, Data Class or Data Element contain any Semantic links, these are summarised in a 'Links' table below the details panel when the relevant data is selected in the Model Tree. This table displays a hyperlink to the target data the Semantic link refers to, as well as the type and total number of links.</p> <p>Links can be edited, created or removed in the 'Links' tab below the details panel. To edit the type of link or the target, click the 'Edit' pencil icon to the right. </p> <p>To add a link click '+ Add Link' and you will be able to specify the type of link and select the target. Click the green tick to confirm and save your changes. </p> <p></p>"},{"location":"glossary/terminology-data-type/terminology-data-type/","title":"Terminology Data Type","text":""},{"location":"glossary/terminology-data-type/terminology-data-type/#what-is-a-terminology-data-type","title":"What is a Terminology Data Type?","text":"<p>A Terminology Data Type is one of the four possible Data Types within Mauro Data Mapper. It is used to describe a structured collection of Enumerated Values which have relationships between different data terms. </p>"},{"location":"glossary/terminology-data-type/terminology-data-type/#how-are-terminology-data-types-used","title":"How are Terminology Data Types used?","text":"<p>Terminology Data Types have now been expanded in Mauro Data Mapper to \u2018Model Reference\u2019. Model References can now point to a Terminology, a CodeSet or a ReferenceDataModel.</p>"},{"location":"glossary/terminology-data-type/terminology-data-type/#terminology","title":"Terminology","text":"<p>A Terminology is a vocabulary, or a collection of allowable Terms as well as any relationships between them. A Term typically has a coded key and a human-readable value, along with some other information. Any relationships between pairs of Terms can be defined, including the relationship stating that one Term has a broader or narrower meaning than another Term.  </p> <p>For example, a hierarchy of Terms denoting patient diagnoses might include a general Term to indicate some form of Diabetes. This may be related to more specific Terms to indicate a particular form of Diabetes such as Type 1 Diabetes.</p> <p>Terminologies are represented by a book icon in Mauro Data Mapper and you can browse through several different databases by selecting \u2018Healthcare Terminologies\u2019 in the Model Tree. </p> <p> </p> <p>Further details of the particular Terminology can be found in its details panel when selected.</p> <p> </p> <p>To view information relating to a specific Term, select the relevant Term in the Model Tree and its corresponding details panel will be displayed. </p> <p>Each Term has a:</p> <ul> <li> <p>Label     This is the unique name of the Term.</p> </li> <li> <p>Aliases     Alternative names that can help locate the Term when searched for.</p> </li> <li> <p>Terminology     The Terminology that this Term is associated with.</p> </li> <li> <p>Description     A definition either written in html or plain text which explains any contextual details relating to the Term.</p> </li> <li> <p>URL     A URL to the original definition of the Term.</p> </li> <li> <p>Classifications     These are effectively tags that you can apply to the Term.</p> </li> </ul> <p>Below the details panel, is a list of Relationships between the selected Term and other Terms within the specified Terminology. </p> <p> </p>"},{"location":"glossary/terminology-data-type/terminology-data-type/#codeset","title":"CodeSet","text":"<p>A CodeSet is a selection of terms, which may be taken from one or more Terminologies. For example, a CodeSet would be all the Terms that describe the different variants of Diabetes. </p>"},{"location":"glossary/terminology-data-type/terminology-data-type/#referencedatamodel","title":"ReferenceDataModel","text":"<p>ReferenceDataModels are similar to a large database containing lots of detailed information and properties that would be too difficult to accurately manage in an Enumeration list. </p> <p>For example, consider the NHS organisation codes. This list includes all the organisation codes that represent each hospital, GP surgery and other practices. Alongside this organisation code could also be a name, an address and the details of the main contact at each hospital or surgery. Therefore, each data item has many different properties associated with it.   </p>"},{"location":"glossary/version/version/","title":"Version","text":""},{"location":"glossary/version/version/#what-does-version-mean","title":"What does Version mean?","text":"<p>A Version of a model represents the final state of that model. Changes made to a model throughout its lifetime can be tracked by their Version numbers to provide a history of changes made. All models with a version number are described as Finalised.</p> <p>As an example, given a new Data Model called \"Test Data Model\", the Versions could be created as such:</p> <ol> <li>Initial creation of \"Test Data Model\", then finalised to be Version 1.0.0</li> <li>A new draft is then created to make some alterations to the description of the model. This is then finalised to be Version 2.0.0</li> <li>Another new draft is then created, this time to alter the description and add Data Classes. This is then finalised to be Version 3.0.0</li> </ol> <p>...and so on.</p> <p>In this way, it is possible to trace the state of each Version of a model from its creation to now.</p> <p>Version numbers are sequential and typically follow the same pattern as Semantic Versioning using major.minor.patch, such as 1.2.3:</p> <ul> <li>Major update - represents a major change to the model, for example a major restructure. This updates the major number, e.g. 1.X.Y -&gt; 2.0.0</li> <li>Minor update - represents a minor, less encompasing change to the model, for example significantly updating text content but not altering the structure. This updates the minor number e.g. 1.0.X -&gt; 1.1.0</li> <li>Patch update - represents a very minor change to the model, for example fixing a gramatical mistake in a description. This updates the patch number e.g. 1.0.0 -&gt; 1.0.1</li> </ul> <p>It is also possible to define your own custom versioning scheme.</p>"},{"location":"glossary/version/version/#why-are-versions-useful","title":"Why are Versions useful?","text":"<p>Versions and Version numbers are useful to definitively state that a model has a set contents to it and will not change; the only way to change a versioned model is to create a new version of it. This allows models to be very easily traced and makes publication of metadata very structured. Versions also allow readers of your models to easily tell what has changed between versions of a model.</p>"},{"location":"glossary/version/version/#how-are-versions-created","title":"How are Versions created?","text":"<p>To create a new version of a model, please refer to the user guide Branching, versioning and forking Data Models.</p>"},{"location":"glossary/versioned-folder/versioned-folder/","title":"Versioned folder","text":""},{"location":"glossary/versioned-folder/versioned-folder/#what-is-a-versioned-folder","title":"What is a Versioned Folder?","text":"<p>A Versioned Folder shares the same traits as both a Folder and a catalogue model. It is both:</p> <ul> <li>A Folder, in that it is a container for holding other catalogue items, and</li> <li>A catalogue item that can be finalised, versioned, branched and/or forked to make new draft versions.</li> </ul> <p>A Versioned Folder is a version controlled folder, allowing the entire contents of the folder to be finalised and set to particular versions in one operation. Finalising a Versioned Folder has the same effect as finalising any other model, locking the folder (and its contents) to a set version.</p> <p></p>"},{"location":"glossary/versioned-folder/versioned-folder/#why-use-versioned-folders","title":"Why use Versioned Folders?","text":"<p>A Versioned Folder gives the benefits of both organisation and workflow management in one.</p> <p>There are several scenarios where a Versioned Folder may be useful:</p> <ol> <li>Your catalogue may be very large and contain many interconnected models that must be managed together.</li> <li>Larger sections of your catalogue may need to be version controlled as a whole, rather than individually finalising many smaller data models. Synchronisation of version numbers is also a big benefit to this approach.</li> <li>A Versioned Folder may act as the repository for a large model collection which should be published as one collection. Using a Versioned Folder also allows for the more complex workflow strategies of branching and forking but across an entire collection.</li> </ol>"},{"location":"installing/administration/","title":"Administration","text":""},{"location":"installing/administration/#checking-version-information","title":"Checking version information","text":"<p>Inside the web interface, the Plugins and Modules tab on the administrator dashboard provides information about the loaded components of the Mauro installation.  The first list, Plugins, provides a list of names and versions of components which integrate with known plugin hooks. For example Importers, Exporters and Email Providers.  The Modules list below provides a more comprehensive account of all Java / Grails components installed, including system libraries.  Each Module may include one or more Plugins.</p> <p>To provide automated reporting, the API call behind this dashboard may be called separately.  The <code>GET</code> urls below will return lists of plugins, for each named type:</p> <p>/api/admin/providers/importers /api/admin/providers/exporters /api/admin/providers/dataLoaders /api/admin/providers/emailers</p> <p>The endpoint below will return the list of modules:</p> <p>/api/admin/modules</p>"},{"location":"installing/administration/#backing-up-the-database","title":"Backing up the database","text":"<p>In order to back up the data from a running Mauro system, it is usually sufficient to take a simple backup of the underlying Postgres database,  which can be achieved through standard Postgres utilities (for example, using <code>pg_dump</code>).</p> <p>Within the Docker repository, a simple script in <code>postgres/bin/snapshot-data.sh</code> can be used within the docker container to take a copy of the  underlying postgres database. This creates a file in a new folder <code>/database</code> on the container which can be copied back out to the host machine.</p> <p>Alternatively, you can run an <code>exec</code> command directly from the host machine. For example the command listed below:</p> <pre><code>docker-compose exec postgres pg_dump -U postgres maurodatamapper | gzip -9  &gt; db-backup-$(date +%d-%m-%y).sql.gz </code></pre> <p>This will execute the <code>pg_dump</code> command on the postgres container, connecting to the <code>maurodatamapper</code> database.  The result will be zipped using the  <code>gzip</code> command, creating a file with today's timestamp on it.</p> <p>Backup requirements vary, but a typical use-case is to combine one of the backup commands listed above with a script to manage regular backups at  timed intervals and deleting old backups once a certain period has passed.  Example scripts which may be adapted can be  found on the official Postgres Wiki here.</p>"},{"location":"installing/administration/#re-building-the-search-index","title":"Re-building the search index","text":"<p>The search index improves the performance of searching and this content is stored in-memory (and persisted to files on disk at suitable intervals).  In some places in the Core, it may also be used to speed up access to particular model contents.</p> <p>The index is built using Apache Lucene but managed in the code through Hibernate.  This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary. For example if searching provides  incorrect or inconsistent results.  </p> <p>Administrators may rebuild the Lucene index through the user interface. To do this, click the white arrow by your user profile to the right of the menu header. Select 'Configuration' from the dropdown menu.</p> <p></p> <p>This will take you to configuration page where you can click the 'Lucene' tab and then select 'Rebuild Index'. </p> <p></p> <p>Please do not leave the page whilst reindexing is in progress. The time required is dependent on the number of models saved in the system, but may  take between 5 and 10 minutes for a large system.</p> <p>Alternatively, an API call may be made: see here for details.  This <code>POST</code> call may be made with an  existing session cookie, by passing username / password parameters as part of the call, or by passing an API Key.  Only those with system  administrator role may perform this action.</p> <p>The contents of the search index can be hard to inspect for debugging purposes! We use a tool called  Marple but be sure to use a compatible version.</p>"},{"location":"installing/administration/#docker-administration","title":"Docker administration","text":""},{"location":"installing/administration/#cleaning-up-docker","title":"Cleaning up docker","text":"<p>Continually building docker images will leave a lot of loose snapshot images floating around, occasionally make use of:</p> <ul> <li>Clean up stopped containers - <code>docker rm $(docker ps -a -q)</code></li> <li>Clean up untagged images - <code>docker rmi $(docker images | grep \"^&lt;none&gt;\" | awk \"{print $3}\")</code></li> <li>Clean up dangling volumes - <code>docker volume rm $(docker volume ls -qf dangling=true)</code></li> </ul> <p>You can make life easier by adding the following to the appropriate bash profile file:</p> <pre><code>alias docker-rmi='docker rmi $(docker images -q --filter \"dangling=true\")'\nalias docker-rm='docker rm $(docker ps -a -q)'\nalias docker-rmv='docker volume rm $(docker volume ls -qf dangling=true)'\n</code></pre> <p>Remove all stopped containers first then remove all tagged images.</p> <p>A useful tool is Dockviz, ever since docker did away with <code>docker images --tree</code> you can't see all the layers of images and therefore how many disconneected images you have.</p> <p>Add the following to the appropriate bash profile file:</p> <pre><code>alias dockviz=\"docker run --privileged -it --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz\"\n</code></pre> <p>Then in a new terminal you can run <code>dockviz images -t</code> to see the tree. The program also does dot notation files for a graphical view as well.</p>"},{"location":"installing/administration/#multiple-compose-files","title":"Multiple compose files","text":"<p>When you supply multiple files, <code>docker-compose</code> combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their successors.</p> <pre><code># Apply the .dev yml file, create and start the containers in the background\n$ docker-compose -f docker-compose.yml -f docker-compose.dev.yml -d &lt;COMMAND&gt;\n\n# Apply the .prod yml file, create and start the containers in the background\n$ docker-compose -f docker-compose.yml -f docker-compose.prod.yml -d &lt;COMMAND&gt;\n</code></pre> <p>We recommend adding the following lines to the appropriate bash profile file:</p> <p><pre><code>alias docker-compose-dev=\"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\"\nalias docker-compose-prod=\"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\"\n</code></pre> This will allow you to start compose in dev mode without all the extra file definitions.</p>"},{"location":"installing/administration/#debugging-and-advanced-configuration","title":"Debugging and advanced configuration","text":"<p>Here we present some useful hints for extending or customising the Docker setup for different configurations or use cases:</p> <p>Development file override</p> <p>The <code>docker-compose.dev.yml</code> can be used to override the standard <code>docker-compose.yml</code> file for development.  In its initial configuarion,  it opens up the ports in the Postgres container for manual connection from the host machine.  This <code>.dev</code> compose file rebuilds all of the images,  whereas the standard compose file and <code>.prod</code> versions do not build new images.</p> <p>Make use of the wait_scripts</p> <p>While <code>-links</code> and <code>depends_on</code> make sure the services a service requires are brought up first Docker only waits till they are running NOT till they are actually ready.  The wait scripts provided test responses on given ports to make sure that a given service is actually available and ready to  interact.</p> <p>Use <code>COPY</code> over <code>ADD</code></p> <p>Docker recommends using COPY instead of ADD unless the source is a URL or a tar file which ADD can retrieve and/or unzip.</p> <p>Use of <code>ENTRYPOINT</code> &amp; <code>CMD</code></p> <ul> <li>If not requiring any dependencies then just set <code>CMD [\"arg1\", ...]</code> and the args will be passed to the <code>ENTRYPOINT</code></li> <li>If requiring dependencies then set the <code>ENTRYPOINT</code> to the wait script and the <code>CMD</code> to <code>CMD [\"process\", \"arg1\", ...]</code></li> </ul> <p>Try to keep images as small as possible</p> <p>As a general rule, we try to use the base images (e.g. of Tomcat, Postgres) and install additional packages at runtime or start-up.  This increases portability and cuts down on disk usage when deploying updates.</p> <p>Exposing ports</p> <p>Exposing ports to other services must be carefully considered, to avoid unnecessary security vulnerabilities</p> <p>If the port only needs to be available to other docker services then use <code>expose</code>.</p> <p>If the port needs to be open to the host machine or beyond, then use <code>ports</code>.</p> <p>If the <code>ports</code> option is used this opens the port from the service to the outside world, it does not affect <code>exposed</code> ports between services, so if a service (e.g. postgres with port 5432) exposes a port then any service which used <code>link</code> to <code>postgres</code> will be able to find the database at <code>postgresql://postgres:5432</code></p>"},{"location":"installing/docker-install/","title":"Docker Install","text":"<p>Information</p> <p>See the Docker Setup section before installing Mauro using Docker.</p>"},{"location":"installing/docker-install/#git-repository","title":"Git repository","text":"<p>Depending on the operating system of the server you are running on, you may first need to install <code>git</code> to checkout the Mauro application.  You  can read more about installing <code>git</code> on different operating systems here: Getting Started - Installing Git</p> <p>The Mauro Docker configuration repository can be found here:  https://github.com/MauroDataMapper/mdm-docker.  Where you clone it is up to you, but on a *nix  system we recommend cloning into <code>/opt/</code> (for optional software packages).</p> <p>Different branches provide different configurations. We recommend checking out the <code>main</code> branch which will provide the latest releases of back-end  and front-end.  Alternatively, you can check out a specific tag to install a specific release. Tagged releases of  Mauro Docker take the form <code>YYYY.Q[.P]</code>, where YYYY.Q is the year and quarter of the release, and .P is an optional point release. For example, 2023.1.1 is point release 1 in Q1 of 2023.</p> <p>Information</p> <p>If you're running on an internal server with SSH access forbidden by a firewall, you can use the following link to access the repository via  HTTPS: SSH over HTTPS document.</p>"},{"location":"installing/docker-install/#overview","title":"Overview","text":"<p>The Docker Compose configuration defines two interacting containers:</p> <ul> <li>Postgres 12 [<code>postgres</code>] - Postgres Database</li> <li>Mauro Data Mapper [<code>maurodatamapper</code>] - Mauro Data Mapper</li> </ul> <p>The first of these is a standard Postgres container with an external volume for persistent storage.  The second builds on the standard Apache  Tomcat container, which hosts built versions of the Mauro application.  The Postgres container must be running whenever the Mauro application  starts.  The Mauro container persists logs and Lucene indexes to shared folders which can be found in the Docker repository folder. </p>"},{"location":"installing/docker-install/#default-username-password","title":"Default username / password","text":"<p>The Docker installation is empty on initialisation - it comes with one pre-configured user: with the username <code>admin@maurodatamapper.com</code> and the password <code>password</code>.  </p> <p>Warning</p> <p>We strongly recommend changing this password on first login, and then setting up personal user accounts for individual users.</p>"},{"location":"installing/docker-install/#building","title":"Building","text":"<p>Once cloned then running the standard <code>docker compose</code> build command will build the images necessary to run the services.</p> <pre><code># Build the entire system\n$ docker compose build\n</code></pre>"},{"location":"installing/docker-install/#additional-backend-plugins","title":"Additional Backend Plugins","text":"<p>Additional plugins can be found at the Mauro Data Mapper Plugins organisation page. A complete list with versions can also be found in the installation documentation. Please note that while we will do our best to keep this page up-to-date, there may be circumstances where it is behind. Therefore, we recommend using our official GitHub Plugins organisation to find the latest releases and all available plugins.</p> <p>Each of these can be added as <code>runtimeOnly</code> dependencies by adding them to the <code>ADDITIONAL_PLUGINS</code> build argument for the <code>mauro-data-mapper</code> service build.</p> <p>These dependencies should be provided in a semicolon separated list in the Gradle style, they will be split and each will be added as a <code>runtimeOnly</code> dependency.</p> <p>Example:</p> <pre><code> mauro-data-mapper:\nbuild:\ncontext: mauro-data-mapper\nargs:\nADDITIONAL_PLUGINS: \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:5.2.0\"\n</code></pre> <p>This will add the Excel plugin to the <code>dependencies.gradle</code> file:</p> <pre><code>runtimeOnly uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:5.2.0\n</code></pre>"},{"location":"installing/docker-install/#dynamic-versions","title":"Dynamic Versions","text":"<p>You can use dynamic versioning to add dependencies, however this comes with a risk that it pulls a version which does not comply with your expected version of <code>mdm-application-build/mdm-core</code> ,which may cause conflicts with other plugins. Therefore, we do not advise this approach.</p> <p>Example</p> <pre><code> mauro-data-mapper:\nbuild:\ncontext: mauro-data-mapper\nargs:\nADDITIONAL_PLUGINS: \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:5.+\"\n</code></pre> <p>This will add the latest minor version of the Excel plugin.</p>"},{"location":"installing/docker-install/#theme","title":"Theme","text":"<p>Mauro comes with a default user interface theme - with the standard blue branding, and default text on the home page.  This can be overridden in  the <code>docker-compose.yml</code> file, with instructions provided in the Branding guide.  The default theme is called  <code>default</code> and can be set with:</p> <pre><code> MDM_UI_THEME_NAME: \"default\"\n</code></pre>"},{"location":"installing/docker-install/#running-multiple-instances","title":"Running multiple instances","text":"<p>If running multiple Mauro Docker instances then they can all make use of the same initial images, therefore you only need to run the <code>docker compose build</code> command once per server.</p>"},{"location":"installing/docker-install/#run-environment","title":"Run environment","text":"<p>By adding variables to the <code>&lt;service&gt;.environment</code> section of the <code>docker-compose.yml</code> file, you can pass them into the container as environment variables. These will override any existing configuration variables which are used by default. Any defaults and normally used environment variables can be found in the relevant service's Dockerfile at the <code>ENV</code> command.</p>"},{"location":"installing/docker-install/#postgres-service","title":"postgres service","text":"<ul> <li> <p><code>POSTGRES_PASSWORD</code> </p> <p>This sets the postgres user password for the service. As per the documentation at Postgres Docker Hub,  it must be set for a docker postgres container. We have set a default but you can override if desired.  If you do override it, you will also need to change the <code>PGPASSWORD</code> environment variable in the mauro-data-mapper section.</p> </li> <li> <p><code>DATABASE_USERNAME</code> </p> <p>This is the username which will be created inside the Postgres instance to own the database which the MDM service will use.  The username is also used by the MDM service to connect to the postgres instance,  therefore if you change this you must** also supply it in the environment args for the MDM service.</p> </li> <li> <p><code>DATABASE_PASSWORD</code></p> </li> </ul> <p>This is the password set for the <code>DATABASE_USERNAME</code>. It is the password used by the MDM service to connect to this postgres container.</p>"},{"location":"installing/docker-install/#mauro-data-mapper-service","title":"mauro-data-mapper service","text":"<p>There are a large amount of variables which either need to be set or can be overridden depending on what plugins have been installed and what features you want. Therefore, you can find all the information on configuring MDM here.</p> <p>There are 2 environment variables which are not used directly by MDM and these are both optional to be overridden in the compose file.</p> <ul> <li><code>PGPASSWORD</code></li> </ul> <p>This is the postgres user's password for the postgres server. This is an environment variable set to allow the MDM service to wait till the postgres   service has completely finished starting up. It is only used to confirm the Postgres server is running and databases exist. After this it is not   used again.</p> <p>Note</p> <p>If you change <code>POSTGRES_PASSWORD</code> you must change this to match. This can only** be overridden in the <code>docker-compose.yml</code> file.</p> <ul> <li><code>CATALINA_OPTS</code></li> </ul> <p>Java Opts to be passed to Tomcat.</p> <p>Note</p> <p>This can only be overridden in the <code>docker-compose.yml</code> file.</p>"},{"location":"installing/docker-install/#environment-notes","title":"Environment Notes","text":""},{"location":"installing/docker-install/#database","title":"Database","text":"<p>The system is designed to use the Postgres service provided in the docker-compose file, therefore there should be no need to alter any of these settings. Only make alterations if running Postgres as a separate service outside of Docker Compose.</p>"},{"location":"installing/docker-install/#email","title":"Email","text":"<p>The standard email properties will allow emails to be sent to a specific SMTP server.</p>"},{"location":"installing/docker-install/#docker-reference","title":"Docker Reference","text":""},{"location":"installing/docker-install/#running","title":"Running","text":"<p>Before running please read the parameters section first.</p> <p>With <code>docker</code> (with <code>docker compose</code>) installed, run the following:</p> <pre><code># Build all the images\n$ docker compose build\n\n# Start all the components up\n$ docker compose up -d\n\n# To only start 1 service\n# This will also start up any of the services the named service depends on (defined by `links` or `depends_on`)\n$ docker compose up [SERVICE]\n\n# To push all the output to the background add `-d`\n$ docker compose up -d [SERVICE]\n\n# Stop background running and remove the containers\n$ docker compose down\n\n# To update an already running service\n$ docker compose build [SERVICE]\n$ docker compose up -d --no-deps [SERVICE]\n</code></pre> <p>If you run everything in the background use <code>Kitematic</code> to see the individual container logs. You can do this if running in the foreground and it is easier as it splits each of the containers up.</p> <p>If only starting a service when you stop, the service docker will not stop the dependencies that were started to allow the named service to start.</p> <p>The default compose file will pull the correct version images from Bintray, or a locally defined docker repository.</p> <p>For more information about administration of your running Docker instance, please see the Administration guide</p>"},{"location":"installing/docker-setup/","title":"Docker Setup","text":""},{"location":"installing/docker-setup/#system-requirements","title":"System requirements","text":"<p>The simplest installation method is to run our preconfigured application using Docker. Any operating system, on a server or desktop, running Docker can run Mauro Data Mapper, but please note that some organisations may restrict the use of Docker on virtual machines.</p> <p>We advise a minimum of 2 CPUs and 4GBs RAM just to run this system. This does not allow for the requirements to have an operating system running as well. Therefore we recommend a 4 CPU and 8GB RAM server.</p> <p>The default install of Docker inside Linux configures the Docker Engine with unlimited access to the server's resources. However, if running in Windows or macOS the Docker Toolbox will need to be configured.</p>"},{"location":"installing/docker-setup/#installing-docker-and-docker-compose","title":"Installing Docker and Docker Compose","text":"<p>You will need to install Docker and Docker Compose.  Docker Compose is included as part of the standard 'Docker Desktop' for Windows and macOS.</p> <p>To run Mauro Data Mapper, we recommend the following minimum versions:</p> <ul> <li>Docker Engine: 20.10.21 or higher</li> <li>Docker Compose: preferably v2.x.x (<code>docker compose</code> command); alternatively v1.27.x (<code>docker-compose</code> command) or higher </li> </ul> <p>Warning</p> <p>If you are running on Ubuntu (20.04 or earlier), the default version of <code>docker-compose</code> installed with apt-get is currently 1.25.0 or earlier, and you might get the error message: <pre><code>Building docker compose ERROR: Need service name for --build-arg option\n</code></pre> In this case, you should uninstall <code>docker-compose</code> and re-install directly from Docker, following the instructions here.</p>"},{"location":"installing/docker-setup/#docker-machine-configuration","title":"Docker Machine configuration","text":"<p>The default <code>docker-machine</code> in a Windows or macOS environment is configured to make use of one CPU and 1GB RAM. This is not enough RAM to reliably run the Mauro Data Mapper system and so should be increased.</p> <p>On Linux the docker machine is the host machine so there is no need to build or remove anything.</p>"},{"location":"installing/docker-setup/#native-docker","title":"Native Docker","text":"<p>If using the Native Docker then edit the preferences of the Docker application and increase the RAM to at least 4GB. You will probably need to restart Docker after doing this.</p>"},{"location":"installing/docker-setup/#docker-toolbox","title":"Docker Toolbox","text":"<p>If using the Docker Toolbox then you will need to perform the following in a 'docker' terminal:</p> <pre><code># Stop the default docker machine\n$ docker-machine stop default\n\n# Remove the default machine\n$ docker-machine rm default\n\n# Replace with a more powerful machine (4096 is the minimum recommended RAM, if you can give it more then do so)\n$ docker-machine create --driver virtualbox --virtualbox-cpu-count \"-1\" --virtualbox-memory \"4096\" default\n</code></pre>"},{"location":"installing/docker-setup/#use-the-default-docker-machine","title":"Use the default Docker Machine","text":"<p>When controlling using Docker Machine via your terminal shell it is useful to set the <code>default</code> docker machine. Type the following at the command line, or add it to the appropriate bash profile file:</p> <pre><code>eval \"$(docker-machine env default)\"\n</code></pre> <p>If not you may see the following error:</p> <p><code>Cannot connect to the Docker daemon. Is the docker daemon running on this host?</code></p> <p>For more information about administration of your running Docker instance, please see the Administration guide</p>"},{"location":"installing/migration/","title":"Migrating Old Data","text":"<p>A tool has been built to migrate data from old instances of Metadata Catalogue to Mauro Data Mapper.  This is a docker-based tool which applies a  number of SQL scripts to move data into a new database schema and transform the data into a new format suitable for Mauro.</p> <p>The GitHub repository for the migration is here:  https://github.com/MauroDataMapper/mc-to-mdm-migration.</p>"},{"location":"installing/migration/#manual-process","title":"Manual Process","text":"<p>Please see the documents in the <code>guide</code> folder for further information.</p>"},{"location":"installing/migration/#automated-process","title":"Automated Process","text":"<p>You can use one of the two available scripts in the top of the repository to run the migration from start to finish.</p> <p>You have two options as with the manual process documents, depending on if you're running Metadata Catalogue and Mauro Data Mapper inside or outside Docker.</p> <p>The scripts execute the stages described in the guide, so if you're unsure as to what parameters you should feed in then please read the guides to find out more.</p> <p>The scripts all execute using the defaults as if you have built the system using Docker.</p>"},{"location":"installing/migration/#docker-based-postgresql","title":"Docker Based PostgreSQL","text":"<pre><code># Help/Usage\n./run-complete-migration-docker.sh --help\n# Default parameters run\n./run-complete-migration-docker.sh\n</code></pre>"},{"location":"installing/migration/#remotelocal-postgresql","title":"Remote/Local PostgreSQL","text":"<pre><code># Help/Usage\n./run-complete-migration-remote.sh --help\n# Default parameters run\n./run-complete-migration-remote.sh\n</code></pre>"},{"location":"installing/migration/#notes","title":"Notes","text":"<p>Please be aware that DataFlows  are currently not migrated using this system, and may need to be manually migrated, or exported / imported.</p>"},{"location":"installing/plugins/","title":"Plugins","text":"<p>We host a number of community plugins, the code for which is available in our GitHub 'Plugins' organisation. Commits to master branches kick off a build process on our continuous integration server and successfully-built artefacts are hosted on our instance of Artifactory.</p> <p>Below is a list of all the available plugins, along with their latest version number, release date and any dependencies each has. More details about the changes in each release can be found on the Release Notes page. To install a plugin, use the 'artefact name' as directed in the Installing page.</p>"},{"location":"installing/plugins/#importers-exporters","title":"Importers / Exporters","text":"Plugin nameVersion Release date Artefact name Dependencies ART-DECOR2.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-artdecor:2.2.0</code> Core &gt;= 5.2.0 AWS Glue2.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-awsglue:2.2.0</code> Core &gt;= 5.2.0 CSV4.3.0 2023-02-24 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-csv:4.3.0</code> Core &gt;= 5.3.0 Digital Object Identifiers2.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-digital-object-identifiers:2.2.0</code> Core &gt;= 5.2.0 Excel5.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:5.2.0</code> Core &gt;= 5.2.0 FHIR2.3.0 2023-02-24 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-fhir:2.3.0</code> Core &gt;= 5.3.0 MS SQL8.2.0 2023-02-24 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-sqlserver:8.2.0</code> Core &gt;= 5.3.0 MySQL7.2.0 2023-02-24 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-mysql:7.2.0</code> Core &gt;= 5.3.0 Oracle SQL6.2.0 2023-02-24 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-oracle:6.2.0</code> Core &gt;= 5.3.0 PostgreSQL7.2.0 2023-02-24 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-postgresql:7.2.0</code> Core &gt;= 5.3.0 XMI1.0.1 2023-04-26 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-xmi-visualparadigm:1.0.1</code> Core &gt;= 5.3.0 XSD1.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-xsd:1.2.0</code> Core &gt;= 5.2.0"},{"location":"installing/plugins/#profiles","title":"Profiles","text":"Plugin NameVersion Release Date Artefact name Dependencies DCAT2.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-profile-dcat:2.2.0</code> Core &gt;= 5.2.0 Dementia Platform2.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-profile-dementia-platform:2.2.0</code> Core &gt;= 5.2.0 HDR UK2.3.0 2023-02-24 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-profile-hdruk:2.3.0</code> Core &gt;= 5.3.0 Schema.org2.3.0 2023-02-24 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-profile-schema-org:2.3.0</code> Core &gt;= 5.3.0"},{"location":"installing/plugins/#security","title":"Security","text":"Plugin nameVersion Release date Artefact name Dependencies Keycloak Integrated Authentication3.3.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-keycloak:3.3.0</code> Core &gt;= 5.2.0 OpenID Connect Authentication2.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-openid-connect:2.2.0</code> Core &gt;= 5.2.0"},{"location":"installing/plugins/#technical-other","title":"Technical / Other","text":"Plugin nameVersion Release date Artefact name Dependencies Freemarker Templating2.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-freemarker:2.2.0</code> Core &gt;= 5.2.0 SPARQL2.2.0 2022-08-16 <code>uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-sparql:2.2.0</code> Core &gt;= 5.2.0"},{"location":"installing/updating/","title":"Updating to the latest version","text":"<p>Updating an already running system can be performed in one of two ways. </p>"},{"location":"installing/updating/#pull-from-latest-tag","title":"Pull from latest tag","text":"<p>The preferred method would be to pull the latest version tag from the mdm-docker repository and then rebuild the mauro-data-mapper service. However this may be hard if multiple changes have been made to the <code>docker-compose.yml</code> and you are not familiar enough with git to handle stashing and merging.</p> <pre><code># Update an already built system\n# Fetch the latest commits\n$ git fetch\n# Stash any local changes\n$ git stash\n# Checkout/pull the version you want to update to\n# e.g. git checkout B4.4.1_F6.0.0\n$ git checkout &lt;TAG&gt;\n# Unstash local changes, you may need to resolve any merge conflicts\n$ git stash pop\n# Build the new image\n$ docker-compose build mauro-data-mapper\n# Start the update\n$ docker-compose up -d mauro-data-mapper\n</code></pre>"},{"location":"installing/updating/#update-script","title":"Update script","text":"<p>The alternative method is to use the update command script and pass in the new versions you want to update to. The downside with this method is if we have made any changes to the Dockerfiles or base versions you will not have them.</p> <pre><code># Update an already built system\n# e.g ./update -b 4.4.1 -f 6.0.0\n$ ./update -b &lt;BACKEND_VERSION&gt; -f &lt;FRONTEND VERSION&gt;\n</code></pre> <p>This will rebuild just the Mauro Data Mapper image with the latest version.</p>"},{"location":"installing/updating/#database-migrations","title":"Database migrations","text":"<p>Occasionally, database migrations are required when updating to a new version. These run automatically when the application restarts, making use of the Flyway versioning system. No manual steps are required from the user.</p>"},{"location":"installing/branding/branding/","title":"Branding","text":""},{"location":"installing/branding/branding/#introduction","title":"Introduction","text":"<p>The Mauro Data Mapper Web interface can be customised to match an organisation's brand or styling. Creating a new theme requires developer effort, the steps of which are detailed below.</p>"},{"location":"installing/branding/branding/#angular-material","title":"Angular Material","text":"<p>The Angular Material framework, which the UI uses for layout and controls, can support multiple themes (colour palettes and typography) as well as dynamic switching of themes. However, the themes must be precompiled into the application as part of the stylesheets for the site. This means that themes must be prepared by a developer.</p> <p>See also:</p> <ul> <li>Angular Material theming guide</li> <li>Theming your components</li> </ul>"},{"location":"installing/branding/branding/#themes","title":"Themes","text":""},{"location":"installing/branding/branding/#overview","title":"Overview","text":"<p>Themes in Mauro Data Mapper are organised in the project by name, and uses folder and naming conventions to identify where files and CSS selectors are. The conventions used are:</p> <ul> <li>SASS files: <code>src/style/themes/{name}.scss</code></li> <li>CSS theme class: <code>.{name}-theme</code></li> <li>Asset files: <code>src/assets/themes/{name}/*.*</code></li> </ul> <p>So, as an example, a custom theme called <code>nhs-digital</code> would be:</p> <ul> <li>SASS files: <code>src/style/themes/nhs-digital.scss</code></li> <li>CSS theme class: <code>.nhs-digital-theme</code></li> <li>Asset files: <code>src/assets/themes/nhs-digital/*.*</code></li> </ul>"},{"location":"installing/branding/branding/#switching-themes","title":"Switching Themes","text":"<p>To determine the theme to use for the site, update the <code>src/environments/environment.ts</code>:</p> <pre><code>export const environment = {\n// ...\nthemeName: 'nhs-digital'\n// ...\n};\n</code></pre> <p>Note</p> <p>If the theme name is not provided, this will fall back to <code>default</code>.</p> <p>An alternative for switching the theme for production UI builds is to set an environment variable called <code>MDM_UI_THEME_NAME</code>. This allows the theme setting to be defined as part of a wider build process, such as creating a Docker image.</p>"},{"location":"installing/branding/branding/#importing-themes","title":"Importing Themes","text":"<p>Each theme requires a SASS file to define the colour palette and typography for the Angular Material theming system to use. This SASS file can also override any other CSS selectors that do not affect the Material controls.</p> <p>The <code>src/style/styles.scss</code> file is the place to import all theme files. The key parts are:</p> <pre><code>@import \"~@angular/material/theming\";\n\n// Include the common styles for Angular Material. Only required to include this once!\n@include mat-core();\n\n/* \nThemes \nImport more theme SASS files here to make them available to the app\n*/\n@import \"style/themes/default.scss\";\n@import \"style/themes/nhs-digital.scss\";\n// ...etc\n\n// More SASS files imported here...\n</code></pre> <p>Note</p> <p>Remember that Angular Material requires all themes to be precompiled, so all SASS files must be imported to include them as options. Only one theme will appear at a time though.</p>"},{"location":"installing/branding/branding/#creating-themes","title":"Creating Themes","text":"<p>To create a new theme it is advised to copy <code>src/style/themes/default.scss</code> and make the necessary adjustments. Then <code>@import</code> the new theme file in <code>src/style/styles.scss</code>.</p> <p>The specifics of the Angular Material theme system should be referenced for better explanation. However, in summary, you will need to define:</p> <ol> <li>The colour palettes required to represent:<ol> <li>Primary colours</li> <li>Accented colours</li> <li>Warning colours</li> </ol> </li> <li>The typography settings to use for text/font</li> </ol> <p>The Material mixins should then be included within a top-level CSS selector named after the theme. This CSS selector will be applied to the body of the HTML page and therefore affect all DOM elements below it. </p> <p>The theme file should effectively include the following steps:</p> <pre><code>// Required for Angular Material\n@import \"~@angular/material/theming\";\n\n// Required for custom theming of MDM components\n@import \"../components/custom\";\n\n/* Define colour platte maps here ... */\n$nhs-digital-theme-primary: mat-palette(...);\n$nhs-digital-theme-accent: mat-palette(...);\n$nhs-digital-theme-warn: mat-palette(...);\n\n/* Create the colour theme */\n$nhs-digital-theme: mat-light-theme(\n$nhs-digital-theme-primary,\n$nhs-digital-theme-accent,\n$nhs-digital-theme-warn\n);\n\n/* Define the typography settings to use... */\n$nhs-digital-typography: mat-typography-config(...);\n\n/* \nDefine the top-level CSS class name. Name this in the format \".{name}-theme\"\nso the application can automatically use it for the page \n*/\n.nhs-digital-theme {\n\n// Use the created theme variables to configure Angular Material\n@include angular-material-theme($nhs-digital-theme);\n@include angular-material-typography($nhs-digital-typography);\n\n// Some MDM components have also been added to the Angular Material theme system. Include this mixin to have these match the same theme\n@include mdm-custom-components-theme($nhs-digital-theme);\n\n/* Add any further CSS class overrides here... */\n}\n</code></pre>"},{"location":"installing/branding/branding/#assets","title":"Assets","text":"<p>Themeable assets, such as logos and images, should be stored under <code>src/assets/themes</code> under the specific sub-folder named after the theme. The file names used should be consistent across all themes e.g. <code>logo.png</code>.</p> <p>To consistently reference the correct asset path to use, the <code>ThemingService</code> can be injected into your component/service and use the <code>getAssetPath()</code> function to get the correct path for an asset according to the current theme.</p>"},{"location":"installing/branding/branding/#edit-page-content","title":"Edit page content","text":"<p>Some of the static content may be adjusted by an administrator to allow the end users to make content edits instead of relying on developers. Follow the steps below to edit page content.</p> <p>Sign in and click the white arrow by your user profile on the right of the menu header. Select 'Configuration' from the dropdown menu.</p> <p></p> <p>Click the 'Properties' tab to view a list of properties, with the cog icon indicating a system property. </p> <p></p> <p>To edit or delete a property, click the three vetical dots to the right of the relevant row and choose either 'Edit' or 'Delete' from the dropdown menu. </p> <p>To add a new property click the '+Add' button at the top right of the page which will take you to an 'Add Property' form. </p> <p></p> <p>Firstly, select a property you want to add from the dropdown menu.  </p> <p>Then enter a 'Key', 'Category' and 'Value'. Tick the 'Publicly visible' box if it applies. Once the form is completed, click 'Add property' and a green notifciaiton box should appear at the bottom right of your screen, confirming the changes. </p> <p>Note</p> <p>It may be necessary to manually adjust the HTML source to set correct styling/markup for the content. This can be done by clicking on  the `` 'Change Mode' toolbar button in each form edit field.</p> <p>There are configuration properties available to modify the following:</p> <ul> <li>Homepage</li> <li>Logo</li> <li>Footer</li> </ul>"},{"location":"installing/branding/branding/#home-page","title":"Home Page","text":"<p>The homepage of Mauro Data Mapper displays a summary of the tool as well as the most important links to help users easily navigate to the main sections. The homepage can be modified to include custom content to suit your organisation. </p> <p>The homepage is split into two columns and four main sections which can each be modified. </p> <p></p> <p>Note</p> <p>It is recommended to include correct HTML source and styling in these sections.</p> <ul> <li> <p>Introduction - <code>content.home.intro.left</code>     This is usually some introduction text on the left column of the starting page</p> </li> <li> <p>Introduction image - <code>content.home.intro.right</code>     This is usually an image or illustration on the right column of the starting page that compliments the introduction text</p> </li> <li> <p>Features heading - <code>content.home.detail.heading</code>     This is usually a heading for the 'Features' section underneath the introduction</p> </li> <li> <p>Feature boxes - <code>content.home.detail.column1 - 3</code>     These are three feature boxes that usually appear in the 'Features' section underneath the introduction</p> </li> </ul>"},{"location":"installing/branding/branding/#logo","title":"Logo","text":"<p>A static asset should also be provided for the logo in the navbar component as a default, however an image URL may also be provided if the logo is hosted in another location e.g. CDN. To set a logo, modify the <code>theme.logo.url</code> property.</p> <p>The <code>theme.logo.width</code> property is also provided to adjust the size of the logo in the space provided. Note that:</p> <ul> <li>The value entered for <code>theme.logo.width</code> must be supported by CSS e.g. <code>20px</code>, <code>1.2em</code>, etc</li> <li>The maximum width of the logo is <code>120px</code>. If the image is larger than this then it will be scaled down to fit</li> </ul>"},{"location":"installing/branding/branding/#footer","title":"Footer","text":"<p>The copyright notice can be altered in the footer by changing the <code>content.footer.copyright</code> property.</p>"},{"location":"installing/branding/branding/#defaults","title":"Defaults","text":"<p>If no property values are provided for the above, then suitable defaults are used instead based on the current theme. To revert back to default values for any property, simply delete the property from the configuration table.</p>"},{"location":"installing/configuring/application.yml/","title":"MDM Application YML","text":"<p>Below is the default <code>application.yml</code> file included in the main application.</p> <pre><code>database:\nhost: localhost\nport: 5432\nname: maurodatamapper\n---\n#Default for plugins/applications\n---\nmaurodatamapper:\nsecurity:\npublic: false\nauthority:\nname: 'Mauro Data Mapper'\nurl: http://localhost\ngrails:\nprofile: rest-api\ncodegen:\ndefaultPackage:  uk.ac.ox.softeng.maurodatamapper.application\ngorm:\nreactor:\n# Whether to translate GORM events into Reactor events\n# Disabled by default for performance reasons\nevents: false\nfailOnError: true\nresources:\npattern: /**\ninfo:\napp:\nname: '@info.app.name@'\nversion: '@info.app.version@'\ngrailsVersion: '@info.app.grailsVersion@'\nspring:\njmx:\nunique-names: true\nmain:\nbanner-mode: \"off\"\ngroovy:\ntemplate:\ncheck-template-location: false\ndevtools:\nrestart:\nadditional-exclude:\n- '*.gsp'\n- '**/*.gsp'\n- '*.gson'\n- '**/*.gson'\n- 'logback.groovy'\n- '*.properties'\n\n# Spring Actuator Endpoints are Disabled by Default\nmanagement:\nendpoints:\nenabled-by-default: false\nweb:\nexposure:\ninclude:\n- 'health'\n- 'shutdown'\njmx:\nexposure:\ninclude: '*'\nendpoint:\nshutdown:\nenabled: true\nhealth:\nenabled: true\n\n---\ngrails:\nmime:\ndisable:\naccept:\nheader:\nuserAgents:\n- Gecko\n- WebKit\n- Presto\n- Trident\ntypes:\njson:\n- application/json\n- text/json\nhal:\n- application/hal+json\n- application/hal+xml\nxml:\n- text/xml\n- application/xml\natom: application/atom+xml\ncss: text/css\ncsv: text/csv\njs: text/javascript\nrss: application/rss+xml\ntext: text/plain\nall: '*/*'\nurlmapping:\ncache:\nmaxsize: 1000\ncontrollers:\ndefaultScope: singleton\nconverters:\nencoding: UTF-8\nexceptionresolver:\nparams:\nexclude:\n- password\n- tempPassword\ncors:\nenabled: true\n# The following are the defaults\n# allowedOrigins: [] # Cannot use allowedOrigins with *, they have to be clearly stated origins\nallowedOriginPatterns: [ '*' ]\nallowedMethods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS', 'HEAD']\nallowedHeaders: ['origin', 'content-type', 'accept', 'authorization', 'pragma', 'cache-control']\n#exposedHeaders: null\n#maxAge: 1800\n#allowCredentials: true\nviews:\nmarkup:\nautoEscape: true\nprettyPrint: false\nautoIndent: false\nautoNewLine: false\n---\nhibernate:\ncache:\nqueries: false\nuse_second_level_cache: true\nuse_query_cache: true\nregion:\nfactory_class: org.hibernate.cache.jcache.JCacheRegionFactory\njavax:\ncache:\nprovider: org.ehcache.jsr107.EhcacheCachingProvider\nmissing_cache_strategy: create\nsearch:\ndefault.indexBase: '/tmp/lucene/mdm_application'\n\n---\ndataSource:\npooled: true\njmxExport: true\nformatSql: true\ndriverClassName: org.postgresql.Driver\ndialect: org.hibernate.dialect.PostgreSQL10Dialect\nusername: maurodatamapper\npassword: MauroDataMapper1234\ndbCreate: none\nurl: 'jdbc:postgresql://${database.host}:${database.port}/${database.name}'\n---\n</code></pre>"},{"location":"installing/configuring/build.yml/","title":"Build YML","text":"<p>Below is the default <code>build.yml</code> file built into the MDM image.</p> <pre><code>---\n# Database connection details\n---\ndatabase:\nhost: postgres\nport: 5432\nname: maurodatamapper\ndataSource:\nusername: maurodatamapperuser\npassword: \"this is provided but hidden in these docs\"\n---\n# MDM configuration properties\n---\nmaurodatamapper:\nauthority:\nname: 'Mauro Data Mapper'\nurl: http://localhost\n---\n# Standard Email configuration\n---\nsimplejavamail:\nsmtp:\nusername:\npassword:\nhost:\nport: 587\ntransportstrategy: SMTP_TLS\n---\n# mdm-plugin-email-proxy Configuration\n---\n#emailServiceUrl: \n#emailServiceUsername: \n#emailServicePassword: \n---\n# CORS\n# See http://docs.grails.org/latest/guide/theWebLayer.html#cors\n---\ngrails:\ncors:\nenabled: true\n# The following are the defaults\n# allowedOrigins: [] # Cannot use allowedOrigins with *, they have to be clearly stated origins\nallowedOriginPatterns: [ '*' ]\nallowedMethods: [ 'GET', 'POST', 'PUT', 'DELETE', 'OPTIONS', 'HEAD' ]\nallowedHeaders: [ 'origin', 'content-type', 'accept', 'authorization', 'pragma', 'cache-control' ]\n#exposedHeaders: null\n#maxAge: 1800\n#allowCredentials: true\nhibernate:\nsearch:\ndefault:\nindexBase: '/lucene'\n</code></pre>"},{"location":"installing/configuring/changing-database/","title":"Changing Database","text":"<p>There are 3 variations of changing the database which may be considered:</p> <ul> <li>Using the postgres service but using different database name, username and/or password.</li> <li>Using an external database server/service outside of the docker-compose network.</li> </ul> <p>Information</p> <p>We cannot recommend using a databasing system other than postgreSQL as we have custom SQL inside the API which uses postgres specific features. </p> <p>Warning</p> <p>The postgres user which MDM uses to connect must be a postgres superuser. This is because we enable and disable foreign keys during major deletes to allow the deletions to happen in a reasonble time.</p>"},{"location":"installing/configuring/changing-database/#changing-the-postgres-service-variables","title":"Changing the postgres service variables","text":"<p>The following properties can be altered inside the postgres service. Any properties which can be defined at runtime can be set in the <code>docker-compose.yml</code> <code>environment</code> block. There is a fixtures file which is run when the container first starts up, and only when it is first started. Once the container has been started and the postgres volume exists, then the fixtures file will not be run again. If you wish to change or rebuild, then you will need to remove the docker volume for the postgres service.</p> <code>POSTGRES_PASSWORD</code> This is the password for the postgres user. Changing this password requires you to change the <code>PGPASSWORD</code> in the mauro-data-mapper environment. <code>DATABASE_NAME</code> This is the name of the database which will be created for the provided username/password. This can be defined at runtime and will be the database which all the MDM data will be stored into, the database will be created when the postgres container first starts. <code>DATABASE_USERNAME</code> This is the username for the user the MDM service will use to talk to the postgres service.  This can be defined at runtime and will be created as a superuser.  <code>DATABASE_PASSWORD</code> This is the password set for the given username. This can be defined at runtime. <p>Changing the above 3 database properties will require the following environment block variables to be set for MDM to set the following 3 properties,  these will override and set the grails <code>yml</code> property defined removing the need for you to set it in the <code>build.yml</code> or <code>runtime.yml</code> files.</p> postgres property mdm property grails property <code>DATABASE_NAME</code> <code>DATABASE_NAME</code> <code>database.name</code> <code>DATABASE_USERNAME</code> <code>DATASOURCE_USERNAME</code> <code>dataSource.username</code> <code>DATABASE_PASSWORD</code> <code>DATASOURCE_PASSWORD</code> <code>dataSource.password</code>"},{"location":"installing/configuring/changing-database/#using-an-external-postgres-service","title":"Using an external postgres service","text":"<p>Information</p> <p>An example of this might be when using Amazon Web Services (AWS) to provide the database hosting solution.</p> <p>If using this option we recommend commenting out or deleting the postgres service from the <code>docker-compose.yml</code> file to avoid any confusion. You will also need to remove the following block from the MDM service, as this section tells the MDM service to wait till the postgres service is running and adds a network link between the 2 containers.</p> <pre><code>depends_on:\n- postgres\n</code></pre> <p>You will need to set the following properties in the <code>environments</code> block of the MDM service, this will inform the service where to find postgres and to wait till it can talk to it before proceeding. They will also provide the connection details required, any of the following properties set will override the stated grails yml properties (the property name in brackets), so you should not set them in the <code>build.yml</code> or <code>runtime.yml</code> file as the environment properties will override whatever is there.</p> <code>DATABASE_HOST</code> (<code>database.host</code>) This is the full hostname or IP address of the server running the postgres service. <code>DATABASE_POST</code> (<code>database.port</code>) This is the port of the server which postgres can be found at. <code>DATABASE_NAME</code> (<code>database.name</code>) This is the name of the database which all the MDM data will be stored into. <code>DATASOURCE_USERNAME</code> (<code>dataSource.username</code>) This is the username for the user the MDM service will use to talk to the postgres database. <code>DATABASE_PASSWORD</code> (<code>dataSource.password</code>) This is the password set for the given username."},{"location":"installing/configuring/mdm-core-config/","title":"MDM Core","text":""},{"location":"installing/configuring/mdm-core-config/#required-to-be-overridden","title":"Required to be overridden","text":"<p>The following variables need to be overriden or set when building or starting up a new mauro-data-mapper image.</p> <code>grails.cors.allowedOrigins</code> Should be set to a single fully qualified domain name (FQDN) URL which is the host where MDM will be accessed from. If using a proxy to break SSL then the origin would be the hostname where the proxy sits, not the host of the server running the docker containers. The origin must include the protocol, i.e. <code>https</code> or <code>http</code>.  At the same time the <code>grails.cors.allowedOriginPatterns</code> property should be overriden to <code>[]</code> to ensure the \"allow all\" option is prevented. <code>grails.cors.allowedOriginPatterns</code> MUST be overriden to <code>[]</code> to prevent \"allow all\" <code>maurodatamapper.authority.name</code> A unique name used to distinguish a running MDM instance. <code>maurodatamapper.authority.url</code> The full URL to the location of the catalogue. This is considered a unique identifier to distinguish any instance from another and therefore no 2 instances should use the same URL. <code>maurodatamapper.email.from.address</code> The email address to use when sending emails to let recipients know who sent the email.  This should be set to override the email address/username used in <code>simplejavamail.smtp.username</code>. <code>simplejavamail.smtp.username</code> To allow the catalogue to send emails this needs to be a valid username for the <code>simplejavamail.smtp.host</code>. <code>simplejavamail.smtp.password</code> To allow the catalogue to send emails this needs to be a valid password for the <code>simplejavamail.smtp.host</code> and <code>simplejavamail.smtp.username</code>. <code>simplejavamail.smtp.host</code> This is the FQDN of the mail server to use when sending emails."},{"location":"installing/configuring/mdm-core-config/#optional-overrides","title":"Optional Overrides","text":"<p>The below, along with any property found in any config file, can be overridden. We have supplied a brief description of any properties which cannot be found in the grails or spring documentation.</p> <code>database.host</code> The host of the database. If using docker-compose this should be left as <code>postgres</code> or changed to the name of the database service. <code>database.port</code> The port of the database. <code>database.name</code> The name of the database which the catalogue data will be stored in. <code>dataSource.username</code> Username to use to connect to the database. See the Postgres service environment variables for more information. <code>dataSource.password</code> Password to use to connect to the database. See the Postgres service environment variables for more information. <code>simplejavamail.smtp.port</code> The port to use when sending emails. <code>simplejavamail.smtp.transportstrategy</code> The transport strategy to use when sending emails. <code>hibernate.search.default.indexBase</code> The directory to store the lucene index files in."},{"location":"installing/configuring/overview/","title":"Overview","text":"<p>There are two preferred methods of overriding the defaults for configuring a MDM instance using <code>docker-compose.yml</code>.</p> <p>Warning</p> <p>This replaces all of the previous releases environment variables setting in <code>docker-compose.yml</code>.</p> <p>Information</p> <p>Each plugin may supply some mandatory fields which need to be overridden. Please see each section to identify which fields these may be.  Grails and Spring supply ample documentation for all their standard properties which can all also be overridden via the build or runtime files.  Our documentation is intended to supply information for our own plugins or properties which we require to be overridden.</p> <p>The preference order (by MDM) for loaded sources of properties is:</p> <ol> <li>Environment Variables.</li> <li><code>runtime.yml</code> - See the default included here.</li> <li><code>build.yml</code> - See the default included here.</li> <li><code>application.yml</code> - See the default included here.</li> <li><code>plugin.yml</code> - there are multiple versions of these as each plugin we build may supply their own.</li> </ol>"},{"location":"installing/configuring/overview/#environment-variables","title":"Environment Variables","text":"<p>Any grails configuration property found in any of the <code>yml</code> files can be overridden through environment variables. They simply need to be provided in the \"dot notation\" form or (more appropriately) the \"uppercase underscore separated\" form rather than the \"YML new line\" format.</p> <p>e.g. application.yml</p> <pre><code>database:\nhost: localhost\n</code></pre> <p>would be overridden by docker-compose.yml</p> <pre><code>services:\nmauro-data-mapper:\nenvironment:\ndatabase.host: another-host\n</code></pre> <p>or</p> <pre><code>services:\nmauro-data-mapper:\nenvironment:\nDATABASE_HOST: another-host\n</code></pre>"},{"location":"installing/configuring/overview/#buildyml-file","title":"build.yml File","text":"<p>The <code>build.yml</code> file is built into the MDM service when the image is built and is a standard grails configuration file. Therefore, any properties which can be safely set at build time for the image should be set into this file. This includes any properties which may be shared between multiple instances of MDM which all start from the same image.</p> <p>Our recommendation is that if only running 1 instance of MDM from 1 cloned repository then you should load all your properties into the <code>build.yml</code> file. For this reason, we have supplied the <code>build.yml</code> file with all the properties which we either require to be overridden or expect may want to be overridden.</p>"},{"location":"installing/configuring/overview/#runtimeyml-file","title":"runtime.yml File","text":"<p>The <code>runtime.yml</code> file will be loaded into the container via the <code>docker-compose.yml</code> file. This is intended as the replacement for environment variable overrides, where each running container might have specifically set properties which differ from a common shared image.</p> <p>Danger</p> <p>Do not change the environment variable <code>runtime.config.path</code>, as this denotes the path inside the container where the config file will be found. If you wish to load a different <code>runtime.yml</code> file then you should alter the <code>volumes</code> mapping in the <code>docker-compose.yml</code> file.</p>"},{"location":"installing/configuring/overview/#defining-a-different-location-for-the-runtimeyml-to-load","title":"Defining a different location for the runtime.yml to load","text":"<p>If you wish to store your <code>runtime.yml</code> file in an alternative location to the folder inside the mdm-docker cloned repository,  then you will need to alter the <code>volumes</code> mapping in the <code>docker-compose.yml</code> file.  The line to change is:</p> <p><code>- ./mauro-data-mapper/config/runtime.yml:/usr/local/tomcat/conf/runtime.yml</code></p> <p>The first part of this is the local location (outside docker container) where the yml file can be found, you should not alter the second half (after the <code>:</code>) as this is where the yml file is mounted into the container and where MDM expects to find it.</p>"},{"location":"installing/configuring/runtime.yml/","title":"Runtime YML","text":"<p>Below is the default <code>application.yml</code> file applied to the docker container at runtime.</p> <pre><code># Nothing overridden by default at runtime\n</code></pre>"},{"location":"model/glossary/","title":"Glossary","text":"<p>The Metadata Catalogue supports a variety of concepts which are not new, but are frequently referred to in different tools or applications by other names, or with different semantics.  In this article we give definitions to the terms we use, and describe their intended usage.  This page may also serve as a lightweight guide to the underlying data model - the UML diagrams provide guidance to those wishing to understand the underlying data structures and relationships.</p>"},{"location":"model/glossary/#data-models","title":"Data Models","text":"<p>A Data Model is a description of an existing collection of data, or the specification of data that is to be collected.  We use the same notation for both types, so that they may easily be compared or linked, and we annotate the Data Model to describe them as either a Data Asset  or a Data Standard respectively.  A Data Model has a label, which is used to uniquely identify it within the catalogue.  It may also have a description, a number of alternate names (aliases), an author, and an organisation.  </p> <p>A Data Model contains a number of Data Classes: groupings or collections of data points that share some common context: for example appearing in the same table of a database, or the same section in a form.  A data class has a name, a description, some aliases, and may contain further  (sub-) data classes.  It has a maximum and minimum multiplicity, determining how many times data in this class may appear.  For example, optional data may have a minimum multiplicity of 0 and a maximum multiplicity of 1; mandatory data may have a minimum multiplicity of 1; data which may occur any number of times is given a multiplicity of '*' (represented as -1 internally). The name of a data class is unique within its context (parent Data Model, or Data Class).</p> <p>A data class contains a number of Data Elements: the description of an individual field, variable, column, or property.  A data element has a label, which must be unique within its containing Data Class.  It has a description, some alternate names, and a multiplicity.  The range of possible values that it may take are described within its data type: a reference to a Data Type stored within this model.</p> <p></p> <p>A Data Type may be one of three sorts:</p> <ul> <li>a Primitive Type such as a String, a Date or an Integer.</li> <li>an Enumerated Type: a constrained set of values such as you might see for a gender or an ethnicity.  Each Enumeration Type defines a number  of Enumeration Values: a coded key, and a human-readable value.</li> <li>a Reference Type referring to another class within the same model - used to describe relationships between Data Classes within a model.  </li> </ul>"},{"location":"model/glossary/#terminologies","title":"Terminologies","text":"<p>A Terminology can represent a complex ontology, a structured collection of enumerated values, or something inbetween.  A terminology is stored in the catalogue in a similar manner to a Data Model: it has a unique label, a description, an author, an organisation, and some alternate names.  A Terminology contains a number of Terms which themselves have a code, a human-readable definition, a URL which can be used to point to a definitive definition, and a description, which may be more verbose than the stated definition.  </p> <p>Pairs of terms inside a terminology may be related: within the Terminology we store a number of Term Relationships - each of which has a source and target term.  The Term Relationship is annotated with the Relationship Type - one of a set of types which are defined separately for each terminology.</p>"},{"location":"model/glossary/#codesets","title":"Codesets","text":"<p>A CodeSet is a named collection of terms which may be pulled from multiple terminologies.  It has similar fields to a Data Model: A label (which must be unique within the system), a description, an author, organisation, and alternate names.</p>"},{"location":"model/glossary/#folders","title":"Folders","text":"<p>A Folder is the principle mechanism for classifying and organising structures within the Metadata Catalogue.  Folders may contain sub-folders,  and each folder may contain Data Models, Terminologies or Code Sets.   A folder has a label, which must be unique within its container (parent  folder, or within the top-level of the folder hierarchy).   </p>"},{"location":"model/glossary/#data-flows","title":"Data Flows","text":"<p>Currently undocumented</p>"},{"location":"model/glossary/#additional-features","title":"Additional features","text":"<p>All items in the Metadata Catalogue conform to a simple notion of Catalogue Item which has additional properties.  These are outlined in the following subsections.</p> <p></p>"},{"location":"model/glossary/#metadata-properties","title":"Metadata / Properties","text":"<p>Each Catalogue Item holds an extensible list of Properties (historically called Metadata), which can store arbitrary additional information  about the Catalogue Item.  Each property is identified by a namespace, and a key, and stored a value.  </p>"},{"location":"model/glossary/#attachments","title":"Attachments","text":"<p>Each Catalogue Item can also hold Attachments: files relating to the item in question - potentially providing additional documentation or  context.  Each attachment has a title, a description, and the file itself.       </p>"},{"location":"model/glossary/#comments","title":"Comments","text":"<p>Any Catalogue Item may have Comments attached.  Each comment has a title and some text, and these comments may themselves have further comments (replies in the sense of a forum or messageboard).  </p>"},{"location":"model/glossary/#classifiers","title":"Classifiers","text":"<p>Classifiers provide a means for further classifying or tagging items within the catalogue.  Classifiers contain a label and a description, and may hold further sub-classifiers.  These classifiers may be managed separately by users, having their own read/write permissions.   </p>"},{"location":"model/glossary/#semantic-links","title":"Semantic Links","text":"<p>Currently undocumented</p>"},{"location":"model/glossary/#summary-metadata","title":"Summary Metadata","text":"<p>Currently undocumented</p>"},{"location":"plugins/cli/remote-database-importers/","title":"Command-line database importers","text":"<p>As well as providing Mauro endpoints for importing from remote databases, the database import plugins can be used as standalone command-line interface (CLI) tools to import to a remote Mauro instance from a database.</p> <p>Each database-specific import plugin provides a command line tool for Unix-like and Windows platforms that can import from a remote database. The command line parameters are the same for the different tools, and they each read a configuration file with some common parameters, and some database-specific parameters.</p>"},{"location":"plugins/cli/remote-database-importers/#command-line-usage","title":"Command-line usage","text":"<p>The provided command line tools are:</p> Database plugin CLI tool name mdm-plugin-database-mysql <code>mysql-remote-database-importer</code> mdm-plugin-database-oracle <code>oracle-remote-database-importer</code> mdm-plugin-database-postgresql <code>postgres-remote-database-importer</code> mdm-plugin-database-sqlserver <code>sqlserver-remote-database-importer</code> <p>Usage, replacing <code>remote-database-importer</code> with a CLI tool named above:</p> <p>remote-database-importer -c &lt;FILE&gt; -u &lt;USERNAME&gt; -p &lt;PASSWORD&gt; -w &lt;DATABASE_PASSWORD&gt;</p> <p>Import database to Mauro Data Mapper Connect to a database, import to a DataModel and push to the Mauro server</p> <code>-c</code>, <code>--config</code> &lt;FILE&gt; Config file defining the import configuration (required) <p><code>-h</code>, <code>--help</code></p> <code>-p</code>, <code>--password</code> &lt;PASSWORD&gt; Password for Mauro Data Mapper instance (required) <code>-u</code>, <code>--username</code> &lt;USERNAME&gt; Username for Mauro Data Mapper instance (required) <p><code>-v</code>, <code>--version</code></p> <code>-w</code>, <code>--databasePassword</code> &lt;DATABASE_PASSWORD&gt; Password for the database to import (required)"},{"location":"plugins/cli/remote-database-importers/#configuration-parameters","title":"Configuration parameters","text":"<p>The configuration file (params <code>-c</code>, <code>--config</code>) uses the Java properties format (an example is below) and all the importers take the following parameters in this file:</p> Parameter Usage <code>import.database.name</code> Database name to import <code>import.database.names</code> Database names to import (comma separated list, if importing multiple databases) <code>import.database.host</code> Database hostname <code>import.database.username</code> Database username <code>import.database.ssl</code> Enable SSL for database connection (<code>true</code> or <code>false</code>) <code>import.database.port</code> Database port (if not using default) <code>export.server.url</code> URL of remote Mauro instance to import to <code>export.folder.path</code> Folder path to export to <code>export.dataModel.name</code> Data model name to export (optional) <code>export.dataModel.finalised</code> Export data model as finalised (<code>true</code> or <code>false</code>, default is <code>true</code>) <p>Some of the importers also take database-specific parameters:</p>"},{"location":"plugins/cli/remote-database-importers/#oracle-remote-database-importer-configuration-parameters","title":"<code>oracle-remote-database-importer</code> configuration parameters","text":"Parameter Usage import.database.owner Database owner to import from"},{"location":"plugins/cli/remote-database-importers/#postgres-remote-database-importer-configuration-parameters","title":"<code>postgres-remote-database-importer</code> configuration parameters","text":"Parameter Usage import.database.schemas Database schemas to import from (comma separated list, optional)"},{"location":"plugins/cli/remote-database-importers/#sqlserver-remote-database-importer-configuration-parameters","title":"<code>sqlserver-remote-database-importer</code> configuration parameters","text":"Parameter Usage import.database.schemas Database schemas to import from (comma separated list, optional)"},{"location":"plugins/cli/remote-database-importers/#example-usage","title":"Example usage","text":"<p>An example of invoking <code>postgres-remote-database-importer</code>:</p> <pre><code>./postgres-remote-database-importer -c config.properties -u admin@maurodatamapper.com -p password -w MauroDataMapper1234\n</code></pre> <p>Where <code>config.properties</code> contains:</p> <pre><code>import.database.name=metadata_simple\nimport.database.host=localhost\nimport.database.username=maurodatamapper\nimport.database.ssl=false\nexport.server.url=http://localhost:8080\nexport.folder.path=8b5523d9-dfd2-4185-a49c-51b9e2f4c515\nexport.dataModel.name=Model from Postgres DB\n</code></pre> <p>A DataModel named 'Model from Postgres DB' is imported from the database 'metadata_simple' on the Postgres database at <code>localhost</code> (on default port 5432), to the Mauro instance at <code>http://localhost:8080</code>, and is placed in the top level folder with ID <code>8b5523d9-dfd2-4185-a49c-51b9e2f4c515</code>.</p>"},{"location":"plugins/cli/remote-database-importers/#building-command-line-tools","title":"Building command-line tools","text":"<p>To build a command line tool from one of the database importer plugin projects, run <code>gradle distTar</code> to build a Unix distribution or <code>gradle distZip</code> to build a Windows distribution. The outputs are found in the <code>build/distributions</code> directory. Extract the output and run the script inside the <code>bin</code> directory.</p>"},{"location":"plugins/configuring/doi/","title":"Digital Object Identifiers","text":"<p>The following properties can be defined in the config yaml files or set using the RESTful API or the web UI.</p> <p>Information</p> <p>Once MDM has started up the only way to change the properties is via the RESTFful API/Web UI. See the DOI Userguide for more information.</p> <p>Warning</p> <p>The following property must also be set via the RESTful API or the web UI for the DOI plugin to work: <code>site.url</code>.</p>"},{"location":"plugins/configuring/doi/#available-properties","title":"Available Properties","text":"<code>maurodatamapper.digitalobjectidentifiers.username</code> The username used to authenticate with when communicating with the DOI registry. <code>maurodatamapper.digitalobjectidentifiers.password</code> The password used to authenticate with when communicating with the DOI registry. <code>maurodatamapper.digitalobjectidentifiers.endpoint</code> The full HTTP web address for the DOI reigstry. e.g. <code>https://api.test.datacite.org</code>. <code>maurodatamapper.digitalobjectidentifiers.prefix</code> The assigned DOI prefix to be used for all registered resources. e.g. <code>10.80079</code>."},{"location":"plugins/configuring/fhir/","title":"FHIR Importer","text":"<p>The configurable YAML properties for the FHIR plugin are for the handling of the micronaut connections between the FHIR server and MDM. Defaults are provided via the <code>plugin.yml</code> and it is unlikely you need to override them. These are micronaut properties and as such please see the  micronaut documentation to understand what they are and how they affect the system.</p>"},{"location":"plugins/configuring/fhir/#defaults-provided-in-the-pluginyml","title":"Defaults provided in the plugin.yml","text":"<pre><code>micronaut:\ncodec:\njson:\nadditional-types:\n- 'application/json+fhir;charset=utf-8'\nhttp:\nclient:\nmax-content-length: 33554430\nread-timeout: PT30S\n</code></pre>"},{"location":"plugins/configuring/oic/","title":"OpenID Connect","text":"<p>OpenID Connect has a range of properties which can be set in the yaml files. Some of these take effect each time you start MDM, some are only used once and if they've been enabled before then they will be ignored on subsequent startups. We supply defaults to ensure basic functionality however these can be overridden using the yaml files.</p> <p>We store the basic information to bootstrap certain providers and set them up without any interaction through the admin interface/RESTful API. The providers are currently</p> <ul> <li>Google</li> <li>Microsoft (via microsoftonline)</li> <li>KeyCloak</li> </ul>"},{"location":"plugins/configuring/oic/#control-properties","title":"Control Properties","text":"<code>maurodatamapper.openidConnect.session.timeout</code> This is the length of time which sessions will be kept active before being timed out, the default is 24 hours this overrides the usual session timeout of 30 minutes which  is set for all non-OIC authenticated users. We do not recommend setting this lower than <code>24h</code> as this is the timeout after which users will be required to log back in via  the OIC window. Inside this timeout the API will keep the session alive connecting to the OIC provider as necessary to refresh the user's token."},{"location":"plugins/configuring/oic/#bootstrapped-providers","title":"Bootstrapped Providers","text":"<p>Each of the providers has pre-configured defaults which are used to add them automatically if they are enabled. These defaults (provided in each section for reference only) can be changed once the system has started up by using the admin interface/RESTful API, see the OIC Userguide for more information.</p>"},{"location":"plugins/configuring/oic/#google","title":"Google","text":"<code>maurodatamapper.openidConnect.google.enabled</code> Defaults to false. If enabled then the other google properties will need to be provided. <code>maurodatamapper.openidConnect.google.clientId</code> The client id used to identify and authenticate the MDM service. <code>maurodatamapper.openidConnect.google.clientSecret</code> The client id used to identify and authenticate the MDM service. <ul> <li><code>discoveryDocumentUrl</code>: https://accounts.google.com/.well-known/openid-configuration</li> <li><code>imageUrl</code>: https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg</li> </ul>"},{"location":"plugins/configuring/oic/#microsoft","title":"Microsoft","text":"<p>Information</p> <p>Post-processing occurs with Microsoft to replace the <code>{tenant}</code> in the <code>issuerUrl</code> provided by the discoveryDocumentUrl with the <code>clientId</code>. If manually configuring a Microsoft provider you may have to manually edit the discovery document via the admin interface to perform the same. </p> <code>maurodatamapper.openidConnect.microsoft.enabled</code> Defaults to false. If enabled then the other google properties will need to be provided. <code>maurodatamapper.openidConnect.microsoft.tenantId</code> The Directory (tenant) id assigned to the Azure AD <code>maurodatamapper.openidConnect.microsoft.accountId</code> The account id or type to use for this app. This is either going to be something like the tenant id, \"organizations\" or \"common\",  it will be decided by the Authentication -&gt; Supported Account Types. It will be used to build the discovery document URL.  If unsure what to use, click the \"Endpoints\" button in the Azure overview and examine the \"OpenID Connect metadata document\" endpoint, this field should be whatever is in the URL between <code>login.microsoftonline.com/</code> and <code>/v2.0</code>. <code>maurodatamapper.openidConnect.microsoft.clientId</code> The client id used to identify and authenticate the MDM service. <code>maurodatamapper.openidConnect.microsoft.clientSecret</code> The client id used to identify and authenticate the MDM service. <ul> <li><code>discoveryDocumentUrl</code>: https://login.microsoftonline.com/${accountId}/v2.0/.well-known/openid-configuration</li> <li><code>imageUrl</code>: https://upload.wikimedia.org/wikipedia/commons/9/98/Microsoft_logo.jpg</li> </ul>"},{"location":"plugins/configuring/oic/#keycloak","title":"KeyCloak","text":"<p>Information</p> <p>The <code>baseUrl</code> and <code>realm</code> fields are used to build the discoveryDocumentUrl for the boostrapped provider.  If creating a Keycloak provider via the Admin interface/RESTful API these properties are not needed as you can define the full URL to the discovery document</p> <code>maurodatamapper.openidConnect.keycloak.enabled</code> Defaults to false. If enabled then the other google properties will need to be provided. <code>maurodatamapper.openidConnect.keycloak.baseUrl</code> The full URL where the keycloak provider can be found. <code>maurodatamapper.openidConnect.keycloak.realm</code> The realm configured inside the keycloak provider for the MDM service to use. <code>maurodatamapper.openidConnect.keycloak.clientId</code> The client id used to identify and authenticate the MDM service. <code>maurodatamapper.openidConnect.keycloak.clientSecret</code> The client id used to identify and authenticate the MDM service. <ul> <li><code>discoveryDocumentUrl</code>: <code>${baseUrl}/realms/${realm}/.well-known/openid-configuration</code></li> <li><code>imageUrl</code>: https://upload.wikimedia.org/wikipedia/commons/2/29/Keycloak_Logo.png</li> </ul>"},{"location":"plugins/configuring/oic/#defaults-provided-in-the-pluginyml","title":"Defaults provided in the plugin.yml","text":"<pre><code>maurodatamapper:\nopenidConnect:\nsession:\ntimeout: 24h\ngoogle:\nenabled: false\nmicrosoft:\nenabled: false\ntenant: common\nkeycloak:\nenabled: false\n</code></pre>"},{"location":"plugins/rest-api/doi/","title":"Digital Object Identifier (DOI)","text":""},{"location":"plugins/rest-api/doi/#introduction","title":"Introduction","text":"<p>The Digital Object Identifier (DOI) plugin provides additional API endpoints that allow the integration of any Mauro multi-facet aware catalogue item to be recorded in a DOI system; this then allows unique, permanent identifiers to those catalogue items that can then be distributed across documents or articles over the web to act as links back to the Mauro content. The DOI system always ensures that a DOI name will always refer to a Mauro catalogue item as long as the DOI name is known. The plugin uses DataCite as the repository of Mauro DOI names; more information can be found at the DataCite website.</p> <p>In order to use Digital Object Identifiers, the Mauro instance must:</p> <ol> <li>Have the Mauro Digital Object Identifier plugin installed.</li> <li>Set up and configure DataCite to acts as the DOI system.</li> </ol> <p>The full details of this set up can be viewed in the user guide Using Digital Object Identifiers.</p>"},{"location":"plugins/rest-api/doi/#profile","title":"Profile","text":""},{"location":"plugins/rest-api/doi/#profile-summary","title":"Profile summary","text":"<p>The plugin automatically exposes a profile called Digital Object Identifiers DataCite Dataset Schema. Depending on whether the profile is used or unused on a catalogue item, the summary of the profile can be found with one of these endpoints:</p> <p>/api/{multiFacetAwareDomainType}/{id}/profiles/used</p> <p>/api/{multiFacetAwareDomainType}/{id}/profiles/unused</p> Response body (JSON) <pre><code>[\n{\n\"name\": \"DigitalObjectIdentifiersProfileProviderService\",\n\"version\": \"1.1.0\",\n\"displayName\": \"Digital Object Identifiers DataCite Dataset Schema\",\n\"namespace\": \"uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile\",\n\"allowsExtraMetadataKeys\": false,\n\"knownMetadataKeys\": [\n\"identifier\",\n\"prefix\",\n\"suffix\",\n\"status\",\n\"state\",\n\"titles/mainTitle\",\n\"descriptions/mainDescription\",\n\"version\",\n\"creators/creator/creatorName\",\n\"creators/creator/creatorNameType\",\n\"creators/creator/givenName\",\n\"creators/creator/familyName\",\n\"creators/creator/nameIdentifier\",\n\"creators/creator/affiliation\",\n\"publisher\",\n\"publicationYear\",\n\"resourceType\",\n\"titles/title\",\n\"titles/titleType\",\n\"descriptions/description\",\n\"descriptions/descriptionType\",\n\"contributors/contributor/contributorName\",\n\"contributors/contributor/contributorNameType\",\n\"contributors/contributor/contributorType\",\n\"contributors/contributor/givenName\",\n\"contributors/contributor/familyName\",\n\"contributors/contributor/nameIdentifier\",\n\"contributors/contributor/affiliation\",\n\"language\"\n],\n\"providerType\": \"Profile\",\n\"metadataNamespace\": \"org.datacite\",\n\"domains\": [\n\"VersionedFolder\",\n\"Folder\",\n\"Classifier\",\n\"ReferenceType\",\n\"PrimitiveType\",\n\"ModelDataType\",\n\"EnumerationType\",\n\"EnumerationValue\",\n\"DataElement\",\n\"DataClass\",\n\"DataModel\",\n\"DataFlow\",\n\"DataElementComponent\",\n\"DataClassComponent\",\n\"Terminology\",\n\"TermRelationshipType\",\n\"TermRelationship\",\n\"Term\",\n\"CodeSet\",\n\"ReferenceDataModel\",\n\"ReferenceDataElement\",\n\"ReferencePrimitiveType\",\n\"ReferenceEnumerationType\",\n\"ReferenceEnumerationValue\"\n],\n\"editableAfterFinalisation\": true\n}\n]\n</code></pre>"},{"location":"plugins/rest-api/doi/#get-the-profile","title":"Get the profile","text":"<p>To get the profile, use the usual profiles endpoint:</p> <p>/api/{multiFacetAwareDomainType}/{id}/profiles/{namespace}/{name}</p> <p>For the case of the DOI profile:</p> <ul> <li>The {namespace} will be <code>uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile</code>.</li> <li>The {name} will be <code>DigitalObjectIdentifiersProfileProviderService</code>.</li> </ul> <p>Information</p> <p>The <code>GET</code> endpoint will always return the profile data, even if the profile has not yet been assigned to the catalogue item. In the case where it has not been assigned yet, an empty profile structure will be returned containing all the fields to prepare.</p> Response body (JSON) <pre><code>{\n\"sections\": [\n{\n\"name\": \"Predefined/Supplied Fields\",\n\"description\": \"Fixed fields which cannot be changed.\",\n\"fields\": [\n{\n\"fieldName\": \"Identifier\",\n\"metadataPropertyName\": \"identifier\",\n\"description\": \"A persistent identifier that identifies a resource. This will be filled in by the API upon submission\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": true,\n\"currentValue\": \"10.80079/ynk3-sz81\"\n},\n{\n\"fieldName\": \"Prefix\",\n\"metadataPropertyName\": \"prefix\",\n\"description\": \"DOI prefix. The first part of the identifier\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": true,\n\"currentValue\": \"10.80079\"\n},\n{\n\"fieldName\": \"Suffix\",\n\"metadataPropertyName\": \"suffix\",\n\"description\": \"DOI suffix. The last part of the identifier\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": true,\n\"currentValue\": \"ynk3-sz81\"\n},\n{\n\"fieldName\": \"Status\",\n\"metadataPropertyName\": \"status\",\n\"description\": \"Status of DOI: draft, final, retired, not submitted.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": true,\n\"currentValue\": \"retired\"\n},\n{\n\"fieldName\": \"State\",\n\"metadataPropertyName\": \"state\",\n\"description\": \"State of DOI inside DataCite: draft, findable, registered (Registered indicates a retired DOI).\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": true,\n\"currentValue\": \"registered\"\n},\n{\n\"fieldName\": \"Main Title\",\n\"metadataPropertyName\": \"titles/mainTitle\",\n\"description\": \"The main title by which the resource is known, derived from the label field of the resource.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"text\",\n\"derived\": true,\n\"derivedFrom\": \"label\",\n\"uneditable\": true,\n\"currentValue\": \"DOI Test Model 5\"\n},\n{\n\"fieldName\": \"Main Description\",\n\"metadataPropertyName\": \"descriptions/mainDescription\",\n\"description\": \"The main description for the resource, derived from the description field of the resource.\",\n\"maxMultiplicity\": -1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"text\",\n\"derived\": true,\n\"derivedFrom\": \"description\",\n\"uneditable\": true,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Version\",\n\"metadataPropertyName\": \"version\",\n\"description\": \"Version number of the resource. If the primary resource has changed the version number increases. Register a new identifier for a major version change. Individual stewards need to determine which are major vs. minor versions. May be used in conjunction with properties 11 and 12 (AlternateIdentifier and RelatedIdentifier) to indicate various information updates. May be used in conjunction with property 17 (Description) to indicate the nature and file/record range of version.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": true,\n\"derivedFrom\": \"modelVersion\",\n\"uneditable\": true,\n\"currentValue\": \"1.0.0\"\n}\n]\n},\n{\n\"name\": \"Primary Creator\",\n\"description\": \"Resource Creator. This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\",\n\"fields\": [\n{\n\"fieldName\": \"Name\",\n\"metadataPropertyName\": \"creators/creator/creatorName\",\n\"description\": \"The main researchers involved working on the data, or the authors of the publication in priority order. May be a corporate/institutional or personal name.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"Peter Monks\"\n},\n{\n\"fieldName\": \"Name Type\",\n\"metadataPropertyName\": \"creators/creator/creatorNameType\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": [\n\"Organizational\",\n\"Personal\"\n],\n\"regularExpression\": null,\n\"dataType\": \"enumeration\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"Personal\"\n},\n{\n\"fieldName\": \"Given Name\",\n\"metadataPropertyName\": \"creators/creator/givenName\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Family Name\",\n\"metadataPropertyName\": \"creators/creator/familyName\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Identifier\",\n\"metadataPropertyName\": \"creators/creator/nameIdentifier\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Affiliation\",\n\"metadataPropertyName\": \"creators/creator/affiliation\",\n\"description\": \"Affiliation of creator, company or institution they represent\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n}\n]\n},\n{\n\"name\": \"Additional Mandatory Fields\",\n\"description\": null,\n\"fields\": [\n{\n\"fieldName\": \"Publisher\",\n\"metadataPropertyName\": \"publisher\",\n\"description\": \"The name of the entity that holds, archives, publishes prints, distributes, releases, issues, or produces the resource. This property will be used to formulate the citation, so consider the prominence of the role. For software, use Publisher for the code repository. If there is an entity other than a code repository, that 'holds, archives, publishes, prints, distributes, releases, issues, or produces' the code, use the property Contributor/contributorType/hostingInstitution for the code repository.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"OCC\"\n},\n{\n\"fieldName\": \"Publication Year\",\n\"metadataPropertyName\": \"publicationYear\",\n\"description\": \"The year when the data was or will be made publicly available. If an embargo period has been in effect, use the date when the embargo period ends.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": null,\n\"regularExpression\": \"\\\\d{4}\",\n\"dataType\": \"int\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"2021\"\n},\n{\n\"fieldName\": \"Resource Type\",\n\"metadataPropertyName\": \"resourceType\",\n\"description\": \"The type of a resource\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 1,\n\"allowedValues\": [\n\"Audiovisual\",\n\"Book\",\n\"BookChapter\",\n\"Collection\",\n\"ComputationalNotebook\",\n\"ConferencePaper\",\n\"ConferenceProceeding\",\n\"DataPaper\",\n\"Dataset\",\n\"Dissertation\",\n\"Event\",\n\"Image\",\n\"InteractiveResource\",\n\"Journal\",\n\"JournalArticle\",\n\"Model\",\n\"OutputManagementPlan\",\n\"PeerReview\",\n\"PhysicalObject\",\n\"Preprint\",\n\"Report\",\n\"Service\",\n\"Software\",\n\"Sound\",\n\"Standard\",\n\"Text\",\n\"Workflow\",\n\"Other\"\n],\n\"regularExpression\": null,\n\"dataType\": \"enumeration\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"Collection\"\n}\n]\n},\n{\n\"name\": \"Additional Optional Title Section\",\n\"description\": \"This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\",\n\"fields\": [\n{\n\"fieldName\": \"Title\",\n\"metadataPropertyName\": \"titles/title\",\n\"description\": \"A name or title by which a resource is known.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Title Type\",\n\"metadataPropertyName\": \"titles/titleType\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": [\n\"AlternativeTitle\",\n\"Subtitle\",\n\"TranslatedTitle\",\n\"Other\"\n],\n\"regularExpression\": null,\n\"dataType\": \"enumeration\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n}\n]\n},\n{\n\"name\": \"Additional Optional Description Section\",\n\"description\": \"This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\",\n\"fields\": [\n{\n\"fieldName\": \"Description\",\n\"metadataPropertyName\": \"descriptions/description\",\n\"description\": \"All additional information that does not fit in any of the other categories. May be used for technical information.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"text\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Description Type\",\n\"metadataPropertyName\": \"descriptions/descriptionType\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": [\n\"Abstract\",\n\"Methods\",\n\"SeriesInformation\",\n\"TableOfContents\",\n\"TechnicalInfo\",\n\"Other\"\n],\n\"regularExpression\": null,\n\"dataType\": \"enumeration\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n}\n]\n},\n{\n\"name\": \"Primary Contributor\",\n\"description\": \"Resource Contributor. This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\",\n\"fields\": [\n{\n\"fieldName\": \"Name\",\n\"metadataPropertyName\": \"contributors/contributor/contributorName\",\n\"description\": \"The institution or person responsible for collecting, creating, or otherwise contributing to the development of the dataset.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Name Type\",\n\"metadataPropertyName\": \"contributors/contributor/contributorNameType\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": [\n\"Organizational\",\n\"Personal\"\n],\n\"regularExpression\": null,\n\"dataType\": \"enumeration\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Contributor Type\",\n\"metadataPropertyName\": \"contributors/contributor/contributorType\",\n\"description\": \"Mandatory if the contributor name is provided.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": [\n\"ContactPerson\",\n\"DataCollector\",\n\"DataCurator\",\n\"DataManager\",\n\"Distributor\",\n\"Editor\",\n\"HostingInstitution\",\n\"Other\",\n\"Producer\",\n\"ProjectLeader\",\n\"ProjectManager\",\n\"ProjectMember\",\n\"RegistrationAgency\",\n\"RegistrationAuthority\",\n\"RelatedPerson\",\n\"ResearchGroup\",\n\"RightsHolder\",\n\"Researcher\",\n\"Sponsor\",\n\"Supervisor\",\n\"WorkPackageLeader\"\n],\n\"regularExpression\": null,\n\"dataType\": \"enumeration\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Given Name\",\n\"metadataPropertyName\": \"contributors/contributor/givenName\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Family Name\",\n\"metadataPropertyName\": \"contributors/contributor/familyName\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Identifier\",\n\"metadataPropertyName\": \"contributors/contributor/nameIdentifier\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n},\n{\n\"fieldName\": \"Affiliation\",\n\"metadataPropertyName\": \"contributors/contributor/affiliation\",\n\"description\": null,\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n}\n]\n},\n{\n\"name\": \"Additional Optional Fields\",\n\"description\": \"Optional metadata fields for DOI profiles.\",\n\"fields\": [\n{\n\"fieldName\": \"Language\",\n\"metadataPropertyName\": \"language\",\n\"description\": \"Primary language of the resource. Allowed values are taken from  IETF BCP 47, ISO 639-1 language codes. For English, use 'en'.\",\n\"maxMultiplicity\": 1,\n\"minMultiplicity\": 0,\n\"allowedValues\": null,\n\"regularExpression\": null,\n\"dataType\": \"string\",\n\"derived\": false,\n\"derivedFrom\": null,\n\"uneditable\": false,\n\"currentValue\": \"\"\n}\n]\n}\n],\n\"id\": \"36e74acc-a862-43d0-8e2b-e54422c4239f\",\n\"label\": \"DOI Test Model 5\",\n\"domainType\": \"DataModel\"\n}\n</code></pre>"},{"location":"plugins/rest-api/doi/#save-the-profile","title":"Save the profile","text":"<p>Using the JSON structure from the <code>GET</code> endpoint above and entering values into the <code>sections/fields/currentValue</code> JSON properties, you can save the changes made to the profile, and attach the profile to the catalogue item at the same time, like so.</p> <p>/api/{multiFacetAwareDomainType}/{id}/profiles/{namespace}/{name}</p> <p>Where:</p> <ul> <li>The {namespace} will be <code>uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile</code>.</li> <li>The {name} will be <code>DigitalObjectIdentifiersProfileProviderService</code>.</li> </ul> <p>If successful, this will return the same JSON response as the <code>GET</code> endpoint above.</p>"},{"location":"plugins/rest-api/doi/#remove-the-profile","title":"Remove the profile","text":"<p>To remove the DOI profile from the catalogue item:</p> <p>/api/{multiFacetAwareDomainType}/{id}/profiles/{namespace}/{name}</p> <p>Where:</p> <ul> <li>The {namespace} will be <code>uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile</code>.</li> <li>The {name} will be <code>DigitalObjectIdentifiersProfileProviderService</code>.</li> </ul>"},{"location":"plugins/rest-api/doi/#doi-management","title":"DOI Management","text":"<p>The profile endpoints above are standard endpoints used in Mauro. This section covers the custom endpoints added by the DOI plugin.</p>"},{"location":"plugins/rest-api/doi/#get-doi-status","title":"Get DOI status","text":"<p>For any catalogue item, the current status of the DOI name on that item can be queried as follows:</p> <p>/api/{multiFacetAwareDomainType}/{id}/doi</p> <p>If a status exists, a <code>200 OK</code> response will be returned with this response body:</p> Response body (JSON) <pre><code>{\n\"identifier\": \"10.1109/5.771073\",\n\"status\": \"final\"\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>identifier (String): The Digital Object Identifier (DOI) name assigned to this catalogue item as generated by the DOI system. If a DOI has not been generated yet, this will not be provided.</li> <li>state (Constant): The current DOI state of this catalogue item, which can one of:<ul> <li>not submitted - no DOI has been requested yet.</li> <li>draft - a draft DOI has been assigned, allowing further changes to the profile.</li> <li>final - a DOI has been assigned and locked, finalising this catalogue item's profile.</li> <li>retired - a previous DOI has been retired and can no longer be used for cross-reference.</li> </ul> </li> </ul>"},{"location":"plugins/rest-api/doi/#submit-doi-status","title":"Submit DOI status","text":"<p>Submitting the catalogue item and its DOI profile can only be done for publicly readable and finalised catalogue items. This is to ensure that:</p> <ol> <li>Every citation has access to the catalogue item.</li> <li>The catalogue item cannot be modified any further, keeping the citation the same for all future uses.</li> </ol> <p>Information</p> <p>It may be possible to not have assigned the Digital Object Identifiers DataCite Dataset Schema to a catalogue item before it was finalised. To solve this issue, the DOI profile has special permissions to allow it to be edited post-finalisation, so long as the DOI is not yet in the Final state.</p> <p>Therefore, you may use the <code>POST /api/{multiFacetAwareDomainType}/{id}/profiles/{namespace}/{name}</code> endpoint as many times as necessary to make alterations to the profile before continuing further.</p> <p>To submit the catalogue item's profile to the DOI system to obtain a DOI name, use this endpoint.</p> <p>/api/{multiFacetAwareDomainType}/{id}/doi?submissionType={type}</p> <p>This endpoint requires no request body; the profile should already have been saved before this point. The submissionType can be one of the following:</p> <ul> <li>draft - submit a draft profile to obtain the DOI name.</li> <li>finalise - submit a final profile to obtain the DOI name.</li> <li>retire - retire an existing DOI name to no longer be in active use.</li> </ul> <p>The response returned will be the contents of the catalogue item the DOI was for. The plugin will also automatically populate the DOI profile <code>identifier</code> metadata key for the catalogue item.</p>"},{"location":"plugins/rest-api/doi/#remove-doi","title":"Remove DOI","text":"<p>To remove all DOI metadata:</p> <p>/api/{multiFacetAwareDomainType}/{id}/doi</p>"},{"location":"plugins/rest-api/doi/#doi-resolution","title":"DOI Resolution","text":"<p>The <code>identifier</code> in the DOI profile will hold the unique identifier as registered in the DOI system, for example <code>10.1109/5.771073</code>. The DOI system has a guaranteed method of locating every registered DOI name through a non-changing URL; this will actually redirect back to your client URL by providing the DOI name in the URL - in Mauro, this would be similar to <code>https://{domain}/#/doi/10.1109/5.771073</code></p> <p>To actually locate the catalogue item assigned to this DOI name, this endpoint will fetch the Mauro catalogue item to which the DOI name is mapped.</p> <p>/api/doi/{identifier}</p> <p>The JSON response will contain the full catalogue item details for that DOI name. See the REST API for the exact details returned per catalogue item domain type.</p>"},{"location":"plugins/rest-api/freemarker/","title":"Freemarker Templating","text":""},{"location":"plugins/rest-api/freemarker/#introduction","title":"Introduction","text":"<p>The Freemarker plugin provides additional API endpoints that allow custom queries to be run and templates to be applied to the result. This can be used as a lightweight way to create custom exporters or provides an easy way to allow interchange between different systems. The functionality makes use of a popular and powerful templating engine: Apache FreeMarker.</p>"},{"location":"plugins/rest-api/freemarker/#warning","title":"Warning","text":"<p>Warning</p> <p>Allowing users to execute their own code on the server is, in general, bad practice. Badly-written templates could result in the server becoming unresponsive or consuming additional resources. The template framework has also been carefully configured to ensure a minimal amount of  data is accessible, but malicious templates may be able to access more data than intended.</p> <p>To mitigate this risk, the Freemarker templating plugin is an optional extra and when installed, the API endpoints are only accessible by registered users with administrator access. It is recommended that template developers test on a local instance of Mauro in the first instance and carefully iterate on templates. This will ensure any complex queries or loops do not accidentally cause a denial of service to the Mauro instance.</p> <p>We have plans to improve the templating plugin in the future to provide greater levels of safety as well as allowing non-administrators to have access to the functionality in a safe manner.</p>"},{"location":"plugins/rest-api/freemarker/#basic-usage","title":"Basic usage","text":"<p>The main API endpoint provided by the plugin is as follows:</p> <p>/api/{domainType}/{id}/template</p> <p>In order to call this endpoint, the user must be authenticated as a user with an administrator role, either through a login call and a persistent session cookie, or using an API key. The body of the call must be text corresponding to a valid Apache FreeMarker template. The <code>domainType</code> and <code>id</code> parameters will point to a particular \u2018resource\u2019, that will be queried and have the template applied. </p> <p>Within the template, the resulting resource will be accessible through a variable with the same name as the domain type. See the examples below  for more details. Successful return of the API call will have, as body text, the result of applying the given template to the queried resource. </p> <p>Any errors in the template will result in an error being thrown, with a <code>message</code> field indicating the nature of the error and the line number of  the template in which the error was found.</p>"},{"location":"plugins/rest-api/freemarker/#template-language-reference","title":"Template language reference","text":"<p>The Apache FreeMarker pages contain a general-purpose  introduction to the FreeMarker Template Language (FTL), as well as a  detailed reference manual for the syntax. </p> <p>Other online tutorials are provided by Lars Vogel and some useful tricks are given in  this article by Baeldung. </p> <p>You may also wish to visit our Zulip community for help and advice. We would also like to publish  reusable templates donated by the community on these pages. </p>"},{"location":"plugins/rest-api/freemarker/#example-1-a-simple-web-page","title":"Example 1: A simple web page","text":"<p>In this first example, a basic template will be applied to a Data Model in order to generate a simple HTML page for a given Data Model.</p> <p>The API call:</p> <p>/api/dataModel/{id}/template</p> <p>Is made with an appropriate API key, and the post body:</p> <pre><code>&lt;html&gt;\n&lt;body&gt;\n  &lt;h1&gt;\n    ${dataModel.label}\n  &lt;/h1&gt;\n  &lt;p&gt;\n    ${(dataModel.description)!}\n  &lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Note in the template, the variable <code>dataModel</code> represents the Data Model object with the <code>id</code> given as the URL parameter. Note also, the use of <code>!</code> when accessing the description o the Data Model. This is to guard against the case where the description is unset (null).</p> <p>The body of the API call response is as follows:</p> <pre><code>&lt;html&gt;\n&lt;body&gt;\n  &lt;h1&gt;\n    Simple Data Model\n  &lt;/h1&gt;\n  &lt;p&gt;\n    This is the description of a simple data model\n  &lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Note that this example also illustrates some shortcomings of the approach for complex transformations. If the description were to be html formatted already, for example, starting with a <code>&lt;p&gt;</code> tag, the resulting html may be invalid. Some simple transformations are possible within the templating language, particularly with the use of complex macros, but the use of this plugin should be reserved for simple textual templating.  </p> <p>More complex algorithms should be implemented within a custom plugin or external script, where programmatic functions and libraries are more  easily accessible.</p>"},{"location":"plugins/rest-api/freemarker/#example-2-a-csv-file-of-terms-in-a-terminology","title":"Example 2: A CSV file of terms in a terminology","text":"<p>This second example uses a slightly more complex template to iterate over the terms of a terminology, printing the code and definition for each.</p> <p>The API call:</p> <p>/api/terminology/{id}/template</p> <p>Is made with an appropriate API key and with the following text in the body:</p> <pre><code>Code,Definition\n&lt;#list terminology.terms?sort_by(\"code\") as term&gt;\n${term.code},${term.definition}\n&lt;/#list&gt;\n</code></pre> <p>The <code>&lt;#list&gt;</code> tag indicates a template chunk that is to be repeated for each item in the list - in this case the list of terms given by  <code>terminology.term</code>.  The <code>?sort_by</code> annotation indicates that the set of terms should be ordered before iteration.  Within these tags, the term in  question is given the variable name <code>term</code>.</p> <p>The result is a CSV file of terms, for example:</p> <pre><code>Code,Definition\nCTT00,Complex Test Term 00\nCTT1,Complex Test Term 1\nCTT10,Complex Test Term 10\nCTT100,Complex Test Term 100\n</code></pre>"},{"location":"plugins/rest-api/freemarker/#templating-diffs","title":"Templating Diffs","text":"<p>The plugin also provides an endpoint for templating the difference between two models.  The API endpoint:</p> <p>/api/{domainType}/{sourceId}/diff/{targetId}/template</p> <p>Compares two objects and allows the result to be templated.  The method provides the following variables for use within the template:</p> <code>sourceModel</code> The model to be used as the source of the diff (left-hand side), identified by <code>sourceId</code> <code>targetModel</code> The model to be used as the target of the diff (right-hand-side), identified by <code>targetId</code> <code>diff</code> The result of the comparison between the two models, stored as a set of <code>diffs</code> <p>The structure of the <code>diff</code> object is still to be described in more detail, but a simple example is given below.</p>"},{"location":"plugins/rest-api/freemarker/#example-3-listing-differences","title":"Example 3: Listing differences","text":"<p>This third example gives a flavour of the templating required to examine the differences between two Data Models.  A more detailed example is  needed here, but we illustrate for a very simple template, comparing two very simple (and similar) models.</p> <p>Given the API call:</p> <p>/api/dataModel/{sourceId}/diff/{targetId}/template</p> <p>With appropriate authentication and the POST body:</p> <pre><code>${sourceModel.label}\n${targetModel.label}\n&lt;#list diff.diffs as fieldDifference&gt; \n  ${fieldDifference.getFieldName()} :: ${fieldDifference.left} &lt;&gt; ${fieldDifference.right}\n&lt;/#list&gt;\n</code></pre> <p>This will print first the label of the source model, then the label of the target model.  For each basic difference between the two models, it  will print the field name, the value of that field in the left-hand model, and the value of that field in the right-hand model.</p> <p>For two very simple models this might give an output such as:</p> <pre><code>Example DataModel 1\nExample DataModel 2\nlabel :: Example DataModel 1 &lt;&gt; Example DataModel 2\ndescription :: My first description &lt;&gt; My second description\n</code></pre>"},{"location":"plugins/rest-api/openid-connect/","title":"OpenID Connect","text":""},{"location":"plugins/rest-api/openid-connect/#introduction","title":"Introduction","text":"<p>The page covering authentication explains how to authenticate with a basic username/password for a user created directly in Mauro. An alternative authentication method is to use an OpenID Connect identity service to authenticate users with an external provider/account system, and then authorize them to use Mauro.</p> <p>In order use OpenID Connect identity providers, the Mauro instance must:</p> <ol> <li>Have the Mauro OpenID Connect Authentication plugin installed.</li> <li>Set up and configure one or more identity services that support the OpenID Connect protocol.</li> <li>Add the configuration details of each identity provider to Mauro.</li> </ol> <p>The full details of this set up can be viewed in the user guide Using OpenID Connect.</p>"},{"location":"plugins/rest-api/openid-connect/#authenticating-users","title":"Authenticating users","text":"<p>Assuming that the Mauro instance has been configured correctly and there is at least one OpenID Connect provider configured in Mauro, endpoints will be exposed to handle authentication using those external services.</p>"},{"location":"plugins/rest-api/openid-connect/#using-authorization-endpoints","title":"Using authorization endpoints","text":"<p>There is an endpoint to fetch a list of OpenID Connect providers available in Mauro, which will provide the authorization endpoints:</p> <p>/api/openidConnectProviders</p> Response body (JSON) <pre><code>[\n{\n\"id\": \"0500cd44-6ca9-4bca-aa55-bbb188278d79\",\n\"label\": \"Google\",\n\"standardProvider\": true,\n\"authorizationEndpoint\": \"https://accounts.google.com/o/oauth2/v2/auth?scope=openid+email+profile&amp;response_type=code&amp;state=045d33af-2dc7-48cd-9111-a09c94faee49&amp;nonce=%C3%B9%02%C2%BF%C5%93%07%C3%A9%C3%B4%27%EF%BF%BD%2C%22%C2%B3%C3%8D%07%C3%98%E2%80%A2%C3%91%C3%8C%C3%83%EF%BF%BD%26%CB%9C%C3%A4%05%C3%A17%C2%B1%19Q%C3%A13%E2%80%9C&amp;client_id=375980182300-tc8sb8c1jelomnkmvqtkkqpl4g8lkp06.apps.googleusercontent.com\",\n\"imageUrl\": \"https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg\"\n},\n{\n\"id\": \"61fa9235-7281-4738-87f1-763bf60e1d79\",\n\"label\": \"Keycloak\",\n\"standardProvider\": true,\n\"authorizationEndpoint\": \"https://jenkins.cs.ox.ac.uk/auth/realms/test/protocol/openid-connect/auth?scope=openid+email+profile&amp;response_type=code&amp;state=0f4e7148-28c4-407d-bad8-47052769e721&amp;nonce=%C3%B9%02%C2%BF%C5%93%07%C3%A9%C3%B4%27%EF%BF%BD%2C%22%C2%B3%C3%8D%07%C3%98%E2%80%A2%C3%91%C3%8C%C3%83%EF%BF%BD%26%CB%9C%C3%A4%05%C3%A17%C2%B1%19Q%C3%A13%E2%80%9C&amp;client_id=mdm\",\n\"imageUrl\": \"https://upload.wikimedia.org/wikipedia/commons/2/29/Keycloak_Logo.png\"\n}\n]\n</code></pre> <p>The <code>authorizationEndpoint</code> is the key property value in each case, as this provides the URL to redirect to in order to start authenticating in the external service. The <code>authorizationEndpoint</code> value must include an additional query parameter added by yourself called <code>redirect_uri</code>: this is the URL to the page that the identity provider will redirect back to once it has authenticated a user, also providing the session state needed for Mauro to authorize the user session.</p> <p>Information</p> <p>Before redirecting to the external identity server, track the <code>id</code> value of the OpenID Connect provider somewhere. This will be required later when authorizing the Mauro user session.</p>"},{"location":"plugins/rest-api/openid-connect/#mauro-authorization","title":"Mauro authorization","text":"<p>When the OpenID Connect identity provider redirects back to the client (via the <code>redirect_uri</code> provided), it will provide three query parameters in the URL:</p> <ul> <li><code>state</code></li> <li><code>session_state</code></li> <li><code>code</code></li> </ul> <p>All these parameters must be captured and the passed to the standard Mauro authentication endpoint via the request body:</p> <p>/api/authentication/login</p> Request body (JSON) <pre><code>{ \"openidConnectProvider\": \"0500cd44-6ca9-4bca-aa55-bbb188278d79\",\n\"state\": &lt;query param value&gt;,\n\"sessionState\": &lt;query param value&gt;,\n\"code\": &lt;query param value&gt;,\n\"redirectUri\": \"http://my.app.com/authorize\"\n}\n</code></pre> <p>Information</p> <p>The <code>redirectUri</code> passed to the login request body must exactly match that passed as the <code>redirect_uri</code> query parameter at the point of redirecting to the OpenID Connect service.</p> <p>If successful, the <code>/api/authentication/login</code> endpoint will return the same response as a standard login using a username and password. The user is now signed into Mauro and has an active session. The user also uses the same permissions and user groups model for accessing catalogue content as any other user.</p>"},{"location":"plugins/rest-api/openid-connect/#administration","title":"Administration","text":"<p>Information</p> <p>The following endpoints can only be accessed by an administrator user.</p>"},{"location":"plugins/rest-api/openid-connect/#getting-providers","title":"Getting providers","text":"<p>To get a list of available OpenID Connect providers registered in Mauro, the following endpoint can be used:</p> <p>/api/admin/openidConnectProviders</p> Response body (JSON) <pre><code>{\n\"count\": 2,\n\"items\": [\n{\n\"id\": \"0500cd44-6ca9-4bca-aa55-bbb188278d79\",\n\"lastUpdated\": \"2021-06-24T09:01:38.914Z\",\n\"label\": \"Google\",\n\"standardProvider\": true,\n\"imageUrl\": \"https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg\"\n},\n{\n\"id\": \"61fa9235-7281-4738-87f1-763bf60e1d79\",\n\"lastUpdated\": \"2021-06-24T09:01:39.336Z\",\n\"label\": \"Keycloak\",\n\"standardProvider\": true,\n\"imageUrl\": \"https://upload.wikimedia.org/wikipedia/commons/2/29/Keycloak_Logo.png\"\n}            ]\n}\n</code></pre> <p>To get the full details of a particular OpenID Connect provider:</p> <p>/api/admin/openidConnectProviders/{id}</p> Response body (JSON) <pre><code>{\n\"id\": \"0500cd44-6ca9-4bca-aa55-bbb188278d79\",\n\"lastUpdated\": \"2021-06-24T09:01:38.914Z\",\n\"label\": \"Google\",\n\"standardProvider\": true,\n\"discoveryDocumentUrl\": \"https://accounts.google.com/.well-known/openid-configuration\",\n\"clientId\": \"375980182300-tc8sb8c1jelomnkmvqtkkqpl4g8lkp06.apps.googleusercontent.com\",\n\"clientSecret\": \"&lt;secret value&gt;\",\n\"authorizationEndpointParameters\": {\n\"id\": \"dae8564f-8272-4b99-ba1d-03c66f3ab34a\",\n\"lastUpdated\": \"2021-06-24T09:01:38.906Z\",\n\"scope\": \"openid email profile\",\n\"responseType\": \"code\"\n},\n\"discoveryDocument\": {\n\"id\": \"52108737-6fda-408e-b167-f6912bd37a18\",\n\"lastUpdated\": \"2021-06-24T09:01:38.91Z\",\n\"issuer\": \"https://accounts.google.com\",\n\"authorizationEndpoint\": \"https://accounts.google.com/o/oauth2/v2/auth\",\n\"tokenEndpoint\": \"https://oauth2.googleapis.com/token\",\n\"userinfoEndpoint\": \"https://openidconnect.googleapis.com/v1/userinfo\",\n\"endSessionEndpoint\": \"https://oauth2.googleapis.com/revoke\",\n\"jwksUri\": \"https://www.googleapis.com/oauth2/v3/certs\"\n},\n\"imageUrl\": \"https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg\"\n}\n</code></pre>"},{"location":"plugins/rest-api/openid-connect/#creating-a-provider","title":"Creating a provider","text":"<p>To add an OpenID Connect provider to Mauro depends on what discovery information is available. There are two types of provider in Mauro:</p> <ul> <li>Standard - a <code>discoveryDocumentUrl</code> is provided, which allows Mauro to discover all other necessary endpoints to complete the setup of the provider (authorization, tokens, etc).</li> <li>Non-Standard - when a <code>discoveryDocumentUrl</code> is not available, the individual endpoints needed to complete the OpenID Connect provider setup can be provided manually.</li> </ul> <p>Both use the following endpoint, but sends different request body content.</p> <p>/api/admin/openidConnectProviders</p> <p>To create a Standard provider, include a similar request body:</p> Request body (JSON) <pre><code>{ \"label\": \"Provider name\",\n\"imageUrl\": \"http://image.com/logo.png\",\n\"standardProvider\": true,\n\"clientId\": \"&lt;Provided by service&gt;\",\n\"clientSecret\": \"&lt;Provided by service&gt;\",\n\"discoveryDocumentUrl\": \"https://url.to.some/discovery-document\"\n}\n</code></pre> <p>Information</p> <p><code>imageUrl</code> is an optional field.</p> <p>To create a Non-Standard provider:</p> Request body (JSON) <pre><code>{ \"label\": \"Provider name\",\n\"imageUrl\": \"http://image.com/logo.png\",\n\"standardProvider\": false,\n\"clientId\": \"&lt;Provided by service&gt;\",\n\"clientSecret\": \"&lt;Provided by service&gt;\",\n\"discoveryDocument\": {\n\"issuer\": \"&lt;url&gt;\",\n\"authorizationEndpoint\": \"&lt;url&gt;\",\n\"tokenEndpoint\": \"&lt;url&gt;\",\n\"userinfoEndpoint\": \"&lt;url&gt;\",\n\"endSessionEndpoint\": \"&lt;url&gt;\",\n\"jwksUri\": \"&lt;url&gt;\"\n}\n}\n</code></pre> <p>Information</p> <p><code>userinfoEndpoint</code> and <code>endSessionEndpoint</code> are optional, all other discovery endpoints are required.</p> <p>If successful, both return the same response as getting a provider.</p>"},{"location":"plugins/rest-api/openid-connect/#update-delete","title":"Update / Delete","text":"<p>To edit the properties of an OpenID Connect provider, use the following endpoints, with a request body similar to the JSON described in Creating a provider.</p> <p>/api/admin/openidConnectProviders/{id}</p> <p>To delete an OpenID Connect provider, use the following endpoint.</p> <p>/api/admin/openidConnectProviders/{id}</p>"},{"location":"plugins/rest-api/sparql/","title":"SPARQL","text":""},{"location":"plugins/rest-api/sparql/#introduction","title":"Introduction","text":"<p>The SPARQL plugin provides a compliant endpoint for executing SPARQL queries against an RDF rendering of the data. The underlying data store of Mauro Data Mapper is Postgres, and this plugin uses the D2RQ library for extracting the information as RDF triples. </p> <p>The REST API ensures that the mapping is configured with correct access constraints so that users may only see a subset of the triples corresponding to their access privileges. In the most basic cases, a system administrator can query across the whole data corpus in triple form; an unauthenticated user on an instance with no publicly available models will not be able to see any triples.</p> <p>Warning</p> <p>This plugin allows users to execute their own queries against the data store. A malicious user may write arbitrarily complex queries which could cause Mauro to slow down or become unresponsive. If you install this plugin, you may wish to ensure the Mauro instance is behind a firewall, and ensure that users know what they are doing!</p>"},{"location":"plugins/rest-api/sparql/#api-endpoints","title":"API Endpoints","text":"<p>The plugin provides two (equivalent) endpoints which accept a SPARQL query as part of the request body and return results in a variety of formats.</p> <p>/api/sparql /api/sparql</p> <p>The Accept header determines which format is returned, according to the table below:</p> Accept Header Value Response Format application/sparql-results+xml         xml         application/xml        XML text/csv         csv        CSV application/sparql-results+json         json         application/json        JSON <p>The default format is for results to be returned in JSON format.</p> <p>In future, it may be possible to extend this plugin to support RDF/XML and other formats.</p>"},{"location":"plugins/rest-api/sparql/#response-format","title":"Response format","text":"<p>For the request body given in Example 1 below, the response body will have one of the following formats:  </p> Response body (JSON) <pre><code>{\n\"head\": {\n\"vars\": [\n\"s\",\n\"p\",\n\"o\"\n]\n},\n\"results\": {\n\"bindings\": [\n{\n\"s\": {\n\"type\": \"uri\",\n\"value\": \"http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5\"\n},\n\"p\": {\n\"type\": \"uri\",\n\"value\": \"http://metadata-catalogue.org/label\"\n},\n\"o\": {\n\"type\": \"literal\",\n\"value\": \"Example Data Model\"\n}\n},\n...\n]\n}\n}\n</code></pre> Response body (XML) <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;sparql xmlns=\"http://www.w3.org/2005/sparql-results#\"&gt;\n&lt;head&gt;\n&lt;variable name=\"s\"/&gt;\n&lt;variable name=\"p\"/&gt;\n&lt;variable name=\"o\"/&gt;\n&lt;/head&gt;\n&lt;results&gt;\n&lt;result&gt;\n&lt;binding name=\"s\"&gt;\n&lt;uri&gt;http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5&lt;/uri&gt;\n&lt;/binding&gt;\n&lt;binding name=\"p\"&gt;\n&lt;uri&gt;http://metadata-catalogue.org/label&lt;/uri&gt;\n&lt;/binding&gt;\n&lt;binding name=\"o\"&gt;\n&lt;literal&gt;Example Data Model&lt;/literal&gt;\n&lt;/binding&gt;\n&lt;/result&gt;\n...\n  &lt;/results&gt;\n&lt;/sparql&gt;\n</code></pre> Response body (CSV) <pre><code>s,p,o\nhttp://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5,http://metadata-catalogue.org/label,Example Data Model\n...\n</code></pre>"},{"location":"plugins/rest-api/sparql/#example-queries","title":"Example Queries","text":"<p>This page is not intended to provide a tutorial to writing SPARQL queries - some other tutorials are available online and linked below.  These  examples serve as a starting point for exploring the triple space.</p>"},{"location":"plugins/rest-api/sparql/#example-1-arbitrary-triples","title":"Example 1: Arbitrary triples","text":"<p>Select the first 20 triples from the entire graph:</p> <pre><code>SELECT ?s ?p ?o WHERE { \n    ?s ?p ?o \n} limit 20\n</code></pre>"},{"location":"plugins/rest-api/sparql/#example-2-restricting-types","title":"Example 2: Restricting types","text":"<p>Select the labels of all Data Models:</p> <pre><code>PREFIX mdm: &lt;http://metadata-catalogue.org/&gt;\n\nSELECT ?o WHERE {\n    ?s mdm:label ?o .\n    ?s a mdm:datamodel\n}\n</code></pre>"},{"location":"plugins/rest-api/sparql/#example-3-relating-entities","title":"Example 3: Relating entities","text":"<p>Find the <code>id</code> of the classes which belong to a model called \"Complex Test DataModel\":</p> <pre><code>PREFIX mdm: &lt;http://metadata-catalogue.org/&gt;\n\nSELECT ?dcl WHERE { \n    ?dm mdm:label \"Complex Test DataModel\" .\n    ?dm a mdm:datamodel .\n    ?dc mdm:child_class_of ?dm .\n    ?dc mdm:id ?dcl\n}\n</code></pre>"},{"location":"plugins/rest-api/sparql/#example-4-relations-multiple-results","title":"Example 4: Relations, multiple results","text":"<p>Find the Enumeration Values which have a 'Value' of \"Not known\". Find their keys and the label of the containing data type:</p> <pre><code>PREFIX mdm: &lt;http://metadata-catalogue.org/&gt;\n\nSELECT ?dtl ?evk  WHERE { \n    ?ev a mdm:enumerationvalue .\n    ?ev mdm:value \"Not known\" .\n    ?ev mdm:key ?evk .\n    ?dt mdm:component_value ?ev .\n    ?dt mdm:label ?dtl\n}\n</code></pre>"},{"location":"plugins/rest-api/sparql/#example-5-finding-metadata","title":"Example 5: Finding metadata","text":"<p>Find the <code>schema.org</code> <code>abstract</code> property from a Data Model:</p> <pre><code>PREFIX mdm: &lt;http://metadata-catalogue.org/&gt;\nPREFIX so: &lt;http://metadata-catalogue.org/schema.org/&gt;\n\nSELECT ?a WHERE { \n    ?dm a mdm:datamodel .\n    ?dm mdm:label \"Complex Test DataModel\" .\n    ?dm so:abstract ?a .\n}\n</code></pre>"},{"location":"plugins/rest-api/sparql/#example-6-transitive-searches","title":"Example 6: Transitive searches","text":"<p>The first query finds the immediate child classes of a model:</p> <pre><code>PREFIX mdm: &lt;http://metadata-catalogue.org/&gt;\n\nSELECT ?l WHERE { \n    ?dm a mdm:datamodel .\n    ?dm mdm:label \"Complex Test DataModel\" .\n    ?dc mdm:child_class_of ?dm .\n    ?dc mdm:label ?l\n}\n</code></pre> <p>Compare this with the second, which transitively follows the <code>child_class_of</code> relationship to find child classes of that class:</p> <pre><code>PREFIX mdm: &lt;http://metadata-catalogue.org/&gt;\n\nSELECT ?l WHERE {\n?dm a mdm:datamodel .\n?dm mdm:label \"Complex Test DataModel\" .\n?dc mdm:child_class_of+ ?dm .\n?dc mdm:label ?l\n}\n</code></pre>"},{"location":"plugins/rest-api/sparql/#example-7-transitive-searches-on-terminologies","title":"Example 7: Transitive Searches on Terminologies","text":"<p>This search recursively finds all terms narrower than another, and provides their code and definition:</p> <pre><code>PREFIX mdm: &lt;http://metadata-catalogue.org/&gt;\n\nSELECT ?code ?definition WHERE { \n    ?ut mdm:code \"N20-N23\" .\n    ?tm mdm:label \"International Classification of Diseases (ICD) Version 10 Edition 5\" .\n    ?t mdm:term_of ?tm .\n    ?ut mdm:term_of ?tm .\n    ?t mdm:narrowerThan+ ?ut .\n    ?t mdm:code ?code .\n    ?t mdm:definition ?definition .\n} ORDER BY ASC(?code)\n</code></pre>"},{"location":"plugins/rest-api/sparql/#links-to-sparql-tutorials","title":"Links to SPARQL Tutorials","text":"<p>There are many good tutorials available online - this is not intended to represet a comprehensive list. But here are some some that we've found useful in the past:</p> <ul> <li>Apache Jena: SPARQL Tutorial</li> <li>Stardog: Learn SPARQL</li> <li>W3C: SPARQL By Example</li> </ul>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/","title":"Digital Object Identifiers","text":"<p>This user guide explains how to set-up the Mauro Digital Object Identifier plugin to allow you to use, edit and remove DOI profiles. You will also find out how to create, submit and retire profile names. </p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#1-overview","title":"1. Overview","text":"<p>Using an optional plugin for Mauro, it is possible to submit Mauro catalogue items from your catalogue to a  Digital Object Identifier (DOI) system. This allows persistent, unique identifiers to be recorded in a DOI system to act as network links back to your Mauro catalogue items. The benefits are that the catalogue item may change over time, but the DOI name will remain fixed, always resolving back to the correct catalogue item, making them very useful for citations.</p> <p>The Mauro plugin for managing DOI names connects to the DataCite system to create and manage DOIs with DataCite Fabrica.</p> <p>DOI names can only be applied to public and finalised Mauro catalogue items, to ensure that their contents remain fixed in time. The following catalogue items can have DOI names attached to them:</p> <ul> <li>Data Model</li> <li>Data Class</li> <li>Data Element</li> <li>Data Type</li> <li>Code Set</li> <li>Terminology</li> <li>Term</li> <li>Reference Data Model</li> <li>Versioned Folder</li> </ul>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#2-administrator-setup","title":"2. Administrator setup","text":"<p>Note</p> <p>This section applies to administrators of Mauro only.</p> <p>There are a number of steps an administrator must carry out before users can submit DOI names.</p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#21-install-the-plugin","title":"2.1 Install the plugin","text":"<p>The Mauro Digital Object Identifier plugin must be installed on the Mauro instance. See the instructions for installing plugins, as well as the GitHub repo README file.</p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#22-enable-the-feature","title":"2.2 Enable the feature","text":"<p>As this is an optional feature, the user interface will not show DOI profiles and features by default. To allow DOI submission to be used, you will first need to click the white arrow next to your user icon and then select 'Configuration' from the dropdown menu. </p> <p></p> <p>This will bring up the 'Configuration' panel. Click the 'Properties' tab and then click '+Add'.</p> <p></p> <p>An 'Add Property' form will appear which you will need to complete. Click the 'Select the property to add' box and select 'feature.use_digital_object_identifiers' from the dropdown menu, which will automatically fill in additional fields. Select 'Yes' from the 'Value' dropdown menu and then click the 'Add property' button. A green notification box will appear at the bottom right of your screen confirming that the 'Property was saved successfully'.</p> <p></p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#23-datacite-membership","title":"2.3 DataCite membership","text":"<p>In order to create DOIs you will need to be a member (regular, or consortium-based) of DataCite. Please read the Become a Member DataCite pages to understand how to do this.</p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#24-mauro-configuration","title":"2.4 Mauro configuration","text":"<p>Once enabled and your DataCite API is configured, there are some additional API properties to configure in Mauro before using the plugin. These can be found in the Configuration &gt; Properties administration table, under the Digital Object Identifier Properties group.</p> <p></p> <ul> <li>username - the username to connect to the DataCite API for authentication.</li> <li>password - the password to connect to the DataCite API for authentication.</li> <li>prefix - the unique prefix identifier representing this organisation. DataCite will provide this.</li> <li>endpoint - the base URL to the DataCite API.</li> </ul> <p>Initially, the plugin will automatically add these with the value of \"NOT_SET\"; change these values once you have them available.</p> <p>One final API property to ensure is set is the Site URL under the Site group. The Site URL property is used by the plugin to submit to DataCite, so that all links to the requested DOI get redirected back to the correct Mauro instance.</p> <p>Warning</p> <p>It is important to set the Site URL property, otherwise the Digital Object Identifier (DOI) plugin will be unable to submit profiles to DataCite correctly.</p> <p></p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#3-using-the-profile","title":"3. Using the profile","text":"<p>Assuming all the setup steps were carried out above, catalogue items should now be able to use a new profile called 'Digital Object Identifiers DataCite Dataset Schema'.</p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#31-add-the-profile","title":"3.1 Add the profile","text":"<p>Note</p> <p>The DOI profile can only be added by editors of Mauro, and only when the catalogue item is in a draft state (not finalised). If the profile has not been created before the catalogue item is finalised, there is another opportunity to edit the profile before submitting for a DOI name, as explained below.</p> <p>To add the DOI profile to the catalogue item, first select the catalogue item in the Model Tree that you wish to modify. Once that item's details panel is displayed on the right of your screen, select the 'Description' tab. </p> <p>Expand the 'Default profile' list and click 'Add new profile...'. Select 'Digital Object Identifiers DataCite Dataset Schema' and click 'Save Changes'. This will display the full DOI profile fields to enter. You can enter values into the fields now, or click 'Save' to store the partially complete profile.</p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#32-editing-the-profile","title":"3.2 Editing the profile","text":"<p>The DOI profile will present a number of fields which will be recorded as metadata against the catalogue item. These profile field values will also be submitted to the DOI system when a DOI name is requested, so it is important to ensure that the values entered into the profile are as accurate as possible.</p> <p>The profile fields are grouped into a number of sections:</p> <ol> <li>Predefined/Supplied Fields</li> <li>Primary Creator</li> <li>Additional Mandatory Fields</li> <li>Additional Optional Title Section</li> <li>Additional Optional Description Section</li> <li>Primary Contributor</li> <li>Additional Optional Fields</li> </ol> <p>The Predefined/Supplied Fields are special, read-only fields in the profile. These are used and modified internally by Mauro to track important details about the DOI name, such as the state, identifier, and so on. These are provided in read-only view to the user but are automatically controlled during DOI submission.</p> <p>There are a number of sections and fields in the profile, though only a few a strictly mandatory to submit to the DOI system (marked with a \"*\" symbol next to their field name). Clicking on the help icons next to each field name will provide a description of what the field represents.</p> <p>The profile will appear in read-only form when viewing the catalogue item. If you want to edit the profile, first make sure the 'Digital Object Identifiers DataCite Dataset Schema' profile is selected in the profile selection list and then click the 'Edit' button. </p> <p></p> <p>This will bring up an 'Edit Profile' form which you will then be able to make changes to. Once you have finished, click 'Save'. </p> <p>Information</p> <p>You are able to save changes to a profile without ensuring that the profile is fully valid. For example, some fields may say 'This field is mandatory', but changes can be saved temporarily without such fields being filled in. To test that the profile is valid, click on the 'Validate' button, then review any validation alerts that appear in each field.</p> <p></p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#34-remove-the-profile","title":"3.4 Remove the profile","text":"<p>If you decide against using the DOI profile, it can be removed from the catalogue item. Once again, make sure that the 'Digital Object Identifiers DataCite Dataset Schema' profile is selected in the profile selection list. Then click the three vertical dot menu to the right of the 'Edit' button and then select 'Remove profile' from the dropdown menu.</p> <p></p> <p>A notification box will then appear asking if you are sure you want to remove this profile. Click 'Yes, remove' and the profile will then be removed along with all the metadata stored for that profile. A notification box will appear at the bottom right of your screen, confirming that the profile was successfully removed. </p> <p></p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#4-submit-and-create-a-doi-name","title":"4. Submit and create a DOI name","text":"<p>The 'Identifier' profile field will store the DOI name once the catalogue item has been submitted to the DOI system, for instance <code>10.1109/5.771073</code>. To store this DOI name, the catalogue item must first be submitted to the DOI system. </p> <p>To do this, select the relevant catalogue item from the Model Tree that you want to submit. The item must be finalised and publicly readable to anyone. Once the details panel has appeared on the right of your screen, click the 'Description' tab to view the profiles. Then click the three vertical dot menu to the right of the 'Edit' button and select 'DOI submission' from the dropdown menu. </p> <p></p> <p>You will then have two possible states to generate the DOI name:</p> <ul> <li> <p>Draft     Allows for the profile fields to be modified whilst still obtaining a DOI name. Suitable for making edits to the profile whilst it is work-in-progress</p> </li> <li> <p>Final     Fixes the DOI name and the metadata associated with it in the DOI system. The profile is finalised and cannot be modified further</p> </li> </ul> <p>Information</p> <p>The 'Digital Object Identifiers DataCite Dataset Schema' profile does not need to be selected to carry out this step, nor does it have to be created yet. Mauro will automatically handle the creation of the DOI profile if it does not already exist.</p> <p>Before submitting, an 'Edit Profile' form will appear which will allow you to review the profile fields and make any necessary changes before submitting to the DOI system. </p> <p>Information</p> <p>It is recommended to use the Validate button to ensure that all profile fields are valid before submitting.</p> <p>Once you are happy with the profile, click the 'Submit' button and a green notification box will appear at the bottom right of your screen to confirm the change and the 'Identifier' field will be populated with the newly created DOI name.</p> <p>You may continue making changes to any Draft DOI profiles by repeating the same steps above. However, once changing to the Final state, you will have one last opportunity to review/modify the profile before it is fixed in place forever.</p> <p></p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#5-retire-a-doi-name","title":"5. Retire a DOI name","text":"<p>Once a DOI name is recorded in the profile, it is possible to retire that DOI name. When a DOI name is retired, any network links back to that catalogue item will no longer work.</p> <p>Warning</p> <p>Be careful when retiring DOI names - once retired, this cannot be undone.</p> <p>To retire a DOI name, first select the catalogue item in the Model Tree with the DOI name to retire. Once the details panel has appeared on the right of your screen, click the 'Description' tab to view the profiles. Then click the three vertical dot menu to the right of the 'Edit' button and select 'DOI submission' and then 'Retire' from the dropdown menus. </p> <p></p> <p>A notification box will appear asking you to confirm that you would like to retire the DOI name. Click 'Yes, retire' and a green notification box shoud appear at the bottom right of your screen confirming that the DOI name was retired successfully. </p> <p></p>"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#6-doi-name-resolution","title":"6. DOI name resolution","text":"<p>When Mauro catalogue items are submitted and recorded in the DOI system, all network links for those catalogue items are routed through the DOI system back to Mauro. All hyperlinks back to Mauro will point to:</p> <pre><code>https://{domain}/#/doi/{name}\n</code></pre> <p>Where <code>{name}</code> is the DOI name in question, for example:</p> <pre><code>https://{domain}/#/doi/10.1109/5.771073\n</code></pre> <p>Any URL in this format will automatically resolve and redirect to the correct Mauro catalogue item to view.</p>"},{"location":"plugins/user-guides/openid-connect/openid-connect/","title":"OpenID Connect Client","text":""},{"location":"plugins/user-guides/openid-connect/openid-connect/#1-overview","title":"1. Overview","text":"<p>Using an optional plugin for Mauro security, it is possible to extend authentication of users in Mauro to also include OpenID Connect authentication. This allows administrators to:</p> <ul> <li>Configure external providers that support the OpenID Connect standard, such as Google, Microsoft, etc, to be included as 'sign-in' options for Mauro</li> <li>Manage users of different account types to still be authenticated and authorized to use Mauro, using the existing user groups and permissions features</li> </ul> <p>This allows users of Mauro to sign in, or register, using a non-Mauro account, such as a user's existing Google/Microsoft account, and still get access to the Mauro catalogue with suitable Mauro permissions assigned.</p>"},{"location":"plugins/user-guides/openid-connect/openid-connect/#2-administrator-setup","title":"2. Administrator setup","text":"<p>Note</p> <p>This section applies to administrators of Mauro only.</p> <p>There are a number of steps an administrator must carry out before the OpenID Connect authentication can be used by users.</p>"},{"location":"plugins/user-guides/openid-connect/openid-connect/#21-install-the-plugin","title":"2.1 Install the plugin","text":"<p>The Mauro OpenID Connect Authentication must be installed on the Mauro instance. See the instructions for installing plugins, as well as the GitHub repo README file.</p>"},{"location":"plugins/user-guides/openid-connect/openid-connect/#22-enable-the-feature","title":"2.2 Enable the feature","text":"<p>As this is an optional feature, the user interface will not show OpenID Connect configuration elements by default. To allow OpenID Connect to be used, you will first need to click the white arrow next to your user icon and then select 'Configuration' from the dropdown menu.</p> <p></p> <p>This will bring up the 'Configuration' panel. Click the 'Properties' tab and then click '+Add'.</p> <p></p> <p>An 'Add Property' form will appear which you will need to complete. Click the 'Select the property to add' box and select 'feature.use_open_id_connect' from the dropdown menu, which will automatically fill in additional fields. Select 'Yes' from the 'Value' dropdown menu and then click 'Add property'.</p> <p></p> <p>A green notification box will appear at the bottom right of your screen confirming that the 'Property was saved successfully' and the OpenID Connect option will now be available in the administration navigation menu.</p> <p></p>"},{"location":"plugins/user-guides/openid-connect/openid-connect/#23-create-openid-connect-providers","title":"2.3 Create OpenID Connect providers","text":"<p>Each OpenID Connect provider service must be created and configured via the site for each provider e.g. Google, Microsoft etc. There are three providers which can be bootstrapped (added automatically when the aplication starts up) using the build configuration files.</p> <p>These three providers are: </p> <ul> <li> <p>Keycloak     Using <code>${openidConnectConfig.baseUrl}/realms/${openidConnectConfig.realm}/.well-known/openid-configuration</code></p> </li> <li> <p>Google     Using https://accounts.google.com/.well-known/openid-configuration</p> </li> <li> <p>Microsoft     Using https://login.microsoftonline.com/common/.well-known/openid-configuration)</p> </li> </ul>"},{"location":"plugins/user-guides/openid-connect/openid-connect/#24-add-providers-to-mauro","title":"2.4 Add providers to Mauro","text":"<p>Once the OpenID Connect provider service has been configured, the details can be entered into Mauro. Remember that you can have more than one of each type of provider as long as each has a unique Label. Click the white arrow next to your user icon and then select 'OpenID Connect' from the dropdown menu. This will bring up a list of 'Open ID Connect Providers'. Click the '+ Add' button and a 'Add OpenID Connect Provider' form will appear which you will need to complete. </p> <p></p> <ul> <li> <p>Label     An identifying label for this provider. This will also be the text used when displaying the login button in the login form, so use a suitable name</p> </li> <li> <p>Image URL (optional)     The URL to a public image that can be used as an icon for the login button.     A live preview button is available to test this field</p> </li> <li> <p>Client ID     The unique client ID that was provided by the OpenID Connect service to be used</p> </li> <li> <p>Client Secret     The unique client secret/password that was provided by the OpenID Connect service to be used</p> </li> </ul> <p>Next, enter the 'Discovery' details for the OpenID Connect service. There are two 'Discovery' forms:</p> <ul> <li> <p>Standard     Providing a URL to a standardised dicovery document allows Mauro to automatically connect to the service and ascertain the necessary endpoints required for authorization, obtaining tokens etc</p> </li> <li> <p>Non-Standard     If the service does not provide a discovery document, then the alternative is to manually enter the endpoints required</p> </li> </ul> <p></p> <p>Tick the 'Use discovery document for endpoints' checkbox as appropriate and enter the necessary details.</p> <p>Alternatively, you can enter advanced details by expanding the 'Authorization endpoint parameters' section. If not explicitly defined, Mauro will pick suitable defaults for these values. Once finished, click the 'Add provider' button to create the provider in Mauro.</p> <p></p>"},{"location":"plugins/user-guides/openid-connect/openid-connect/#25-editdelete-providers","title":"2.5 Edit/delete providers","text":"<p>Once created, OpenID Connect providers can be edited or deleted in Mauro through the 'OpenID Connect' administration page. Click the white arrow next to your user icon and then select 'OpenID Connect' from the dropdown menu. This will bring up a list of 'Open ID Connect Providers'. Click the vertical dot menu to the right of the provider you wish to edit or delete and select the relevant option from the dropdown menu. </p> <p></p>"},{"location":"plugins/user-guides/openid-connect/openid-connect/#3-testing","title":"3. Testing","text":"<p>Once the OpenID Connect providers have been added to Mauro, they can be tested by viewing the login form. Click the 'Log in' button at the top right of the navigation bar.</p> <p>A login form will appear and will now show the standard email/password fields, plus a list of all OpenID Connect providers added to Mauro. Click on any of these provider buttons to be automatically redirected to that provider service and authenticate using their account.</p> <p></p> <p>If successfully authenticated, the user will be automatically redirected back to Mauro and signed in just like any other user.</p>"},{"location":"resources/architecture/","title":"Technical Architecture","text":""},{"location":"resources/architecture/#overview","title":"Overview","text":"<p>Mauro Data Mapper is built using a fairly common layered design. At the heart is a standard, relational database where all data is primarily stored. </p> <p>All interaction with the database is controlled through business logic within the 'Core' layer. Interfaces around the core allow interaction at different levels of abstraction</p> <p></p>"},{"location":"resources/architecture/#relational-database","title":"Relational Database","text":"<p>Mauro Data Mapper has been built on top of PostgreSQL and in particular is tried and tested against PostgreSQL version 12. However, we've taken care not to use any clever features or plugins so that the majority of the code should run against any recent version of PostgreSQL.</p> <p>Furthermore, since the interaction with the relational database is based upon the Hibernate ORM it could even be possible to rebuild against other database implementations, but we've yet to try. The only exception is during integration testing, in which an in-memory h2 database is used in order to speed up testing.</p> <p>System administrators with access to the database can access the data directly, and this is the preferred route for taking backups. However, editing or interpreting the data directly through the database is not recommended, as this will bypass the business logic in the core, with potential loss of system integrity.</p>"},{"location":"resources/architecture/#core","title":"Core","text":"<p>The Core component is built using Grails (version 4), which is a Java-based Model-View-Controller framework. Code is typically written in Groovy, which itself compiles down to Java. Much of the Grails framework is built on top of the widely-used Spring components.</p> <p>The Core codebase defines the object-oriented domain model, which specifies the structure and constraints on the underlying model. All program logic is contained within Services and Controllers, with Views defining the structure of any outputs to procedures or requests.</p>"},{"location":"resources/architecture/#rest-api","title":"REST API","text":"<p>The REST API is a logical layer, defined completely within the Core component and is the standard way of interacting with the platform. A standard REST-style interface makes use of standard HTTP commands, for example <code>GET</code>, <code>POST</code>, <code>PUT</code>, and <code>DELETE</code>. Each REST endpoint is defined by a Controller and View within the Grails Core. Some endpoints are aliased for ease of use, or backwards compatibility, and there is genericity built in to make programming against the API easier. Plugins may extend the API with new endpoints.</p> <p>Each endpoint typically receives and responds in JSON; some can use XML but this is less well tested. Custom data formats apply in particular circumstances - for example when dealing with file attachments.</p>"},{"location":"resources/architecture/#programming-apis","title":"Programming APIs","text":"<p>The programming APIs wrap REST commands in programming constructs to make it easier for programmers to interact with Mauro Data Mapper without being concerned with the technical details of the REST API. Of the three current APIs, the Java library is most mature and is able to re-use components of the Grails Core for a faithful representation of the underlying object model. </p> <p>The Java API is suitable for programming complex import and export routines and has built in support for a number of batch operations that are not easily achieved through the web user interface. The Java client also supports connections to multiple instances, making it a good tool for implementing more sophisticated federation mechanisms.</p> <p>The Typescript API is a much simpler wrapper around those endpoints used by the web interface. It is hosted as a separate component and can be installed using <code>npm</code>, the standard package manager for javascript applications.</p>"},{"location":"resources/architecture/#user-interface","title":"User Interface","text":"<p>The web-based user interface is defined in Angular 14 using the Angular Material library for look-and-feel. It is a self-contained, single-page web application which makes use of some additional typescript libraries (for rendering diagrams, providing notifications, etc) on top of the standard ones provided by Angular. </p> <p>It only uses the REST API (via the typescript client library) to communicate with the Core, and is built in a modular fashion to allow easy extensibility. Many of the components can be easily re-used in the creation of other web interfaces.</p>"},{"location":"resources/architecture/#grails-plugins","title":"Grails Plugins","text":"<p>The Grails Core provides an easy mechanism for extension, through standard Grails plugins. A number of pre-defined extension points are available. For example, to implement new importers, exporters, profiles, or authentication mechanisms. However plugins may also arbitrarily extend the REST API with custom functionality, making use of the services and controllers defined within the core. </p> <p>A number of plugins are defined and made available through the central GitHub plugins organisation; developers may feel free to use and adapt those, or write their own, sharing if they wish. Note that some technical plugins such as the SPARQL or Apache Freemarker plugins allow users to perform their own queries against the database and arbitrarily complex queries may affect the performance of the server.</p> <p>The Core component is itself made up of a number of plugins, and can be disassembled for particular use cases. For example, it is possible to disable support for API Keys by compiling a version of the Core with that plugin removed.</p>"},{"location":"resources/architecture/#search-index","title":"Search Index","text":"<p>The search index improves the performance of searching, this content is stored in-memory (and persisted to files on disk at suitable intervals). In some places in the Core, it may also be used to speed up access to particular model contents. </p> <p>The index is built using Apache Lucene but managed in the code through Hibernate. This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary.</p> <p>The contents of the search index can be hard to inspect for debugging purposes! We use a tool called Marple but be sure to use a compatible version!</p>"},{"location":"resources/client/java/","title":"Groovy / Java","text":""},{"location":"resources/client/java/#introduction","title":"Introduction","text":"<p>The Java / Groovy client library wraps the REST API in Java methods to make it easy for Java developers to interact with a Mauro instance. The library makes use of the mdm-core grails application, essentially loading a local, in-memory copy of the Mauro Core to take advantage of the services and controllers from Core, as well as re-using the domain model, and in-built validation.</p> <p>The REST API library can connect to a remote Mauro instance, and is typically used to perform bulk operations such as importing and exporting models, such as when scripting a complex import. This is often the easiest way to experiment with importing before building a Grails Plugin. The code can also be used to connect to multiple Mauro instances, for example to copy models from one instance to another.</p>"},{"location":"resources/client/java/#api-documentation","title":"API documentation","text":"<p>The API is documented using GroovyDoc and the complete documentation can be found here.</p>"},{"location":"resources/client/java/#add-dependency","title":"Add dependency","text":"<p>In order to include the Mauro client library in your Java / Groovy project, use the following dependency in Gradle or Maven:</p> Gradle <pre><code>repositories {\n    ...\n    maven {url \"https://jenkins.cs.ox.ac.uk/artifactory/libs-release\"}\n    ...\n}\n\ndependencies {\n    ...\n    compile \"uk.ac.ox.softeng.maurodatamapper:mdm-api-java-restful:{version}\"\n}\n</code></pre> Maven <pre><code>&lt;distributionManagement&gt;\n&lt;snapshotRepository&gt;\n&lt;id&gt;snapshots&lt;/id&gt;\n&lt;name&gt;jenkins.cs.ox.ac.uk-snapshots&lt;/name&gt;\n&lt;url&gt;http://jenkins.cs.ox.ac.uk/artifactory/libs-snapshot-local&lt;/url&gt;\n&lt;/snapshotRepository&gt;\n...\n&lt;/distributionManagement&gt;\n...\n&lt;dependency&gt;\n&lt;groupId&gt;uk.ac.ox.softeng.maurodatamapper.plugins&lt;/groupId&gt;\n&lt;artifactId&gt;mdm-api-java-restful&lt;/artifactId&gt;\n&lt;version&gt;{version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Look on our Release Notes page to find the latest version number</p>"},{"location":"resources/client/java/#getting-started","title":"Getting started","text":"<p>Information</p> <p>The examples given here are in Groovy. Conversion to equivalent Java is a fairly simple task.</p> <p>The simplest way to get started is by creating a client manually - using the url of the server that the instance is hosted on, and a username / password.</p> <pre><code>import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient\n\nclass TestConnection {\n\nstatic void main(String[] args) {\nString baseUrl = \"http://localhost:8080\"\nString username = \"...\"\nString password = \"...\"\nnew DataMapperClient(baseUrl, username, password).withCloseable { client -&gt;\n// client is now connected with a session to a Mauro instance\n// Now we can do things with our client\nclient.createFolder(\"Test Folder\")\n}\n}\n}\n</code></pre> <p>The <code>BindingMauroDataMapperClient</code> creates a connection to a Mauro instance, and maintains a session where necessary. The class implements the <code>Closeable</code> interface which means that any session will be closed at the end of the <code>withCloseable</code> closure.</p>"},{"location":"resources/client/java/#authentication-and-passing-arguments","title":"Authentication and passing arguments","text":"<p>As in the example above, you can connect to the Mauro instance using a username and password. In this case, any session cookie returned will be stored and used for future calls automatically. You can also connect using an API Key  and this will be passed in the parameters for every call.</p> <p>It's usually more convenient to pass arguments such as usernames, passwords or API keys in as parameters, rather than hard-coding them into the  application.  We take advantage of PicoCli to provide options for passing these parameters to an application.</p> <p>In the most basic case, consider the following application:</p> <pre><code>import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient\n\nclass TestConnection2 extends MdmCommandLineTool&lt;MdmConnectionOptions&gt; {\n\nstatic void main(String[] args) {\nTestConnection2 testConnection2 = new TestConnection2(args)\nTestConnection2.doStuff()\n}\n\nvoid doStuff() {\noptions.getMauroDataMapperClient().withCloseable { client -&gt;\n// Do something with our client here...\n}\n}\n}\n</code></pre> <p>The class extends <code>MdmCommandLineTool</code>, which provides an object of type <code>MdmConnectionOptions</code>, representing the options passed in on the command  line.  This class contains definitions for the following options:</p> -U, --clientBaseUrl, --client.baseUrl The base url of the Mauro instance to connect to - for example <code>http://www.example.com/mauro/</code>.  Any trailing <code>/api</code> will be added automatically. -u, --clientUsername, --client.username The username for logging into this Mauro instance. -p, --clientPassword, --client.password The password for logging into this Mauro instance. -a, --clientApiKey, --client.apiKey The API key for logging into this Mauro instance. -h, --help Displays a help message describing these options -v, --verbose Runs the application in 'verbose' mode, giving additional logging for debug purposes -D, --debug Provide advanced debug information as logs -P, --properties Provides further parameters via a properties file - for example: <code>--properties ./config.properties</code>.  See below for more details. <p>The final option allows these properties to be passed in a standard java properties file.  Properties should be provided using the format provided  in the final options above - for example:</p> <p><pre><code>client.baseUrl=http://localhost:8080/\nclient.apiKey=767c6e02-4ad6-4480-8b42-a36160143a24\n</code></pre> The <code>MdmConnectionOptions</code> class provides a default mechanism to create a new <code>BindingMauroDataMapperClient</code> from the provided values. The class can be extended to provide additional, application-specific options.  To provide specific functionality for these options, you can also  extend the <code>MdmCommandLineTool</code> class.</p>"},{"location":"resources/client/java/#dealing-with-multiple-connections","title":"Dealing with multiple connections","text":"<p>Occasionally, it's useful to deal with multiple connections to a single catalogue, or connections to more than one catalogue instance.  The client  can store multiple named connections and all methods have an optional final parameter to choose which connection to use.</p> <p>For example, the following code connects to two instances of Mauro, naming the connections as 'source' and 'target':</p> <pre><code>BindingMauroDataMapperClient bindingMauroDataMapperClient = new BindingMauroDataMapperClient()\nbindingMauroDataMapperClient.openConnection(\"source\", sourceProperties)\nbindingMauroDataMapperClient.openConnection(\"target\", targetProperties)\n</code></pre> <p>Subsequent method calls can pass the name of the connection as a final argument - for example the following code copies a DataModel from the  \"source\" instance of Mauro to the \"target\":</p> <pre><code>DataModel dataModel = bindingMauroDataMapperClient.exportAndBindDataModelById(dataModelId, \"source\")\n\nbindingMauroDataMapperClient.importDataModel(\ndataModel, folderId, dataModelName, finalised, importAsNewDocumentationVersion, \"target\")\n</code></pre> <p>In every method, if no connection is specified by name, the default connection (internally named \"_default\") is used.</p>"},{"location":"resources/client/java/#binding-vs-non-binding-clients","title":"Binding vs. non-binding clients","text":"<p>The library provides two different clients: the first is the simpler <code>MauroDataMapperClient</code>.  This provides a number of methods for interacting with the Mauro REST API in a more native form: dealing with responses in <code>Map</code> form.  The alternative is the more complex <code>BindingMauroDataMapperClient</code> which extends <code>MauroDataMapperClient</code> with additional methods for binding responses into the appropriate Mauro domain types.</p> <p>For example, compare the following methods:</p> <p><code>Map exportDataModel(UUID id, String connectionName = defaultConnectionName)</code></p> <p>and</p> <p><code>DataModel exportAndBindDataModelById(UUID id, String connectionName = defaultConnectionName)</code></p> <p>The two methods access the same REST endpoint: the former returns the response JSON in map form; the latter takes that response and binds it to an  object of class <code>DataModel</code>.  The latter is obviously easier to process, but the former provides a faster response.  </p> <p>The former variant also provides an important advantage:  when a Map is bound to a Data Model within grails, it is associated with the internal,  in-memory database and validated.  At this point, any identifiers, or 'last modified' dates associated with any component of that DataModel will  be dropped in favour of local variants.  So, for example, if you wanted to re-use the identifiers of the DataClasses contained within that  DataModel (for example, in order to update their descriptions individually in the remote instance), then you should use the non-binding version of  the method.</p> <p>In general, since the <code>BindingMauroDataMapperClient</code> extends the <code>MauroDataMapperClient</code> class, the binding version is all that is required.  The  binding client has additional methods for manually binding results of calls after intermediate processing. </p>"},{"location":"resources/client/net/","title":".NET","text":"<p>The .Net client library wraps the REST API in .net methods to make it easy for .Net developers to interact with a Mauro instance. The library makes use of the .Net Core API application with Controller &amp; Model to call API methods</p> <p>The REST API library can connect to a remote Mauro instance, and is typically used to perform bulk operations such as importing and exporting models. The code can also be used to connect to multiple Mauro instances, for example to copy models from one instance to another.</p>"},{"location":"resources/client/net/#api-documentation","title":"API documentation","text":"<p>The API documentation is built using DocFx and is available here:</p>"},{"location":"resources/client/net/#adding-as-a-dependency","title":"Adding as a dependency","text":"<p>In order to include the Mauro client library in your .Net project, you will need to include the following dependencies:</p> <ul> <li>CsvHelper</li> <li>Newtonsoft.Json</li> <li>System.Text.Json</li> </ul> <p>A .dll file built from the code can be referenced as <code>mdm-api-dotnet-restful</code></p>"},{"location":"resources/client/net/#api-client","title":"API Client","text":"<p>MauroDataMapperClient provides constructors to login either by</p> <ol> <li>UserId, Password, BaseUrl &amp; Connection Name</li> <li>Properties Object with Properties - UserName, Password &amp; BaseUrl</li> </ol> <p>If you connect to an instance using a username and password, any session cookie returned will be stored and used for future calls automatically. It's usually more convenient to pass arguments such as usernames, passwords as command-line parameters, rather than hard-coding them into the application.</p>"},{"location":"resources/client/net/#api-client-methods","title":"API Client Methods","text":"<p>The methods of the API Client make use of the <code>System.Net.Http</code> package. This library is described in more detail here. Each accepts an object of the class <code>HttpRequestMessage</code>, and each returns an instance of <code>HttpResponseMessage</code></p>"},{"location":"resources/client/net/#net-console-application","title":".Net Console Application","text":"<p>The <code>mdm-api-dotnet-console</code> application has been built as a Windows-friendly tool for bulk CSV import.  In order to include this as part of an  application, you need to add a reference to the <code>mdm-api-dotnet-restful</code> .dll file.</p> <p>This application has its own configuration settings:</p> <ul> <li>Folder &amp; File location</li> <li>Whether to create each CSV definition as a DataClass or a DataModel</li> </ul> <p>The server address &amp; user credentials are read from the <code>app.config</code> file. Once files are read, and their definitions uploaded to Mauro, they  are moved to the 'Archive' folder.</p>"},{"location":"resources/client/typescript/","title":"TypeScript","text":""},{"location":"resources/client/typescript/#introduction","title":"Introduction","text":"<p>The TypeScript client library wraps the REST API in TypeScript classes/functions to make it easy for  JavaScript and TypeScript developers to interact with a Mauro instance. </p> <p>The TypeScript library that implements communication with the back-end  server is available as a standalone repository for incorporation into other applications. For example other web interfaces, or back-end  functionality using <code>node.js</code>. This is in fact the client library that the Mauro Data Mapper user interface uses.</p> <p>The GitHub repository is called <code>mdm-resources</code> and is available within the Mauro Data Mapper organisation.</p>"},{"location":"resources/client/typescript/#api-documentation","title":"API documentation","text":"<p>The API is documented using TypeDoc and the complete documentation can be found here.</p>"},{"location":"resources/client/typescript/#layout","title":"Layout","text":"<p>Methods to call API functions are roughly broken down by resource type, with filenames conforming to the pattern:</p> <p><code>mdm-{resourceType}.resource.ts</code></p> <p>There are additional utility functions available in <code>mdm-resource.ts</code>. There are also type definitions to assist with requests and responses, which can be found in filenames of the format <code>mdm-{resourceType}.model.ts</code>.  An <code>index.ts</code> file lists all files for inclusion.</p>"},{"location":"resources/client/typescript/#resources","title":"Resources","text":"<p>Each <code>mdm-{resourceType}.resource.ts</code> file defines a new class extending the super class MdmResource, and provides  methods for each endpoint.  These make use of the simpleGet(),  simplePost(), etc methods defined in the super class.</p> <p>Every class that extends MdmResource can optionally provide these in the  constructor:</p> <ul> <li>MdmResourcesConfiguration - object to define configuration options for every HTTP request.</li> <li>MdmRestHandler - object to the REST handler that will process the requests. If not provided, the DefaultMdmRestHandler will be used - see the REST Handlers section for further  details.</li> </ul>"},{"location":"resources/client/typescript/#including-in-applications","title":"Including in applications","text":"<p>If you are using NPM or Yarn, then you need the following line in your <code>.npmrc</code> or <code>.yarnrc</code> file:</p> <pre><code>@maurodatamapper:registry=https://npm.pkg.github.com`\n</code></pre> <p>You can then add a line such as the following to your <code>package.json</code> file:</p> <pre><code>\"dependencies\" : {\n...  \"@maurodatamapper/mdm-resources\": \"github:MauroDataMapper/mdm-resources#{version}\"\n}\n</code></pre> <p>Where <code>{version}</code> refers to a git tag or branch name.  </p> <p>Within a TypeScript file, you can then add an <code>import</code> statement such as the following:</p> <pre><code>import { MdmResourcesConfiguration } from '@maurodatamapper/mdm-resources';\n</code></pre> <p>Or, as illustrated in the Mauro UI application, create a custom service to pull all the classes into a single location (see  <code>mdm-resources.service.ts</code> within the <code>mdm-ui</code> project).</p>"},{"location":"resources/client/typescript/#rest-handlers","title":"REST Handlers","text":"<p><code>mdm-resources</code> provides a default implementation of the MdmRestHandler called  DefaultMdmRestHandler. This implementation uses the  fetch API to complete HTTP requests and return  promises on each response.</p> <p>This default implementation is usually sufficient for most scenarios, but it is also possible to replace this with your own implementation. Reasons why you might want to do this are:</p> <ol> <li>To use something other than <code>fetch()</code>. For example, Angular applications tend to use the built-in  HTTP Client to return observable streams instead of promises.</li> <li>To intercept any Mauro HTTP requests/responses to perform some custom operations, such as error handling on failed responses.</li> </ol> <p>To use a custom REST handler, follow the steps below:</p> <pre><code>// Define the class that implements `IMdmRestHandler`\nexport class CustomMdmRestHandler implements IMdmRestHandler {\nprocess(url: string, options: IMdmRestHandlerOptions) {\n// (Optional) pre-process step (e.g. logging)\n\nconst response = /* Send HTTP request */\n\n// (Optional) post-process step (e.g. logging, error handling)\n\nreturn response;\n}\n}\n\n// For every MDM resource created, pass in the custom REST handler instance instead\nconst dataModelsResource = new MdmDataModelResource(null, new CustomMdmRestHandler());\n</code></pre>"},{"location":"resources/client/typescript/#handling-responses","title":"Handling Responses","text":"<p>All endpoints have type definitions that explicitly state their inputs for requests but generalise their response outputs. This is due to the ability to customise the REST handler, which may return different wrapper objects around the core response definitions. Nonetheless, <code>mdm-resources</code> does provide type definitions for responses and will include them in the documentation comments and type reference documentation for the use of the downstream developer.</p> <p>As an example, given this endpoint function type definition:</p> <pre><code>export class MdmDataModelResource extends MdmResource {\nget(dataModelId: string, query?: QueryParameters, options?: RequestSettings): any {\n//...\n}\n}\n</code></pre> <p>Response types can be explicitly added to return results so that further type checking can be performed, as in these examples:</p> fetch <pre><code>const dataModels = new MdmDataModelResource();\ndataModels\n.get('679d582c-9f6c-4ce5-99c7-7fad79374637')\n.then((response: DataModelDetailResponse) =&gt; {\n// MdmDataModelResponse.get() states it will return `any` but will actually return\n// Promise&lt;DataModelDetailResponse&gt;.\n\n// Adding explicit type information will resolve further code that uses the response.\n\n// Do something here with the response...\n})\n</code></pre> Angular <pre><code>const dataModels = new MdmDataModelResource();\ndataModels\n.get('679d582c-9f6c-4ce5-99c7-7fad79374637')\n.subscribe((response: DataModelDetailResponse) =&gt; {\n// MdmDataModelResponse.get() states it will return `any` but will actually return\n// Observable&lt;DataModelDetailResponse&gt;.\n\n// Adding explicit type information will resolve further code that uses the response.\n\n// Do something here with the response...\n})\n</code></pre>"},{"location":"rest-api/admin/","title":"Admin functions","text":"<p>There are a number of endpoints which are specific to administrators: understanding the configuration of the particular instance; discovering the  avaiable plugins, etc.</p>"},{"location":"rest-api/admin/#currently-logged-in-users","title":"Currently logged in users","text":"<p>/api/admin/activeSessions</p> <p>This endpoint returns a list of all logged in users.</p> <p>/api/admin/activeSessions</p> <p>If called in <code>post</code> mode, you can pass in user credentials, rather than basing on an existing session.</p>"},{"location":"rest-api/admin/#configuration","title":"Configuration","text":"<p>To find out more about the current instance of the catalogue: what version is running; the version of Java that's runing; the JDBC drivers  currently available; call the following endpoint:</p> <p>/api/admin/status</p>"},{"location":"rest-api/admin/#modules","title":"Modules","text":"<p>To find out which modules are installed, call the following endpoint:</p> <p>/api/admin/modules</p> <p>The <code>post</code> version of the endpoint can be called in order to pass authentication credentials at the same time:</p> <p>/api/admin/modules</p>"},{"location":"rest-api/admin/#plugins","title":"Plugins","text":"<p>To find out which plugins are currently installed, use one of the following endpoints:</p> <p>/api/admin/plugins/exporters /api/admin/plugins/emailers /api/admin/plugins/dataLoaders /api/admin/plugins/importers</p>"},{"location":"rest-api/admin/#system-actions","title":"System actions","text":"<p>/api/admin/rebuildHibernateSearchIndexes</p> <p>This endpoint forces the rebuild of the indexes used for searching (built using Hibernate Search and Lucene).  This is only necessary when synchronisation between database and indexes is lost; when the search functionality is not returning correct results.  Authentication credentials can be passed as part of the request body.</p>"},{"location":"rest-api/admin/#properties","title":"Properties","text":"<p>There are a number of system-wide properties that can be updated by administrators, such as the text of any emails sent and the email address  from which catalogue emails appear to be sent.</p> <p>Properties are composed of keys and values. Keys can be any string with the following restrictions:</p> <ul> <li>Must be lowercase alpha characters</li> <li>No spaces are allowed</li> <li>May include periods ('.') and/or underscores ('_')</li> <li>Must be unique</li> </ul>"},{"location":"rest-api/admin/#getting-properties","title":"Getting properties","text":"<p>Properties can be viewed at the following endpoint:</p> <p>/api/admin/properties</p> <p>If successful, the response body will list the available properties:</p> Response body (JSON) <pre><code>{\n\"count\": X,\n\"items\": [\n{\n\"id\": \"c7de1358-a4ce-4d72-abca-04013f7f4acc\",\n\"key\": \"test.property\",\n\"value\": \"Test value\",\n\"category\": \"Test\",\n\"publiclyVisible\": false,\n\"lastUpdatedBy\": \"admin@test.com\",\n\"createdBy\": \"admin@test.com\",\n\"lastUpdated\": \"2021-03-10T15:17:05.459Z\"\n},\n{\n\"id\": \"76becaa3-da04-40d5-a433-51ed203c77b4\",\n\"key\": \"test.property.public\",\n\"value\": \"Public test value\",\n\"category\": \"Test\",\n\"publiclyVisible\": true,\n\"lastUpdatedBy\": \"admin@test.com\",\n\"createdBy\": \"admin@test.com\",\n\"lastUpdated\": \"2021-03-10T15:17:05.558Z\"\n}\n]\n}\n</code></pre> <p>Notice that properties contain a <code>publiclyVisible</code> flag.  This is because properties can be created to either be public or restricted to administrators/systems  (the default being <code>false</code>). Only an authenticated session can use the endpoint above, however an anonymous session may use this endpoint to list all publicly available properties:</p> <p>/api/properties</p> <p>To access a single property, this endpoint is provided:</p> <p>/api/admin/properties/{propertyId}</p>"},{"location":"rest-api/admin/#created-updating-and-deleting","title":"Created, updating and deleting","text":"<p>Properties can be created as follows:</p> <p>/api/admin/properties</p> Request body (JSON) <pre><code>{\n\"key\": \"test.property\",\n\"value\": \"Test value\",\n\"publiclyVisible\": false,\n\"category\": \"Test\"\n}\n</code></pre> <p>If successful, the new property is returned in the response body including the new property <code>id</code>.</p> Response body (JSON) <pre><code>{\n\"id\": \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\",\n\"key\": \"test.property\",\n\"value\": \"Test value\",\n\"category\": \"Test\",\n\"publiclyVisible\": false,\n\"lastUpdatedBy\": \"admin@test.com\",\n\"createdBy\": \"admin@test.com\",\n\"lastUpdated\": \"2021-03-11T17:46:47.654Z\"\n}\n</code></pre> <p>The property can then be updated with the <code>put</code> endpoint:</p> <p>/api/admin/properties</p> Request body (JSON) <pre><code>{\n\"id\": \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\",\n\"key\": \"test.property\",\n\"value\": \"Test value\",\n\"publiclyVisible\": false,\n\"category\": \"Test\"\n}\n</code></pre> <p>And deleted with the <code>delete</code> endpoint:</p> <p>/api/admin/properties/{propertyId}</p>"},{"location":"rest-api/admin/#data-models","title":"Data Models","text":"<p>The following endpoints provide paginated lists of Data Models (for cleaning / monitoring processes).  They list those models which have been  deleted, superseded by a new model, and superseded by new documentation, respectively:</p> <p>/api/admin/dataModels/deleted /api/admin/dataModels/modelSuperseded /api/admin/dataModels/documentSuperseded</p>"},{"location":"rest-api/admin/#emails","title":"Emails","text":"<p>Retrieve the list of emails (recipient, message, date/time) sent by the system:</p> <p>/api/admin/emails</p>"},{"location":"rest-api/apikeys/","title":"API Keys","text":"<p>API keys offer an alternative way to authenticate to the Mauro Data Mapper REST API instead of logging in with a username and password and saving session cookies. This is the recommended method for authenticating when you:</p> <ul> <li>Have long-running processing scripts which could cause sessions to timeout between calls</li> <li>Need to store authentication details in clear text for an external application to use</li> </ul> <p>Each user can create multiple API keys, and so when sharing with multiple applications, can disable access individually. API keys are also  configured with a default expiry date for additional security.   </p>"},{"location":"rest-api/apikeys/#creating-an-api-key","title":"Creating an API Key","text":"<p>API keys may be set up through the web interface or via the API. To generate a first API key, the user must be logged in using a username and  password - either through the web interface, or through the REST API.  </p> <p>On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu.</p> <p></p> <p>This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys.</p> <p></p> <p>For each API belonging to the user, the list displays:</p> <ul> <li> <p>Name   This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys    used for different purposes.</p> </li> <li> <p>Key   This is the key itself, which is a UUID, unique to this user. Keys can be copied to your clipboard by clicking 'Copy' on the right of the key box.</p> </li> <li> <p>Expiry date   This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be    'refreshed' before expiry.</p> </li> <li> <p>Refreshable   Specifies whether an API key can be refreshed once it has expired.</p> </li> <li> <p>Status   Details whether an API key is 'Active' or 'Disabled'.</p> </li> </ul> <p>To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete.</p> <p>Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes.</p> <p></p>"},{"location":"rest-api/apikeys/#using-an-api-key","title":"Using an API Key","text":"<p>To use an API Key, simply add it into the headers of any REST API call.  </p> <p>Info</p> <p>If you use API keys to authenticate, the session cookies are not used to persist identity and so the key should be passed with every call.</p> <p>The header key should be <code>apiKey</code> and the value should be the UUID value of the API key itself.</p>"},{"location":"rest-api/apikeys/#using-postman","title":"Using Postman","text":"<p>If you are using Postman as a client, there are two ways to configure the API key for a request, which both have the same result.  </p> <p>Firstly, select the 'Authorization' tab which will display several fields that you need to complete. From the 'TYPE' dropdown menu, select 'API Key'. In the 'Key' box on the right hand side type <code>apiKey</code>. Enter the value of the API key in the 'Value' field and select 'Header' from the 'Add to' dropdown menu.</p> <p></p> <p>The API key must be passed in the headers, not in the query parameters, which is the second method. This method sets the headers automatically, although you can also set them manually. Select the 'Headers' tab which will display a list of Keys and Values. Again, set the 'Key' field to <code>apiKey</code> and the 'Value' field to the API key value.   </p> <p></p>"},{"location":"rest-api/apikeys/#refreshing-an-expired-api-key","title":"Refreshing an expired API key","text":"<p>When an API key has expired and it has previously been marked as 'Refreshable', then it may be refreshed with a new expiry date. </p> <p>To do this, navigate to the list of 'API keys' via your user profile. Identify which API keys have 'API Key expired' in the 'Expiry date' column. Click the three vertical dots to the right of the relevant API key and you will now have the option to 'Refresh API Key' in the dropdown menu. Select this option and then enter a new number of days for expiry. </p> <p></p>"},{"location":"rest-api/apikeys/#revoking-an-api-key","title":"Revoking an API key","text":"<p>To revoke a particular API key, you can mark it as 'Disabled'. Navigate to the 'API Keys' list and click the three vertical dots to the right of the relevant API key. Select 'Disable' from the dropdown menu. The same option will allow you to re-enable the key if necessary.</p> <p>Info</p> <p>It is good practise to set up different API keys for each application. In this way it is easy to revoke access to a single application without having to recreate all other keys and update other application settings.</p>"},{"location":"rest-api/apikeys/#managing-keys-through-the-rest-api","title":"Managing keys through the REST API","text":"<p>Info</p> <p>Note that API Keys can only be managed by the user that they belong to.</p> <p>Once authenticated, the endpoint for listing existing API keys is:</p> <p>/api/catalogueUsers/{catalogueUserId}/apiKeys</p> <p>This returns a paginated list of API keys as follows:</p> Response body (JSON) <pre><code>{\n\"count\": X,\n\"items\": [\n{\n\"id\": \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\",\n\"apiKey\": \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\",\n\"name\": \"Test API Key\",\n\"expiryDate\": \"2022-02-03\",\n\"expired\": false,\n\"disabled\": true,\n\"refreshable\": false,\n\"createdDate\": \"2021-02-03\"\n},\n...\n]\n}\n</code></pre> <p>The parameters are as described above. The <code>id</code> field is the global primary key identifer for the key.</p> <p>To create a new API key, post to the following endpoint:</p> <p>/api/catalogueUsers/{userId}/apiKeys</p> <p>The body of the post method should be structured as follows:</p> Request body (JSON) <pre><code>{\n\"name\":\"My Name\",\n\"expiresInDays\":365,\n\"refreshable\":true\n}\n</code></pre> <p>Where the parameters are as described above.</p> <p>To enable an existing, disabled API key, you can use it's ID (as described above), with the following endpoint:</p> <p>/api/catalogueUsers/{catalogueUserId}/apiKeys/{apiKeyId}/enable </p> <p>Similarly, to disable an existing, enabled API key, use the following:</p> <p>/api/catalogueUsers/{catalogueUserId}/apiKeys/{apiKeyId}/disable</p> <p>To refresh an API key, provide the number of days before the next expiry with the following endpoint:</p> <p>/api/catalogueUsers/{catalogueUserId}/apiKeys/{apiKeyId}/refresh/{expiresInDays} </p> <p>Finally, to delete an API key identified by a particular UUID:</p> <p>/api/catalogueUsers/{catalogueUserId}/apiKeys/{id}</p>"},{"location":"rest-api/authentication/","title":"Authentication","text":"<p>Mauro Data Mapper stores content that may be either publicly accessible, or have access restricted to particular users or groups.  Therefore  the majority of API requests can be made as an 'anonymous user' (by passing no session information in the request header), or as a 'logged in' user (by passing a valid session key in the request headers).</p> <p>A request to the <code>login</code> will result in a new session token being generated.  This is typically 32 hexadecimal characters in length, and uniquely  identifies the current session.  These tokens should not be shared, and will automatically expire with 30mins of inactivity.  Sessions can be  manually terminated through a call to the <code>logout</code> resource.   At any point the validity of a session may be checked against the server.</p>"},{"location":"rest-api/authentication/#login","title":"Login","text":"<p>To login to the server, <code>POST</code> to the following API endpoint: </p> <p>/api/authentication/login</p> <p>The request body should contain the username, and the password.  The username is not case-sensitive:</p> Request body (JSON) <pre><code>{ \"username\" : \"joe.bloggs@test.com\",\n\"password\" : \"pa55w0rd\"\n}\n</code></pre> Request body (XML) <pre><code>&lt;user&gt;\n&lt;username&gt;joe.bloggs@test.com&lt;/username&gt;\n&lt;password&gt;pa55w0rd\"&lt;/password&gt;\n&lt;/user&gt;\n</code></pre> <p>Information</p> <p>There is an alternative request body that can be sent if authenticating via OpenID Connect identity providers. Please read the OpenID Connect plugin information page for further details.</p> <p>If successful, the response body will contain the user's <code>id</code>, email address, first and last names, and whether or not that  user's account has been disabled (typically false in the case of a successful login).</p> Response body (JSON) <pre><code>{\n\"id\": \"01234567-0123-0123-0123-01234567\",\n\"emailAddress\": \"joe.bloggs@test.com\",\n\"firstName\": \"Joe\",\n\"lastName\": \"Bloggs\",\n\"pending\": false,        \"disabled\": false,\n\"createdBy\": \"admin@test.com\"\n}\n</code></pre> <p>One of the response headers will also contain an identifier for the new session.  The header is of the form:</p> <pre><code>Set-Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B; Path=/; HttpOnly\n</code></pre> <p>The session id (JSESSIONID) can be passed to any subsequent request to ensure that the user's credentials are used.  To supply the cookie, it  should be placed in the <code>Cookie</code> request header:</p> <pre><code>Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B\n</code></pre> <p>Further requests without the session cookie will be treated as anonymous requests.</p>"},{"location":"rest-api/authentication/#session-validation","title":"Session validation","text":"<p>In order to validate whether a session is currently active, or has expired (by logging out, or timed-out due to inactivity): </p> <p>/api/session/isAuthenticated </p> <p>No request body or parameters are required for this request.  The response will include a <code>true</code> or <code>false</code> value depending on the validity of the  session whose <code>JSESSIONID</code> is passed in as part of the request headers.</p> Response body (JSON) <pre><code>{\n\"authenticatedSession\": true\n}\n</code></pre>"},{"location":"rest-api/authentication/#administration-validation","title":"Administration validation","text":"<p>In order to validate whether a session is an administrative role:</p> <p>/api/session/isApplicationAdministration </p> <p>No request body or parameters are required for this request.  The response will include a <code>true</code> or <code>false</code> value depending on the validity of the  session whose <code>JSESSIONID</code> is passed in as part of the request headers.</p> Response body (JSON) <pre><code>{\n\"applicationAdministrationSession\": true\n}\n</code></pre>"},{"location":"rest-api/authentication/#logout","title":"Logout","text":"<p>Every session should ideally be closed manually, rather than leaving it to expire through inactivity.  In order to close a user session, you should  call the logout endpoint, again including the <code>JSESSIONID</code> cookie as part of the request headers:   </p> <p>/api/authentication/logout</p> <p>The response should include the status <code>204: No Content</code> and the successful response will be empty.</p>"},{"location":"rest-api/errors/","title":"Errors","text":"<p>The Mauro Data Mapper API uses standard HTTP response codes to indicate the success or failure of an API request.  In addition, some requests also return additional status information relating to the reasons for any error or  failure that has occurred.    </p> <p>In general, codes of the form <code>2XX</code> indicate success, codes of the form <code>4XX</code> indicate an error with the request, and codes of the form <code>5XX</code>  indicate that an error occurred with the server during the processing of a potentially valid request. Hopefully those in the last category are rare!</p> <p>Further details for each of the common error codes are shown in the tables below.</p>"},{"location":"rest-api/errors/#error-code-tables","title":"Error code tables","text":"Code Meaning Description 200 OK The response succeeded as expected 201 Created The <code>POST</code> method was successful and a new resource was created 204 No Content The server successfully processed the request, but no content was returned - for example when deleting a resource Code Meaning Description 400 Bad Request The server cannot process the request - either a required parameter was missing, or the body was badly formatted 401 Unauthorized The requested resource requires authentication, but none was provided as part of the header information 403 Forbidden The server refused to process the request because the authenticated user does not have the correct permissions 404 Not Found The resource requested could not be found.  This may be because the URL is malformed, or because the HTTP method was not permitted for this particular URL - for example <code>PUT</code> on a resource which may not be edited 408 Request Timeout The server gave up waiting for a request.  This code may occasionally be seen when the upload of a file takes longer than the server is prepared to wait 409 Conflict The server could not process the request because of some conflict in the current state of the resource.  Most commonly this occurs when a user tried to log in, despite already being logged in with a valid session Code Meaning Description 500 Internal Server Error This is a catch-all error message, when the request appears valid but the server was unable to process it.   This may well be caused by a bug in the software; such error messages may be reported through our issue-tracking software 502 Bad Gateway This is a system error relating to the server.  It may be that the Metadata Catalogue is configured incorrectly, or is otherwise not installed correctly 503 Service Unavailable The API server is currently unavailable.  It may have been taken down for maintenance, or is otherwise not running 504 Gateway Timeout See <code>502</code> - the server may be badly configured or is otherwise unavailable"},{"location":"rest-api/importexport/","title":"Import / Export","text":"<p>A number of plugins exist for importing and exporting Data Models, Data Flows, and Terminologies.  The endpoint for each import / export contains  the details of the plugin to be used, which includes the namespace, the name, and the version number. </p>"},{"location":"rest-api/importexport/#data-model","title":"Data Model","text":""},{"location":"rest-api/importexport/#import","title":"Import","text":"<p>The endpoint for importing one or more models is as follows:</p> <p>/api/dataModels/import/{importerNamespace}/{importerName}/{importerVersion}</p> <p>Each importer defines its own set of parameters, relating to the method of import.  For example, an XML or JSON import will require a file  upload; a SQL import will require a list of connection parameters in order to connect to a relational database.</p> <p>As the import parameters may involve file attachements, standard practise is to provide upload parameters as <code>multipart/form-data</code> format, which allows the attachment of files.     </p> <p>The standard importers available with a default installation are as follows:</p> Namespace Name Version ox.softeng.metadatacatalogue.plugins.excel ExcelDataModelImporterService 1.0.0 ox.softeng.metadatacatalogue.core.spi.xml XmlImporterService 2.2 ox.softeng.metadatacatalogue.core.spi.json JsonImporterService 1.1 ox.softeng.metadatacatalogue.plugins.database.postgres PostgresDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.oracle OracleDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.sqlserver SqlServerDatabaseImporterService 2.0.0 <p>These fall into two basic categories - simple file-based importers, or simple database-connection importers.  The parameters for each of these  types are detailed in the sections below.</p>"},{"location":"rest-api/importexport/#simple-file-based-importers","title":"Simple file-based importers","text":"<p>The simple file-based importers include the Excel, XML and JSON importers.  These take the following parameters:</p> Parameter Name Description <code>folderId</code> The UUID identifier for the folder that the new model is to be uploaded to.  This is mandatory. <code>finalised</code> A mandatory boolean value determining whether the new model is to be marked as finalised.  This determines whether the resulting model can be further edited within the interface. <code>importAsNewDocumentationVersion</code> A mandatory boolean value.  If this option is selected, then any models with the same name will be superseded.  If this option is not set, then the importer will produce an error if there are existing models with the same label. <code>importFile</code> The file containing the data to be imported - for example an XML file, JSON file, or Excel spreadsheet. <p>All fields are mandatory.</p>"},{"location":"rest-api/importexport/#simple-database-connection-importers-example","title":"Simple database-connection importers (example)","text":"<p>In order to connect to a database, fields are required to build the connection string, as well as handle the resulting generated model.  Each SQL importer is slightly different, but the SQL Server importer serves as an adequate example:</p> Parameter Name Description <code>folderId</code> The UUID identifier for the folder that the new model is to be uploaded to.  This is mandatory. <code>finalised</code> A mandatory boolean value determining whether the new model is to be marked as finalised.  This determines whether the resulting model can be further edited within the interface. <code>importAsNewDocumentationVersion</code> A mandatory boolean value.  If this option is selected, then any models with the same name will be superseded.  If this option is not set, then the importer will produce an error if there are existing models with the same label. <code>databaseHost</code> The hostname of the server that is running the database <code>databasePort</code> The port that the database is accessed through.  If none is set, then the default port for the specified database type will be used. <code>databaseNames</code> A comma-separated list of database names that are to be analysed and imported.  If multiple databases are specified, the same username and password will be used for all. <code>databaseUsername</code> The usesrname used to connect to the database <code>databasePassword</code> The password used to connect to the database <code>domain</code> The User Domain name for SQL Server.  This should be used rather than prefixing the username with <code>&lt;DOMAIN&gt;/&lt;username&gt;</code> <code>databasesSSL</code> Whether SSL should be used to connect to the database.  The default is false. <code>useNtlmv2</code> Whether to use NLTMv2 when connecting to the database. The default is false. <code>dataModelName</code> If a single database is imported, this field can be used to override its name <code>schemaNames</code> A comma-separated list of the schema names to import.  If not supplied, then all schemas other than 'sys' and 'INFORMATION_SCHEMA' will be imported <p>Other database-connecting import plugins provide a similar list of parameters, to be documented later.</p>"},{"location":"rest-api/importexport/#export","title":"Export","text":""},{"location":"rest-api/importexport/#data-flow","title":"Data Flow","text":""},{"location":"rest-api/importexport/#terminology","title":"Terminology","text":""},{"location":"rest-api/introduction/","title":"REST API Introduction","text":"<p>The Mauro Data Mapper API conforms to standard REST principles.  The API has  resource-oriented URLs, accepts XML and JSON body content (or form-encoded parameters where applicable), and can return data in XML or JSON  formats. Each call uses standard HTTP response codes, authentication, and verbs.</p>"},{"location":"rest-api/introduction/#requests","title":"Requests","text":"<p>To make a REST API request, you combine:</p> <ul> <li>The HTTP method: <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>PATCH</code> or <code>DELETE</code></li> <li>The URL to the API service - for example <code>http://modelcatalogue.cs.ox.ac.uk/demo/api</code></li> <li>The URL to a resource to query, update or delete</li> <li>One or more HTTP request headers, for example the identifier of any session token, or a request to save data back in XML  or  JSON format</li> </ul> <p>Most calls may also require a JSON or XML body representing any new or updated data, or query parameters to filter or restrict the response. </p>"},{"location":"rest-api/introduction/#http-request-headers","title":"HTTP request headers","text":"<p>The commonly-used HTTP request headers used are:</p>"},{"location":"rest-api/introduction/#accept","title":"Accept","text":"<p>This header determines the format of the response body for those requests with structured output.  The syntax is:</p> <pre><code>Accept: application/&lt;format&gt;\n</code></pre> <p>Where <code>&lt;format&gt;</code> can be either <code>xml</code> or <code>json</code>.</p> <p>By default, the format of the response body will match that of the request body, where applicable.  </p>"},{"location":"rest-api/introduction/#content-type","title":"Content-Type","text":"<p>This header specifies the format of the request body, where applicable.  The syntax is:</p> <pre><code>Content-Type: application/&lt;format&gt;\n</code></pre> <p>Where <code>&lt;format&gt;</code> can be either <code>xml</code> or <code>json</code>.</p> <p>By default, the request body is assumed to be JSON unless otherwise specified.</p>"},{"location":"rest-api/introduction/#cookie","title":"Cookie","text":"<p>This header stores the session identifier which persists a login between calls.  For example, having received a session cookie during  login, the token can be used to validate the user.  </p> <pre><code>Cookie: JSESSIONID=&lt;sessionid&gt;\n</code></pre> <p>Typically, a session identifier is 32 characters long and uses hexadecimal characters <code>0-9</code>, <code>A-F</code>.</p>"},{"location":"rest-api/introduction/#tools","title":"Tools","text":"<p>We use Postman for testing API calls during development.  It has an intuitive interface that lets you set  parameters, headers, and message bodies, and preview structured responses.  It can also be used as part of an automated testing or debugging requests.</p> <p>If you're looking for a more lightweight solution, curl is a suitable command-line tool which can be  easily configured to make complex REST API requests.  In this set of documentation, requests are illustrated with the appropriate curl command.</p>"},{"location":"rest-api/introduction/#testing","title":"Testing","text":"<p>There is a test API resource which will show whether the server API is running correctly, and whether the client has been correctly configured.  </p> <p>To test this using <code>curl</code>, run the following command:</p> <pre><code>curl -X GET http://localhost:8080/api/test </code></pre> <p>This will return the following JSON:</p> Response body (JSON) <pre><code>{\n\"message\":\"Not Found\",\n\"error\":404,\n\"path\":null,\n\"object\":null,\n\"id\":null\n}\n</code></pre>"},{"location":"rest-api/pagination/","title":"Pagination","text":"<p>The majority of requests for multiple objects have parameters to manage pagination.  By returning results in separate pages, we can minimise  network traffic and reduce the load on the server.  The size or limit (<code>max</code>) and starting position (<code>offset</code>) of each page can be passed in as a  parameter  to the query.  The response will always return the total number of objects, along with a list of 'items' corresponding to the specified 'page' of  results.</p>"},{"location":"rest-api/pagination/#parameter-format","title":"Parameter format","text":"<p>In these examples we consider the endpoint endpoint for listing all folders:</p> <p>/api/folders</p> <p>To manually specify the <code>offset</code> and <code>max</code> values, these should be passed as form parameters - for example the request:</p> <p>/api/folders?offset=10&amp;max=5</p> <p>Would return folders 10-14 inclusive in the overall list.</p> <p>To specify that all results should be returned, the boolean parameter <code>all</code> can be passed - for example the request:</p> <p>/api/folders?all=true</p> <p>Will return the complete list of visible folders.</p> <p>The <code>all</code> parameter is an alternative, and should not be specified at the same time as <code>offset</code> and <code>max</code>.</p>"},{"location":"rest-api/pagination/#response-format","title":"Response format","text":"<p>Again consider the endpoint endpoint for listing all folders described above.</p> <p>The response body would look something like:</p> Response body (JSON) <pre><code>{\n\"count\": n,\n\"items\": [\n{\n...\n},\n...\n]\n}\n</code></pre> <p>Where <code>n</code> is the total number of folders available.  The number of <code>items</code> returned will be at most <code>max</code> items.</p>"},{"location":"rest-api/pagination/#default-settings","title":"Default settings","text":"<p>If no parameters are passed, the default values are:</p> <ul> <li><code>offset</code> : 0</li> <li><code>max</code>: 10</li> </ul>"},{"location":"rest-api/postman/","title":"Postman Library","text":"<p>The Postman app is a tool for working with external APIs. Originally a plugin for Google Chrome, it now comes as a desktop app for all operating systems, as well as providing a web version.</p>"},{"location":"rest-api/postman/#downloading","title":"Downloading","text":"<p>The Mauro Postman repository provides definitions for Postman in order to test the Mauro APIs and contains sample environment configurations. To use, simply clone the repository into your local system, or download the files as follows:</p> <ul> <li>Navigate to the main branch of the GitHub repository</li> <li>Choose a folder to match the version of <code>mdm-core</code> that you intend to run against</li> <li>Download the two listed JSON files - one for the 'Collection' and another for the 'Environment'</li> </ul>"},{"location":"rest-api/postman/#using-the-collection","title":"Using the collection","text":"<p>Within the Postman app, choose 'File' -&gt; 'Import...' and under the 'File' tab, choose 'Upload Files'. Select the collection JSON file, and click 'Import' to import the new collection. If you've previously imported an older version of this collection, you are asked whether you wish to import as a new copy, or overwrite the previous version.</p> <p></p> <p>In the 'Collections' tab on the left hand side, you can now see a number of folders and subfolders containing configuration for Mauro API endpoints. By clicking on each you can view the endpoint, and optionally execute it against a given server.</p> <p></p> <p>Parameters to the call, including the server name, are indicated by double braces <code>{{ ... }}</code> - e.g. <code>{{base_url}}</code> in the URL of the endpoint.  </p> <p>To instantiate these parameters, you can either replace the text manually, or use an environment to provide consistent replacements across all endpoints. The Mauro Postman Environment provides some default values which can be customised.</p>"},{"location":"rest-api/postman/#using-the-environment","title":"Using the environment","text":"<p>To import the Mauro environment, choose 'File' -&gt; 'Import...' and under the 'File' tab, choose 'Upload Files'. Select the environment JSON file, and click 'Import' to import the new environment.</p> <p></p> <p>Once imported, the enviroment can be selected from the drop-down at the top-right of the screen.  To edit the environment, click 'Manage  Envronments'.  A number of parameters have been pre-set and can be edited here; new parameters can be added to suit your own usage.</p> <p></p>"},{"location":"rest-api/postman/#swagger-openapi","title":"Swagger, OpenAPI","text":"<p>We are yet to create a description of our APIs compatible with Swagger, or OpenAPI.  However, there are tools that should automate the conversion of  Postman collections to these formats - albeit in a manner specific to a particular environment or use case.  Some example tools include:</p> <ul> <li>APIMatic</li> <li>REST United</li> <li>APITransform</li> </ul> <p>The Mauro team have limited experience with these tools and so would welcome any feedback!</p>"},{"location":"rest-api/postman/#submitting-changes","title":"Submitting changes","text":"<p>The Postman library is not yet complete. There are endpoints undocumented, and plenty of improvements that could be made to the environment, or  particular usage scenarios we've not yet catered for.  </p> <p>If you've made changes to the Postman Library and think they would be of more general use,  please do consider submitting a pull request, so we can make them more widely available.</p>"},{"location":"rest-api/resources/catalogue-item/","title":"Catalogue item","text":"<p>A Catalogue Item in the catalogue is an abstract class containing properties that are common to most objects in the catalogue - for example  DataModels, DataClasses, DataElements, DataTypes, EnumerationValues, Terminologies, etc.  These properties include metadata (properties), summary  metadata, permissions, annotations (comments) and so on.  In some cases the url for each endpoint uses the word 'facet'; in others the data  type (DataModel, DataClass, etc) are used.  This page lists all the endpoints and describes the structure of each property. </p>"},{"location":"rest-api/resources/catalogue-item/#metadata","title":"Metadata","text":"<p>The metadata, or properties, of a Catalogue Item are extensible key/value pairs to store any further information about an object - including  technical properies, or field conforming to an external model.  A single item of metadata is structured as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"c9a36d30-2c6a-4dd0-a792-a337a2eca9c8\",\n\"namespace\": \"ox.softeng.metadatacatalogue.dataloaders.hdf\",\n\"key\": \"Volumes\",\n\"value\": \"Varies annually: in 2013/14, 18.2m finished consultant episodes (FCEs) and 15.5m Finished Admission Episodes (FAEs)\",\n\"lastUpdated\": \"2019-10-03T09:15:12.082Z\"\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this property</li> <li>namespace (String): a namespace used to group particular properties - and can be used to filter properties for particular uses</li> <li>key (String): the title or label of this property.  The combination of namespace and key should be unique for this object.</li> <li>value (String): the value that this property holds.  This field may take HTML or MarkDown syntax, and may include links to other objects in  the catalogue.</li> <li>lastUpdated (DateTime): The date/time when this Metadata property was last modified</li> </ul> <p>The endpoints for using metadata properties are listed below.</p> <p>To retrieve all the properties for a particular object, use the following endpoint.  The metadata properties are returned in a paginated list </p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/metadata</p> <p>Where {catalogueItemDomainType} can be one of:</p> <ul> <li><code>folders</code>,</li> <li><code>dataModels</code>,</li> <li><code>dataClasses</code>,</li> <li><code>dataTypes</code>,</li> <li><code>terminologies</code>,</li> <li><code>terms</code>, or</li> <li><code>referenceDataModels</code></li> </ul> <p>To get a specific property (whose id field is known), use the following endpoint:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/metadata/{id}</p> <p>To add a new property to an object you have write-access to, post a structure similar to the one displayed above (ignoring id and  lastUpdated fields, which will be automatically set to the following endpoint:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/metadata</p> <p>To edit an existing property (whose id field is known), use the following endpoint:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/metadata/{id}</p> <p>To delete an existing property (whose id field is known), use the following endpoint:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/metadata/{id}</p> <p>The following endpoint returns all known namespaces for a particular object (given by the id field).  To find all namespaces across the whole  catalogue, the final component of the URL can be left off.</p> <p>/api/metadata/namespaces/{id}?</p>"},{"location":"rest-api/resources/catalogue-item/#permissions","title":"Permissions","text":"<p>Logged in users may query to discover who is able to read or write a particular object (that they themselves have read-access to).  The structure  of a response is as follows:</p> Response body (JSON) <pre><code>{\n\"readableByEveryone\": false,\n\"readableByAuthenticated\": true,\n\"readableByGroups\": [],\n\"writeableByGroups\": [\n{\n\"id\": \"cb1b7f4e-6955-41ba-8f91-2ca92b97c189\",\n\"label\": \"Test Group\",\n\"createdBy\": {\n\"id\": \"dc7a7c25-5622-4cb0-869f-6d0e688b490f\",\n\"emailAddress\": \"joebloggs@test.com\",\n\"firstName\": \"Joe\",\n\"lastName\": \"Bloggs\",\n\"userRole\": \"EDITOR\",\n\"disabled\": false\n}\n}\n],\n\"readableByUsers\": [\n{\n\"id\": \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\",\n\"emailAddress\": \"joebloggs@test.com\",\n\"firstName\": \"Joe\",\n\"lastName\": \"Bloggs\",\n\"userRole\": \"EDITOR\",\n\"disabled\": false\n}\n],\n\"writeableByUsers\": [\n{\n\"id\": \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\",\n\"emailAddress\": \"joebloggs@test.com\",\n\"firstName\": \"Joe\",\n\"lastName\": \"Bloggs\",\n\"userRole\": \"EDITOR\",\n\"disabled\": false\n}\n]\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>readableByEveryone (Boolean): whether the object in question is publicly available - i.e. can be read by any un-authenticated user of the  system</li> <li>readableByAuthenticated (Boolean): whether the object in question can be read by any authenticated (logged-in) user of the system</li> <li>readableByGroups (Set(Group)): the set of groups who have permission to read a particular object.  The group has a label, an identifier, and  the details of the user responsible for creating that group</li> <li>writeableByGroups (Set(Group)): the set of groups who have permission to edit a particular object.  Note that this set of groups is always a  subset of the groups listed in readableByGroups</li> <li>readableByUsers (Set(User)): the set of users who have permission to read a particular object.  The user has an identifier, first name, last  name, email address and flag indicating whether their access is currently valid or disabled</li> <li>writeableByGroups (Set(Group)): the set of users who have permission to edit a particular object.  Note that this set of users is always a  subset of the users listed in readableByUsers</li> </ul> <p>Note</p> <p>Note that read/write permissions are propagated through folders and sub-folders, and the list of permissions given is the inferred list.  So  changes to that list may not always have an affect if they are contradicted by another assertion further up the tree.</p> <p>The endpoint for getting the permissions each of DataModel, ReferenceDataModel, Folder, CodeSet and Classifier are listed below.  The details for updating  permissions are listed on their respective pages. </p> <p>/api/dataModels/{dataModelId}/permissions /api/referenceDataModels/{referenceDataModelId}/permissions /api/folders/{folderId}/permissions /api/codeSets/{codeSetId}/permissions /api/classifiers/{classifierId}/permissions</p>"},{"location":"rest-api/resources/catalogue-item/#annotations","title":"Annotations","text":"<p>Annotations, or comments, can be attached to any item in the catalogue.  The structure is as follows:</p> Response body (JSON) <pre><code>{\n\"count\": 2,\n\"items\": [\n{\n\"id\": \"da3d6229-b152-4cbb-8667-eede523c7eb1\",\n\"description\": \"DataModel finalised by Joe Bloggs on 2018-09-28T20:21:35.995Z\",\n\"createdBy\": {\n\"id\": \"5b96991a-d350-4470-958a-29bfac557ed0\",\n\"emailAddress\": \"joebloggs@test.com\",\n\"firstName\": \"Joe\",\n\"lastName\": \"Bloggs\",\n\"userRole\": \"EDITOR\",\n\"disabled\": false\n},\n\"lastUpdated\": \"2018-09-28T20:21:37.655Z\",\n\"label\": \"Finalised Model\"\n},\n{\n\"id\": \"670e7c31-00fd-425f-903f-6d024845e63e\",\n\"createdBy\": {\n\"id\": \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\",\n\"emailAddress\": \"joebloggs@test.com\",\n\"firstName\": \"Joe\",\n\"lastName\": \"Bloggs\",\n\"userRole\": \"EDITOR\",\n\"disabled\": false\n},\n\"lastUpdated\": \"2018-07-17T15:51:45.643Z\",\n\"label\": \"Is this model is ready for finalisation?\"\n}\n]\n}\n</code></pre>"},{"location":"rest-api/resources/catalogue-item/#listing-annotations","title":"Listing annotations","text":"<p>To get all the annotations for a particular object, use the following endpoint. The results are returned in a paginated list</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/annotations</p> <p>Where {catalogueItemDomainType} can be one of:</p> <ul> <li><code>folders</code>,</li> <li><code>dataModels</code>,</li> <li><code>dataClasses</code>,</li> <li><code>dataTypes</code>,</li> <li><code>terminologies</code>,</li> <li><code>terms</code>, or</li> <li><code>referenceDataModels</code></li> </ul> <p>To get the details of a particular annotation / comment, whose id is known, use the following endpoint:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/annotations/{id}</p> <p>To create a new top-level annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and  lastUpdated fields)</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/annotations</p> <p>To delete an annotation / comment whose identifier is known, use the following delete endpoint:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/annotations/{id}</p>"},{"location":"rest-api/resources/catalogue-item/#child-annotations-responses","title":"Child annotations (responses)","text":"<p>Comments can have child comments (or replies).  To get all the child comments for a particular comment, use the following endpoint. The  results are returned in a paginated list</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/annotations/{annotationId}/annotations</p> <p>To get the details of a particular child annotation / comment, whose id is known, use the following endpoint:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/annotations/{annotationId}/annotations/{id}</p> <p>To create a new child annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and  lastUpdated fields)</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/annotations/{annotationId}/annotations</p> <p>To delete a child annotation / comment whose identifier is known, use the following delete endpoint:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/annotations/{annotationId}/annotations/{id}</p>"},{"location":"rest-api/resources/catalogue-item/#searching","title":"Searching","text":"<p>An advanced search is powered by Lucene.  The parameters for an advanced search can be provided as query parameters of a get request, as follows:</p> Parameter Description <code>searchTerm</code> (String): the search string - this can take a number of standard search operators - for example \"smoking + pregnancy\" <code>limit</code> (Integer): the number of results returned in a paginated list <code>offset</code> (Integer): the index of the first result returned in a paginated list <code>domainTypes</code> (Set(String)): the catalogue object types that should be searched. These can include: <code>DataModel</code>, <code>DataClass</code>, <code>DataElement</code>, <code>DataType</code>, <code>EnumerationType</code>. <code>labelOnly</code> (Boolean): whether the search should only query the label of all objects <code>dataModelTypes</code> (Set(String)): the types of data model that should be searched - for example <code>Data Asset</code>, <code>Data Standard</code> <code>classifiers</code> (Set(String)): a set of classifier labels, such that all results must be classified by one of those tags <code>lastUpdatedAfter</code> (DateTime): Only include objects in the search results if they have been modified more recently than the given date <code>lastUpdatedBefore</code> (DateTime): Only include objects in the search results if they have been modified earlier than the given date <code>createdAfter</code> (DateTime): Only include objects in the search results if they were created more recently than the given date <code>createdBefore</code> (DateTime): Only include objects in the search results if they were created earlier than the given date <p>The response will be a paginated list of items, where each item has the following structure:</p> Response body (JSON) <pre><code>{\n\"id\": \"127bdf61-cbfe-47dc-9854-fdce276f13bf\",\n\"domainType\": \"DataElement\",\n\"label\": \"AGE AT ONSET OF SYMPTOMS (CHILDREN TEENAGERS AND YOUNG ADULTS CANCER)\",\n\"description\": \"AGE BAND AT SMOKING QUIT DATE is derived as the number of completed years between the PERSON BIRTH DATE of the PERSON and the SMOKING QUIT DATE of the Person Stop Smoking Episode.  Permitted National Codes:  01  Under 18 years of age  02  18 to 34 years of age  03  35 - 44 years of age  04  45 - 59 years of age  05  60 and over years of age\",\n\"breadcrumbs\": [\n{\n\"id\": \"078955c7-6c0f-4fc2-a30e-55629a85b9da\",\n\"label\": \"NHS Data Dictionary\",\n\"domainType\": \"DataModel\",\n\"finalised\": true\n},\n{\n\"id\": \"012e8dd5-b4b1-4d26-82aa-17430baf2e2b\",\n\"label\": \"All Data Elements\",\n\"domainType\": \"DataClass\"\n}\n]\n}\n</code></pre> <p>where the fields are defined as follows:</p> <ul> <li>id (UUID): The identifier of the returned object</li> <li>domainType (String): The type of the returned object - for example \"DataModel\", \"DataClass\", \"DataElement\"</li> <li>label (String): The name of the returned object</li> <li>description (String): The description of the returned object.  This may include formatting specified in HTML, or MarkDown.</li> <li>breadcrumbs (List(Breadcrumb)): An ordered list of, e.g. DataModels and DataClasses to show the location of an object in the hierarchy of a  model.  This will include, for each component of the breadcrumb, an id, a label and a domainType.  </li> </ul> <p>To search across the whole catalogue, use the following endpoint, optionally passing the above query parameters:</p> <p>/api/tree/folders/search</p> <p>Similarly, to search within a particular data model, use the following:</p> <p>/api/dataModels/{dataModelId}/search</p> <p>Finally, to search within a specific Data Class, use the following:</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/search</p>"},{"location":"rest-api/resources/catalogue-item/#item-history","title":"Item history","text":"<p>The edit history for various catalogue items can be retrieved using the endpoints listed below.  The format of a response is a paginated list of  edits, with the following structure:</p> Response body (JSON) <pre><code>{\n\"dateCreated\": \"2018-07-17T15:53:17.276Z\",  \"createdBy\": {\n\"id\": \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\",\n\"emailAddress\": \"ollie.freeman@gmail.com\",\n\"firstName\": \"Oliver\",\n\"lastName\": \"Freeman\",\n\"userRole\": \"EDITOR\",\n\"disabled\": false\n}\n\"description\": \"[Data Standard:HIC: Hepatitis v2.0.0] changed properties [folder]\"\n}\n</code></pre> <p>The fields have the following definition:</p> <ul> <li>dateCreated (DateTime): the date and time when this modification was made</li> <li>createdBy (User): the user responsible for making the edit.  This will include their id, emailAddress, firstName, lastName **, **userRole and whether the user account is currently disabled.  It may also include the profile image of the user in question</li> <li>description (String): The human-readable description of the edit made.  These descriptions are automatically generated by the catalogue </li> </ul> <p>The endpoints for getting the edit history for each of DataModel, Terminology, Folder, CodeSet, Classifier and UserGroup are listed below. </p> <p>/api/dataModels/{dataModelId}/edits /api/terminologies/{terminologyId}/edits /api/folders/{folderId}/edits /api/codeSets/{codeSetId}/edits /api/classifiers/{classifierId}/edits /api/userGroups/{userGroupId}/edits</p>"},{"location":"rest-api/resources/catalogue-item/#reference-files","title":"Reference files","text":"<p>Reference files (or attachments) can be stored alongside various catalogue items to supplement information about the catalogue item. Reference  files have the following structure:</p> Response body (JSON) <pre><code>{\n\"id\": \"eea67c19-1833-4125-9934-b06f45844c20\",\n\"domainType\": \"ReferenceFile\",\n\"fileName\": \"uploadedFile.png\",\n\"fileSize\": 80899,\n\"fileType\": \"image/png\",\n\"lastUpdated\": \"2021-05-13T12:50:37.523Z\"\n}\n</code></pre> <p>The fields have the following definition:</p> <ul> <li>id (UUID): The identifier of the returned object</li> <li>domainType (String): The type of the returned object - in this case, \"ReferenceFile\"</li> <li>fileName (String): The name of the uploaded file</li> <li>fileSize (Number): The size of the uploaded reference file, in bytes</li> <li>fileType (String): The MIME type of the uploaded reference file</li> <li>lastUpdated (DateTime): the date and time when this modification was made</li> </ul> <p>To upload and attach a new reference file to a catalogue item, use the following endpoint and request payload:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/referenceFiles</p> Request body (JSON) <pre><code>{\n\"fileName\": \"uploadedFile.png\",\n\"fileSize\": 80899,\n\"fileType\": \"image/png\",\n\"fileContents\": [ // Array of bytes\n]\n}\n</code></pre> <p>Where {catalogueItemDomainType} can be one of:</p> <ul> <li><code>dataModels</code>,</li> <li><code>terminologies</code>,</li> <li><code>codeSets</code>, or</li> <li><code>referenceDataModels</code></li> </ul> <p>To get either a paginated list of reference files for a catalogue item, or an individual reference file known by {id}:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/referenceFiles /api/{catalogueItemDomainType}/{catalogueItemId}/referenceFiles/{id}</p> <p>To delete a reference file from a catalogue item whose identifier is known, use the following delete endpoint:</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/referenceFiles/{id}</p>"},{"location":"rest-api/resources/catalogue-item/#summary-metadata","title":"Summary Metadata","text":"<p>/api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata /api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata /api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata/{id} /api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata/{id} /api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata/{id}</p>"},{"location":"rest-api/resources/catalogue-item/#summary-metadata-reports","title":"Summary Metadata Reports","text":"<p>/api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata/{summaryMetadataId}/summaryMetadataReports /api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata/{summaryMetadataId}/summaryMetadataReports /api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata/{summaryMetadataId}/summaryMetadataReports/{id} /api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata/{summaryMetadataId}/summaryMetadataReports/{id} /api/{catalogueItemDomainType}/{catalogueItemId}/summaryMetadata/{summaryMetadataId}/summaryMetadataReports/{id}</p>"},{"location":"rest-api/resources/classifier/","title":"Classifier","text":"<p>A Classifier is a container type and can be represented as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"Classification\",\n\"label\": \"classifier\",        \"description\": \"Represents a classifier.\",        \"lastUpdated\": \"2021-04-28T10:10:13.945Z\"        }\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this classifier</li> <li>domainType (Type): The domain type of this catalogue object. Will always be <code>Classification</code> in this case.</li> <li>label (String): The human-readable identifier of this classifier.</li> <li>description (String): A long description of the classifier, and any important characteristics of the data.  This field may include HTML, or  MarkDown.</li> <li>lastUpdated (DateTime): The date/time when this Classifier was last modified</li> </ul> <p>As well as the endpoints listed below, a Classifier is also a CatalogueItem, and so a Classifier identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/classifier/#child-classifiers","title":"Child Classifiers","text":"<p>A classifier may contain child classifiers. Endpoints are provided to differentiate between parent and child classifiers.</p>"},{"location":"rest-api/resources/classifier/#getting-information","title":"Getting information","text":"<p>The following endpoints returns a paginated list of all the Classifiers. The first requests all root classifiers in Mauro, the second requests the classifiers for a parent classifier.</p> <p>/api/classifiers</p> <p>/api/classifiers/{classifierId}/classifiers</p> <p>These endpoints provide the detailed information about a particular Classifier; the first requests a root classifier in Mauro, the second requests a classifier from a parent classifier.</p> <p>/api/classifiers/{id}</p> <p>/api/classifiers/{classifierId}/classifiers/{id}</p> <p>Finally, these endpoints request a list of catalogue items mapped to a classifier, and the inverse to list all classifiers mapped to a catalogue item, respectively.</p> <p>/api/classifiers/{classifierId}/catalogueItems</p> <p>/api/{catalogueItemDomainType}/{catalogueItemId}/classifiers</p>"},{"location":"rest-api/resources/classifier/#create-update-delete","title":"Create / Update / Delete","text":"<p>To create a new Classifier from scratch, use the following post endpoints, depending on whether to create one with or without a parent.</p> <p>/api/classifiers</p> <p>/api/classifiers/{classifierId}/classifiers</p> <p>To edit the properties of a Classifier, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent.</p> <p>/api/classifiers/{id}</p> <p>/api/classifiers/{classifierId}/classifiers/{id}</p> <p>To delete a Classifier, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator.</p> <p>/api/classifiers/{id}?permanent={true/false}</p> <p>/api/classifiers/{classifierId}/classifiers/{id}?permanent={true/false}</p>"},{"location":"rest-api/resources/classifier/#security","title":"Security","text":"<p>/api/classifiers/{classifierId}/readByAuthenticated /api/classifiers/{classifierId}/readByAuthenticated /api/classifiers/{classifierId}/readByEveryone /api/classifiers/{classifierId}/readByEveryone</p>"},{"location":"rest-api/resources/codeset/","title":"Codeset","text":"<p>In its simplest form, a Code Set can be represented as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"CodeSet\",\n\"label\": \"Sample Codeset\",\n\"aliases\": [\n\"sample\"\n],\n\"description\": \"Example of a Codeset\",\n\"author\": \"NHS Digital\",\n\"organisation\": \"NHS Digital\",\n\"documentationVersion\": \"2.0.0\",\n\"lastUpdated\": \"2019-10-03T12:00:05.95Z\",\n\"classifiers\": [\n{\n\"id\": \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\",\n\"label\": \"NIHR Health Data Finder\",\n\"lastUpdated\": \"2019-10-03T09:15:37.323Z\"\n}\n],\n\"type\": \"CodeSet\",\n\"finalised\": false,\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this code set</li> <li>domainType (Type): The domain type of this catalogue object - always <code>CodeSet</code> in this case</li> <li>label (String): The human-readable identifier of this code set.  The combination of <code>label</code> and <code>documentationVersion</code> are unique across the catalogue</li> <li>aliases (Set(String)): Any other names by which this code set is known</li> <li>description (String): A long description of the code set, and any important characteristics of the data.  This field may include HTML, or  MarkDown.</li> <li>author (String): The names of those creating and maintaining this code set (not any underlying dataset itself)</li> <li>organisation (String): The name of the organisation holding the dataset</li> <li>documentationVersion (Version): The version of the description of an underlying dataset</li> <li>lastUpdated (DateTime): The date/time when this code set was last modified</li> <li>classifiers (Set(Classifier)): The id, label and lastUpdated date of any classifiers used to tag or categorise this code set  (see classifiers)</li> <li>type (CodeSet Type): Will always be defined as <code>CodeSet</code>.</li> <li>finalised (Boolean): Whether this code set has been 'finalised', or is in draft mode</li> </ul> <p>Endpoints which return multiple code sets typically include sufficient fields for generating links on the interface - a separate call to return the  details of the Code Set is usually required. </p> <p>As well as the endpoints listed below, a Code Set is also a CatalogueItem, and so a Code Set identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/codeset/#list-all-code-sets","title":"List all code sets","text":"<p>The following endpoint returns a paginated list of all code set objects readable by the current user:  </p> <p>/api/codeSets</p> <p>This endpoint returns all the code sets within a particular folder; again, this result is paginated.</p> <p>/api/folders/{folderId}/codeSets</p>"},{"location":"rest-api/resources/codeset/#get-information-about-a-particular-code-set","title":"Get information about a particular code set","text":"<p>This endpoint provides the default information about a code set, as per the JSON at the top of the page.</p> <p>/api/codeSets/{id}</p>"},{"location":"rest-api/resources/codeset/#create-code-set","title":"Create code set","text":"<p>To create a new code set from scratch, use the following post endpoint.  Within the body of this call, you should include a folder identifier.</p> <p>/api/codeSets</p> <p>There are two ways of versioning code set in the catalogue.  To create an entirely new version of a model, please use the following endpoint:</p> <p>/api/codeSets/{codeSetId}/newBranchModelVersion</p> <p>The name must be different to the original model.</p> <p>To create a new 'documentation version', use the following endpoint:</p> <p>/api/codeSets/{codeSetId}/newDocumentationVersion</p> <p>By default, this will supersede the original data model.</p> <p>It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set:</p> <p>/api/codeSets/{codeSetId}/newBranchModelVersion</p> Request body (JSON) <pre><code>{\n\"branchName\": \"newBranch\"\n}\n</code></pre> <p>To create a fork of the original data model:</p> <p>/api/codeSets/{codeSetId}/newForkModel</p> Request body (JSON) <pre><code>{\n\"label\": \"newForkLabel\"        }\n</code></pre>"},{"location":"rest-api/resources/codeset/#update-code-set","title":"Update code set","text":"<p>To edit the primitive properties of a code set, use the following endpoint, with a body similar to the JSON described at the top of this page:</p> <p>/api/codeSets/{id}</p> <p>To move a code set from one folder to another, call the following, using the id fields for the code set, and the new folder:</p> <p>/api/folders/{folderId}/codeSets/{codeSetId}</p> <p>Alternatively, you can call this equivalent endpoint:</p> <p>/api/codeSets/{codeSetId}/folder/{folderId}</p> <p>To move a code set from a draft state to 'finalised', use the following endpoint:</p> <p>/api/codeSets/{codeSetId}/finalise</p>"},{"location":"rest-api/resources/codeset/#sharing","title":"Sharing","text":"<p>To allow a code set to be read by any authenticated user of the system, use the following endpoint:</p> <p>/api/codeSets/{codeSetId}/readByAuthenticated</p> <p>... and to remove this flag, use the following:</p> <p>/api/codeSets/{codeSetId}/readByAuthenticated</p> <p>Similarly, to allow the code set to be publicly readable - ie. readable by any unauthenticated user of the system,  use the following endpoint: </p> <p>/api/codeSets/{codeSetId}/readByEveryone</p> <p>... and the following to remove this flag:</p> <p>/api/codeSets/{codeSetId}/readByEveryone</p>"},{"location":"rest-api/resources/codeset/#delete-code-set","title":"Delete code set","text":"<p>To delete a code set, use the following endpoint.  The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete  is used if the user is an administrator.</p> <p>/api/codeSets/{id}?permanent={true/false}</p> <p>An administrator is able to restore a 'soft' deleted code set using the following endpoint:</p> <p>/api/admin/codeSets/{id}/undoSoftDelete</p>"},{"location":"rest-api/resources/codeset/#import-export-a-code-set","title":"Import / export a code set","text":"<p>To export a code set using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin.  The  following endpoint can be used to export multiple code sets: </p> <p>/api/codeSets/export/{exporterNamespace}/{exporterName}/{exporterVersion}</p> <p>To export a single code set, you can use the following endpoint with the id of the code sets specified:</p> <p>/api/codeSets/{codeSetId}/export/{exporterNamespace}/{exporterName}/{exporterVersion}</p> <p>Similarly, to import one or more code sets, the namespace, name and version of the import plugin must be known.  The body of this method should  be the parameters for the import, including any files that are required.</p> <p>/api/codeSets/import/{importerNamespace}/{importerName}/{importerVersion}</p>"},{"location":"rest-api/resources/data-class/","title":"Data class","text":"<p>A DataClass can be represented as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"DataClass\",\n\"label\": \"parent\",        \"description\": \"Represents a parent data class.\",\n\"aliases\": [\n\"root\"\n],\n\"lastUpdated\": \"2021-04-28T10:10:13.945Z\"\n\"model\": \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\",\n\"parentDataClass\": \"363c202b-e6d9-4098-a5bf-78194d57b70d\",\n\"minMultipicity\": 0,\n\"maxMultiplicity\": -1,\n\"classifiers\": [\n{\n\"id\": \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\",\n\"label\": \"NIHR Health Data Finder\",\n\"lastUpdated\": \"2019-10-03T09:15:37.323Z\"\n}\n],\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this data class</li> <li>domainType (Type): The domain type of this catalogue object. Will always be <code>DataClass</code> in this case.</li> <li>label (String): The human-readable identifier of this class.</li> <li>description (String): A long description of the data class, and any important characteristics of the data.  This field may include HTML, or  MarkDown.</li> <li>aliases (Set(String)): Any other names by which this data class is known</li> <li>lastUpdated (DateTime): The date/time when this DataClass was last modified</li> <li>model (UUID): The unique identifier of the owning data model</li> <li>parentDataClass (UUID): The unique identifier of the data class of which this is a child of. If the data class does not have a parent, this field is undefined/not provided.</li> <li>minMultiplicity (Number): Defines the minimum uses of this data class may be applied to a data model. See the multipicity.</li> <li>maxMultiplicity (Number): Defines the maximum uses of this data class may be applied to a data model. See the multipicity.</li> <li>classifiers (Set(Classifier)): The id, label and lastUpdated date of any classifiers used to tag or categorise this data class  (see classifiers)</li> </ul> <p>As well as the endpoints listed below, a DataClass is also a CatalogueItem, and so a DataClass identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/data-class/#child-classes","title":"Child Classes","text":"<p>A DataClass may be composed of child classes to define more complex definitions for a DataModel. Endpoints are provided to differentiate between parent and child data classes.</p>"},{"location":"rest-api/resources/data-class/#multiplicity","title":"Multiplicity","text":"<p>Each DataClass defines its multiplicity to state how many occurances of this class should be expected in a model. Multipicities are defined in the notation <code>x..y</code>, where:</p> <ul> <li><code>x</code> represents the minimum multipicity.</li> <li><code>y</code> represents the maximum multipicity.</li> </ul> <p>A minimum multipicity must be provided and be greater than or equal to <code>0</code>.</p> <p>A maximum multiplicity may be provided that is greater than or equal to <code>1</code>. To represent unbounded multipicity, the <code>*</code> symbol is used - numerically, for endpoints, this is represented as the integer <code>-1</code>.</p> <p>Some examples of multipicities and what they represent:</p> <ul> <li><code>0..*</code> - an optional, unbounded data class. This may be present or not, and has not limit on how many are present.</li> <li><code>1..*</code> - a required, unbounded data class. Similar to above but with the added constraint that at least one must be present.</li> <li><code>0..1</code> - an optional, singular data class. Either the class is present in the model or not.</li> <li><code>1..1</code> - a required, singular data class. This represents that the class must be present in the model.</li> </ul>"},{"location":"rest-api/resources/data-class/#getting-information","title":"Getting information","text":"<p>The following endpoints returns a paginated list of all the DataTypes within a particular DataModel. The first requests the data classes for a data model, the second requests the data classes for a parent data class.</p> <p>/api/dataModels/{dataModelId}/dataClasses</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataClasses</p> <p>These endpoints provide the detailed information about a particular DataClass; the first requests a DataType under a particular DataModel, the second requests a DataClass from a parent DataClass.</p> <p>/api/dataModels/{dataModelId}/dataClasses/{id}</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataClasses/{id}</p>"},{"location":"rest-api/resources/data-class/#create-update-delete","title":"Create / Update / Delete","text":"<p>To create a new DataClass from scratch, use the following post endpoints, depending on whether to create one directly under a DataModel or under a parent DataClass respectively.</p> <p>/api/dataModels/{dataModelId}/dataClasses</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataClasses</p> <p>To edit the properties of a DataClass, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one directly under a DataModel or under a parent DataClass respectively.</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataClasses/{id}</p> <p>/api/dataModels/{dataModelId}/dataClasses/{id}</p> <p>To delete a DataClass, use the following endpoint, depending on whether to delete one directly under a DataModel or under a parent DataClass respectively.</p> <p>/api/dataModels/{dataModelId}/dataClasses/{id}</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataClasses/{id}</p>"},{"location":"rest-api/resources/data-class/#copying","title":"Copying","text":"<p>Instead of creating a new DataClass from scratch, it is also possible to copy an existing DataClass from another DataModel. Use the following endpoints to accomplish this, depending on whether to copy one directly under a DataModel or under a parent DataClass respectively. The dataModelId and dataClassId refers to the target DataModel or parent DataClass to copy to; otherDataModelId and otherDataCLassId refers to the source DataModel/Class to copy from.</p> <p>/api/dataModels/{dataModelId}/dataClasses/{otherDataModelId}/{otherDataClassId}</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataClasses/{otherDataModelId}/{otherDataClassId}</p>"},{"location":"rest-api/resources/data-element/","title":"Data element","text":"<p>A DataElement can be represented as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"DataElement\",\n\"label\": \"element\",        \"description\": \"Description of the Data Element.\",\n\"lastUpdated\": \"2021-04-28T10:10:13.945Z\"\n\"model\": \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\",\n\"dataClass\": \"afb3dcda-fd7d-40b9-857c-23fb5af8cbbf\",\n\"dataType\": {\n\"id\": \"c85d78d3-cac8-449b-a22b-52da144b9a8f\",\n\"domainType\": \"PrimitiveType\",\n\"label\": \"integer\"\n}\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this data element</li> <li>domainType (Type): The domain type of this catalogue object. This is always <code>DataElement</code> in this case.</li> <li>label (String): The human-readable identifier of this element.</li> <li>description (String): A long description of the data element, and any important characteristics of the data.  This field may include HTML, or  MarkDown.</li> <li>lastUpdated (DateTime): The date/time when this DataElement was last modified</li> <li>model (UUID): The unique identifier of the parent data model</li> <li>dataClass (UUID): The unique identifier of the parent data class</li> <li>dataType (Object): The type definition of this data element. The object returned matches the JSON defined in Data type</li> </ul> <p>As well as the endpoints listed below, a DataElement is also a CatalogueItem, and so a DataElement identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/data-element/#getting-information","title":"Getting information","text":"<p>The following endpoint returns a paginated list of all the DataElements within a particular DataClass.</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataElements</p> <p>This endpoint provides the detailed information about a particular DataElement under a DataClass.</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataElements/{id}</p>"},{"location":"rest-api/resources/data-element/#create-update-delete","title":"Create / Update / Delete","text":"<p>To create a new DataElement from scratch, use the following post endpoint.</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataElements</p> <p>To edit the properties of a DataElement, use the following endpoint, with a body similar to the JSON described at the top of this page:</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataElements/{id}</p> <p>To delete a DataElement, use the following endpoint.</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataElements/{id}</p>"},{"location":"rest-api/resources/data-element/#copying","title":"Copying","text":"<p>Instead of creating a new DataElement from scratch, it is also possible to copy an existing DataElement from another DataClass. Use the following endpoint to accomplish this. The dataModelId and dataClassId refers to the target DataClass to copy to; otherDataModelId, otherDataClassId and dataElementId refer to the source DataModel/Class/Element to copy from.</p> <p>/api/dataModels/{dataModelId}/dataClasses/{dataClassId}/dataElements/{otherDataModelId}/{otherDataClassId}/{dataElementId}</p>"},{"location":"rest-api/resources/data-flow-component/","title":"Data flow component","text":"<p>/api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/dataClassFlows/{dataClassId}/dataFlowComponents /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/dataFlowComponents /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/dataFlowComponents /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/dataFlowComponents/{id} /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/dataFlowComponents/{id} /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/dataFlowComponents/{id} /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/dataFlowComponents/{dataFlowComponentId}/{type}/{dataElementId} /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/dataFlowComponents/{dataFlowComponentId}/{type}/{dataElementId}</p>"},{"location":"rest-api/resources/data-flow/","title":"Data flow","text":"<p>/api/dataModels/{dataModelId}/dataFlows/import/{importerNamespace}/{importerName}/{importerVersion} /api/dataModels/{dataModelId}/dataFlows/export/{exporterNamespace}/{exporterName}/{exporterVersion} /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/dataClassFlows /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/diagramLayout /api/dataModels/{dataModelId}/dataFlows/{dataFlowId}/export/{exporterNamespace}/{exporterName}/{exporterVersion} /api/dataModels/{dataModelId}/dataFlows /api/dataModels/{dataModelId}/dataFlows /api/dataModels/{dataModelId}/dataFlows/{id} /api/dataModels/{dataModelId}/dataFlows/{id} /api/dataModels/{dataModelId}/dataFlows/{id}</p>"},{"location":"rest-api/resources/data-model/","title":"Data model","text":""},{"location":"rest-api/resources/data-model/#datamodel-object-description","title":"DataModel object description","text":"<p>In its simplest form, a DataModel can be represented as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"DataModel\",\n\"label\": \"Diagnostic Imaging Dataset\",\n\"aliases\": [\n\"DID\"\n],\n\"description\": \"Central collection of detailed information about diagnostic imaging tests carried out on NHS patients (such as x-rays and MRI scans).  Any organisation providing diagnostic imaging tests to NHS patients in England, i.e.:\\n* NHS (Foundation) trusts / hospitals\\n* NHS-funded activity with independent sector providers\\nNOT included are breast screening services or any other diagnostic imaging tests not typically recorded on the local provider's Radiology Information Systems.\\nDiagnostic Imaging Dataset (DID) does not store the images themselves, or the outcomes/diagnoses related to these images.\",\n\"author\": \"NHS Digital\",\n\"organisation\": \"NHS Digital\",\n\"editable\": true,\n\"documentationVersion\": \"2.0.0\",\n\"lastUpdated\": \"2019-10-03T12:00:05.95Z\",\n\"classifiers\": [\n{\n\"id\": \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\",\n\"label\": \"NIHR Health Data Finder\",\n\"lastUpdated\": \"2019-10-03T09:15:37.323Z\"\n}\n],\n\"type\": \"Data Asset\",\n\"finalised\": false,\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this data model</li> <li>domainType (Type): The domain type of this catalogue object - always \"DataModel\" in this case</li> <li>label (String): The human-readable identifier of this model.  The combination of label and documentationVersion are unique across the catalogue</li> <li>aliases (Set(String)): Any other names by which this datamodel is known</li> <li>description (String): A long description of the data model, and any important characteristics of the data.  This field may include HTML, or  MarkDown.</li> <li>author (String): The names of those creating and maintaining this Datamodel (not any underlying dataset itself)</li> <li>organisation (String): The name of the organisation holding the dataset</li> <li>editable (Boolean): Whether the current user (see authentication) is allowed to edit this DataModel</li> <li>documentationVersion (Version): The version of the description of an underlying dataset</li> <li>lastUpdated (DateTime): The date/time when this DataModel was last modified</li> <li>classifiers (Set(Classifier)): The id, label and lastUpdated date of any classifiers used to tag or categorise this data model  (see classifiers)</li> <li>type (DataModel Type): Whether this DataModel is a \"Data Asset\", or a \"Data Standard\"</li> <li>finalised (Boolean): Whether this DataModel has been 'finalised', or is in draft mode</li> </ul> <p>Endpoints which return multiple models typically include sufficient fields for generating links on the interface - a separate call to return the  details of the DataModel is usually required. </p> <p>As well as the endpoints listed below, a DataModel is also a CatalogueItem, and so a DataModel identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/data-model/#list-all-data-models","title":"List all data models","text":"<p>The following endpoint returns a paginated list of all DataModel objects readable by the current user:  </p> <p>/api/dataModels</p> <p>This endpoint returns all the DataModels within a particular folder; again, this result is paginated.</p> <p>/api/folders/{folderId}/dataModels</p>"},{"location":"rest-api/resources/data-model/#get-information-about-a-particular-data-model","title":"Get information about a particular data model","text":"<p>This endpoint provides the default information about a DataModel, as per the JSON at the top of the page.</p> <p>/api/dataModels/{id}</p> <p>The 'hierarchy' endpoint provides a structured representation of the entire datamodel - child DataClasses, sub-DataClasses, and their child  DataElements.   </p> <p>Warning</p> <p>This call can take a long time for large data models</p> <p>/api/dataModels/{dataModelId}/hierarchy</p> <p>The 'types' endpoint lists all the datatypes for a given datamodel.  This returns primitive, reference, enumeration and terminology types owned by  the data model, whether used or not.</p> <p>/api/dataModels/types</p>"},{"location":"rest-api/resources/data-model/#create-data-model","title":"Create data model","text":"<p>To create a new data model from scratch, use the following post endpoint.  Within the body of this call, you should include a folder identifier.</p> <p>/api/dataModels</p> <p>There are two ways of versioning Data Models in the catalogue.  To create an entirely new version of an existing model, please use the following endpoint with no request body:</p> <p>/api/dataModels/{dataModelId}/newBranchModelVersion</p> <p>The name must be different to the original model.</p> <p>To create a new 'documentation version', use the following endpoint:</p> <p>/api/dataModels/{dataModelId}/newDocumentationVersion</p> <p>By default, this will supersede the original data model.</p> <p>It is also possible to branch and fork Data Models to create drafts before finalising them. To create a new branch from an existing Data Model:</p> <p>/api/dataModels/{dataModelId}/newBranchModelVersion</p> Request body (JSON) <pre><code>{\n\"branchName\": \"newBranch\"\n}\n</code></pre> <p>To create a fork of the original data model:</p> <p>/api/dataModels/{dataModelId}/newForkModel</p> Request body (JSON) <pre><code>{\n\"label\": \"newForkLabel\"        }\n</code></pre>"},{"location":"rest-api/resources/data-model/#update-data-model","title":"Update data model","text":"<p>To edit the primitive properties of a data model, use the following endpoint, with a body similar to the JSON described at the top of this page:</p> <p>/api/dataModels/{id}</p> <p>To move a data model from one folder to another, call the following, using the id fields for the data model, and the new folder:</p> <p>/api/folders/{folderId}/dataModels/{dataModelId}</p> <p>Alternatively, you can call this equivalent endpoint:</p> <p>/api/dataModels/{dataModelId}/folder/{folderId}</p> <p>To move a data model from a draft state to 'finalised', use the following endpoint:</p> <p>/api/dataModels/{dataModelId}/finalise</p>"},{"location":"rest-api/resources/data-model/#sharing","title":"Sharing","text":"<p>To allow a model to be read by any authenticated user of the system, use the following endpoint:</p> <p>/api/dataModels/{dataModelId}/readByAuthenticated</p> <p>... and to remove this flag, use the following:</p> <p>/api/dataModels/{dataModelId}/readByAuthenticated</p> <p>Similarly, to allow the model to be publicly readable - ie. readable by any unauthenticated user of the system,  use the following endpoint: </p> <p>/api/dataModels/{dataModelId}/readByEveryone</p> <p>... and the following to remove this flag:</p> <p>/api/dataModels/{dataModelId}/readByEveryone</p>"},{"location":"rest-api/resources/data-model/#delete-data-model","title":"Delete data model","text":"<p>To delete a data model, use the following endpoint.  The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete  is used if the user is an administrator.</p> <p>/api/dataModels/{id}?permanent={true/false}</p> <p>An administrator is able to restore a 'soft' deleted code set using the following endpoint:</p> <p>/api/admin/dataModels/{id}/undoSoftDelete</p>"},{"location":"rest-api/resources/data-model/#import-export-a-data-model","title":"Import / export a data model","text":"<p>To export a data model using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin.  The  following endpoint can be used to export multiple data models: </p> <p>/api/dataModels/export/{exporterNamespace}/{exporterName}/{exporterVersion}</p> <p>To export a single model, you can use the following endpoint with the id of the data model specified:</p> <p>/api/dataModels/{dataModelId}/export/{exporterNamespace}/{exporterName}/{exporterVersion}</p> <p>Similarly, to import one or more data models, the namespace, name and version of the import plugin must be known.  The body of this method should  be the parameters for the import, including any files that are required.</p> <p>/api/dataModels/import/{importerNamespace}/{importerName}/{importerVersion}</p>"},{"location":"rest-api/resources/data-model/#finalise-a-data-model","title":"Finalise a data model","text":"<p>To finalise a data model means to lock it to a particular version and make it read-only; only new versions can be created to make further modifications after that point.  Use this endpoint with a similar payloads described below to finalise a data model.</p> <p>/api/dataModels/{id}/finalise</p> <p>To automatically let Mauro choose the next version number, set the versionChangeType property to either <code>'Major'</code>, <code>'Minor'</code> or <code>'Patch'</code>.</p> Request body (JSON) <pre><code>{\n\"versionChangeType\": \"Major\" | \"Minor\" | \"Patch\"\n}\n</code></pre> <p>Mauro uses Semantic Versioning rules to determine the next appropriate version number based on the versionChangeType value provided.</p> <p>To optionally choose your own version number, provide this payload. If versionChangeType is <code>'Custom'</code>, then version must also be provided.</p> Request body (JSON) <pre><code>{\n\"versionChangeType\": \"Custom\",\n\"version\": \"1.2.3.4\"\n}\n</code></pre> <p>In all cases you may also supply an optional tag name to assign with the finalised version to help provide more context, as follows:</p> Request body (JSON) <pre><code>{\n\"versionChangeType\": \"Major\" | \"Minor\" | \"Patch\",\n\"versionTag\": \"My first version\"\n}\n</code></pre>"},{"location":"rest-api/resources/data-model/#merging-data-models","title":"Merging data models","text":"<p>If creating branches of data models, it is possible to merge the data values from one data model to another. The first step is to calculate the differences between two data models, as follows:</p> <p>/api/dataModels/{sourceId}/mergeDiff/{targetId}?isLegacy=false</p> Response body (JSON) <pre><code>{\n\"sourceId\": \"f9a4e390-6259-4616-b725-d45524851a82\",\n\"targetId\": \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\",\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch\",\n\"label\": \"Model Version Tree DataModel\",\n\"count\": 8,\n\"diffs\": [\n{\n\"fieldName\": \"author\",\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch@author\",\n\"sourceValue\": \"Mauro User\",\n\"targetValue\": \"Dante\",\n\"commonAncestorValue\": \"Dante\",\n\"isMergeConflict\": false,\n\"type\": \"modification\"\n},                        {\n\"fieldName\": \"organisation\",\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch@organisation\",\n\"sourceValue\": \"Mauro\",\n\"targetValue\": \"Baal\",\n\"commonAncestorValue\": \"Baal\",\n\"isMergeConflict\": false,\n\"type\": \"modification\"\n},\n{\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch|ann:Test Comment\",\n\"isMergeConflict\": false,\n\"isSourceModificationAndTargetDeletion\": false,\n\"type\": \"creation\"\n},\n{\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch|dc:Test Data Class\",\n\"isMergeConflict\": false,\n\"isSourceModificationAndTargetDeletion\": false,\n\"type\": \"creation\"\n},\n{\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch|md:v1Versioning.com.mdk1\",\n\"isMergeConflict\": false,\n\"isSourceDeletionAndTargetModification\": false,\n\"type\": \"deletion\"\n},\n{\n\"fieldName\": \"value\",\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch|md:org.datacite.creator@value\",\n\"sourceValue\": \"Peter Monks\",\n\"targetValue\": \"Mauro Administrator\",\n\"commonAncestorValue\": null,\n\"isMergeConflict\": true,\n\"type\": \"modification\"\n},\n{\n\"fieldName\": \"value\",\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch|md:test.com.testProperty@value\",\n\"sourceValue\": \"Oliver Freeman\",\n\"targetValue\": \"Peter Monks\",\n\"commonAncestorValue\": null,\n\"isMergeConflict\": true,\n\"type\": \"modification\"\n},\n{\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\",\n\"isMergeConflict\": false,\n\"isSourceModificationAndTargetDeletion\": false,\n\"type\": \"creation\"\n}\n]\n}\n</code></pre> <p>The diffs collection will hold each change found between the two data models and how they relate.</p> <p>All changes need to be manually organised into patches so that they can be applied to the target data model. Then the following endpoint is used to commit:</p> <p>/api/dataModels/{sourceId}/mergeInto/{targetId}?isLegacy=false</p> Request body (JSON) <pre><code>{\n\"changeNotice\": \"Change comment\",\n\"deleteBranch\": false,\n\"patch\": {\n\"sourceId\": \"f9a4e390-6259-4616-b725-d45524851a82\",\n\"targetId\": \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\",\n\"label\": \"Model Version Tree DataModel\",\n\"count\": 3,\n\"patches\": [\n{\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\",\n\"isMergeConflict\": false,\n\"isSourceModificationAndTargetDeletion\": false,\n\"type\": \"creation\"\n},\n{\n\"fieldName\": \"author\",\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch@author\",\n\"sourceValue\": \"Mauro User\",\n\"targetValue\": \"Mauro User\",\n\"commonAncestorValue\": \"Dante\",\n\"isMergeConflict\": false,\n\"type\": \"modification\"\n},  {\n\"path\": \"dm:Model Version Tree DataModel$interestingBranch|md:v1Versioning.com.mdk1\",\n\"isMergeConflict\": false,\n\"isSourceDeletionAndTargetModification\": false,\n\"type\": \"deletion\"\n},\n]\n}\n}\n</code></pre> <p>The key point is to set the targetValue of every patch item to change - this value is what will be written to the target data model when committing the merge.</p>"},{"location":"rest-api/resources/data-type/","title":"Data type","text":"<p>A DataType can be represented as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"PrimitiveType\",\n\"label\": \"integer\",        \"description\": \"Represents a number.\",\n\"lastUpdated\": \"2021-04-28T10:10:13.945Z\"\n\"model\": \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\",\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this data type</li> <li>domainType (Type): The domain type of this catalogue object. Could be <code>PrimitiveType</code> or <code>EnumerationType</code>.</li> <li>label (String): The human-readable identifier of this type.</li> <li>description (String): A long description of the data type, and any important characteristics of the data.  This field may include HTML, or  MarkDown.</li> <li>lastUpdated (DateTime): The date/time when this DataType was last modified</li> <li>model (UUID): The unique identifier of the parent data model</li> </ul> <p>As well as the endpoints listed below, a DataType is also a CatalogueItem, and so a DataType identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/data-type/#default-data-types","title":"Default Data Types","text":"<p>When creating DataModels, a default data type provider can be used to automatically define data types for a data model. To list all available data type providers:</p> <p>/api/dataModels/defaultDataTypeProviders</p>"},{"location":"rest-api/resources/data-type/#getting-information","title":"Getting information","text":"<p>The following endpoint returns a paginated list of all the DataTypes within a particular DataModel.</p> <p>/api/dataModels/{dataModelId}/dataTypes</p> <p>This endpoint provides the detailed information about a particular DataType under a DataModel.</p> <p>/api/dataModels/{dataModelId}/dataTypes/{id}</p>"},{"location":"rest-api/resources/data-type/#create-update-delete","title":"Create / Update / Delete","text":"<p>To create a new DataType from scratch, use the following post endpoint.</p> <p>/api/dataModels/{dataModelId}/dataTypes</p> <p>To edit the properties of a DataType, use the following endpoint, with a body similar to the JSON described at the top of this page:</p> <p>/api/dataModels/{dataModelId}/dataTypes/{id}</p> <p>To delete a DataType, use the following endpoint.</p> <p>/api/dataModels/{dataModelId}/dataTypes/{id}</p>"},{"location":"rest-api/resources/data-type/#copying","title":"Copying","text":"<p>Instead of creating a new DataType from scratch, it is also possible to copy an existing DataType from another DataModel. Use the following endpoint to accomplish this. The dataModelId refers to the target DataModel to copy to; otherDataModelId and dataTypeId refer to the source DataModel/Type to copy from.</p> <p>/api/dataModels/{dataModelId}/dataTypes/{otherDataModelId}/{dataTypeId}</p>"},{"location":"rest-api/resources/enumeration-value/","title":"Enumeration value","text":"<p>/api/dataModels/{dataModelId}/enumerationTypes/{enumerationTypeId}/enumerationValues /api/dataModels/{dataModelId}/enumerationTypes/{enumerationTypeId}/enumerationValues /api/dataModels/{dataModelId}/enumerationTypes/{enumerationTypeId}/enumerationValues/{id} /api/dataModels/{dataModelId}/enumerationTypes/{enumerationTypeId}/enumerationValues/{id} /api/dataModels/{dataModelId}/enumerationTypes/{enumerationTypeId}/enumerationValues/{id}</p>"},{"location":"rest-api/resources/folder/","title":"Folder","text":"<p>A Folder is a container type and can be represented as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"Folder\",\n\"label\": \"folder\",        \"description\": \"Represents a folder.\",        \"lastUpdated\": \"2021-04-28T10:10:13.945Z\",\n\"hasChildFolders\": true\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this folder</li> <li>domainType (Type): The domain type of this catalogue object. Will always be <code>Folder</code> in this case.</li> <li>label (String): The human-readable identifier of this folder.</li> <li>description (String): A long description of the folder, and any important characteristics of the data.  This field may include HTML, or  MarkDown.</li> <li>lastUpdated (DateTime): The date/time when this folder was last modified</li> <li>hasChildFolders (Boolean): Determines if this folder contains child folders.</li> </ul> <p>As well as the endpoints listed below, a Folder is also a CatalogueItem, and so a Folder identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/folder/#child-folders","title":"Child Folders","text":"<p>A folder may contain child folders. Endpoints are provided to differentiate between parent and child folders.</p>"},{"location":"rest-api/resources/folder/#getting-information","title":"Getting information","text":"<p>The following endpoints returns a paginated list of all the folders. The first requests all root folders in Mauro, the second requests the folders for a parent folder.</p> <p>/api/folders</p> <p>/api/folders/{folderId}/folders</p> <p>These endpoints provide the detailed information about a particular folder; the first requests a root folder in Mauro, the second requests a folder from a parent folder.</p> <p>/api/folders/{id}</p> <p>/api/folders/{folderId}/folders/{id}</p>"},{"location":"rest-api/resources/folder/#create-update-delete","title":"Create / Update / Delete","text":"<p>To create a new folder from scratch, use the following post endpoints, depending on whether to create one with or without a parent.</p> <p>/api/folders</p> <p>/api/folders/{folderId}/folders</p> <p>To edit the properties of a folder, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent.</p> <p>/api/folders/{id}</p> <p>/api/folders/{folderId}/folders/{id}</p> <p>To delete a folder, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator.</p> <p>/api/folders/{id}?permanent={true/false}</p> <p>/api/folders/{folderId}/folders/{id}?permanent={true/false}</p>"},{"location":"rest-api/resources/folder/#security","title":"Security","text":"<p>/api/folders/{folderId}/readByAuthenticated /api/folders/{folderId}/readByAuthenticated /api/folders/{folderId}/readByEveryone /api/folders/{folderId}/readByEveryone</p>"},{"location":"rest-api/resources/plugin/","title":"Plugin","text":"<p>/api/public/plugins/dataFlowImporters /api/public/plugins/dataFlowExporters /api/public/plugins/dataModelImporters /api/public/plugins/dataModelExporters</p>"},{"location":"rest-api/resources/plugin/#importer","title":"Importer","text":"<p>/api/importer/parameters/{ns}?/{name}?/{version}?</p>"},{"location":"rest-api/resources/semantic-link/","title":"Semantic link","text":"<p>/api/terminologies/{terminologyId}/terms/{termId}/semanticLinks /api/terminologies/{terminologyId}/terms/{termId}/semanticLinks /api/terminologies/{terminologyId}/terms/{termId}/semanticLinks/{id} /api/terminologies/{terminologyId}/terms/{termId}/semanticLinks/{id} /api/terminologies/{terminologyId}/terms/{termId}/semanticLinks/{id} /api/catalogueItems/{catalogueItemId}/semanticLinks /api/catalogueItems/{catalogueItemId}/semanticLinks /api/catalogueItems/{catalogueItemId}/semanticLinks/{id} /api/catalogueItems/{catalogueItemId}/semanticLinks/{id} /api/catalogueItems/{catalogueItemId}/semanticLinks/{id}</p>"},{"location":"rest-api/resources/term-relationship/","title":"Term relationship","text":"<p>/api/terminologies/{terminologyId}/termRelationshipTypes/{termRelationshipTypeId}/termRelationships /api/terminologies/{terminologyId}/terms/{termId}/termRelationships /api/terminologies/{terminologyId}/termRelationshipTypes/{termRelationshipTypeId}/termRelationships/{id} /api/terminologies/{terminologyId}/terms/{termId}/termRelationships/{id}</p>"},{"location":"rest-api/resources/term-relationship/#term-relationship-types","title":"Term relationship types","text":"<p>/api/terminologies/{terminologyId}/termRelationshipTypes /api/terminologies/{terminologyId}/termRelationshipTypes/{id}</p>"},{"location":"rest-api/resources/term/","title":"Term","text":"<p>A Term is part a Terminology and can be represented as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"Term\",\n\"model\": \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\",\n\"code\": \"CT1\",\n\"definition\": \"Custom Term\",\n\"label\": \"CT1: Custom Term\",\n\"lastUpdated\": \"2021-04-28T10:10:13.945Z\"        }\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this term</li> <li>domainType (Type): The domain type of this catalogue object. Will always be <code>Term</code> in this case.</li> <li>model (UUID): The unique identifier of the owning data model</li> <li>code (String): A unique code identifier for the term.</li> <li>definition (String): The definition/name of this term.</li> <li>label (String): The human-readable identifier of this term. This is the combination of <code>code</code> and <code>definition</code>.</li> <li>lastUpdated (DateTime): The date/time when this term was last modified</li> </ul> <p>As well as the endpoints listed below, a term is also a CatalogueItem, and so a term identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/term/#getting-information","title":"Getting information","text":"<p>The following endpoint returns a paginated list of all the terms within a particular Terminology.</p> <p>/api/terminologies/{terminologyId}/terms</p> <p>This endpoint provides the detailed information about a particular term.</p> <p>/api/terminologies/{terminologyId}/terms/{id}</p>"},{"location":"rest-api/resources/term/#create-update-delete","title":"Create / Update / Delete","text":"<p>To create a new term from scratch, use the following post endpoint with a JSON request body similar to above.</p> <p>/api/terminologies/{terminologyId}/terms</p> <p>To edit the properties of a term, use the following endpoint, with a body similar to the JSON described at the top of this page.</p> <p>/api/terminologies/{terminologyId}/terms/{id}</p> <p>To delete a term, use the following endpoint.</p> <p>/api/terminologies/{terminologyId}/terms/{id}</p>"},{"location":"rest-api/resources/term/#relationships","title":"Relationships","text":"<p>/api/terminologies/{terminologyId}/terms/{termId}/termRelationships /api/terminologies/{terminologyId}/terms/{termId}/termRelationships/{id} /api/terminologies/{terminologyId}/terms/{termId}/termRelationships /api/terminologies/{terminologyId}/terms/{termId}/termRelationships/{id} /api/terminologies/{terminologyId}/terms/{termId}/termRelationships/{id}</p>"},{"location":"rest-api/resources/terminology/","title":"Terminology","text":"<p>In its simplest form, a Terminology can be represented as follows:</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"Terminology\",\n\"label\": \"Sample Terminology\",\n\"aliases\": [\n\"sample\"\n],\n\"description\": \"Example of a Terminology\",\n\"author\": \"NHS Digital\",\n\"organisation\": \"NHS Digital\",\n\"documentationVersion\": \"2.0.0\",\n\"lastUpdated\": \"2019-10-03T12:00:05.95Z\",\n\"classifiers\": [\n{\n\"id\": \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\",\n\"label\": \"NIHR Health Data Finder\",\n\"lastUpdated\": \"2019-10-03T09:15:37.323Z\"\n}\n],\n\"type\": \"Terminology\",\n\"finalised\": false,\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this terminology</li> <li>domainType (Type): The domain type of this catalogue object - always <code>Terminology</code> in this case</li> <li>label (String): The human-readable identifier of this terminology.  The combination of <code>label</code> and <code>documentationVersion</code> are unique across the catalogue</li> <li>aliases (Set(String)): Any other names by which this terminology is known</li> <li>description (String): A long description of the description, and any important characteristics of the data.  This field may include HTML, or  MarkDown.</li> <li>author (String): The names of those creating and maintaining this terminology (not any underlying dataset itself)</li> <li>organisation (String): The name of the organisation holding the terminology</li> <li>documentationVersion (Version): The version of the description of an underlying dataset</li> <li>lastUpdated (DateTime): The date/time when this terminology was last modified</li> <li>classifiers (Set(Classifier)): The id, label and lastUpdated date of any classifiers used to tag or categorise this terminology  (see classifiers)</li> <li>type (Terminology Type): Will always be defined as <code>Terminology</code>.</li> <li>finalised (Boolean): Whether this terminology has been 'finalised', or is in draft mode</li> </ul> <p>Endpoints which return multiple terminologies typically include sufficient fields for generating links on the interface - a separate call to return the  details of the terminology is usually required. </p> <p>As well as the endpoints listed below, a terminology is also a CatalogueItem, and so a terminology identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/terminology/#list-all-terminologies","title":"List all terminologies","text":"<p>The following endpoint returns a paginated list of all terminologies readable by the current user:  </p> <p>/api/terminologies</p> <p>This endpoint returns all the terminologies within a particular folder; again, this result is paginated.</p> <p>/api/folders/{folderId}/terminologies</p>"},{"location":"rest-api/resources/terminology/#get-information-about-a-particular-terminology","title":"Get information about a particular terminology","text":"<p>This endpoint provides the default information about a terminology, as per the JSON at the top of the page.</p> <p>/api/terminologies/{id}</p>"},{"location":"rest-api/resources/terminology/#create-terminologies","title":"Create terminologies","text":"<p>There are two ways of versioning terminologies in the catalogue.  To create an entirely new version of a model, please use the following endpoint:</p> <p>/api/terminology/{terminologyId}/newBranchModelVersion</p> <p>The name must be different to the original model.</p> <p>To create a new 'documentation version', use the following endpoint:</p> <p>/api/terminology/{terminologyId}/newDocumentationVersion</p> <p>By default, this will supersede the original terminology.</p> <p>It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set:</p> <p>/api/terminology/{terminologyId}/newBranchModelVersion</p> Request body (JSON) <pre><code>{\n\"branchName\": \"newBranch\"\n}\n</code></pre> <p>To create a fork of the original terminology:</p> <p>/api/terminology/{terminologyId}/newForkModel</p> Request body (JSON) <pre><code>{\n\"label\": \"newForkLabel\"        }\n</code></pre>"},{"location":"rest-api/resources/terminology/#update-terminology","title":"Update terminology","text":"<p>To edit the primitive properties of a terminology, use the following endpoint, with a body similar to the JSON described at the top of this page:</p> <p>/api/terminology/{id}</p> <p>To move a terminology from one folder to another, call the following, using the id fields for the terminology, and the new folder:</p> <p>/api/folders/{folderId}/terminologies/{terminologyId}</p> <p>Alternatively, you can call this equivalent endpoint:</p> <p>/api/terminologies/{terminologyId}/folder/{folderId}</p> <p>To move a terminology from a draft state to 'finalised', use the following endpoint:</p> <p>/api/terminologies/{terminologyId}/finalise</p>"},{"location":"rest-api/resources/terminology/#sharing","title":"Sharing","text":"<p>To allow a terminology to be read by any authenticated user of the system, use the following endpoint:</p> <p>/api/terminologies/{terminologyId}/readByAuthenticated</p> <p>... and to remove this flag, use the following:</p> <p>/api/terminologies/{terminologyId}/readByAuthenticated</p> <p>Similarly, to allow the terminology to be publicly readable - ie. readable by any unauthenticated user of the system,  use the following endpoint: </p> <p>/api/terminologies/{terminologyId}/readByEveryone</p> <p>... and the following to remove this flag:</p> <p>/api/terminologies/{terminologyId}/readByEveryone</p>"},{"location":"rest-api/resources/terminology/#delete-terminology","title":"Delete terminology","text":"<p>To delete a terminology, use the following endpoint.  The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete  is used if the user is an administrator.</p> <p>/api/terminologies/{id}?permanent={true/false}</p> <p>An administrator is able to restore a 'soft' deleted terminology using the following endpoint:</p> <p>/api/admin/terminologies/{id}/undoSoftDelete</p>"},{"location":"rest-api/resources/terminology/#import-export-a-terminology","title":"Import / export a terminology","text":"<p>To export a terminology using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin.  The  following endpoint can be used to export multiple code sets: </p> <p>/api/terminologies/export/{exporterNamespace}/{exporterName}/{exporterVersion}</p> <p>To export a single terminology, you can use the following endpoint with the id of the terminologies specified:</p> <p>/api/terminologies/{terminologyId}/export/{exporterNamespace}/{exporterName}/{exporterVersion}</p> <p>Similarly, to import one or more terminologies, the namespace, name and version of the import plugin must be known.  The body of this method should  be the parameters for the import, including any files that are required.</p> <p>/api/terminologies/import/{importerNamespace}/{importerName}/{importerVersion}</p>"},{"location":"rest-api/resources/user-group/","title":"User group","text":"<p>/api/userGroups/{userGroupId}/catalogueUsers /api/userGroups/{userGroupId}/catalogueUsers/{catalogueUserId} /api/userGroups/{userGroupId}/catalogueUsers/{catalogueUserId} /api/userGroups /api/userGroups /api/userGroups/{id} /api/userGroups/{id} /api/userGroups/{id}</p>"},{"location":"rest-api/resources/user/","title":"User","text":"<p>/api/catalogueUsers /api/catalogueUsers /api/catalogueUsers/{id} /api/catalogueUsers/{id} /api/catalogueUsers/{id}</p>"},{"location":"rest-api/resources/user/#admin-functionality","title":"Admin functionality","text":"<p>/api/catalogueUsers/adminRegister /api/catalogueUsers/pending /api/catalogueUsers/userExists/{emailAddress} /api/catalogueUsers/{catalogueUserId}/adminPasswordReset /api/catalogueUsers/{catalogueUserId}/resetPasswordLink /api/catalogueUsers/{catalogueUserId}/rejectRegistration /api/catalogueUsers/{catalogueUserId}/approveRegistration /api/catalogueUsers/{catalogueUserId}/changePassword /api/catalogueUsers/{catalogueUserId}/userPreferences /api/catalogueUsers/{catalogueUserId}/userPreferences</p>"},{"location":"rest-api/resources/user/#search","title":"Search","text":"<p>/api/catalogueUsers/search/{searchTerm?} /api/catalogueUsers/search</p>"},{"location":"rest-api/resources/user/#profile-images","title":"Profile images","text":"<p>/api/catalogueUsers/{catalogueUserId}/image /api/catalogueUsers/{catalogueUserId}/image /api/catalogueUsers/{catalogueUserId}/image /api/catalogueUsers/{catalogueUserId}/image</p>"},{"location":"rest-api/resources/versioned-folder/","title":"Versioned folder","text":""},{"location":"rest-api/resources/versioned-folder/#description","title":"Description","text":"<p>A Versioned Folder is a container type and can be represented as a mixture of a Folder and a Data Model, since it shares the functional characteristics of both - a container for holding other catalogue items, and a model that can be version controlled.</p> Response body (JSON) <pre><code>{\n\"id\": \"81d00110-40e3-4ec0-b279-6004fa1b9b52\",\n\"domainType\": \"VersionedFolder\",\n\"label\": \"Sample Versioned Folder\",\n\"aliases\": [\n\"SVF\"\n],\n\"description\": \"First sample version controlled folder.\",        \"branchName\": \"main\",\n\"documentationVersion\": \"2.0.0\",\n\"modelVersion\": \"2.0.0\"\n\"lastUpdated\": \"2019-10-03T12:00:05.95Z\",\n\"classifiers\": [\n{\n\"id\": \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\",\n\"label\": \"Samples\",\n\"lastUpdated\": \"2019-10-03T09:15:37.323Z\"\n}\n],\n\"finalised\": true,\n\"hasChildFolders\": true\n}\n</code></pre> <p>The fields are as follows:</p> <ul> <li>id (UUID): The unique identifier of this versioned folder.</li> <li>domainType (Type): The domain type of this catalogue object - always \"VersionedFolder\" in this case</li> <li>label (String): The human-readable identifier of this folder.  The combination of label and documentationVersion are unique across the catalogue</li> <li>aliases (Set(String)): Any other names by which this versioned folder is known</li> <li>description (String): A long description of the versioned folder, and any important characteristics of the folder contents.  This field may include HTML, or  MarkDown.</li> <li>documentationVersion (Version): The version of the description of an underlying versioned folder.</li> <li>modelVersion (Version): The version of the folder of an underlying versioned folder.</li> <li>lastUpdated (DateTime): The date/time when this Versioned Folder was last modified</li> <li>classifiers (Set(Classifier)): The id, label and lastUpdated date of any classifiers used to tag or categorise this versioned folder  (see classifiers)</li> <li>finalised (Boolean): Whether this Versioned Folder has been 'finalised', or is in draft mode</li> <li>hasChildFolders (Boolean): Determines if this folder contains child folders.</li> </ul> <p>Endpoints which return multiple versioned folders typically include sufficient fields for generating links on the interface - a separate call to return the  details of the Versioned Folder is usually required. </p> <p>As well as the endpoints listed below, a Versioned Folder is also a CatalogueItem, and so a Versioned Folder identifier can also be used as the parameter to any  of those endpoints</p>"},{"location":"rest-api/resources/versioned-folder/#getting-information","title":"Getting information","text":"<p>The following endpoint returns a paginated list of all versioned folders readable by the current user:  </p> <p>/api/versionedFolders</p> <p>This endpoint returns all the Versioned Folders within a particular folder; again, this result is paginated.</p> <p>/api/folders/{folderId}/versionedFolders</p> <p>This endpoint provides the default information about a Versioned Folder, as per the JSON at the top of the page.</p> <p>/api/versionedFolders/{id}</p>"},{"location":"rest-api/resources/versioned-folder/#create-versioned-folder","title":"Create versioned folder","text":"<p>To create a new versioned folder from scratch, use the following post endpoint.  Within the body of this call, you should include a folder identifier.</p> <p>/api/versionedFolder</p> <p>There are two ways of versioning Versioned Folders in the catalogue.  To create an entirely new version of an existing folder, please use the following endpoint with no request body:</p> <p>/api/versionedFolders/{id}/newBranchModelVersion</p> <p>The name must be different to the original folder.</p> <p>To create a new 'documentation version', use the following endpoint:</p> <p>/api/versionedFolders/{id}/newDocumentationVersion</p> <p>By default, this will supersede the original versioned folder.</p> <p>It is also possible to branch and fork Versioned Folders to create drafts before finalising them. To create a new branch from an existing Versioned Folder:</p> <p>/api/versionedFolders/{id}/newBranchModelVersion</p> Request body (JSON) <pre><code>{\n\"branchName\": \"newBranch\"\n}\n</code></pre> <p>To create a fork of the original versioned folder:</p> <p>/api/versionedFolders/{id}/newForkModel</p> Request body (JSON) <pre><code>{\n\"label\": \"newForkLabel\"        }\n</code></pre>"},{"location":"rest-api/resources/versioned-folder/#update-versioned-folder","title":"Update versioned folder","text":"<p>To edit the primitive properties of a versioned folder, use the following endpoint, with a body similar to the JSON described at the top of this page:</p> <p>/api/versionedFolders/{id}</p> <p>To move a versioned folder from one folder to another, call the following, using the id fields for the versioned folder, and the new folder:</p> <p>/api/folders/{folderId}/versionedFolders/{id}</p> <p>Alternatively, you can call this equivalent endpoint:</p> <p>/api/versionedFolders/{id}/folder/{folderId}</p>"},{"location":"rest-api/resources/versioned-folder/#sharing","title":"Sharing","text":"<p>To allow a versioned folder to be read by any authenticated user of the system, use the following endpoint:</p> <p>/api/versionedFolders/{id}/readByAuthenticated</p> <p>... and to remove this flag, use the following:</p> <p>/api/versionedFolders/{id}/readByAuthenticated</p> <p>Similarly, to allow the versioned folder to be publicly readable - ie. readable by any unauthenticated user of the system,  use the following endpoint: </p> <p>/api/versionedFolders/{id}/readByEveryone</p> <p>... and the following to remove this flag:</p> <p>/api/versionedFolders/{id}/readByEveryone</p>"},{"location":"rest-api/resources/versioned-folder/#delete-versioned-folder","title":"Delete versioned folder","text":"<p>To delete a versioned folder, use the following endpoint.  The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete  is used if the user is an administrator.</p> <p>/api/versionedFolders/{id}?permanent={true/false}</p> <p>An administrator is able to restore a 'soft' deleted code set using the following endpoint:</p> <p>/api/admin/versionedFolders/{id}/undoSoftDelete</p>"},{"location":"rest-api/resources/versioned-folder/#finalise-a-versioned-folder","title":"Finalise a versioned folder","text":"<p>To finalise a versioned folder means to lock it to a particular version and make it read-only; only new versions can be created to make further modifications after that point. This also applies to the child contents of this versioned folder too, all child items will share the same finalised state and version number assigned. Use this endpoint with a similar payloads described below to finalise a versioned folder.</p> <p>/api/versionedFolders/{id}/finalise</p> <p>To automatically let Mauro choose the next version number, set the versionChangeType property to either <code>'Major'</code>, <code>'Minor'</code> or <code>'Patch'</code>.</p> Request body (JSON) <pre><code>{\n\"versionChangeType\": \"Major\" | \"Minor\" | \"Patch\"\n}\n</code></pre> <p>Mauro uses Semantic Versioning rules to determine the next appropriate version number based on the versionChangeType value provided.</p> <p>To optionally choose your own version number, provide this payload. If versionChangeType is <code>'Custom'</code>, then version must also be provided.</p> Request body (JSON) <pre><code>{\n\"versionChangeType\": \"Custom\",\n\"version\": \"1.2.3.4\"\n}\n</code></pre> <p>In all cases you may also supply an optional tag name to assign with the finalised version to help provide more context, as follows:</p> Request body (JSON) <pre><code>{\n\"versionChangeType\": \"Major\" | \"Minor\" | \"Patch\",\n\"versionTag\": \"My first version\"\n}\n</code></pre>"},{"location":"rest-api/resources/versioned-folder/#merging-versioned-folders","title":"Merging versioned folders","text":"<p>If creating branches of versioned folders, it is possible to merge the data values from one versioned folder to another. The first step is to calculate the differences between two versioned folders, as follows:</p> <p>/api/versionedFolders/{sourceId}/mergeDiff/{targetId}?isLegacy=false</p> Response body (JSON) <pre><code>{\n\"sourceId\": \"f9a4e390-6259-4616-b725-d45524851a82\",\n\"targetId\": \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\",\n\"path\": \"vf:Model Version Tree Folder$interestingBranch\",\n\"label\": \"Model Version Tree Folder\",\n\"count\": 6,\n\"diffs\": [            {\n\"path\": \"vf:Model Version Tree Folder$interestingBranch|ann:Test Comment\",\n\"isMergeConflict\": false,\n\"isSourceModificationAndTargetDeletion\": false,\n\"type\": \"creation\"\n},\n{\n\"path\": \"vf:Model Version Tree Folder$interestingBranch|dm:Test Data Model\",\n\"isMergeConflict\": false,\n\"isSourceModificationAndTargetDeletion\": false,\n\"type\": \"creation\"\n},\n{\n\"path\": \"vf:Model Version Tree Folder$interestingBranch|md:v1Versioning.com.mdk1\",\n\"isMergeConflict\": false,\n\"isSourceDeletionAndTargetModification\": false,\n\"type\": \"deletion\"\n},\n{\n\"fieldName\": \"value\",\n\"path\": \"vf:Model Version Tree Folder$interestingBranch|md:org.datacite.creator@value\",\n\"sourceValue\": \"Peter Monks\",\n\"targetValue\": \"Mauro Administrator\",\n\"commonAncestorValue\": null,\n\"isMergeConflict\": true,\n\"type\": \"modification\"\n},\n{\n\"fieldName\": \"value\",\n\"path\": \"vf:Model Version Tree Folder$interestingBranch|md:test.com.testProperty@value\",\n\"sourceValue\": \"Oliver Freeman\",\n\"targetValue\": \"Peter Monks\",\n\"commonAncestorValue\": null,\n\"isMergeConflict\": true,\n\"type\": \"modification\"\n},\n{\n\"path\": \"vf:Model Version Tree Folder$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\",\n\"isMergeConflict\": false,\n\"isSourceModificationAndTargetDeletion\": false,\n\"type\": \"creation\"\n}\n]\n}\n</code></pre> <p>The diffs collection will hold each change found between the two versioned folders and how they relate.</p> <p>All changes need to be manually organised into patches so that they can be applied to the target versioned folder. Then the following endpoint is used to commit:</p> <p>/api/versionedFolders/{sourceId}/mergeInto/{targetId}?isLegacy=false</p> Request body (JSON) <pre><code>{\n\"changeNotice\": \"Change comment\",\n\"deleteBranch\": false,\n\"patch\": {\n\"sourceId\": \"f9a4e390-6259-4616-b725-d45524851a82\",\n\"targetId\": \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\",\n\"label\": \"Model Version Tree Folder\",\n\"count\": 4,\n\"patches\": [\n{\n\"path\": \"vf:Model Version Tree Folder$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\",\n\"isMergeConflict\": false,\n\"isSourceModificationAndTargetDeletion\": false,\n\"type\": \"creation\"\n},\n{\n\"fieldName\": \"description\",\n\"path\": \"vf:Model Version Tree Folder$interestingBranch@description\",\n\"sourceValue\": \"\",\n\"targetValue\": \"Test description\",\n\"commonAncestorValue\": \"\",\n\"isMergeConflict\": false,\n\"type\": \"modification\"\n},  {\n\"path\": \"vf:Model Version Tree Folder$interestingBranch|md:v1Versioning.com.mdk1\",\n\"isMergeConflict\": false,\n\"isSourceDeletionAndTargetModification\": false,\n\"type\": \"deletion\"\n},\n{\n\"path\": \"vf:Model Version Tree Folder$interestingBranch|dm:Test Data Model\",\n\"isMergeConflict\": false,\n\"isSourceModificationAndTargetDeletion\": false,\n\"type\": \"creation\"\n},\n]\n}\n}\n</code></pre> <p>The key point is to set the targetValue of every patch item to change - this value is what will be written to the target versioned folder when committing the merge.</p>"},{"location":"tutorials/introduction/","title":"Introduction","text":"<p>These tutorials are intended to provide more information about particular aspects of functionality. Unlike the  User Guides, which explain how to achieve things in Mauro Data Mapper, these Tutorials will explain why such  functionality is available and how it is intended to be used.</p> <p>Our current Tutorials:</p> Semantic Links In this tutorial we explain why Semantic links are important, and how they can be used to assist re-use of existing data. Properties and Profiles This tutorial explains how to add custom information to models and model components using properties, and how to group and constrain properties  using profiles."},{"location":"tutorials/properties-profiles/","title":"Properties and profiles","text":""},{"location":"tutorials/properties-profiles/#overview","title":"Overview","text":"<p>Mauro provides a minimal set of fields that can be stored against any item in the catalogue. These are typically a Label, some other names (Aliases) and a description. It is expected that for any particular use of Mauro, there may be additional fields that users would want to store against a Data Model, or component of a model. To facilitate this, Mauro allows every catalogue item to be extensible, by allowing any arbitrary fields to be added as the properties of that object.</p>"},{"location":"tutorials/properties-profiles/#example-1","title":"Example 1","text":"<p>As a first example, consider the use of Mauro as a repository for research datasets and their discovery. For each dataset, the Data Model corresponding to that dataset should have the following information stored against it:</p> <ul> <li>Responsible organisation name</li> <li>Dataset collection date</li> <li>Size of complete dataset in Megabytes</li> <li>Link to data request form</li> </ul> <p>Furthermore, each Data Element within that Data Model should record the following fields:</p> <ul> <li>A boolean indicating whether the field contains identifiable information</li> <li>A score denoting the completeness of the field - the number of non-null values</li> </ul>"},{"location":"tutorials/properties-profiles/#example-2","title":"Example 2","text":"<p>Consider the use of Mauro as a data asset register. Each model within Mauro represents a data asset held by the organisation. For each model (each data asset held within the organisation), the following fields should be recorded:</p> <ul> <li>Department name</li> <li>Contact email address</li> <li>Server IP address</li> <li>Backup status</li> </ul>"},{"location":"tutorials/properties-profiles/#properties","title":"Properties","text":"<p>Each Mauro item can hold a number of properties, stored as a pair of key and value fields. For example, a Data Model property may have the key: \u201cResponsible Organisation Name\u201d and the value: \u201cUniversity of Oxford\u201d. Another Data Model may use the same key and a different value such as \u201cUniversity of Manchester\u201d. </p> <p>Each property also has a namespace which is a field that can disambiguate the use of the same key for different uses. For instance, the examples above may choose to use the key location for their intended use case without clashing. Namespaces typically take the form of a url (usually owned or controlled by, the use case). More information can be found here:  Namespace on Wikipedia.</p> <p>Mauro enforces a basic constraint upon properties where for any given catalogue item, there must be no two attached properties with the same namespace and key. There are no constraints on values: each is stored internally as a string and can be used to store JSON or XML for more complex values.</p>"},{"location":"tutorials/properties-profiles/#profiles","title":"Profiles","text":"<p>A profile is a group of related properties, typically sharing the same namespace. A profile allows these properties to be grouped into sections and for each defined key may add a description, a default value and a validation constraint. The user interface is able to take known profiles and present a cleaner interface for data entry against them. This involves hiding the namespace, grouping keys, providing the description for each field and providing type-specific data entry controls such as date pickers. Validation can be performed for all the fields on submission.</p> <p>As part of their specification, profiles can determine which types of catalogue item can use them. For example, you may have a profile which  should only be stored against Data Models, or one which may be used for Data Classes and Data Elements only. </p>"},{"location":"tutorials/properties-profiles/#static-vs-dynamic","title":"Static vs Dynamic","text":"<p>Profiles in Mauro can be defined in one of two ways: static or dynamic. A static profile is defined as part of a plugin, typically either stand-alone, bundled as part of an importer or exporter, or with additional functionality / REST endpoints for processing the values. These profiles are usually defined through a JSON file, and plenty of examples can be found in the shared plugin repositories, such as in the SQL importers and the defined profile plugins.</p> <p>Alternatively, profiles can be defined dynamically as Data Models within Mauro. Any model which uses the Profile Specification Profile is recognised as a profile. Each Data Class defined at the top-level of the profile specification model represents a profile section and each Data Element within those Data Classes defines the property keys themselves. As with any normal model, the Profile Specification Model can be finalised and versioned, and it is recommended that such models are finalised before use. Find out how to create Profile Specification models in our 'Dynamic Profiles' user guide.</p> <p>Static profiles are useful when additional functionality is to be tied to the particular property values, so it is important that keys are not changed independently. Dynamic profiles are much easier for an end-user to create, without the need to program a new Mauro Plugin. They are also easier to re-use and extend. However, their definition must be carefully controlled to ensure the conceptual integrity of the data stored against  them.</p>"},{"location":"tutorials/semantic-links/","title":"Semantic Links","text":""},{"location":"tutorials/semantic-links/#introduction","title":"Introduction","text":"<p>When defining the meaning of data, it can be helpful to point to definitions elsewhere that are already known or understood. For example, we might like to consider that the 'Date of Birth' field in one dataset might mean exactly the same as 'Birth Date' in another. In practice, however, we rarely find that two Data Elements share exactly the same context. Measurements might be taken using different apparatus; answers to a question on a form may differ depending on how the question is phrased or presented; the timing or ordering of data collection may alter the possible values.</p> <p>To provide a more practical approach, in Mauro Data Mapper we can link two definitions to indicate that one refines the other: it says everything that the other definition says, and possibly more. In the general case, this allows us to define abstract definitions of data with minimal context (for example as a dictionary or a data specification) and relate more concrete definitions (for example the design of a data collection form or the description of a data asset).</p> <p>We may go further and for any definitions A and B, indicate that both A refines B and B refines A. The two links together imply that the two fields really are identical and share the exact same context. In practice, we've found this to be an overly strong statement and use it very rarely.</p>"},{"location":"tutorials/semantic-links/#example","title":"Example","text":"<p>As an example, consider the three definitions given in the diagram below:</p> <p></p> <p>Here, we assume that the name of the Data Element which could be 'Word Count' and the Data Type, which could be 'Positive Integer' is the same in each case, and we are focusing simply on the explanatory text. The refinement arrows represent assertions that the definitions to the left and right are both refinements of the definition at the centre. In this example, the description 'number of words in document' defines a more abstract notion; the descriptions 'number of words in document according to Microsoft' and 'number of words in document according to Apple' provide extra context about the means of calculation.</p> <p>Such assertions cannot be derived automatically from the explanatory text. These explanations may be subjective and will only give a partial account of the context. It may be that several people with expertise in how the data is captured, recorded and analysed may all need to be consulted to provide an accurate assertion about the relationship between two descriptions.</p> <p>Note that there may not be any link defined between two more concrete definitions. In this example, there is no direct relationship between the Microsoft and Apple interpretations of the value - only that they both share some common abstract interpretation. Of course there may be some mapping of values, or conversion algorithm, which allows data collected according to one definition to be converted into a form matching another definition. In this case, perhaps opening the document on another computer and re-running the word count. Simple mappings, for example when converting units of measurement, can be uncontroversial, but in general they will also be subjective or suitable only for a particular purpose.</p>"},{"location":"tutorials/semantic-links/#interpretation","title":"Interpretation","text":"<p>There are two immediate practical applications of linking information. Considering the example above, first suppose that some data assets exist for each of the two concrete definitions above - some documents whose word count has been calculated by either Microsoft of Apple software. If the analysis only requires a value for the 'number of words in a document', then both types of data will be equally applicable and can be included in the analysis. If, however, the requirement is for counts according to Microsoft, then those documents whose word count has been calculated by Apple may not be suitable.</p> <p>Another application might be in the provision of data according to some specification. If a data specification requires values according to our abstract description: 'the number of words in document' and if we already have values for those documents as calculated by Microsoft software, then those values are suitable to provide. If, however, the specification is more concrete, and requires those word counts to have been determined by Microsoft software, then any word counts calculated by other software, or, more importantly, any document word count whose provenance is not known, will not be suitable to provide.</p>"},{"location":"tutorials/semantic-links/#linking-in-mauro","title":"Linking in Mauro","text":"<p>In Mauro Data Mapper, Semantic links can be recorded against any model components, but are typically used between two Data Elements in different Data Models. Or between an enumeration value in a model and a term in a Terminology. Other mappings, for example to indicate refinement between two Data Classes, may be much harder to interpret and may only be of use in particular circumstances.</p> <p>As described above, the assertion of Semantic links may require domain knowledge and cannot be automatically inferred from the text of a description. In Mauro Data Mapper, links are not created automatically during ingest of models, but may be manually asserted between items individually. However, Mauro includes a tool which will make suggestions based on the description text of the fields and also the name of the field, the name of the Data Type and containing Data Class, and any enumeration values. The user interface allows users to select the best match, or choose from other alternatives ranked by \u2018closest match\u2019. Although this doesn't remove the need for a human to make the final decision, it can save time searching for the correct item within a target Data Model.</p>"},{"location":"tutorials/semantic-links/#does-not-refine","title":"Does not refine","text":"<p>As our models may represent incomplete information about the artefacts they represent and the semantic relationships between them, we cannot infer that no refinement exists simply because there is no refines link present. To record the assertion that no refinement exists, we can use the **does not refine **link.</p> <p>As a further example, consider the three attribute definitions given in the diagram below. Here, we have a more specific definition at the centre of the diagram: \u2018number of words in a document ignoring hyphens\u2019. We have also an assertion that this refines the definition with the explanatory text \u2018number of words in a document\u2019. If we accept this assertion, then any data collected against the new definition can be used in any situation in which the original definition was accepted.</p> <p></p> <p>We have also an assertion that the definition \u2018number of words in a document according to Microsoft\u2019 is not a refinement of this new, more specific definition. The \u2018word count\u2019 feature in Microsoft's Word application treats hyphenated phrases as single words: it does not ignore hyphens. In contrast, the same feature in Apple's Pages application does ignore hyphens, treating them as if they were spaces. For example, Word will count the phrase \u2018strongly-connected\u2019 as one word, whereas Pages will count it as two words.</p> <p>In general, specifying 'does not refine' as a universal statement - suggesting that there are no circumstances where item A may be used according  to definition B - is a strong statement whose use will be limited.  However, there can be value in disambiguating or asserting a distinction  between two similarly-defined items whose descriptions may otherwise cause confusion. </p> <p>To find out how to add a Semantic link between two descriptions of data, see our Semantic links user guide </p>"},{"location":"tutorials/document-assets/","title":"Index","text":"<p>This document explains how to create a standard description for a health dataset using the metadata catalogue.   Such a description will come in  two parts:</p> <ol> <li>a description of the dataset as a whole</li> <li>descriptions of the individual data items within the dataset, and of the structural relationships between them </li> </ol> <p>There is no single, applicable standard for the first part.  For the moment, pending the development of the gateway interface, we require only a  minimal set of properties, sufficient to uniquely identify the dataset in question.  A slightly longer list will be required once the gateway   specification has been agreed. </p> <p>For the second part, we are able to adapt and extend an existing international standard for metadata registration: ISO/IEC 11179.  We specify a  list of properties that should be recorded for each item, and each relationship; this list will remain unchanged, although the information   provided may be updated over time. </p> <p>If the dataset is held in a relational datastore, then we may be able to determine the name and type of each data item, and the structural  relationships between them, automatically.  We will not, however, be able to determine an adequate, human-readable explanation of each item  ; this will need to be entered by hand and/or carefully extracted from existing, electronic documentation. </p>"},{"location":"tutorials/document-assets/#describing-the-dataset","title":"Describing the dataset","text":"<p>Each of the \u2018top level\u2019 descriptions in the catalogue is called a model.  Models are stored in a familiar folder (or directory) structure .  Folders may have sub-folders.  The folder tree is shown to the left of the screen.</p> <p></p> <p>To create a new model of a health dataset, first choose the folder in which you wish to store it.  To create a new folder at the top-level, click   the \u2018plus\u2019 button at the top of the tree view.</p> <p></p> <p>To create a sub-folder of an existing folder, right-click the folder and choose \u2018Add folder\u2019.  When creating a new folder, you must give it a name , and should enter a short description describing the purpose of the folder.</p> <p></p> <p>To create a new model, right-click upon the folder in which you want it to appear, and choose \u2018Add Data Model\u2019:</p> <p></p> <p>You will be presented with a short form to enter the details of your new model:</p> <p></p> <p>Choose a label for your model.  This should be enough to uniquely identify the dataset that you are describing.   You will be able to add other  names or labels later on. </p> <p>The author field should be used to record the names of those creating and maintaining this data model (not the dataset itself).</p> <p>The organisation field should be used to record the name of the organisation holding the dataset.</p> <p>In the description field, enter a short (e.g. 2 paragraphs), human-readable description of the data stored within that dataset, and any important  characteristics of the data.</p> <p>For type, choose \u2018data asset\u2019. </p> <p>Once you have entered all mandatory fields, labelled with \u2018*\u2019, you can click \u2018Next\u2019 to choose a default set of data types to be imported into  your model.</p> <p> </p> <p>For example, you may choose to import the default datatypes of a MS SQL Server database.  If you are unsure at this stage, you can leave the field  blank - you can always import these later on.</p> <p>Click \u2018Submit\u2019 to finish creating the new data model.  This will take you to the \u2018overview page\u2019 of your new model.</p> <p></p> <p>You are welcome to record further characteristics of the dataset - this could help the gateway providers when the come to design their interface .   If you wish to do this, </p> <p>Click the \u2018Properties\u2019  tab on the panel below, and choose the \u2018+\u2019 button to add each new property.</p> <p></p> <ul> <li>namespace:    This will be used to select the correct profile / property list.</li> <li>key:  This is the property name - e.g. \u2018contact email\u2019.  You should add a property for each of the names listed below, but may add further  properties if you wish.</li> <li>value:    This is the value of the given property - e.g. \u2018enquiries-mydataset@hub.org\u2019.</li> </ul>"},{"location":"tutorials/document-assets/#describing-the-data-items","title":"Describing the data items","text":"<p>Data items are created and managed within data classes.  If a dataset is managed as a collection of tables, then you may wish to create a class  for each table.  This is the default approach.  Alternatively, you may wish to create a set of classes to provide a more abstract account of the   data set - grouping and presenting the data items in a way that is quite different from the way in which they are stored and managed.</p> <p>To create a new class, select a data model from the model tree, choose the \u2018DataClasses\u2019 tab, and click the \u2018+\u2019 button:</p> <p></p> <p></p> <p>In documenting an existing data set, you will be creating rather than copying classes, so choose the first option and click \u2018Next\u2019.   You should  then choose a name or label for your class. </p> <p> </p> <p>Again, you will be able to add further names later, as aliases, if you wish. The description of a class may explain what kind of data items are  grouped together here; alternatively, it may explain some common context for the items it contains, to avoid the need to include that information   in the description of each individual item. </p> <p>The multiplicity values specify the number of instances of that class that may appear in an instance of the model.   For example, if a class were  to correspond to a table in a relational database, the multiplicity values would be the minimum, and the maximum, number of rows allowed in the   table.  In a model of a dataset, there is usually no need to specify the multiplicity of a class.</p> <p>Once all mandatory fields have been completed, you may click \u2018Submit\u2019 to create the new data class.  This will take you to the page for the newly -created class.  You can click the link back to the parent data model to continue adding further classes of data as necessary.</p> <p>You may also choose to add further \u2018child\u2019 classes to this class: choose the 'Content\u2019 tab on the DataClass page, and click the \u2018+\u2019 symbol to add  a new data class.</p> <p></p> <p>Data items are represented as \u2018data elements\u2019 within the model.  To start adding data elements to a class, visit its DataClass page, and click the  \u2018+\u2019 button on the \u2018Content\u2019 tab.  This will give you two options - to create a new contained \u2018child\u2019 class, as above, or to create a new data   element - choose the second option.</p> <p>As with creating a new class, this will give you the option to copy an existing data element from elsewhere in the model (or from another model  which you have read access to).   In creating a model of an existing dataset, you will almost certainly want to create a new data element. </p> <p></p> <p>The next form that appears will ask for the details of the new data element:</p> <p></p> <p>For label, enter the preferred name of the data item.  This could be the name of a column in a relational database, for example.  You can add  further names as aliases later.</p> <p>In description, you should describe the data item and the values it may take.  This may include information about provenance - the context of  collection, and any subsequent processing, but should also say something about intended interpretation or possible use.  This description should   explain the data point in terms that might be useful to prospective data users.</p> <p>The multiplicity of a data element specifies the number of values that it may take at the same time.   For example, the number of \u2018date of death \u2019 items contained within a \u2018patient\u2019 record might be at least 0, and at most 1.  The min field should be a positive whole number; the max  field should be a positive whole number no smaller than min, or may be \u2018*\u2019 to indicate that no maximum number is set.  If you are unsure, this   field may be left blank.</p> <p>The data type field describes the values that this data item may take.  A data type may be either:</p> <ul> <li>primitive: for example a String, an Integer, or a Date.  </li> <li>enumerated: chosen from a given list of values, which may be described using codes or free-text.  For example M = Male, F = Female, U  = Unknown</li> <li>reference-valued: a reference to another class of data.  For example, the \u2018Registered GP\u2019 data element of a \u2018Patient\u2019 may refer to a separate  class of \u2018GP\u2019 (containing name, surgery, address, etc)</li> </ul> <p>When creating a new data element, you can choose to use an existing data type that belongs to the data model in question, or you can create a new  data type (by providing its name, the list of values, or a class to reference, respectively).  From the data model page it is also possible to   import data types from another model, or some pre-defined sets of data types (such as those found in MS SQLServer databases, for example).</p> <p>Click \u2018Submit\u2019 to create the new Data Element.  Repeat this process to add the other data elements for each class.</p>"},{"location":"tutorials/document-assets/#entering-this-information-offline","title":"Entering this information offline","text":"<p>While creating a model of an existing dataset, you may find it more convenient to enter and share the information above using an Excel spreadsheet  - and then upload the contents of that spreadsheet into the catalogue.   You can create more than one model using the same spreadsheet.</p> <p>A blank spreadsheet should be included with this document, together with an example of a completed spreadsheet, describing the Diagnostic Imaging  Dataset.   </p> <p>The first sheet o the spreadsheet (which must be called DataModels) should introduce one or more data models.  Each subsequent sheet should  describe the contents of one of these data models.</p> <p>In the DataModels sheet, there should be one row for each data model described, and the following columns can be completed:</p> <ul> <li>SHEET_KEY: the name of the sheet (in this spreadsheet) describing the contents of the data model</li> <li>Name: the name or label of the data model (as explained above)</li> <li>Description: a brief description of the dataset </li> <li>Author: the author of this data model of the dataset</li> <li>Organisation: the organisation holding the dataset </li> <li>Type: this should be \u2018Data Asset\u2019</li> </ul> <p>In the blank spreadsheet supplied, there is a KEY_1 sheet with the column headings required for each of the subsequent sheets.   You can rename   or copy this sheet.  Whatever name is chosen should be included in the list of data models presented in the opening \u2018DataModels\u2019 sheet (or the    contents will not be uploaded).    The following columns should be completed:</p> <ul> <li>DataClass Path (essential): this should be the path from the top level of the model to the data class described in the current row, or the  class containing the item described in the current row; for a top-level class, it will be simply the class name; for child classes, it will be a   list of class names, using \u201c|\u201d as a delimiter.</li> <li>DataElement Name (essential for data elements): if the row is describing a data element, rather than a data class, then the name of the element  should be inserted here </li> <li>Description: an explanation of the intended interpretation (and perhaps also the context of collection or provenance) of the DataClass or  DataElement</li> <li>Minimum Multiplicity (may be left blank): the minimum number of instances of the class or element, usually 0 or 1 </li> <li>Maximum Multiplicity (may be left blank): the maximum number of instances, with -1 (rather than *) used to indicate that there is no upper  bound </li> <li>DataType Name (essential for data elements): the name of the data type of the data element being described</li> <li>DataType Description (needed only for the first time that the data type in question is mentioned): the description of the data type   </li> <li>Reference to DataClass Path: if the data type is another class (if the data element is a reference to an instance of another class) then  insert the path to that class here</li> <li>Enumeration Key and Enumeration Value: if the data type is an enumeration, then you may add several pairs of entries in these two columns,  one for each key-value pair in the enumeration; in each case, the key is the text or string that may appear in a column of the dataset, and the value is its expansion or explanatory text.   Once you have done this, you will need to select the corresponding cells in the DataClass Path column - the first column - and merge them.   You may wish to merge the corresponding cells in the other columns as well, for a clearer presentation of the information: for example, </li> </ul> <p> </p> <p>A completed spreadsheet can be imported into the catalogue using the \u2018import\u2019 button, located on the toolbar at the top of the screen:</p> <p></p> <p></p> <p>Select the Excel importer. </p> <p></p> <p>Having chosen to import from a spreadsheet, you will have the opportunity to provide some more information:</p> <ul> <li>Folder: this is the folder in which the newly created model(s) will reside.  The drop-down menu lets you choose a folder which you have  access to</li> <li>Finalised: we recommend that you keep models as \u2018draft\u2019 until the gateway presentation of model descriptions has been decided</li> <li>Import: as new documentation version  check this option if you intend to replace the current (catalogue) version of an existing model  description</li> <li>File: Choose the spreadsheet file for upload.  You may drag/drop a file from your file browser into the box here.</li> </ul> <p>Press \u2018Submit\u2019 to import the model.</p> <p>Excel files can be safely used to \"round-trip\" data model descriptions.  You can export a model from the catalogue in spreadsheet form, edit the spreadsheet, and import the new version of the spreadsheet to produce an updated version of the model - in this case, you need to select \u201cNew Documentation Version\u201d.</p>"},{"location":"tutorials/document-assets/#extracting-metadata-from-a-relational-database","title":"Extracting metadata from a relational database","text":"<p>If the dataset to be described is held in a relational database, and you have direct access to that database, you can also use the catalogue to extract basic metadata - table names, column names, types, and structural relationships - from the database itself.   The effort of manual data entry can then be focussed upon producing adequate accounts of the account of the intended interpretation of each data element.   Clicking on the \u2018import\u2019 button as above, you can select an importer for most types of relational database.</p> <p></p> <p>Having chosen to import from a relational database, the next set of options allow you to configure the import.  Fields marked with a \u2018*\u2019 are  mandatory.  The fields should be completed as follows: </p> <ul> <li>Folder: this is the folder in which the newly created model(s) will reside.  The drop-down menu lets you choose a folder which you have  access to</li> <li>Data Model Name: this will be the name of the new data model.  If no name is specified, the name of the database will be used.</li> <li>Finalised: if the newly-imported model is immediately \u2018finalised\u2019, you will no-longer be able to make changes to it.  We recommend that  you keep models as \u2018draft\u2019; otherwise if you wish to edit the descriptions you will need to create a new version.</li> <li>Import as new documentation version: this determines how the newly imported model supersedes any existing model with the same name.  We  recommend you check this option if you intend to overwrite an older version of the model.</li> <li>Database Name(s): please enter the database name.  If you\u2019d like to import multiple databases in one go, please enter the list of names , separated by a comma.</li> <li>Database Host: this is the IP address, or the server name, of the machine that the database is installed on.</li> <li>Username: this is the username which is used to connect to the database.  Ideally, a user with read-only access will be used.  This is not  stored within the Metadata Catalogue application.</li> <li>Password: the password used by the user to connect to the database.  This is not stored within the Metadata Catalogue application.</li> <li>Database port: the port which the database is communicating on - eg. 1433 for MS SQL Server.  If no port is specified, the default  port for the database will be used (MS SQL Server: 1433, Postgres: 5432, OracleDB: 1521)</li> <li>SSL: whether the database requires an encrypted connection (usually false).</li> </ul>"},{"location":"tutorials/document-assets/#other-features","title":"Other features","text":"<p>The catalogue toolkit has a range of related functions for creating and updating models of datasets and data standards.   As the features of the  gateway interface are determined, we will update this documentation to address any additional metadata requirements, and to describe any  additional functions that have become relevant.   We will update it also to describe any improvements made to the functions described above.    </p>"},{"location":"user-guides/","title":"User Guides","text":"<p>In this section you will find a variety of user guides which explain how to use Mauro Data Mapper. You don't need to follow them all in sequence - just dive into a guide that interests you!</p> <p>Our current User Guides:</p> Create a Data Model In this user guide we explain how to use the web interface to create a new folder and a new Data Model from scratch, including selecting some default Data Types. Organising Data Models This user guide summarises how to organise Data Models including rearranging folders, adding classifiers and selecting favourites.  Finalising Data Models This user guide explains how to finalise catalogue items such as Versioned Folders and Data Models. Branching, versioning and forking Data Models This user guide explains how to create a new draft of a Finalised Data Model by either creating a new Version, a new Branch or a new Fork. It also explains how to view a merge graph of a model.  Merging Data Models This user guide explains how to merge Data Models including comparing and committing changes as well as how to successfully resolve conflicts. Document a Health Dataset This longer guide explains how to add structure to your Data Model such as incorporating Data Classes and Data Elements. Bulk editing This user guide will explain how to modify multiple catalogue items at once using Profiles. Exporting Data Models This user guide will explain how to export a Data Model using a variety of exporters including XML, JSON, XML Schema, Grails and Excel. Import a Data Model from Excel This guide presents the steps for uploading a Data Model from an Excel Spreadsheet, and explains how you can use the import / export  capabilities to 'round-trip' documentation with external users. How to search In this guide we walk you through the searching capabilities of Mauro Data Mapper, including the different search types, the syntaxes used as well as tips on how to conduct specific searches.  Semantic links This is a short guide which explains the steps required to add, edit and delete Semantic links.  Publish/Subscribe This guide explains how to connect to another Mauro Data Mapper instance to consume its Atom feed of Federated Data Models. Each Federated Data Model can then be subscribed to for use in your own Mauro Data Mapper instance. User Profile This short guide explains how to update your User profile within the Mauro Data Mapper web interface and how to change your login password. Admin functionality This user guide will walk you through all the options and settings that are available to administrators on Mauro Data Mapper. User permissions This user guide explains the multiple levels of access to catalogue items and adminstration supported in Mauro Data Mapper. Feature switches This user guide will walk you through feature switches that allow administrators to enable or disable certain Mauro features. Dynamic Profiles This user guide walks through the steps for creating and managing dynamic  profiles. These are profiles which are defined by a model elsewhere in the system. Digital Object Identifiers This user guide explains how to set-up the Mauro Digital Object Identifier plugin to allow you to use, edit and remove DOI profiles. You will also find out how to create, submit and retire profile names."},{"location":"user-guides/add-a-semantic-link/semantic-links/","title":"Semantic links","text":"<p>This user guide will explain the steps you need to follow to add a Semantic link between two descriptions of data.</p>"},{"location":"user-guides/add-a-semantic-link/semantic-links/#1-add-a-semantic-link","title":"1. Add a Semantic link","text":"<p>Firstly, navigate to the source Data Model, Data Class or Data Element that you want to create the link from. Select the relevant data item in the Model Tree to display the details panel on the right. </p> <p>Any existing Semantic links are summarised in the 'Links' table below the details panel. To add, edit or remove a Semantic link, select the 'Links' tab, which will also display a list of existing Semantic links.</p> <p></p> <p>Click the '+ Add Link' button at the top right of the 'Links' table which will add a new row. </p> <p></p> <p>Complete the fields as described below:</p> <ul> <li> <p>Source     This is the Data Model, Data Class or Data Element which you are linking from.</p> </li> <li> <p>Link     From the dropdown menu, select the type of Semantic link between 'Refines' or 'Does Not Refine'. </p> <p>A 'Refines' link is used when the description of the source 'Refines' that of the target. In other words, everything that is true about the target description is also true about the source description, with the source description often adding more information or context.</p> <p>A 'Does Not Refine' link is used when the description of the source is not intended to refine that of the target.</p> </li> <li> <p>Target     Select the target description that you want to link to. </p> <p>To do this, click 'Add target' which will open up a seperate box. Choose whether the target is a Data Model or a Data Class and then select the relevant item from the Model Tree. </p> <p>This will automatically populate the 'Target' field and once completed, click the green tick to save and a green notification box should appear at the bottom right of your screen confirming that the 'Link saved successfuly'.</p> </li> </ul>"},{"location":"user-guides/add-a-semantic-link/semantic-links/#2-delete-a-semantic-link","title":"2. Delete a Semantic link","text":"<p>To delete an existing Semantic link, navigate to the relevant link in the 'Links' tab underneath the details panel. </p> <p>Click the 'Delete' bin icon where you will then be asked to confirm the change. Click the green tick and a green notification box should appear at the bottom right of your screen confirming that the 'Link deleted successfuly'.</p>"},{"location":"user-guides/add-a-semantic-link/semantic-links/#3-edit-a-semantic-link","title":"3. Edit a Semantic link","text":"<p>To edit an existing Semantic link, navigate to the relevant link in the 'Links' tab underneath the details panel. </p> <p>Click the 'Edit' pencil icon which will allow you to change the 'Link' and 'Target' columns. Click the green tick to the right to confirm your changes and a green notification box should appear at the bottom right of your screen confirming that the 'Link updated successfuly'.</p>"},{"location":"user-guides/admin-functionality/admin-functionality/","title":"Admin functionality","text":"<p>This user guide will walk you through all the options and settings that are available to adminsitrators on Mauro Data Mapper.</p>"},{"location":"user-guides/admin-functionality/admin-functionality/#1-dashboard","title":"1. Dashboard","text":"<p>To access your admin dashboard, log in to Mauro Data Mapper and click the white arrow by your user profile to the right of the menu header. From the dropdown menu under 'Admin settings' select 'Dashboard'.</p> <p></p> <p>Your dashboard displays two tabs. One shows your 'Active sessions' and the other shows the 'Plugins and Modules' in the repository. </p> <p></p>"},{"location":"user-guides/admin-functionality/admin-functionality/#2-model-management","title":"2. Model management","text":"<p>As an administrator, you can delete several elements such as Data Models and Terminologies. </p> <p>To do this, select 'Model management' from the user profile dropdown menu. Select the relevant options to filter elements by type and status and the Model Tree displayed at the bottom of the page will automatically filter. </p> <p></p> <p>Once you've found the element you wish to delete, click the checkbox until a green tick appears. You can do this for multiple elements and a summary list will be displayed on the right. Here, you will then have the option to either 'Delete Permanently' or 'Mark as Deleted'.</p>"},{"location":"user-guides/admin-functionality/admin-functionality/#3-emails","title":"3. Emails","text":"<p>To access your emails, select 'Emails' from the user profile dropdown menu. This will take you to your inbox where you can compose, edit and delete messages. </p>"},{"location":"user-guides/admin-functionality/admin-functionality/#4-manage-users","title":"4. Manage users","text":"<p>As an administrator you can add, activate and deactive users. Firstly, select 'Manage users' from the user profile dropdown menu. This will navigate you to a full list of all the users within the repository. Each user's full name, email, organisation, role, any groups they are associated with as well as the status of their account will be displayed. </p> <p></p> <p>To order the list, you can click the small arrow to the right of each column heading. This will display the list in chronological order according to that category. To order from A-Z click the arrow until it points up. To order from Z-A click the arrow until it points down.</p> <p>You can also filter the user list, by clicking the filter symbol, to the right of 'repository'. This will display three boxes at the top of the list where you can then enter either a 'Name', 'Email', or 'Organisation' to filter the list by.</p> <p></p>"},{"location":"user-guides/admin-functionality/admin-functionality/#41-add-a-new-user","title":"4.1 Add a new user","text":"<p>To add a new user click the '+Add user' button at the top right of the 'Manage users' page. This will bring up an 'Add User' form which you will need to complete. </p> <p></p> <p>Enter the email address, first name, last name, organisation and role for the new user. </p> <p>You then need to add the user to the correct group to define what permissions they should have. To do this, click the 'Choose a Group' box and select the relevant group from the dorpdown menu. </p> <p></p> <p>Once all the fields have been completed, click 'Add user' and a green notification box should appear at the bottom right of your screen confirming that the 'User saved successfully'. </p>"},{"location":"user-guides/admin-functionality/admin-functionality/#42-reset-password","title":"4.2 Reset password","text":"<p>To reset a user's password, click the 'Actions' button to the right of the relevant row and select 'Reset password' from the dropdown menu. </p> <p>A green notification box will appear at the bottom right of your screen, confirming that a 'Reset password email sent successfully'. The user can then follow the instructions in the email to reset their password. </p> <p></p>"},{"location":"user-guides/admin-functionality/admin-functionality/#43-activate-or-deactivate-a-user","title":"4.3 Activate or Deactivate a user","text":"<p>The details of whether a user's account is 'Active' or 'Disabled' is displayed in the 'Status' column.</p> <p>To activate a user's account, click the 'Actions' button to the right of the relevant user and select 'Activate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully'. The status of the user will now change to 'Active'. </p> <p></p> <p>To deactivate a user's account, click the 'Actions' button to the right of the relevant user and select 'Deactivate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully'. The status of the user will now change to 'Disabled'. </p> <p></p>"},{"location":"user-guides/admin-functionality/admin-functionality/#5-pending-users","title":"5. Pending users","text":"<p>To approve or reject users, select 'Pending users' from the user profile dropdown menu. This will navigate you to a list of all the users that are waiting for approval. </p> <p>To approve or reject a user, click the 'Actions' button to the right of the relevant user and select either 'Approve user' or 'Reject user' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming the change. </p> <p></p>"},{"location":"user-guides/admin-functionality/admin-functionality/#6-manage-groups","title":"6. Manage groups","text":"<p>As an administrator you can also add, edit and delete groups. To do this, select 'Manage groups' from the user profile dropdown menu. This will navigate you to a list of all the groups currently stored in the repository.</p> <p></p> <p>To order the list, click the small arrow to the right of 'Name'. This will display the list in chronological order from A-Z when the arrow points up and from Z-A when the arrow points down.</p> <p>You can also filter the user list, by clicking the filter symbol to the right of 'repository'. This will display a 'Name' box at the top of the list where you can then enter a group name and the list will automatically filter.</p>"},{"location":"user-guides/admin-functionality/admin-functionality/#61-add-a-group","title":"6.1 Add a group","text":"<p>To add a new group, click the '+Add' button at the top right of the 'Manage groups' page. Enter the 'Name' and 'Description' of the new group and click 'Save group'. A green notifiction box will appear at the bottom right of your screen confirming that the 'Group saved successfully'. </p> <p></p>"},{"location":"user-guides/admin-functionality/admin-functionality/#62-edit-a-group","title":"6.2 Edit a group","text":"<p>To edit a group, click the 'Actions' button to the right of the relevant group and select 'Edit group' from the dropdown menu.</p> <p></p> <p>This will take you to a summary page of the group, where you can amend the name and description. Make sure to click 'Save group' to save any changes.</p> <p></p> <p>Also displayed is a list of members, which can be found under the 'Members' tab. Here, you can see the status, details and role of each member. You can aslo delete members by clicking the red bin icon to the right of the relevant row. Any past amendments to the group can be viewed under the 'History' tab.</p>"},{"location":"user-guides/admin-functionality/admin-functionality/#62-delete-a-group","title":"6.2 Delete a group","text":"<p>To delete a group, click the 'Actions' button to the right of the relevant row and select 'Delete group'. A green notification box will appear at the bottom right of your screen to confirm that the 'Group deleted successfully'.</p>"},{"location":"user-guides/admin-functionality/admin-functionality/#7-configuration","title":"7. Configuration","text":"<p>As an administrator you can edit the various email templates associated with Mauro Data Mapper including:</p> <ul> <li>Admin confirm user registration email</li> <li>Admin registered user email</li> <li>User invited to edit email</li> <li>User invited to view email</li> <li>User self registered email</li> <li>Forgotten password email</li> <li>Reset password email</li> </ul> <p>To access these templates, select 'Configuration' from the user profile dropdown menu. If you make any changes remember to press 'Save email configuration' at the bottom of the page. </p> <p>You can also rebuild the Lucene Search Index by clicking the 'Lucene' tab and then the 'Rebuild index' button. </p> <p></p>"},{"location":"user-guides/branch-version-fork/branch-version-fork/","title":"Branching, versioning and forking Data Models","text":"<p>This user guide explains how to create a new draft of a Finalised Data Model by either creating a new Version, a new Branch or a new Fork. It also explains how to view a merge graph of a model. </p>"},{"location":"user-guides/branch-version-fork/branch-version-fork/#1-overview","title":"1. Overview","text":"<p>When a Data Model is finalised it cannot be modified any further in that state. It represents the final state/contents of that model for a particular version.</p> <p>To find out how to Finalise catalogue items, visit our 'Finalising Data Models' user guide.</p> <p>If you want to edit a Finalised model, you will need to create a new draft which you can then work on. There are three ways to create a new draft:</p> <ol> <li>Create a new Version</li> <li>Create a new Branch</li> <li>Create a new Fork</li> </ol> <p>Selecting the right method depends on what you want to achieve:</p> <ul> <li> <p>Create a new Version if:</p> <ul> <li>You are a single-person team making edits</li> <li>You want to make changes to a model in a linear fashion</li> <li>You want a simpler workflow i.e. not requiring merging of branches</li> </ul> </li> <li> <p>Create a new Branch if:</p> <ul> <li>You work in a multi-person team who are making edits at the same time</li> <li>You eventually plan on merging all branches back together into the main branch to finalise</li> <li>You want to review everyone's changes before they get merged into the main branch   </li> </ul> </li> <li> <p>Create a new Fork if:</p> <ul> <li>You plan to take the model in a new direction. For example, under a new authority or if you want to use an existing model as the starting point for a new model</li> <li>You do not plan on merging this continuation back into the original workflow</li> </ul> </li> </ul>"},{"location":"user-guides/branch-version-fork/branch-version-fork/#2-criteria","title":"2. Criteria","text":"<p>The following catalogue items can be put into a draft state:</p> <ul> <li>Data Models</li> <li>Terminologies</li> <li>Code Sets</li> <li>Reference Data Models</li> <li>Versioned Folders</li> </ul> <p>Any of the catalogue items above can be used to create a new draft, so long as they are:</p> <ol> <li>Already Finalised</li> <li>Have a version number</li> </ol>"},{"location":"user-guides/branch-version-fork/branch-version-fork/#21-creating-a-draft-of-a-versioned-folder","title":"2.1 Creating a draft of a Versioned Folder","text":"<p>When creating a new draft of a Versioned Folder, every model within this folder will also be put into a draft state as well. </p> <p>This is shown by 'Draft' highlighted in yellow to the right of the model's name in the details panel.  </p> <p></p> <p>However, a Data Model within a Versioned Folder cannot be individually put into a draft state as it is version controlled by it's parent folder. Therefore, only when the Versioned Folder is put into a draft state will the model within it become a draft too. </p>"},{"location":"user-guides/branch-version-fork/branch-version-fork/#3-creating-a-new-draft","title":"3. Creating a new draft","text":"<p>To create a new draft, first select the relevant catalogue item in the Model Tree. As mentioned above, make sure that this item has already been Finalised. Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel and select 'Create a New Version' from the dropdown menu.</p> <p></p> <p>A 'New version' form will appear which you will need to complete. When selecting the type of new version that you want to create, there are three 'Actions' to choose from in the dropdown menu. These are: 'New Version', 'New Branch' or a 'New Fork'.</p> <p></p> <p>If you want to create a 'New Version', select this option from the dropdown menu and then click 'Create'. A green notification box should appear at the bottom right of your screen confirming that the 'New version created successfully'.</p> <p></p> <p>If you want to create a 'New Branch', select this option from the dropdown menu. Enter a new 'Branch name' which helps describe it and then click 'Create'.  A green notification box should appear at the bottom right of your screen confirming that the 'New branch created successfully'.</p> <p>When creating a new Branch, if a main branch doesn't already exist, then this will automatically be created as well. The main branch acts as the main line, or trunk of the changes made to a model.</p> <p></p> <p>If you want to create a 'New Fork', select this option from the dropdown menu. Then enter a 'New label' for the model and click 'Create'.  A green notification box should appear at the bottom right of your screen confirming that the 'New fork created successfully'.</p> <p></p> <p>Once completed, the new item will then be displayed in the draft state.</p>"},{"location":"user-guides/branch-version-fork/branch-version-fork/#4-merge-graph","title":"4. Merge graph","text":"<p>To help you visualise and track Versions, Branches and Forks, Mauro has the ability to illustrate the relationships between each version of a model via a merge graph.</p> <p>To access this merge graph, select a catalogue item that is in a draft state in the Model Tree. Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Merge' from the dropdown menu and then 'Show merge graph' from the secondary dropdown menu. </p> <p></p> <p>A 'Versioning graph' window will then appear. This illustrates the different versions of the model throughout it's history and how all the versions link together.</p> <p>You can zoom in or out by using the '+' and '-' buttons at the top left of the window, or within the graph itself. You can refresh the graph by selecting the circular arrows or the 'RESET' button at the top left. </p> <p>You can also download a copy of the graph or view it in full screen using the buttons at the top right of the window.</p> <p></p> <p>The various colours of the version boxes represent different states of the model:</p> <ul> <li> <p>Dark orange     The current version/branch of the model being viewed</p> </li> <li> <p>Yellow     Finalised version of a model</p> </li> <li> <p>Light yellow     Draft branch of a model</p> </li> <li> <p>Dark blue     Fork of a model</p> </li> </ul> <p></p>"},{"location":"user-guides/bulk-editing/bulk-editing/","title":"Bulk editing","text":"<p>This user guide will explain how to modify more than one catalogue item at once using Profiles. </p> <p>The Mauro Data Mapper bulk editor can support editing the following top-level domains:</p> <ol> <li>Data Models</li> <li>Data Classes</li> <li>Terminologies</li> <li>Code Sets</li> </ol> <p>From these, the following child items may be edited:</p> <ol> <li>Data Types</li> <li>Data Classes</li> <li>Data Elements</li> <li>Terms</li> </ol>"},{"location":"user-guides/bulk-editing/bulk-editing/#1-select-a-model","title":"1. Select a Model","text":"<p>Select a model currently in a draft state from the Model Tree. Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Bulk Edit' from the dropdown menu.</p> <p></p>"},{"location":"user-guides/bulk-editing/bulk-editing/#2-selecting-items-and-profiles","title":"2. Selecting Items and Profiles","text":"<p>Before proceeding to the bulk editor, you must first select your item(s) and the Profiles to edit. First choose which domain type to use for the items; only one child domain type can be edited at a time, and the available options will depend on which top-level model you chose.</p> <p></p> <p>Selecting a domain type will then search for possible catalogue items of that type under the model. Click on one or more items from the list to select which items to edit. There are also ways of selecting (or clearing) all items and filtering to aid navigation if the list is large.</p> <p>Finally, select one of more Profiles to edit against these items. Most profiles are located due to their domain type or via the Mauro configuration you have (for example, profiles may be imported via plugins, or dynamically created by yourself). A special profile in this list is the \"Default profile\" - this provides the same fields as you would find when editing a catalogue item on its own, such as Description, Aliases, and so on.</p> <p>Click on the Next button to continue.</p> <p>Information</p> <p>At least one item and profile must be selected before you can continue.</p> <p></p>"},{"location":"user-guides/bulk-editing/bulk-editing/#3-bulk-editing","title":"3. Bulk editing","text":"<p>The next screen will display a tab for each chosen profile. Inside each tab will be:</p> <ol> <li>An editable grid listing each element along with each profile field.</li> <li>The ability to validate and save your changes within that tab/profile.</li> <li>A visual count of how many current validation issues were found, or a visual cue stating the profiles are valid.</li> </ol> <p></p> <p>Any validation errors found will be highlighted.</p> <p></p> <p>Simply click on any grid cell to display an editing cursor, type in the value, and either click away from that cell or press the 'Enter' key to accept that change.</p> <p>Here are some other useful tips and information:</p>"},{"location":"user-guides/bulk-editing/bulk-editing/#profile-field-descriptions","title":"Profile field descriptions","text":"<p>Some profile fields may describe what they are. Hover your mouse cursor over a column heading and a tooltip may appear to provide further information.</p> <p></p>"},{"location":"user-guides/bulk-editing/bulk-editing/#cell-values","title":"Cell values","text":"<p>Mostly you will be editing text cells, though Profiles do support other types of data, which the bulk editor can also support.</p> <p>Note</p> <p>The different data types supported is currently limited, though further support will come in the future.</p> <p>For example:</p> <ul> <li>Text areas for editing large sections of text, such as descriptions. It is posible to edit large text fields using a simple pop-up editor, or by clicking the Pop-out button to use full editing tools:</li> </ul> <p></p> <p></p> <ul> <li>Checkboxes to support boolean types:</li> </ul> <p></p> <ul> <li>Select lists to support enumerations:</li> </ul> <p></p> <ul> <li>Calendar pickers to support dates:</li> </ul> <p></p> <p>Some cells may also appear uneditable, this will be because of the way the Profile works - either because the profile field is read-only, or some profile fields can be configured to infer metadata from other parts of the model.</p> <p></p>"},{"location":"user-guides/bulk-editing/bulk-editing/#validation","title":"Validation","text":"<p>When you click on the 'Validate' button, the Profiles as currently edited will be verified by Mauro. Any issues found will be highlighted with a red border around the appropriate cells, and an issue total will state how many issues were found altogether.</p> <p>Information</p> <p>Validating profiles may show issues, but this will not prevent you from saving your changes - this action merely acts as a warning to the editor.</p> <p></p>"},{"location":"user-guides/bulk-editing/bulk-editing/#4-save-your-changes","title":"4. Save your changes","text":"<p>Once you are happy with the changes you have made in a profile tab, click on the 'Save' button. A message will appear to show the operation was successful, and you may continue making more changes, or click the 'Close' button to go back to the model.</p> <p></p>"},{"location":"user-guides/create-a-data-model/create-a-data-model/","title":"Create a Data Model","text":"<p>This user guide will explain the steps you need to follow to create a new Data Model. </p>"},{"location":"user-guides/create-a-data-model/create-a-data-model/#1-create-a-new-folder","title":"1. Create a new folder","text":"<p>Data Models are stored in their own folders and subfolders which are displayed in the Model Tree. Therefore, to create a new Data Model, first you need to either create a new folder, or add a subfolder.</p> <p>Ensure the 'Models' tab is selected above the Model Tree. To create a new top-level folder click the 'Create a new Folder' icon at the top right of the  Model Tree. To create a new subfolder, right click on an existing folder and select '+ Create' and then 'Folder' from the dropdown menu.</p> <p></p> <p>A 'Create a new Folder' dialogue box will appear. Enter a name (Label) for the folder and tick 'Version control this folder' if you wish to create a Versioned Folder. Once complete, click 'Add folder' to save your changes.</p> <p>The 'Version control this folder' option will only appear if adding to the top level of the Model Tree or under another folder. Versioned folders cannot be created under other Versioned folders.</p> <p></p>"},{"location":"user-guides/create-a-data-model/create-a-data-model/#2-add-data-model","title":"2. Add Data Model","text":"<p>To add a Data Model, right click the relevant folder and select 'Add Data Model'. A 'New Data Model' form will appear on the right. </p> <p></p>"},{"location":"user-guides/create-a-data-model/create-a-data-model/#3-complete-new-data-model-form","title":"3. Complete New Data Model form","text":""},{"location":"user-guides/create-a-data-model/create-a-data-model/#31-complete-data-model-details","title":"3.1 Complete Data Model Details","text":"<p>Please complete both the mandatory and optional fields of the 'New Data Model' form. The defintions of each field are detailed below:</p> <ul> <li> <p>Label     Enter a unique name for the Data Model and include any version information, as two Data Models cannot share the same Label.</p> </li> <li> <p>Author     Use this field to record the name(s) of the authors who are creating and maintaining this Data Model.</p> </li> <li> <p>Organisation     Type the name of the organisation responsible for the Data Model, or the underlying data.</p> </li> <li> <p>Description     Enter a detailed description in either plain text or html. Include any important contextual details relating to the Data Model.</p> </li> <li> <p>Select a Data Model Type     Select whether the Data Model is a Data Asset or a Data Standard from the dropdown menu.      A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema.</p> </li> <li> <p>Classifications     Select any relevant Classifications (also known as tags) from the dropdown menu. You can select as many Classifications as you like. </p> </li> </ul> <p>Once all the fields have been populated click 'Next Step' to complete the 'Default Data Types' section of the form. </p>"},{"location":"user-guides/create-a-data-model/create-a-data-model/#32-select-default-data-types","title":"3.2 Select Default Data Types","text":"<p>Select the relevant set of 'Default Data Types' from the dropdown menu. These will be imported into your Data Model. You should select the category that includes all the Data Types that you will likely have within your Data Model, however if you are unsure at this stage, then leave this field blank and import them later on.</p> <p></p>"},{"location":"user-guides/create-a-data-model/create-a-data-model/#4-submit-data-model","title":"4. Submit Data Model","text":"<p>Once completed, click 'Submit Data Model' and your new Data Model will be added. When selected in the Model Tree the details of the Data Model will now be displayed on the right.</p> <p></p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/","title":"Document a Dataset","text":"<p>This user guide will explain the steps you need to follow to manually add a health dataset to Mauro Data Mapper.</p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#1-create-a-data-model","title":"1. Create a Data Model","text":"<p>Datasets are stored in their own Data Models within Mauro Data Mapper. Therefore, you first need to create a new Data Model. To do this, follow the steps in the 'Create a Data Model user guide'.</p> <p>Once you have reached step 3, 'Complete New Data Model form' you will need to select the 'Data Model Type' as Data Asset from the dropdown menu.</p> <p>Fill in the rest of the 'New Data Model' form and submit the Data Model as explained in steps 3.2 and 4.</p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#2-add-a-property","title":"2. Add a property","text":"<p>Once you've created your Data Model, it's important to record further characteristics of the corresponding dataset, particularly to help gateway providers when designing interfaces.</p> <p>To do this, select the Data Model in the Model Tree and then click the 'Properties' tab underneath the Data Model details panel. Click the '+ Add Property' button on the right to add a new row to the property table. </p> <p>Complete the details of the new property as follows:</p> <ul> <li> <p>Namespace     This will be used to select the correct profile / selection of properties.</p> </li> <li> <p>Key     Where an existing namespace has been chosen, select a relevant property name such as 'contact email'.  Otherwise, enter a new property name.</p> </li> <li> <p>Value     This is the value of the given property, for example \u2018enquiries-mydataset@hub.org\u2019.     You can also add a relevant element to the value of a property. Click '+ Add Element' in the 'Value' column and select the element type from the menu. Search for the element you require and once selected, it will automatically import into the 'Value' column of the properties table.     </p> </li> </ul> <p>Once you have filled in the details of the property, click the green 'Save' tick and the new property will be added to the table. </p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#3-create-a-data-class","title":"3. Create a Data Class","text":"<p>Each Data Model is made up of several Data Classes which is where data items are both created and managed. </p> <p>If your dataset is a collection of tables, the conventional approach is to create a new class for each table. Alternatively, you can create a set of classes to provide a more abstract account of the data set, which group and present the data differently to how it is stored and managed.</p> <p>To create a Data Class, select the relevant Data Model in the Model Tree and click the 'Data Classes' tab on the panel below the Data Model details. Click the '+ Add' button on the right and a 'New Data Class' form will appear.</p> <p>There are two ways to import a Data Class into a Data Model. You can either create a new Data Class or copy a Data Class from an existing Data Model. </p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#31-create-a-new-data-class","title":"3.1 Create a New Data Class","text":"<p>To create a new Data Class, select this option in the first section of the 'New Data Class' form and then click the 'Next step' button.</p> <p></p> <p>Now you need to complete the 'Data Class Details' section of the form as follows: </p> <ul> <li> <p>Label     Enter a name for the new Data Class which has to be unique within the Data Model.</p> </li> <li> <p>Description     Complete a description in either html or plain text which explains the types of data items grouped together within this Data Class. Also include contextual details which are common to the data items, to avoid having to add descriptions to each individual data item.</p> </li> <li> <p>Multiplicity      The Multiplicity specifies the minimum and maximum number of times that the Data Class will appear within the Data Model. </p> <p>Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of * which represents -1 internally.       </p> </li> </ul> <p></p> <p>Once you have completed the 'Data Class Details' form, click 'Submit Data Class' and the new Data Class will now be permanently displayed under the 'Data Classes' tab of the Data Model. You can add as many Data Classes as necessary.</p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#32-copy-a-data-class","title":"3.2 Copy a Data Class","text":"<p>To import Data Classes from an existing Data Model, select the 'Copy Data Classes(s) from...' option in the first section of the 'New Data Class' form. Select the relevant Data Model by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can then select the relevant Data Model. Once selected, click 'Next step'.</p> <p>The 'Data Class Details' section of the form will then appear, with a list of all the Data Classes within the selected Data Model. Select the Data Classes you wish to import and then click 'Submit Data Class'. The selected Data Classes will then be imported into your original Data Model, with the progress illustrated by a green loading bar at the bottom of the form. </p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#4-add-a-nested-data-class","title":"4. Add a Nested Data Class","text":"<p>A useful way of managing complex data sets is to use Nested Data Classes which are essentially a Data Class within a Data Class.</p> <p>For example, in a webform, there may be a section called 'Contact details', which would be one Data Class. Within that section however, there may be another labelled 'Correspondence Address', which would be a Nested Data Class.  </p> <p></p> <p>To add a Nested Data Class, click the relevant Data Class from the Model Tree and click the 'Content' tab on the panel below the  model overview. Then click '+ Add' and select 'Add Data Class' from the dropdown menu. Complete the 'New Data Class' form as explained  above in step '3. Create a Data Class'.</p> <p></p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#5-add-data-elements","title":"5. Add Data Elements","text":"<p>Within each Data Class lies several Data Elements which are the descriptions of an individual field, variable, column or property of a data item. </p> <p>To create a Data Element you can use the same approach as creating a Data Class. Select the relevant Data Class in the Model Tree and click the 'Content' tab on the panel below the Data Class details. Click the '+ Add' button on the right and you will be given the choice to either add a Data Class or a Data Element. Select 'Add Data Element' and a 'New Data Element' form will appear. </p> <p>Similar to adding a Data Class, there are two ways to import a Data Element. You can either create a new Data Element or copy Data Elements from an existing Data Class.</p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#51-create-a-new-data-element","title":"5.1 Create a New Data Element","text":"<p>Follow the steps in '3.1 Create a new Data Class' until you have completed the Label, Description and  Multiplicity fields for the Data Element. </p> <p>Each Data Element then needs to be assigned a relevant Data Type. This can either be selected from an existing list, or you can add a new Data Type. </p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#511-select-an-existing-data-type","title":"5.1.1 Select an existing Data Type","text":"<p>Click the 'Search' box and a dropdown list of existing Data Types will appear. Select the relevant Data Type. </p> <p>You can then assign several Classifications to the Data Type by selecting them from the dropdown menu. Once all fields are complete, click 'Submit Data Element' to add the Data Element to the Data Class. Repeat this process to add other Data Elements.   </p> <p></p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#512-add-a-new-data-type","title":"5.1.2 Add a new Data Type","text":"<p>To add a new Data Type, click 'Add a new Data Type' on the 'Data Element Details' form. Fill in the Label and Description fields and select the relevant Data Type. </p> <p>A Data Type can either be:  </p> <ul> <li> <p>Enumeration:     A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value.      If 'Enumeration' has been selected, an additional table will appear where you can add several Enumerations and specify a 'Group', 'Key' and 'Value'. </p> </li> <li> <p>Primitive:     Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019, \u2018Integer\u2019 or \u2018Date\u2019.</p> </li> <li> <p>Reference:     Data which refers to another Data Class within the same Data Model.      If Reference has been selected, the Reference Data Class can be selected from a dropdown menu. </p> </li> <li> <p>Terminology:     A structured collection of Enumeration Values which have relationships between different data terms.      Similarly, if Terminology has been selected, the relevant category can be chosen from a dropdown menu.</p> </li> </ul> <p>You can then assign several 'Classifications' by selecting them from the dropdown menu. </p> <p>Once all fields are complete, click 'Submit Data Element' to add the new Data Element to the Data Class. Go back to step '5.1 Create a new Data Element' and repeat the process to add other Data Elements.</p>"},{"location":"user-guides/document-a-dataset/document-a-dataset/#52-copy-a-data-element","title":"5.2 Copy a Data Element","text":"<p>To import a Data Element from an existing Data Class, select the 'Copy Data Element(s) from...' option in the first section of the 'New Data Element' form. Select the relevant Data Class by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can select the relevant Data Model and Data Class. Once selected, click 'Next step'.</p> <p>The 'Data Element Details' section of the form will then appear, with a list of all the Data Elements within the selected Data Class. Select the Data Elements you wish to import by ticking the relevant boxes and these will then appear in a 'Summary of Selected Data Elements' table at the bottom of the form. Once you have checked this table is correct, click 'Submit Data Element'. </p> <p></p> <p>The selected Data Elements will then be imported into your original Data Class, with the progress illustrated by a green loading bar at the bottom of the form. </p>"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/","title":"Dynamic profiles","text":"<p>Profiles are a way to store additional information about Data Models or their components.  You can read more about profiles in our Properties and profiles tutorial.  This user guide walks through the steps for creating and managing dynamic  profiles. These are profiles which are defined by a model elsewhere in the system.  The first stage is to create a definition model.</p>"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#1-creating-a-profile-definition-model","title":"1. Creating a profile definition model","text":"<p>In this user guide, we'll create a dynamic profile for storing organisation data inventory information.</p> <p>You may decide to create a folder specifically for storing dynamic profile models, but alternatively you may store them within the folder they'll  be used.   Section 6. 'Admin dashboard for dynamic profiles' describes the admin dashboard which allows these specifications to be easily found  at a later date.</p> <p>First, create a Data Model as explained in Section 2. 'Add Data Model' of our 'Create a Data Model' user guide. Add a Label, author and organisation. Select Data Standard from the 'Data Model Type' dropdown and select any relevant classifications. Once completed, click 'Next step'.</p> <p></p> <p>Next, you must choose to import the default set of 'Profile Specification Data Types' from the dropdown list.  These define the supported  basic data types for profile fields which you'll be choosing from when adding Data Elements to your profile definition model. Once finished, click 'Submit Data Model' to create this new Data Model. </p> <p></p> <p>In the next step you will add technical information to assist the server in  storing properties against your profile.</p>"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#2-apply-the-profile-specification-profile","title":"2. Apply the Profile Specification Profile","text":"<p>From the description page, use the profile dropdown to select 'Add New Profile'. Then choose 'Profile Specification Profile (Data Model)' from the dropdown menu. </p> <p></p> <p>Click 'Save Changes' and this will bring up the form for editing the profile fields. There are two fields for completion:</p> <ul> <li> <p>Metadata Namespace     This mandatory field defines the namespace that separates the properties in this profile from others in other profiles.  You should try and  choose a unique name that is based on a URL and the purpose of this profile model. For more information about namespaces, see this article.</p> </li> <li> <p>Applicable for domains     This optional field determines which types of catalogue item can be profiled using this profile. For example, 'DataModel'.  You should separate  multiple domains with a semi-colon (';'). Leave this field blank to allow this profile to be applicable to any catalogue item. </p> </li> </ul> <p>Click 'Save' to finish editing the dynamic profile model details.</p> <p></p>"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#3-add-data-classes-and-data-elements","title":"3. Add Data Classes and Data Elements","text":"<p>Once you've created your profile model, you can start adding fields that will be stored for those catalogue items that use the profile.  You should  create one or more Data Classes to group your Data Elements. Any description you give to those Data Classes will be visible in the editing  interface for that profile.</p> <p>In this example, we'll create classes for 'Basic Information', 'Data Source Owner', and 'Physical Location'.  Create each Data Class as explained in  Section 3. 'Create a Data Class' of our 'Document a Dataset' user guide. </p> <p>Add a Label and a description. You do not need to add Multiplicities to the classes as repeated groups are not currently supported in profile definitions. The structure of the Data Model (red), along with it's Data Classes (blue) is shown below.</p> <p></p> <p>Within each Data Class you can add Data Elements for the fields you wish to store.  Again, create these in the usual way, as explained in Section 5. 'Add Data Elements' of our 'Document a Dataset' user guide. You will need to enter the following details for each element:</p> <ul> <li> <p>Label     The name of this field in the profile.  The 'key' of properties corresponding to this field  (See tutorial) will be derived from this name unless overridden (See Section 4. 'Customise the fields').</p> </li> <li> <p>Description     The description of this property. This will be presented to users as they view or edit profile fields, so is a good opportunity to better  describe the information you'd like collected, or how you expect this field to be interpreted.</p> </li> <li> <p>Multiplicity     The multiplicity will add validation constraints to the field, i.e. <code>0..1</code> defines an optional field; <code>1..1</code> defines a mandatory field.</p> </li> <li> <p>Data Type     For the data type you should choose one of the existing primitive types, as imported in  Section 1. 'Creating a profile definition model'.  Alternatively, you can create a new  Enumeration Data Type and define a list of allowable values.</p> </li> </ul> <p></p>"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#4-customise-the-fields","title":"4. Customise the fields","text":"<p>You can further add to the definition of a property by using another profile on the Data Element - the <code>Profile Specification Profile (Data  Element)</code> profile.  Add this to the element you've just created and you'll be able to enter the following information:</p> <ul> <li> <p>Metadata Property Name     This will override the key used to store against the property in the database and is accessible through the APIs.  You should set this field if  your Data Element label has special characters, or if you require the APIs to provide a particular output format.</p> </li> <li> <p>Default Value     This indicates the value that will be shown in the interface when a user starts entering data for this profile.</p> </li> <li> <p>Regular expression     This field allows you to specify a validation constraint against data entered for the field in question.  We use  standard Java syntax for these.</p> </li> <li> <p>May be edited after finalisation     This functionality is still under construction, but ticking the box will allow this value to be edited on a profile after the containing model has been  finalised.  This is suitable for fields such as 'Contact email' which may change without affecting the semantics of the model.</p> </li> </ul> <p></p> <p>Once completed, click 'Save' to confirm your changes.</p>"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#5-use-the-dynamic-profile","title":"5. Use the dynamic profile","text":"<p>Once you've added all required Data Classes and Data Elements, the dynamic profile will be ready to use.  It is good practise to try using it with  a test model before finalising the specification. This will allow you to verify that the fields are defined correctly.  However, once you're  happy with the design, you should finalise the model as modifying a profile that has data collected against it may have unexpected consequences.</p> <p>You can add the profile to a Data Model and fill out the form provided, as shown in the example below.</p> <p></p> <p>If you subsequently wish to revise the profile model, you can create a new version of it.  Using the same namespace will ensure that fields are  migrated, where possible. Using a new namespace will allow the collection of new fields, with both profiles being maintained separately.</p>"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#6-admin-dashboard-for-dynamic-profiles","title":"6. Admin dashboard for dynamic profiles","text":"<p>Profile specification models are normal models and so it can be tricky to find and maintain them.  An admin dashboard has been created to help  keep track of those models used as profile specifications.</p> <p>To access your admin dashboard, log in to Mauro Data Mapper and click the white arrow by your user profile to the right of the menu header. From the dropdown menu under 'Admin settings' select 'Dashboard'.</p> <p></p> <p>Click the 'Profiles' tab and the namespace for each profile used along with the relevant link to the defining model will be displayed.  </p> <p>To ensure profiles are well defined before use, we intend to add details of superseded profile models as well as show validation status and other hints. </p> <p></p>"},{"location":"user-guides/exporting-data-models/exporting-data-models/","title":"Exporting Data Models","text":"<p>This user guide will explain how to export a Data Model using a variety of exporters including XML, JSON, XML Schema, Grails and Excel.</p>"},{"location":"user-guides/exporting-data-models/exporting-data-models/#1-export-a-data-model","title":"1. Export a Data Model","text":"<p>Select the Data Model you wish to export in the Model Tree to display it's details panel. In the icon menu at the bottom right of the details panel click the 'Export as JSON, XML...' button. This will display a list of exporters. Select the exporter you wish to use. </p> <p></p> <p>Once selected, the Data Model will automatically start exporting into the specificed format, with a green loading bar indicating the progress. Once exported, this progress bar will be replaced with a link to download the exported file. Click this link to download the file directly to your local machine.  </p> <p></p>"},{"location":"user-guides/exporting-data-models/exporting-data-models/#2-export-multiple-data-models","title":"2. Export multiple Data Models","text":"<p>You can also export several Data Models together. To do this, click the 'Export DataModel(s)' icon at the top right of the Model Tree. A 'Data Model(s) Export' form will appear on the right hand side.</p> <p>To select the Data Models you wish to export, click the menu icon to the right of the 'Select Data Model...' box and the Model Tree will be displayed. Select the relevant Data Models and these will automatically appear in the 'Select your Data Model(s):' field. </p> <p></p> <p>Once you have selected the relevant Data Models, you then need to choose an exporter. Note that only Excel (XLSX) Exporter and Simple Excel (XLSX) Exporter support multiple Data Model exports.</p> <p>Click the 'Select an exporter*' box and a dropdown list of all the available exporters will appear. Select the exporter you want and then click the 'Export Data Model(s)' button. </p> <p></p> <p>A green loading bar will appear and once exported a green notification box will confirm that the 'Data Model(s) exported successfully' and a link to download the file will appear at the bottom of the form.  </p>"},{"location":"user-guides/feature-switches/feature-switches/","title":"Feature switches","text":""},{"location":"user-guides/feature-switches/feature-switches/#introduction","title":"Introduction","text":"<p>This user guide will walk you through the management and purpose of feature switches.</p> <p>Note</p> <p>Only administrators can view and control feature switches in the Mauro user interface.</p>"},{"location":"user-guides/feature-switches/feature-switches/#what-are-feature-switches","title":"What are feature switches?","text":"<p>Feature switches in the Mauro user interface allow an administrator to control which particular feature sets are enabled and visible to users of the Mauro instance running. There are several reasons why feature switches may be used:</p> <ol> <li>There are some feature sets in the user interface which are optional or depend on certain plugins or instance configuration to work. </li> <li>Some Mauro instances may not need particular functionality in daily running.</li> <li>There may be features in active development, still experimental or subject to change.</li> </ol> <p>In these cases, an administrator is able to switch on or off feature sets that are required or not needed, depending on the requirements of that Mauro instance.</p>"},{"location":"user-guides/feature-switches/feature-switches/#administration","title":"Administration","text":"<p>Feature switches can be accessed from the 'Configuration' panel in account settings using Mauro API properties.</p> <p></p> <p>All feature switches are:</p> <ol> <li>Categorised under features.</li> <li>Publicly available to any user for reading (but not updating).</li> </ol> <p>To add a feature switch and control feature visibility, select 'Configuration' from the user profile dropdown menu.</p> <p></p> <p>Select the 'Properties' tab and then click '+ Add'. An 'Add Property' form will then appear for you to complete.</p> <p></p> <p>To select a property, click the 'Select the property to add' box and a dropdown menu will appear. Select any key from the list that starts with 'feature'. Once selected, the 'Key' and 'Category' fields will automatically populate. Tick the checkbox next to 'Publicly visible' if required.</p> <p></p> <p>Under 'Value', select either 'Yes' or 'No' from the dropdown menu to enable or disable the feature respectively. Once completed select 'Add property' to save your changes and a green notificaiton box should appear at the bottom right of your screen confirming that the 'Property was saved successfully'.</p> <p></p>"},{"location":"user-guides/feature-switches/feature-switches/#available-features","title":"Available features","text":"<p>The table below lists all the optional features that can be controlled by feature switches.</p> <p>Warning</p> <p>It is advisable that any feature switches marked as 'In development' should only be used in test environments until fully completed.</p> Feature switch Default value <code>feature.use_subscribed_catalogues</code> <p>                     Enables the Subscribed Catalogues and Federated Data Models feature set.                 </p> <code>true</code> <code>feature.use_open_id_connect</code> <p>                     Enables the user interface for administrators to create/edit/remove OpenID Connect provider details to Mauro, and                     for the login form to list these external identity providers to redirect and login.                                     </p> <p>                     As well as enabling this feature switch, you must also install the Mauro OpenID Connect authentication                     plugin for the Mauro instance too.                 </p> <code>false</code> <code>feature.use_digital_object_identifiers</code> <p>                     Enables the management of Digital Object Identifiers (DOIs) using a new profile to store                     DOI metadata and submit catalogue items to a DOI system.                 </p> <p>                     As well as enabling this feature switch, you must also install the Mauro Digital Object Identifiers                     plugin for the Mauro instance too.                 </p> <code>false</code> <code>feature.use_versioned_folders</code> <p>                     Enables the ability to use Versioned Folders.                 </p> <code>true</code> <code>feature.use_merge_diff_ui</code> <p>                     Enables a new user interface for the management of merging data models within the Mauro user interface. See the Merging Data Models user guide for more details.                 </p> <code>true</code>"},{"location":"user-guides/finalising-data-models/finalising-data-models/","title":"Finalising Data Models","text":"<p>This user guide explains how to finalise catalogue items such as Data Models and Versioned Folders.</p>"},{"location":"user-guides/finalising-data-models/finalising-data-models/#1-overview","title":"1. Overview","text":"<p>Finalising a Data Model means that the model is fixed to a specific version and cannot be modified further. Therefore, all the contents of that model are now read only. This ensures that the model represents a final state that will remain as it is forever. However, a finalised Data Model can be modified further by creating a new Version.</p>"},{"location":"user-guides/finalising-data-models/finalising-data-models/#2-criteria","title":"2. Criteria","text":"<p>The following catalogue items can be Finalised:</p> <ul> <li>Data Models</li> <li>Terminologies</li> <li>Code Sets</li> <li>Reference Data Models</li> <li>Versioned Folders</li> </ul> <p>Any of the catalogue items above may be Finalised, so long as they are:</p> <ul> <li>In their Draft state</li> <li>On their main branch</li> </ul>"},{"location":"user-guides/finalising-data-models/finalising-data-models/#21-finalising-a-versioned-folder","title":"2.1 Finalising a Versioned Folder","text":"<p>When Finalising a Versioned Folder, this means that every model within this folder is also Finalised as well. This is shown in the Model Tree by a document symbol and a version number, with 'Finalised' to the right of the model's name in the details panel.  </p> <p>The same version number for the Versioned Folder is automatically assigned to the models within that folder. Version numbers cannot be customised individually on model's within a Versioned Folder.</p> <p></p> <p>However, a Data Model within a Versioned Folder cannot be individually Finalised as it is version controlled by it's parent folder. Therefore, only when the Versioned Folder is Finalised will the model within it become Finalised too.</p>"},{"location":"user-guides/finalising-data-models/finalising-data-models/#3-how-to-finalise-an-item","title":"3. How to Finalise an item","text":"<p>To finalise a catalogue item, first select it in the Model Tree. Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Finalise' from the dropdown menu.</p> <p></p> <p>A 'Finalise Data Model' dialogue box will then appear where you will have several options for choosing the next version number. </p> <p></p> <ul> <li> <p>Major     Represents a major change to the item. For example, restructuring the model.</p> </li> <li> <p>Minor     Represents a minor change to the item. For example, editing the description of a model.</p> </li> <li> <p>Patch     Represents very minor changes or fixes to the item. For example, correcting spelling mistakes.</p> </li> <li> <p>Custom     If the automatic version number above is not sufficient, select this option to manually enter your own.</p> </li> </ul> <p>You then have the option to enter a 'Tag name' to attach to this Finalised version. This is useful if you want to apply additional context alongside the version number. For example, 'January 2021 Release', 'Interim Release' or 'Modelling Milestone'.</p> <p>Once you've completed the form, click 'Finalise Data Model' to confirm your changes and a green notification box should appear at the bottom right of your screen confirming that the 'Data Model finalised successfully'. </p> <p>The item is now in a Finalised state, and the version number chosen will now be attached to the item.</p> <p></p>"},{"location":"user-guides/how-to-search/how-to-search/","title":"How to search","text":"<p>The search function within Mauro Data Mapper is extremely powerful and allows you to search for datasets or specifications throughout the whole catalogue. This user guide will explain how to effectively search for an item as well as the methods Mauro Data Mapper uses for searching. </p>"},{"location":"user-guides/how-to-search/how-to-search/#1-types-of-search","title":"1. Types of search","text":"<p>There are several different ways to search for an item within Mauro Data Mapper. </p>"},{"location":"user-guides/how-to-search/how-to-search/#11-search-model-tree","title":"1.1 Search model tree","text":"<p>The first method of searching is to use the search box on the top left of the finder panel above the Model Tree. Here, you can search 'Models', 'Classifications' and 'Favourites' by clicking the relevant tab and then entering the search term in the search box. </p> <p>As you type your search, the Model Tree will automatically filter, displaying only the relevant results, with the search term highlighted in yellow wherever it appears within the list. </p> <p>When searching Models, the results list will show Data Models and Data Classes which match the search term in any Label, description, comment or property.</p> <p>To refresh the results, click the 'Search' icon or the 'Reload Data Models Tree' icon in the menu to the right of the search box. You can also choose whether you want the results list to feature superseded or old Data Models by clicking the 'Filters' icon and then selecting the relevant options. To remove filtering click the small 'x' on the right of the search box. </p> <p></p>"},{"location":"user-guides/how-to-search/how-to-search/#12-search-catalogue","title":"1.2 Search catalogue","text":"<p>Another method of searching is to use the main search box for the entire catalogue, which can be found under the 'Search' tab in the header. Simply enter your search term(s) and click the 'Search' button to start searching.</p> <p></p> <p>To use more advanced search options, click on the 'Advanced' button to expand the advanced search controls. These will allow you use more refined filters, such as:</p> <ol> <li>A context to restrict to - such as a top level data model.</li> <li>Which domain types to search for.</li> <li>Date ranges to restrict to.</li> </ol> <p>And so on.</p> <p></p> <p>The list of search results will then provide details on all the catalogue items found that match your search criteria. You can interactively adjust the filters along the left side of the screen, as well as control sorting. Click on any 'View details' link to be taken directly to that catalogue item.</p> <p></p>"},{"location":"user-guides/how-to-search/how-to-search/#13-search-within-a-model","title":"1.3 Search within a model","text":"<p>Finally, you can also search within Data Models directly, as well as Folders and Versioned Folders. This is equivalent to performing a catalogue-wide search (as described above) but automatically restricted by a model context. When viewing a Data Model, click on the magnifier tab icon to view the search area, then enter your search term(s). Search results are then listed and automatically constrained to both the parent model and relevant domain types for that model.</p> <p>Clicking on the 'Refine search' link will take you to the main catalogue search results page, allowing you to control the filtering manually.</p> <p></p>"},{"location":"user-guides/how-to-search/how-to-search/#2-how-search-works","title":"2. How search works","text":"<p>The search functionality in Mauro Data Mapper implements the Simple Query Strings of Hibernate search. </p> <p>This is essentially where a simple syntax is used to parse and split the provided query string into terms based on special operators. The query then analyses each term independently before returning matching results. </p>"},{"location":"user-guides/how-to-search/how-to-search/#21-search-syntax","title":"2.1 Search syntax","text":"<p>The search syntaxes that Mauro Data Mapper supports are:</p> <ul> <li>AND using +</li> <li>OR using |</li> <li>NOT using \u2013</li> <li>To search for different suffixes use * e.g. prefix* </li> <li>To search for an exact phrase use \"\" e.g. \"example phrase\"</li> <li>To add precedence to search terms, use () e.g. (important) example</li> <li>To search for similar terms (fuzzy string searching) use ~2 e.g. smoke~2 will return smokes, smake etc</li> <li>To search for similar phrases use \"\" and ~2 e.g. \"cigarette smoke\"~3</li> </ul> <p>For example, if you are interested in information relating to smoking. Searching \u2018smoking\u2019 will return a list of results. However, if you want to be more generic you can type \u2018smoke*\u2019 which will search for smoking, smoker, smoked etc. </p> <p>To look at information relating to smoking and pregnancy you can type 'smok*+pregnancy' which will return results that include both pregnancy and the different variants of smoking.  </p>"},{"location":"user-guides/how-to-search/how-to-search/#22-search-examples","title":"2.2 Search examples","text":"<p>Below are some more examples to further illustrate how to use the search syntaxes effectively:</p> <ul> <li>storm~2 - will return results containing storms or sturm</li> <li>war + (peace | harmony) - will return results containing \"war and either peace or harmony\"</li> <li>storm tree - will return results containing the words storm or tree</li> <li>storm and tree - will return results containing exactly the phrase storm and tree</li> </ul>"},{"location":"user-guides/how-to-search/how-to-search/#23-label-metadata-key","title":"2.3 Label &amp; Metadata Key","text":"<p>The Label and Key entries are indexed using a WordDelimiter analyser. This essentially splits up the search term into individual words at the following points:</p> <ul> <li>Spaces </li> <li>Hypens</li> <li>Numbers</li> <li>Capital letters</li> <li>Full stops</li> </ul> <p>These individual words are referred to as 'Keys' and once the search term has been split up into its various keys, these are then saved and used to conduct the search. Therefore, the results will only match the keys and not the whole search phrase.</p> <p>For example:</p> <ul> <li>Datamodel for test - will be searchable by the keys datamodel, for and test</li> <li>Test DataModel - will be searchable by the keys test, data and model, it will not be searchable by the key datamodel because of the capital letter</li> <li>Test DataModel V1.0.1 - will be searchable by the keys test, data, model, v1, 0 and 1. It will not be searchable by the keys datamodel or v1.0.1</li> <li>subject-34567 - will be searchable by the keys subject and 34567</li> </ul>"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/","title":"Import a Data Model from Excel","text":"<p>This user guide will explain the steps you need to follow to import a health dataset into Mauro Data Mapper using an Excel spreadsheet.</p> <p>To add an existing dataset to Mauro Data Mapper, you can either enter all the information online as explained in the 'Document a Health Dataset user guide', or you may find it more convenient to import information automatically from an Excel spreadsheet. </p> <p>The importing functionality of Mauro Data Mapper allows you to import several Data Models using the same spreadsheet. </p>"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#1-create-data-model-import-file","title":"1. Create Data Model import file","text":"<p>To ensure all the information is imported correctly, the dataset needs to be entered into a spreadsheet in a specific format. To help with this, you can download a zip file of the standard Data Model import file here.</p> <p>This standard spreadsheet contains two types of worksheets. Firstly, there is the Data Model listing sheet, titled \u2018DataModels\u2019. This is effectively a contents page which lists the main details of each Data Model you wish to import. There must only ever be one Data Model listing sheet.</p> <p>The next sheet is the Data Model key sheet which is titled \u2018KEY_1\u2019. This contains all the relevant details of one Data Model listed in the \u2018DataModels\u2019 sheet. Therefore, if you wish to import several Data Models, you will need to add a Data Model key sheet for each additional Data Model and title it \u2018KEY_2\u2019, \u2018KEY_3\u2019 respectively.</p>"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#11-data-model-listing-sheet","title":"1.1 Data Model listing sheet","text":"<p>In the Data Model listing sheet, use one row for each Data Model. Enter the information according to the columns, which are explained below, along with any other properties or metadata which may be relevant. </p> <p></p> <p>The following columns must be completed:</p> <ul> <li> <p>SHEET_KEY     The unique name of each Data Model key sheet such as \u2018KEY_1\u2019, \u2018KEY_2\u2019 etc. </p> </li> <li> <p>Name     The unique name or Label of the Data Model. Remember this should be different to all the existing Data Models within Mauro Data Mapper, unless you are updating an existing Data Model.</p> </li> <li> <p>Description     Enter a description which explains the contextual details of the dataset within the Data Model. </p> </li> <li> <p>Author     Record the name(s) of the authors who are creating and maintaining this Data Model.</p> </li> <li> <p>Organisation     Type the name of the organisation responsible for the Data Model, or the underlying data.</p> </li> <li> <p>Type     This is the type of Data Model, which can either be a Data Asset or a Data Standard. </p> <p>A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema.</p> </li> <li> <p>Adding properties     Any other relevant properties or metadata relating to the Data Model can be included in additional columns to the right of the core columns. Metadata can have the following properties:</p> <ul> <li> <p>Namespace     This will be used to select the correct profile / property selection and should be entered in row 1 of the spreadsheet. If it is left    blank, the default namespace of ox.softeng.metadatacatalogue.plugins.excel will be used. </p> </li> <li> <p>Key     This is a relevant property name such as \u2018contact email\u2019 and must be entered in row 2. If no key is supplied, then the value will not be assigned.</p> </li> <li> <p>Value     This is the Value of the given property, for example \u2018enquiries-mydataset@hub.org\u2019 and should be entered into the relevant row. If multiple rows are being imported and a Namespace and Key column is created, then the property will only be assigned if a Value is supplied. </p> </li> </ul> </li> </ul> <p></p>"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#12-data-model-key-sheet","title":"1.2 Data Model key sheet","text":"<p>There should be one Data Model key sheet for each Data Model, and its name should correspond to the relevant 'SHEET_KEY' on the Data Model listing sheet. Any Data Model listed without the correctly named key sheet will not be imported. </p> <p>Therefore, the best practise is to first copy the 'KEY_1' sheet in the standard excel template, rename it and then add the details of the relevant Data Model. Otherwise, formatting issues could occur, resulting in the data importing incorrectly.  </p> <p></p> <p>The following columns must be completed:</p> <ul> <li> <p>DataClass Path     This is the path from the top level of the Data Model to the Data Class. For a top level Data Class, only the class name should be entered in this field. However, if it is a Nested Data Class, then the name of the parent class along with the child class should be entered and be separated by \u201cI\u201d, for example: </p> <ul> <li>A nested class: 'parentIchild'</li> <li>A nested class within a nested class: 'grandparentIparentIchild'</li> <li>Another nested class within a nested class: 'grandparentIparentIanotherChild'</li> </ul> </li> <li> <p>DataElement Name     If the row is describing a Data Element, instead of a Data Class, then the name of the Data Element should be entered here. </p> <p>If supplied, the remaining information in this row will be used to create the Data Element inside the Data Class provided in the 'DataClass Path' column. However, if left blank, the remaining information in this row will be assigned to the Data Class entered in the 'DataClass Path' column. </p> </li> <li> <p>Description     Enter a description which explains any contextual details relating to the Data Element or Data Class.</p> </li> <li> <p>Minimum Multiplicity     The minimum number of instances of the Data Class or Data Element within the Data Model.      Optional data has a minimum Multiplicity of 0, whereas mandatory data has a minimum Multiplicity of 1.</p> </li> <li> <p>Maximum Multiplicity     The maximum number of instances of the Data Class or Data Element within the Data Model.      Optional data has a maximum Multiplicity of 1, whereas data which occurs any number of times and therefore has no upper bound has a maximum Multiplicity of * which represents -1 internally.</p> </li> <li> <p>DataType Name     This is the name given to the Data Type of the Data Element being described and must be included when entering information for Data Elements. </p> <p>The Data Type describes the range of possible values that the Data Element may take. There are four different Data Types stored within a Data Model:</p> <ul> <li> <p>Enumeration: A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value.   </p> </li> <li> <p>Primitive: Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019, \u2018Integer\u2019 or \u2018Date\u2019.</p> </li> <li> <p>Reference: Data which refers to another Data Class within the same Data Model.</p> </li> <li> <p>Terminology: A structured collection of Enumeration Values which have relationships between different data terms.   </p> </li> </ul> </li> <li> <p>DataType Description     This is a short description of the Data Type. If the same Data Type is used multiple times, then the first description entry will be used so subsequent description fields can be left blank.</p> </li> <li> <p>Reference to DataClass Path     If the Data Element is a \u2018Reference\u2019 Data Type and therefore the Data Element is a reference to an instance of another Data Class, then the path to that Data Class should be entered here. </p> <p>This path must match a path provided in the 'DataClass Path' column using the same format with \u2018I\u2019 as the separator. This field cannot be used in conjunction with \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019. </p> </li> <li> <p>Enumeration Key     If the Data Element is an Enumeration Data Type, then the Key and Value of each Enumeration should be included in the \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019 columns respectively, with one Key and Value per row. </p> <p>The Enumeration Key is the text or string that may appear in a column of the dataset. </p> </li> <li> <p>Enumeration Value     The Enumeration Value is the data that corresponds to the Key. For example, for a yes/no question on a webform, the Value is either \u2018Yes\u2019 or \u2018No\u2019. Each Value is assigned a Key, which in this case could be \u20181\u2019 and \u20180\u2019 respectively. </p> <p>An Enumeration Value must be provided if an Enumeration Key has been entered. </p> <p>Once all the Enumeration Key and Value pairs have been entered you will need to merge the corresponding cells in the 'DataClass Path' column, and you may wish to merge the other relevant cells in the other columns too for consistency.</p> </li> </ul> <p>For example, if you were entering the information for a Data Element that was an Enumeration Data Type then the Data Model key sheet would look similar to the below: </p> <p></p>"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#2-import-data-model","title":"2. Import Data Model","text":"<p>Once the Data Model import file has been completed, click the 'Import a Data Model' icon at the top right of the Model Tree. The 'Import a new Data Model' form will then appear on the right. </p> <p>Click the 'Select an importer' box and select 'Excel (XLSX) importer' from the dropdown menu. This will automatically load the rest of the form that you need to complete. </p> <p></p> <p>You then need to select the relevant folder that you wish to import the Data Model into. You can do this by either typing in the folder name, or clicking the menu icon to the right of the 'Folder location' box. This will display the Model Tree, from which you can then select the relevant folder.  </p> <p>You then need to tick the 'Finalised' box to indicate whether the Data Model you are importing is finalised. Although, it is recommended to keep the model as a draft until the gateway presentation of model descriptions has been decided. </p> <p>If the Data Model you are importing is intended to replace an existing Data Model within the current version of Mauro Data Mapper, then tick the 'Import as New Documentation Version' box. This means that the imported model will supersede any Data Models with the same name and will be assigned the latest documentation version of the existing Data Model. </p> <p>If you are importing a new Data Model, then make sure the 'Name' field in the Data Model listing sheet is unique, otherwise this will cause an error when importing. </p> <p>In the 'Source' box, click 'Choose file' and your file explorer will open. Navigate to the relevant Data Model import file and then select 'Import Model(s)'. </p> <p></p>"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#3-round-tripping-data-models","title":"3. Round Tripping Data Models","text":"<p>Excel files can be safely used to 'Round trip' Data Model descriptions. This is essentially where:</p> <ul> <li>A Data Model is exported in the form of an Excel spreadsheet.</li> <li>This exported spreadsheet is edited or updated with new information.</li> <li> <p>The new version of the spreadsheet is re-imported into Mauro Data Mapper and therefore automatically updates the existing Data Model. </p> <p>Note: 'Import as New Documentation Version' should be ticked when re-importing. </p> </li> </ul> <p>This method is a quick and easy way to update existing Data Models as well as adding extra information. This can be particularly useful when Data Models have previously been imported using an alternative method, and therefore may only have the basic layout. </p>"},{"location":"user-guides/merging-data-models/merging-data-models/","title":"Merging Data Models","text":"<p>This user guide explains how to merge Data Models including comparing and committing changes as well as how to successfully resolve conflicts.</p>"},{"location":"user-guides/merging-data-models/merging-data-models/#1-overview","title":"1. Overview","text":"<p>When working with multiple branches on the same model, it is likely that these multiple branch versions will have modifications such as additions, deletions and edits. Eventually, it will be necessary to merge all these back into one single draft version of the model. This is why every model has a main branch which acts as a single branching point and is also where models are finalised from.</p> <p>Merging models is quite a complex task, as it is important not to lose any potential changes during the merge process. This is why a special user interface has been designed for you to:</p> <ul> <li>Review all changes that have occurred.</li> <li>Assist in resolving conflicts, which are any changes that have occurred in two models that do not automatically align.</li> <li>Commit the changes you wish to make to a model.</li> </ul> <p>Merging can be applied to Data Models or Versioned Folders and includes merging changes within the model hierarchy as well. For example, changes made to Data Classes also count as modifications to a Data Model, and so on.</p>"},{"location":"user-guides/merging-data-models/merging-data-models/#2-terminology","title":"2. Terminology","text":"<p>As merging is an advanced user concept, the following terms will be used throughout this user guide:</p> <ul> <li> <p>Source     The model or folder where changes are coming from. For example, the source would typically be a branch that is not the main branch.</p> </li> <li> <p>Target     The model or folder where changes are going to. This is usually the main branch, but is not always the case.</p> </li> <li> <p>Change     Any field that has changed between Source and Target. These will be:</p> <ul> <li>Addition - something has been added to Source which will then be added to Target</li> <li>Deletion - something has been removed from Source, or has been added to Target</li> <li>Modification - something exists in both Source and Target, but has different values</li> </ul> </li> <li> <p>Conflicts     When a modification exists which cannot automatically be resolved. This typically happens when a piece of text has been changed and the same sentence has been modified. A user must manually resolve such conflicts to avoid losing important information or context</p> </li> <li> <p>Paths     A fully qualified path is the full name of a model field (or a field within the model hierarchy). Paths are used as full unique identifiers to list all the changes within a model or folder</p> </li> <li> <p>Commit     Saving all changes gathered together, from both Source and Target, and updating the end Target model or folder</p> </li> </ul>"},{"location":"user-guides/merging-data-models/merging-data-models/#2-visualise-branches","title":"2. Visualise branches","text":"<p>It can be difficult to keep track of all branches in the Model Tree, especially if there are multiple branches. Therefore, Mauro allows you to visualise the branches as well as previous versions or forks in a merge graph.</p> <p>To find out how to access this merge graph see section '4. Merge graph' of our 'Branching, versioning and forking Data Models' user guide.</p>"},{"location":"user-guides/merging-data-models/merging-data-models/#3-how-to-merge","title":"3. How to merge","text":"<p>Firstly, select the relevant draft Data Model or Versioned Folder that has more than one branch in the Model Tree. Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Merge...' from the dropdown menu and this will navigate you to the merge interface. </p> <p></p>"},{"location":"user-guides/merging-data-models/merging-data-models/#4-the-merge-interface","title":"4. The merge interface","text":"<p>The interface for merging is split into several parts:</p>"},{"location":"user-guides/merging-data-models/merging-data-models/#41-merge-controls","title":"4.1 Merge controls","text":"<p>The top of the interface lists:</p> <ul> <li>The label of the catalogue item that is being merged</li> <li>The Source branch to get changes from</li> <li> <p>The Target branch to merge changes into. You are able to select from any available branch, typically main is initially selected</p> <p>Note: This is disabled if there are any changes being committed, which locks the target. To change the target branch again, cancel all changes being committed, start again and re-calculate the differences</p> </li> <li> <p>The Commit Changes button. Clicking this will finish the merge operation (see below)</p> </li> <li>The Cancel button. Click this to cancel the merge operation and return to the catalogue without saving any changes</li> </ul> <p></p>"},{"location":"user-guides/merging-data-models/merging-data-models/#42-changes-list","title":"4.2 Changes list","text":"<p>The 'Changes' tab lists all the changes made between the chosen Source and Target. All changes use path names to fully explain where they are in relation to the model hierarchy (see below for more details).</p> <p>The changes list displays all the changes that have been made, but each change must be moved to the 'Committing' list in order to actually save the change to the Target. See section '4.5 Comparison view' below.</p> <p></p> <p>The changes list also allows you to:</p> <ul> <li>Filter the list by typing in a name</li> <li>Use all changes taken from the Source branch</li> <li>Use all changes taken from the Target branch</li> </ul>"},{"location":"user-guides/merging-data-models/merging-data-models/#43-committing-list","title":"4.3 Committing list","text":"<p>The 'Committing' tab lists all the changes that are going to be committed to the Target branch when saved. This is similar to the 'Changes' list, but also displays where the committed changes have come from. For example, from Source, Target, or a mixture of both.</p> <p>Note: After an initial comparison, the 'Committing' list may include some changes automatically. This is because Mauro has determined that these changes have no conflicts and so can be included by default, however you can undo these changes manually.</p> <p></p>"},{"location":"user-guides/merging-data-models/merging-data-models/#44-path-names","title":"4.4 Path names","text":"<p>Fully qualified paths are used to describe the field changes that have occurred in a model hierarchy. You can hover the mouse over each section of the path name to find out what they mean, but in summary:</p> <ul> <li>The first few capital letters represent the domain type. For example 'DM' is for 'Data Model', 'DC' is for 'Data Class', 'MD' is for 'Metadata' etc</li> <li>This prefix is followed by the label of that item</li> <li>Anything after a '$' represents the branch name the item is for</li> <li>Anything after a '@' represents the name of the field that has changed</li> <li>These then repeat if hierarchical, split by a '\u00bb' separator</li> </ul> <p></p>"},{"location":"user-guides/merging-data-models/merging-data-models/#45-comparison-view","title":"4.5 Comparison view","text":"<p>You can click on any item from the 'Changes' or 'Committing' lists to display a comparison view below. There are three types of comparison views that can be shown and this will depend on the change you have selected. </p> <p>If you have selected an addition change, as highlighted by a green '+' symbol, then the comparison view will display what will be added to the Target in its entirety.</p> <p></p> <p>If you have selected a deletion change, as highlighted by a red '-' symbol, then the comparison view will display what will be removed from the Target in its entirety.</p> <p></p> <p>If you have selected an edit change, as highlighted by a dark blue pencil symbol, then the comparison view will display the values in both Source and Target, highlighting the differences between the two.</p> <p></p> <p>Using the comparison view you must then choose how this change should be committed. If the only option that appears at the top of the comparison view is an 'Accept' button, then clicking this will move the change to the Committing list. </p> <p>However, if there are changes to consider, then a range of options will appear at the top of the comparison view:</p> <ul> <li> <p>'Use \"Source\"'     Commit the value that is in the Source branch, ignoring the Target copy</p> </li> <li> <p>'Use \"Target\"'     Commit the value that is in the Target branch, ignoring the Source copy</p> </li> <li> <p>'Open Editor'     Open an editor window to manually correct the value to commit to Target. See section '5. Resolving conflicts' below</p> </li> <li> <p>'Link Scrolls'     This will synchronise the scroll bars of the two values to better visualise the changes between them. To switch this off, click this button again</p> </li> </ul>"},{"location":"user-guides/merging-data-models/merging-data-models/#5-resolving-conflicts","title":"5. Resolving conflicts","text":"<p>Sometimes Mauro is unable to determine exactly which change from which branch is required. Or there may be cases where you want some changes from Source and Target included together. To make these decisions, click on the 'Open Editor' button in the top right of the comparison view, this will bring up the Conflict Editor.</p> <p></p> <p>The Conflict Editor will display:</p> <ul> <li>The full path to the field being reviewed</li> <li>The entire value from the Source branch, with differences highlighted (left)</li> <li>The entire value from the Target branch, with differences highlighted (right)</li> <li>The 'Resolved' value (initially the value from Source) which can be manually edited and is located at the bottom of the Conflict Editor. </li> </ul> <p>You can click the differences highlighted in green in the Source and Target views to automatically add them to the Resolved editor, replacing any red highlighted '---' sections.</p> <p>Alternatively, you can manually remove all the red '---' sections from the Resolved editor and adjust any text as you wish. Use the Source and Target views as a guide for how your content should look.</p> <p>Once you are happy with how the resolved text looks, click on the 'Resolve Conflict' button. The text entered in the Resolved editor will now be the committed value for the Target branch. In the 'Committed' list, the field will be tagged with the 'Mixed' suffix to show that the change was manually modified.</p> <p></p>"},{"location":"user-guides/merging-data-models/merging-data-models/#6-commit-your-changes","title":"6. Commit your changes","text":"<p>When you are happy with your 'Committing' list, you can save and commit all changes to the Target branch.</p> <p>Note: Not every listed change has to be pushed to the 'Committing' list, but there must be at least one change to commit. This allows you to perform partial merges, for example to keep track of work in progress.</p> <p>Click on 'Commit Changes' which will display a 'Check in Branch' dialogue box. Here you will find a list of all the changes you are committing for a final review. You can enter an optional comment in the 'Commit Comment' box for this merge change. This will be tracked in the history of the Target catalogue item.</p> <p>You can also select the option to 'Delete Source Branch' once the merge has finished. This can be useful to tidy up remaining branches which are no longer needed once merged back into main.</p> <p>Once you have checked everything, click on the 'Commit' button to save all the selected changes to the Target catalogue item.</p> <p></p>"},{"location":"user-guides/organising-data-models/organising-data-models/","title":"Organising Data Models","text":"<p>This user guide summarises how to organise Data Models including rearranging folders, adding classifiers and selecting favourites. </p>"},{"location":"user-guides/organising-data-models/organising-data-models/#1-overview","title":"1. Overview","text":"<p>There are three ways to organise Data Models in a Mauro catalogue:</p> <ol> <li>Folders</li> <li>Versioned Folders</li> <li>Classifiers</li> </ol>"},{"location":"user-guides/organising-data-models/organising-data-models/#11-folder","title":"1.1 Folder","text":"<p>A folder is a container that can store Data Models and other folders.</p>"},{"location":"user-guides/organising-data-models/organising-data-models/#12-versioned-folders","title":"1.2 Versioned folders","text":"<p>A versioned folder shares the same properties as a folder but also shares additional properties typically used by a Data Model. A versioned folder is able to store models and/or folders, but can also be version controlled like a Data Model. This means that the entire folder can be:</p> <ul> <li>Finalised</li> <li>Branched</li> <li>Versioned</li> <li>Forked</li> <li>Merged</li> </ul> <p>Finalising a versioned folder is the same as finalising every single, individual model within that folder, but in one action.</p>"},{"location":"user-guides/organising-data-models/organising-data-models/#13-classifier","title":"1.3 Classifier","text":"<p>A Classifier is a container type which acts like a tag on a catalogue item. A Classifier can be attached to one or more catalogue items, such as models, as a way of grouping items together.</p>"},{"location":"user-guides/organising-data-models/organising-data-models/#2-create-a-folder","title":"2. Create a folder","text":"<p>To find out how to create a folder see section '1. Create a new Folder' of our 'Create a Data Model' user guide.</p> <p>These instructions apply to both folders and versioned folders, as both are the same in this case.</p>"},{"location":"user-guides/organising-data-models/organising-data-models/#3-create-a-classification","title":"3. Create a Classification","text":"<p>To create a Classifier make sure the 'Classifications' tab above the Model Tree is selected. Then click the 'Create a new Classification' icon to the right of the 'Search classifications' box.</p> <p></p> <p>This will bring up a 'Create a new Classifier' dialogue box. Enter a Classifier name (Label) and click 'Add Classifier'.</p> <p></p>"},{"location":"user-guides/organising-data-models/organising-data-models/#4-moving-folders","title":"4. Moving folders","text":"<p>To move a folder or a versioned folder, left click and hold on the relevant folder in the Model Tree. You can then drag this folder into either another folder, versioned folder or into the top level dropzone that appears. Once the folder has been dragged to the desired location and the top level folder is highlighted, release the left mouse button to confirm the action. </p> <p>There are some restrictions on moving versioned folders. Firstly, a versioned folder cannot be dragged into another versioned folder. Secondly, items cannot be dragged into versioned folder that are finalised.</p>"},{"location":"user-guides/organising-data-models/organising-data-models/#5-adding-to-folders","title":"5. Adding to folders","text":"<p>Catalogue items can be created in folders and versioned folders in a number of different ways. </p>"},{"location":"user-guides/organising-data-models/organising-data-models/#51-create-menu","title":"5.1 Create menu","text":"<p>Right click on the relevant folder or versioned folder and then hover over the '+ Create' option that appears at the top of the menu. A secondary menu will then appear, from which you can add another 'Folder', 'Data Model' or 'Code Set'.</p> <p></p>"},{"location":"user-guides/organising-data-models/organising-data-models/#52-drag-items","title":"5.2 Drag items","text":"<p>In a similar way to moving folders, you can also move existing catalogue items into both folders and versioned folders by dragging them. Left click and hold on the item you wish to move in the Model Tree. Drag it to the desired location and once highlighted, release the left mouse button.  </p>"},{"location":"user-guides/organising-data-models/organising-data-models/#53-import","title":"5.3 Import","text":"<p>You can also import Data Models into folders and versioned folders. To find out how to do this from an Excel spreadsheet, see our 'Import a Data Model from Excel' user guide.</p>"},{"location":"user-guides/organising-data-models/organising-data-models/#6-adding-to-classifiers","title":"6. Adding to Classifiers","text":"<p>To add a catalogue item to a Classifier, select the relevant item in the Model Tree. The description panel will then display on the right hand side of your screen. Under the 'Description' tab, click the 'Edit' button and then select 'Edit' from the dropdown menu.</p> <p></p> <p>This will bring up an 'Edit Default Profile' form. In the 'Classifications' field, tick the classifiers from the dropdown menu that you would like to associate with this item. Once you've selected all the relevant classifiers, click 'Save' to confirm your changes.</p> <p></p>"},{"location":"user-guides/organising-data-models/organising-data-models/#7-add-remove-and-view-favourites","title":"7. Add, remove and view Favourites","text":"<p>To easily access catalogue items, you can mark them as Favourites. To do this, right click on the catalogue item in the Model Tree and then select the 'Add to Favourites' option in the menu.</p> <p></p> <p>This will add a star to the right of the catalogue item name in the Model Tree.</p> <p></p> <p>To remove a catalogue item from your favourites list, right click on the catalogue item in the Model Tree and then select 'Remove from Favourites' from the menu.</p> <p></p> <p>To view a list of all the catalogue items that have been bookmarked as favourites, select the 'Favourites' tab above the Model Tree.</p> <p></p>"},{"location":"user-guides/permissions/permissions/","title":"Permissions","text":"<p>This user guide explains the multiple levels of access to catalogue items and adminstration supported in Mauro Data Mapper. </p>"},{"location":"user-guides/permissions/permissions/#1-access-levels","title":"1. Access levels","text":"<p>In total there are six different access levels within Mauro. These are divided into Model roles, Container roles and Application roles. </p> <p>Each level along with it's permissions are summarised below in ascending order. </p>"},{"location":"user-guides/permissions/permissions/#reader","title":"Reader","text":"<ul> <li>Read models</li> </ul>"},{"location":"user-guides/permissions/permissions/#reviewer","title":"Reviewer","text":"<ul> <li>As Reader (above) plus:</li> <li>Add comments</li> </ul>"},{"location":"user-guides/permissions/permissions/#author","title":"Author","text":"<ul> <li>As Reviewer (above) plus:</li> <li>Add and edit descriptions on models</li> </ul>"},{"location":"user-guides/permissions/permissions/#editor","title":"Editor","text":"<ul> <li>As Author (above) plus:</li> <li>Create models</li> <li>Edit model structures</li> <li>Mark models as 'deleted'</li> </ul>"},{"location":"user-guides/permissions/permissions/#container-administrator","title":"Container Administrator","text":"<ul> <li>As Editor (above) plus:</li> <li>Permanently delete models</li> <li>Add and Edit descriptions on containers (folders, classifications) </li> <li>Delete containers (folders, classifications)</li> </ul>"},{"location":"user-guides/permissions/permissions/#container-group-administrator","title":"Container Group Administrator","text":"<ul> <li>As Container Administrator (above) plus:</li> <li>Add container groups</li> <li>Assign groups to containers</li> <li>Add users to container groups</li> <li>Remove users from container groups</li> </ul>"},{"location":"user-guides/permissions/permissions/#2-users","title":"2. Users","text":"<p>Users can be created in Mauro to control authenticated access to the catalogue. Some or all of the catalogue may be made publicly available, which means that anonymous users may view the contents. However, for any non-public content an authenticated user can only see that catalogue content.</p> <p>When a new Mauro instance is created, a default administrator account will be created to allow you administration access and manage further users. By default this account is:</p> <p>Username: admin@maurodatamapper.com</p> <p>Password: password</p> <p>Warning</p> <p>Please change this password as soon as possible to avoid security issues. To find out how to change your password see section '4. Change your password' of our 'User profile' user guide.</p> <p>An administrator is then able to manage and create further users in Mauro via the administrator dashboard. See section '4. Manage users' of our 'Admin functionality' user guide to find out how to do this.</p> <p>To differentiate user access levels and determine what each user is allowed to action, each user should be assigned to one of these groups:</p> <ul> <li>Readers</li> <li>Editors</li> <li>Administrators</li> </ul>"},{"location":"user-guides/permissions/permissions/#3-groups","title":"3. Groups","text":"<p>Groups are used to manage collections of users and their permissions as a whole. Each Mauro instance is created with the following core user groups defined:</p> <ul> <li> <p>Readers     Users are only able to read the Mauro catalogue content and cannot modify anything.</p> </li> <li> <p>Editors     Users can read and modify the Mauro catalogue content, but are not able to perform administrative actions, such as creating new users.</p> </li> <li> <p>Administrators     Users are able to perform any action in Mauro, including reading and modifying content and administration tasks.</p> </li> </ul> <p>Each user should be placed into one of these groups initially to govern what permissions they have, such as:</p> <ul> <li>Read access</li> <li>Write access</li> <li>Delete access, etc</li> </ul> <p>A user can belong to more than one group, in which case the permissions for each group are then combined. Further groups can also be created to define logical collections of users together. To find out how to manage groups, see section '6. Manage groups' of our 'Admin functionality' user guide.</p> <p>Groups can then be used to assign access to particular catalogue items for a collection of users as a whole, as explained in the next section.</p>"},{"location":"user-guides/permissions/permissions/#4-catalogue-items","title":"4. Catalogue items","text":"<p>Catalogue items can be restricted to be globally accessible to allow them to be either:</p> <ul> <li>Fully public </li> <li>Accessible to authenticated users only </li> <li>Accessible only to certain groups of users </li> </ul> <p>These restrictions can be placed on:</p> <ul> <li>Folders</li> <li>Data Models</li> <li>Code Sets</li> <li>Classifications</li> <li>Terminologies</li> <li>Reference Data Models</li> </ul> <p>These permissions also propogate down the catalogue item hierarchy. For instance, if a folder were made 'Publicly available', then all models and sub-folders within it would automatically become 'Publicly available' as well.</p>"},{"location":"user-guides/permissions/permissions/#5-manage-user-and-group-access","title":"5. Manage User and Group Access","text":"<p>Only Editors can manage the user and group access. To do this, select the relevant Data Model or Folder in the Model Tree. Then click the 'User &amp; Group Access' button at the top right of the details panel.</p> <p></p> <p>This will bring up a 'Restrict user access globally' dialogue box. Here you can select whether you would like the model or folder to be 'Publicly readable', 'Readable by authenticated users' or 'Restrict user access based on groups'.</p> <p></p>"},{"location":"user-guides/permissions/permissions/#51-publicly-readable","title":"5.1 Publicly readable","text":"<p>Any catalogue item made publicly readable allows anyone to view it, whether they are an anonymous or registered user. This is useful when building a catalogue for public consumption.</p> <p>Note that although catalogue items may be publicly readable, only authenticated users in the Editors group or higher are able to modify those catalogue items.</p>"},{"location":"user-guides/permissions/permissions/#52-authenticated-users","title":"5.2 Authenticated users","text":"<p>Any catalogue item made readable by authenticated users guarantees that only registered users can view this item. This is useful when building an internal catalogue for an organisation and viewing access must be governed by an administrator to manage users.</p> <p>Again, only users in the Editors group or higher can modifiy these catalogue items.</p>"},{"location":"user-guides/permissions/permissions/#53-group-access","title":"5.3 Group access","text":"<p>Zero or more user groups may be assigned to a catalogue item to control which collection(s) of users may view the catalogue item. This allows a further level of control compared to the 'Authenticated users' option.</p> <p>To add group access to a catalogue item, click the '+Add Group' button on the 'Restrict user access globally dialogue box'. This will add a row to the group list that you will then need to populate. </p> <p>First, you need to select a group name which determines the collection of users to allow. Click the 'Search...' box under 'Group name' and select the relevant group from the dropdown menu.</p> <p></p> <p>You then need to select which access level you would like to assign to the group. This determines what the collection of users are capable of doing to this catalogue item. Available access levels are listed below, with the permissions of each detailed in '1. Access levels'.</p> <ul> <li>Reader</li> <li>Reviewer</li> <li>Author</li> <li>Editor</li> <li>Container Administrator (Folders and Classifications only)</li> <li>Container Group Administrator (Folders only)</li> </ul> <p>Once you've decided on a suitable access level, click the 'Search...' box under 'Access Level' and select the relevant access option. Then click the green tick to the right of the row to save your changes.</p> <p></p> <p>You can remove group access from a catalogue item by clicking the red bin to the right of the group you wish to remove. </p> <p></p>"},{"location":"user-guides/publish-subscribe/publish-subscribe/","title":"Publish / Subscribe","text":"<p>Mauro Data Mapper instances are able to provide an Atom feed that publishes all finalised versions of Data Models in the catalogue. </p> <p>This enables other Mauro Data Mapper instances to subscribe to these feeds and view the Federated Data Models available from external catalogues. </p> <p>Furthermore, a Mauro Data Mapper instance has the ability to subscribe to one or more of these Federated Data Models to import them into their own catalogues. This user guide will explain how. </p>"},{"location":"user-guides/publish-subscribe/publish-subscribe/#1-feeds-and-api-keys","title":"1. Feeds and API Keys","text":"<p>All Mauro Data Mapper server instances expose the URL <code>api/feeds/all</code> which returns the Atom syndication data. For another server instance to view all available data in that feed, each server instance must generate an API key and provide it to the required external parties. </p>"},{"location":"user-guides/publish-subscribe/publish-subscribe/#11-create-or-copy-an-api-key","title":"1.1 Create or copy an API Key","text":"<p>API keys may be set up through the web interface or via the API. To create an API key, you must be logged in with a username and  password.</p> <p>On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu.</p> <p></p> <p>This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys.</p> <p></p> <p>For each API belonging to the user, the list displays:</p> <ul> <li> <p>Name   This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys    used for different purposes.</p> </li> <li> <p>Key   This is the key itself, which is a UUID, unique to this user. </p> </li> <li> <p>Expiry date   This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be    'refreshed' before expiry.</p> </li> <li> <p>Refreshable   Specifies whether an API key can be refreshed once expired.</p> </li> <li> <p>Status   Details whether an API key is 'Active' or 'Disabled'.</p> </li> </ul> <p>You can either copy an existing key value or create a new one to provide to an external party. </p> <p>To copy the key value to your clipboard click the 'Copy' button in the relevant 'Key' box. To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete.</p> <p>Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes.</p> <p></p> <p>For more information about creating and managing API Keys, please see the API Keys page in the REST API documentation </p>"},{"location":"user-guides/publish-subscribe/publish-subscribe/#2-add-a-subscribed-catalogue","title":"2. Add a Subscribed Catalogue","text":"<p>Note: The following can only be carried out by an administrator. </p> <p>Administrators can manage the set of Subscribed Catalogues a Mauro Data Mapper instance connects to. To manage the catalogues, click the white arrow next to your user profile and select 'Subscribed catalogues' from the dropdown menu.</p> <p></p> <p>This will take you to a list of existing catalogues. Here you can test, edit and delete catalogues by clicking the three vertical dots to the right of each item in the list which will display a dropdown menu.</p> <p></p> <p>For each existing catalogue, the list displays:</p> <ul> <li> <p>Label   A unique label to help identify the subscribed catalogue.</p> </li> <li> <p>Description   A description to help explain what the subscribed catalogue is for.</p> </li> <li> <p>URL   The URL to the server instance to connect to.</p> </li> <li> <p>API Key   The generated key value to authenticate against the server instance.</p> </li> <li> <p>Refresh period   State how often, in days, this server instance should refresh the subscribed catalogue feed to check for new data. If not provided, a suitable default will be used instead.</p> </li> </ul> <p>To add a Subscribed Catalogue, you will need:</p> <ul> <li>The URL to the Mauro Data Mapper server instance to connect to.</li> <li>The API key to authenticate on that server instance.</li> </ul> <p>Note: The URL only needs to refer to the instance and not the absolute URL to the Atom feed.</p> <p>Click the '+Add' button at the top right of the 'Catalogues' list. This will open an 'Add Subscribed Catalogue' form which you will need to complete.</p> <p>Enter the label, description, URL, API Key and refresh period as described above. Once completed, click 'Add subscription' and a green notification box should appear at the bottom right of your screen confirming that the 'subscribed catalogue saved successfully'. </p> <p></p>"},{"location":"user-guides/publish-subscribe/publish-subscribe/#3-federated-data-models","title":"3. Federated Data Models","text":"<p>When at least one subscribed catalogue has been configured, any signed in user may be able to view the Federated Data Models from those catalogues in the Model Tree.</p> <p>The Model Tree will be updated to list two new root nodes:</p> <ul> <li> <p>This catalogue   Lists all the folders and Data Models contained within this Mauro Data Mapper instance.</p> </li> <li> <p>External catalogues   Lists each subscribed catalogue created for this Mauro Data Mapper instance. Expanding each subscribed catalogue will list all readable Federated Data Models that can be individually subscribed to.</p> </li> </ul> <p></p>"},{"location":"user-guides/publish-subscribe/publish-subscribe/#31-subscribe-to-a-federated-data-model","title":"3.1 Subscribe to a Federated Data Model","text":"<p>You can view the subscription status of a Federated Data Model by selecting the relevant model in the Model Tree which will display it's details panel. </p> <p></p> <p>To subscribe to a Federated Data Model so that it is included in the current catalogue, click 'Subscribe' and a 'Subscribe to Data Model' dialogue box will appear.</p> <p></p> <p>Click the menu icon to the right of the box to display the Model Tree. Select the folder you wish to import the Federated Data Model into and click '+Subscribe'. The status of this model will now change to 'Subscribed' in the details panel, meaning that this catalogue will automatically maintain this subscription until the user chooses to unsubscribe.</p>"},{"location":"user-guides/publish-subscribe/publish-subscribe/#32-imported-federated-data-models","title":"3.2 Imported Federated Data Models","text":"<p>Once subscribed to, each Federated Data Model will be automatically imported into the current Mauro Data Mapper catalogue at the chosen target folder, including the links between model versions.</p> <p>It is possible to manually synchronise the subscribed catalogues. To do this, select the subscribed catalogue in the Model Tree. In the relevant details panel, click the 'Synchronise' button.</p> <p>Each subscribed catalogue will also list version updates when the current Mauro Data Mapper instance refreshes the Atom feed, the frequency of which is determined during configuration. New versions of a Data Model will appear with their updated versions listed for you to also subscribe to, which is a manual operation.</p>"},{"location":"user-guides/user-profile/user-profile/","title":"User profile","text":"<p>This user guide will show you how to add and edit information in your user profile.</p>"},{"location":"user-guides/user-profile/user-profile/#1-edit-your-profile","title":"1. Edit your profile","text":"<p>When logged in to Mauro Data Mapper, your user profile is displayed on the top right of the menu header. Click the white arrow and a dropdown menu will appear. The options available in this menu will depend on whether you are an 'Editor' or an 'Administrator'. </p> <p></p> <p>To edit your profile, select the 'My profile' option from the dropdown menu. This will take you to a page displaying your details, with the compulsory fields indicated by a *. These will already be completed according to the information entered when you were registered. </p> <p></p> <p>Complete the fields below:</p> <ul> <li> <p>First and last name     This is used for identification.</p> </li> <li> <p>Organisation     This allows other Mauro Data Mapper users to know which organisation you belong to.</p> </li> <li> <p>Role in Organisation     This allows other Mauro Data Mapper users to know your role within your organisation.</p> </li> </ul> <p>Once you have amended your details, click 'Save profile' and your changes will be updated.</p>"},{"location":"user-guides/user-profile/user-profile/#2-change-your-profile-picture","title":"2. Change your profile picture","text":"<p>To add or edit an image to your profile picture, navigate to the 'My profile' page and click the pencil icon on your profile picture. A dropdown menu will appear with the option to either 'Change image' or 'Remove image'. </p> <p></p> <p>To change the image for your profile picture, select 'Change image' and the file explorer on your computer will open. Navigate to the image you would like to use and then click 'Open'. </p> <p>This image will then be imported into your 'My profile' page, with a preview displayed on the right. You can then resize the image by hovering over the corners of the highlighted box until an arrow appears. Click and hold the left mouse button and drag the box until it is the desired size. </p> <p>To drag the entire highlighted box, hover your cursor inside the box until the 4 headed arrow appears. Click and hold the left mouse button until the box is in the desired location. </p> <p>Once you have finished adjusting your profile picture, click 'Update profile image' and a notification will appear on the bottom right of your screen to confirm the update. Then click 'Save profile' to save all changes.</p> <p>To remove a profile picture, click the pencil icon and select 'Remove image' from the dropdown menu. Again, a notification will appear at the bottom right of your screen to confirm the change. Then click 'Save profile'.  </p>"},{"location":"user-guides/user-profile/user-profile/#3-update-preferences","title":"3. Update preferences","text":"<p>You can also specify some interface preferences that are permanently saved to your user profile. To do this, navigate to the 'Preferences' page on your user profile and choose the options that suit you. </p> <ul> <li> <p>Number of records per table     By default this is set to 20. However depending on screen size, you may wish to view 5, 10 or 50. Select the option that you prefer.</p> </li> <li> <p>Always expand descriptions     The next option allows descriptions to be expanded automatically, regardless of the character count. This is unchecked by default. However, those with larger screens may prefer to always expand descriptions in tables. If so, check this box.  </p> </li> <li> <p>Include Superseded Document Models in Data Model Tree     The final option determines whether you want the Finder Panel to show superseded document models within the Data Model Tree view. This is unchecked by default as typically these models represent invalid or outdated descriptions. However, feel free to check this box to change this. </p> </li> </ul> <p>Once you have selected all your preferences, click 'Save preferences' and these will now be saved for future sessions.</p> <p></p>"},{"location":"user-guides/user-profile/user-profile/#4-change-your-password","title":"4. Change your password","text":"<p>To change your password, go to the 'Password' page under 'Account settings'. Enter your old password in the first box and your new password in the second box. </p> <p>The colour bar underneath will automatically change from red to green when you start typing. The width of the green bar will then change according to the strength of your chosen password. You can use both lower and upper case letters as well as special characters and numbers. </p> <p>If the colour bar changes to dark blue, then the password is too long and won't be accepted. To view or hide your passwords, click the eye icon to the right of each box.</p> <p>Once you have finalised your password, re-enter it into the 'Confirm password' box and then click 'Change password' to confirm the change. </p> <p></p>"},{"location":"user-guides/version-data-models/version-data-models/","title":"Version data models","text":"<p>This user guide will show you how to create working versions of Data Models as well as how to merge completed Data Models into the main branch. This ensures that you can make changes without adversely affecting other users of Mauro Data Mapper.</p>"},{"location":"user-guides/version-data-models/version-data-models/#1-versioning","title":"1. Versioning","text":"<p>Data Models can only be versioned if they meet the below criteria: </p> <ul> <li>The Data Model must be finalised</li> <li>The Data Model must have a version number</li> </ul> <p>If your Data Model satisfies both of these conditions, then you will be able to create a new version. To do this, highlight the relevant Data Model in the Model Tree and click the three vertical dots at the top right of the details panel. Select 'Create a New Version' from the dropdown menu.</p> <p></p> <p>You will then be presented with three options: </p> <ul> <li> <p>New Fork     This will create a copy of the Data Model with a new name and a new 'main' branch. Use this option if you are planning on taking this model in a new direction, or under a new authority.</p> </li> <li> <p>New Version     This will create a copy of the Data Model under the 'main' branch. Use this option if you want to create the next iteration of this model.</p> </li> <li> <p>New Branch     This will create a copy of the Data Model in a new branch, which you can choose the name for. Use this option if you want to make some changes that you subsequently wish to merge back into 'main'.</p> </li> </ul>"},{"location":"user-guides/version-data-models/version-data-models/#11-new-branch","title":"1.1 New Branch","text":"<p>For this user guide, we will select 'New Branch'. A text box will then appear where you can enter a 'Branch name'. This will distinguish your Data Model from the others in your Data Model family. </p> <p></p> <p>Once completed, click 'Add Branch'. This will create a copy of the Data Model for you to edit. If this is the first branch, then a 'main' branch will also be created. This will be the branch that changes from other branches will be merged into. </p>"},{"location":"user-guides/version-data-models/version-data-models/#2-merging","title":"2. Merging","text":"<p>To merge completed changes of a Data Model into the main branch, click the three vertical dots at the top right of the details panel. Select 'Merge' from the dropdown menu and then click 'Merge Model'.</p> <p></p> <p>This will take you to a merge screen which is split into three columns. The left column shows the source branch which you are merging from. The middle column displays the items to be merged and the right column shows the items currently on the main branch.</p> <p></p> <p>The right and left columns will display a tree which represents the Data Model and its changes. There are three types of possible changes which are represented by different colours:</p> <ul> <li> <p>Addition - Green     This is a change present on the source branch but isn\u2019t currently on the target branch. These types of changes will usually be auto merged and therefore won\u2019t need selecting.</p> </li> <li> <p>Modification - Yellow     This is a change to an item which is present on both the source and target branches. These changes aren\u2019t normally auto merged and therefore will require selection by the user.</p> </li> <li> <p>Deletion - Red     These changes will remove items from the main branch. These are never auto merged and will require selection by the user.</p> </li> </ul> <p>As changes can be made to different items within the Data Model, it is recommended that you check the tree in the left column to ensure that all the changes you wish to merge are selected.</p> <p></p>"},{"location":"user-guides/version-data-models/version-data-models/#21-property-changes","title":"2.1 Property changes","text":"<p>If required changes can be selected from the trees in both the left and right  columns, a new editor window will appear. This allows you to make custom changes and then commit these changes to the main branch.</p>"},{"location":"user-guides/version-data-models/version-data-models/#22-check-in","title":"2.2 Check In","text":"<p>Once all the relevant changes have been selected and are highlighted by a green tick, click 'Commit Changes'. A new dialogue box will appear where you can enter a check in comment. This is not required but can be useful for future auditing. You also have the option to delete the source branch by selecting the tick box to the left of the 'Delete Source Branch' option. </p> <p></p> <p>To migrate all the changes to the main branch click 'Commit' and a green notification box should appear at the bottom right of your screen, confirming that the commit was successful. If the commit was unsuccessful, then a red notification box will appear at the bottom right of your screen. Correct the errors and try again. </p>"}]}