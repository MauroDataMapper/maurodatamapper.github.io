{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mauro Data Mapper: Documentation \u00b6 Welcome to the Mauro Data Mapper documentation site. Here you will find everything you need to understand, use and navigate the Mauro Data Mapper tool as well as API documentation for software developers. To browse through the documentation, use the menu on the left. Alternatively, click the buttons below to go straight to our most popular pages. About Discover more about Mauro Data Mapper, its history, and the plans for the future Glossary Understand the terminology and how it's used User Guides Learn how to use Mauro Data Mapper yourself Tutorials Study the full capabilities of Mauro Data Mapper and how it can benefit your work API Documentation Find out how to write scripts and applications to interact with Mauro Data Mapper Community Find support and contribute to our code and documentation","title":"Home"},{"location":"#mauro-data-mapper-documentation","text":"Welcome to the Mauro Data Mapper documentation site. Here you will find everything you need to understand, use and navigate the Mauro Data Mapper tool as well as API documentation for software developers. To browse through the documentation, use the menu on the left. Alternatively, click the buttons below to go straight to our most popular pages. About Discover more about Mauro Data Mapper, its history, and the plans for the future Glossary Understand the terminology and how it's used User Guides Learn how to use Mauro Data Mapper yourself Tutorials Study the full capabilities of Mauro Data Mapper and how it can benefit your work API Documentation Find out how to write scripts and applications to interact with Mauro Data Mapper Community Find support and contribute to our code and documentation","title":"Mauro Data Mapper: Documentation"},{"location":"test_file/","text":"Metadata Catalogue documentation \u00b6 Warning These pages are very much work-in-progress and should not be treated as definitive. Whilst these pages are under construction, the layout, formatting and organisation may change. This documentation is split into several parts: An introduction , to give an overview of the Metadata Catalogue, its intended functionality, and its intended usage. () A set of tutorials, designed to assist users in understanding the catalogue concepts and carrying out common tasks. Visit the first one here Using the Metadata Catalogue to Document Health Datasets A set of more fundamental user guides, explaining in more detail how to interact with the catalogue and detailing the options available. Visit the first one here: Creating a Data Model Detailed guides to the REST-style API - these pages are suitable for developers intending to write scripts or applications that programmatically interact with the system. Read the Introduction here","title":"Metadata Catalogue documentation"},{"location":"test_file/#metadata-catalogue-documentation","text":"Warning These pages are very much work-in-progress and should not be treated as definitive. Whilst these pages are under construction, the layout, formatting and organisation may change. This documentation is split into several parts: An introduction , to give an overview of the Metadata Catalogue, its intended functionality, and its intended usage. () A set of tutorials, designed to assist users in understanding the catalogue concepts and carrying out common tasks. Visit the first one here Using the Metadata Catalogue to Document Health Datasets A set of more fundamental user guides, explaining in more detail how to interact with the catalogue and detailing the options available. Visit the first one here: Creating a Data Model Detailed guides to the REST-style API - these pages are suitable for developers intending to write scripts or applications that programmatically interact with the system. Read the Introduction here","title":"Metadata Catalogue documentation"},{"location":"about/development-roadmap/","text":"Development of the core Mauro platform, its interfaces and plugins, are largely driven by the community. A number of features planned for development are listed below, in no particular order. Some of these features are funded and underway; others may be de-prioritised based on user demand. For more information about any of these features, please contact us . Core \u00b6 Improved support for federation between instances of Mauro More advanced, configurable publication workflows Support for publishing DOIs Visual editors for Data Models Customizable dashboards and metrics Refinement of the Dataflow functionality and visualization Security \u00b6 Improved integration with OAuth / OpenID Connect for authorization Refinement of the API key functionality to restrict the scope of client application interactions Import / export \u00b6 Terminology and Reference Data Model Import plugins for SKOS A wider range of configuration options for exporting XML Schema User templating for Word / PDF exports Wizards for advanced import / export of models, including preview and customisation options Other \u00b6 Support for a wider range of email configurations Python API client library Simplified cloud-based deployment Setup wizard for first-time installation. Support for DOI registration and maintenance.","title":"Development Roadmap"},{"location":"about/development-roadmap/#core","text":"Improved support for federation between instances of Mauro More advanced, configurable publication workflows Support for publishing DOIs Visual editors for Data Models Customizable dashboards and metrics Refinement of the Dataflow functionality and visualization","title":"Core"},{"location":"about/development-roadmap/#security","text":"Improved integration with OAuth / OpenID Connect for authorization Refinement of the API key functionality to restrict the scope of client application interactions","title":"Security"},{"location":"about/development-roadmap/#import-export","text":"Terminology and Reference Data Model Import plugins for SKOS A wider range of configuration options for exporting XML Schema User templating for Word / PDF exports Wizards for advanced import / export of models, including preview and customisation options","title":"Import / export"},{"location":"about/development-roadmap/#other","text":"Support for a wider range of email configurations Python API client library Simplified cloud-based deployment Setup wizard for first-time installation. Support for DOI registration and maintenance.","title":"Other"},{"location":"about/history/","text":"The team at Oxford have been engaged in research and development of Metadata Registries for more than a decade. The Mauro Data Mapper represents the third generation of this technology. In the first generation, we used the eXist database as a simple store of XML representations of metadata items as individual attributes with arbitrary XML document types. This version of the catalogue did not make use of or enforce the constraints of a data modelling language, and the approach to semantic linkage was through classification, rather than individual assertions of refinement. It was applied successfully in two large projects, but proved difficult to maintain. The second generation of the technology was developed for the National Institute for Health Research (NIHR) Health Informatics Collaborative: a national programme aimed at facilitating the re-use of routinely-collected hospital data for medical research. This version of the catalogue allowed arbitrary links and included some of the features of the data modelling language presented here, but without a consistent, formal interpretation. The design included our principle that models should be the units of context and versioning. However, links were not contained within attributes, classes, or even models and hence not subject to model finalisation, making it impossible to ensure consistency. Furthermore, it was possible to import arbitrary items across models, without any guarantee that the definition of these items was independent of context. This third generation of the technology was developed from scratch, based on the experience of the previous implementations and, crucially, was developed in parallel with the formal semantics presented above. The result was a catalogue in which it was possible to guarantee consistency of definitions and to achieve a greater degree of scalability through automation and federation. The team have been engaged with the standards community, particularly the development of the ISO/IEC 11179/3 standard for metadata registration and related specifications. While the model defined in this standard is more closely aligned with the original generation of the tool, all subsequent implementations of Mauro maintain compliance with this standard for interoperability.","title":"History"},{"location":"about/introduction/","text":"What is Mauro Data Mapper? \u00b6 Mauro Data Mapper is a toolkit for the design and documentation of databases, data flows, and data standards, as well as related software artefacts such as data schemas and data forms. It was originally developed for the description of data in clinical research, but it is equally applicable in other settings. Data and software artefacts can be described as linked, versioned Data Models . The links let us re-use and relate data definitions, recording and reasoning about semantic interoperability. The versioning lets us keep track of changes in design, in implementation, or in understanding. Why is metadata important? \u00b6 To fully understand the meaning of data, first we need to know some further information about its context, known as metadata. For example, consider a blood pressure reading. Although this has standard units, the method and state of the patient at the time the measurement was taken will affect the recorded value. Therefore, by outlining this additional information, the reading can be understood and interpreted more accurately. In this way, metadata allows data to be more searchable, comparable and standardised enabling further interoperability. How does Mauro Data Mapper work? \u00b6 The Mauro Data Mapper is a web based tool which stores and manages descriptions of data. These can be descriptions of data already collected, such as databases or csv files. Or these can be descriptions of data you wish to collect or transfer between organisations, such as a specification for a webform or an XML schema. Mauro Data Mapper represents both types of descriptions of data as Data Models . These are defined as a structured collection of metadata and effectively model the data that they describe. Each Data Model consists of several Data Classes , which are groups of data that are related in some way. For example, a group of data that appears in the same table of a database or the same section of a form. Data Classes can sometimes also contain Nested Data Classes . Within each Data Class is then a number of Data Elements which are the descriptions of an individual field or variable. For example, a webform where patients enter their details would be a Data Model . This form could consist of two separate sections such as 'Personal details' and 'Contact details' which would each be a Data Class . The individual entries within each of these sections, such as 'First Name' , 'Last Name' , 'Date of Birth' etc, would each be a Data Element . However, there might be a section within another section on the webform, such as 'Correspondence Address' which lies within 'Contact details' . In this case, 'Correspondence Address' would become a Nested Data Class , where the 'Contact details' Data Class would be the parent. By organising metadata in this way, Mauro Data Mapper allows users to easily search data but also automatically import database schemas and export forms; helping to record data in standardised formats. An open-source community \u00b6 The Mauro platform and plugins are distributed under an open source Apache 2.0 license . We are keen to build an active community of users and developers, and encourage contributions to our code and documentation, and facilitate model sharing. Support \u00b6 The development of Mauro Data Mapper has been funded by the NIHR Oxford Biomedical Research Center as part of the NIHR Health Informatics Collaborative (HIC) . The NIHR HIC is a partnership of 28 NHS trusts and health boards, including the 20 hosting NIHR Biomedical Research Centres (BRCs) , working together to facilitate the equitable re-use of NHS data for translational research. The NIHR HIC has established cross-site data collaborations in areas such as cardiovascular medicine, critical care, renal disease, infectious diseases, and cancer. Mauro Data Mapper , and its previous incarnation, the Metadata Catalogue, has been used for collaboratively editing Data Models for research, and for generating software artefacts such as XML Schema.","title":"Introduction"},{"location":"about/introduction/#what-is-mauro-data-mapper","text":"Mauro Data Mapper is a toolkit for the design and documentation of databases, data flows, and data standards, as well as related software artefacts such as data schemas and data forms. It was originally developed for the description of data in clinical research, but it is equally applicable in other settings. Data and software artefacts can be described as linked, versioned Data Models . The links let us re-use and relate data definitions, recording and reasoning about semantic interoperability. The versioning lets us keep track of changes in design, in implementation, or in understanding.","title":"What is Mauro Data Mapper?"},{"location":"about/introduction/#why-is-metadata-important","text":"To fully understand the meaning of data, first we need to know some further information about its context, known as metadata. For example, consider a blood pressure reading. Although this has standard units, the method and state of the patient at the time the measurement was taken will affect the recorded value. Therefore, by outlining this additional information, the reading can be understood and interpreted more accurately. In this way, metadata allows data to be more searchable, comparable and standardised enabling further interoperability.","title":"Why is metadata important?"},{"location":"about/introduction/#how-does-mauro-data-mapper-work","text":"The Mauro Data Mapper is a web based tool which stores and manages descriptions of data. These can be descriptions of data already collected, such as databases or csv files. Or these can be descriptions of data you wish to collect or transfer between organisations, such as a specification for a webform or an XML schema. Mauro Data Mapper represents both types of descriptions of data as Data Models . These are defined as a structured collection of metadata and effectively model the data that they describe. Each Data Model consists of several Data Classes , which are groups of data that are related in some way. For example, a group of data that appears in the same table of a database or the same section of a form. Data Classes can sometimes also contain Nested Data Classes . Within each Data Class is then a number of Data Elements which are the descriptions of an individual field or variable. For example, a webform where patients enter their details would be a Data Model . This form could consist of two separate sections such as 'Personal details' and 'Contact details' which would each be a Data Class . The individual entries within each of these sections, such as 'First Name' , 'Last Name' , 'Date of Birth' etc, would each be a Data Element . However, there might be a section within another section on the webform, such as 'Correspondence Address' which lies within 'Contact details' . In this case, 'Correspondence Address' would become a Nested Data Class , where the 'Contact details' Data Class would be the parent. By organising metadata in this way, Mauro Data Mapper allows users to easily search data but also automatically import database schemas and export forms; helping to record data in standardised formats.","title":"How does Mauro Data Mapper work?"},{"location":"about/introduction/#an-open-source-community","text":"The Mauro platform and plugins are distributed under an open source Apache 2.0 license . We are keen to build an active community of users and developers, and encourage contributions to our code and documentation, and facilitate model sharing.","title":"An open-source community"},{"location":"about/introduction/#support","text":"The development of Mauro Data Mapper has been funded by the NIHR Oxford Biomedical Research Center as part of the NIHR Health Informatics Collaborative (HIC) . The NIHR HIC is a partnership of 28 NHS trusts and health boards, including the 20 hosting NIHR Biomedical Research Centres (BRCs) , working together to facilitate the equitable re-use of NHS data for translational research. The NIHR HIC has established cross-site data collaborations in areas such as cardiovascular medicine, critical care, renal disease, infectious diseases, and cancer. Mauro Data Mapper , and its previous incarnation, the Metadata Catalogue, has been used for collaboratively editing Data Models for research, and for generating software artefacts such as XML Schema.","title":"Support"},{"location":"about/mauro/","text":"Who is Mauro? \u00b6 Mauro Data Mapper is named after Fra Mauro who was a 15 th century Venetian Monk and also a famous cartographer. A diligent scholar, and enthusiast of ancient greek geographers such as Ptolemy , Mauro created the most detailed map of the world at the time, featuring thousands of texts and illustrations. Unlike other explorers who would embark on expeditions to gather their own data about the world, Mauro adopted a different approach and based himself at the port in Venice. He interviewed travellers who would give their detailed interpretations of the countries they had visited. Mauro then merged this information together to generate maps which were rich with contextual data. Unusually for the time, these maps were based on authoritative sources and proofs of existence, rather than religion and superstition. This philosophy lies at the heart of Mauro Data Mapper, which not only aims to be a hub of detailed metadata, but also allows users to make and share their own insights, understanding and experience. The Fra Mauro map of the world, shown above (source: Wikipedia), was produced between 1448 and 1459 as a commission for King Alfonso V of Portugal. Containing thousands of texts and illustrations, the map would have measured over five square metres. For a higher-resolution version, click here . Fra Mauro's Mappa Mundi along with a copy dated 1804 are stored in the British Library. To find out more about these historic items, watch the video below, courtesy of the British Library. Further reading \u00b6 Wikipedia Wikipedia entry on Fra Mauro A Mapmaker's Dream: The Meditations of Fra Mauro, Cartographer to the Court of Venice James Cowan , 1997 Fra Mauro and the Modern Globe Klaus A. Vogel , No. 57/58, Papers, Read at the 11TH Coronelli Symposium, Venice 2007, and other papers (2011 (for 2009/2010)), pp. 81-92, JSTOR.org Fra Mauro's Mappa Mundi Google Arts & Culture","title":"Mauro"},{"location":"about/mauro/#who-is-mauro","text":"Mauro Data Mapper is named after Fra Mauro who was a 15 th century Venetian Monk and also a famous cartographer. A diligent scholar, and enthusiast of ancient greek geographers such as Ptolemy , Mauro created the most detailed map of the world at the time, featuring thousands of texts and illustrations. Unlike other explorers who would embark on expeditions to gather their own data about the world, Mauro adopted a different approach and based himself at the port in Venice. He interviewed travellers who would give their detailed interpretations of the countries they had visited. Mauro then merged this information together to generate maps which were rich with contextual data. Unusually for the time, these maps were based on authoritative sources and proofs of existence, rather than religion and superstition. This philosophy lies at the heart of Mauro Data Mapper, which not only aims to be a hub of detailed metadata, but also allows users to make and share their own insights, understanding and experience. The Fra Mauro map of the world, shown above (source: Wikipedia), was produced between 1448 and 1459 as a commission for King Alfonso V of Portugal. Containing thousands of texts and illustrations, the map would have measured over five square metres. For a higher-resolution version, click here . Fra Mauro's Mappa Mundi along with a copy dated 1804 are stored in the British Library. To find out more about these historic items, watch the video below, courtesy of the British Library.","title":"Who is Mauro?"},{"location":"about/mauro/#further-reading","text":"Wikipedia Wikipedia entry on Fra Mauro A Mapmaker's Dream: The Meditations of Fra Mauro, Cartographer to the Court of Venice James Cowan , 1997 Fra Mauro and the Modern Globe Klaus A. Vogel , No. 57/58, Papers, Read at the 11TH Coronelli Symposium, Venice 2007, and other papers (2011 (for 2009/2010)), pp. 81-92, JSTOR.org Fra Mauro's Mappa Mundi Google Arts & Culture","title":"Further reading"},{"location":"about/release-notes/","text":"This page describes the changes for each component: 'core', 'ui', and the various centrally-maintained plugins and client libraries. For each, a table is presented with version number, notable new features, notable pull requests, and any dependencies. For more information about the structure and architecture of the code, please see our technical architecture pages. In general, we try to use Semantic Versioning . In particular note that any version tagged as 0.x.y should be considered 'beta' or for testing purposes only. In our code repositories, we use Git Flow , and so the 'main / master ' branch may be considered stable, but 'bleeding edge' features may be available within 'develop' or any feature branch. Please see our Installing Plugins pages for details about build artefacts and dependencies. Core \u00b6 GitHub Version Release Date Major Changes 4.5.0 (GitHub Link) 18th May 2021 Dynamic Profiles Initial endpoints for Versioned Folders Add Facets to all containers 4.4.1 (GitHub Link) 22nd April 2021 Hidden / internal importer parameters ATOM feed for known models Bug fixes 4.3.0 (GitHub Link) 26th Mar 2021 Multiple bug fixes More control over admin system properties Add change notes to the edit history Custom tag names on model branches 4.2.0 (GitHub Link) 4th Mar 2021 DataModel component import feature Dataflow importer / exporter in XML / JSON Fix for Lucene indexing failure Fix for exception during model import Back-end performance improvements 4.1.0 (GitHub Link) 10th Feb 2021 Fix for multi-model imports 4.0.0 (GitHub Link) 1st Feb 2021 Listing major changes since last release of Metadata Catalogue Upgrade to Grails 4 Complete refactoring of code and underlying database Modular code structure Reference Data Model Removed access controls for individuals, increased the options for groups Updated Security Module API Keys / access tokens Much improved support for branching and merging UI \u00b6 GitHub Version Release Date Major Changes 6.1.0 (GitHub Link) 18th May 2021 Properties, annotations and rules available on folders Bug fix for default datatypes 6.0.0 (GitHub Link) 22nd April 2021 Refresh of most model view screens, maximizing screen usage Support for structured profiles Publish / subscribe functionality Support for user-provided themes Many other bug fixes 5.2.0 (GitHub Link) 26th Mar 2021 New screens for administrator system properties 5.1.0 (GitHub Link) 4th Mar 2021 Global progress indicator Fix for Markdown link editing Other fixes / performance improvements 5.0.0 (GitHub Link) 5th Feb 2021 Listing major changes since last release of Metadata Catalogue Upgrade to Angular 9 Modular code structure Refactored, simplified layout to all screens Reference Data Model API Keys / access tokens WYSIWYG HTML editor Much improved support for branching and merging Plugins, client libraries, others \u00b6 Release notes for plugins, client libraries, and other Mauro repositories are not formally listed here for ease of maintenance. However, you can view the latest versions of each plugin on the Plugins page . More details about tagged releases and issues addressed can be found on the individual GitHub repos (see the 'tags' section to view all releases).","title":"Release Notes"},{"location":"about/release-notes/#core","text":"GitHub Version Release Date Major Changes 4.5.0 (GitHub Link) 18th May 2021 Dynamic Profiles Initial endpoints for Versioned Folders Add Facets to all containers 4.4.1 (GitHub Link) 22nd April 2021 Hidden / internal importer parameters ATOM feed for known models Bug fixes 4.3.0 (GitHub Link) 26th Mar 2021 Multiple bug fixes More control over admin system properties Add change notes to the edit history Custom tag names on model branches 4.2.0 (GitHub Link) 4th Mar 2021 DataModel component import feature Dataflow importer / exporter in XML / JSON Fix for Lucene indexing failure Fix for exception during model import Back-end performance improvements 4.1.0 (GitHub Link) 10th Feb 2021 Fix for multi-model imports 4.0.0 (GitHub Link) 1st Feb 2021 Listing major changes since last release of Metadata Catalogue Upgrade to Grails 4 Complete refactoring of code and underlying database Modular code structure Reference Data Model Removed access controls for individuals, increased the options for groups Updated Security Module API Keys / access tokens Much improved support for branching and merging","title":"Core"},{"location":"about/release-notes/#ui","text":"GitHub Version Release Date Major Changes 6.1.0 (GitHub Link) 18th May 2021 Properties, annotations and rules available on folders Bug fix for default datatypes 6.0.0 (GitHub Link) 22nd April 2021 Refresh of most model view screens, maximizing screen usage Support for structured profiles Publish / subscribe functionality Support for user-provided themes Many other bug fixes 5.2.0 (GitHub Link) 26th Mar 2021 New screens for administrator system properties 5.1.0 (GitHub Link) 4th Mar 2021 Global progress indicator Fix for Markdown link editing Other fixes / performance improvements 5.0.0 (GitHub Link) 5th Feb 2021 Listing major changes since last release of Metadata Catalogue Upgrade to Angular 9 Modular code structure Refactored, simplified layout to all screens Reference Data Model API Keys / access tokens WYSIWYG HTML editor Much improved support for branching and merging","title":"UI"},{"location":"about/release-notes/#plugins-client-libraries-others","text":"Release notes for plugins, client libraries, and other Mauro repositories are not formally listed here for ease of maintenance. However, you can view the latest versions of each plugin on the Plugins page . More details about tagged releases and issues addressed can be found on the individual GitHub repos (see the 'tags' section to view all releases).","title":"Plugins, client libraries, others"},{"location":"about/research/","text":"The core of Mauro Data Mapper is based on sound theoretical principles developed by the Oxford team over the course of the last decade. Formal, rigorous definitions of semantic interoperability , have been peer-reviewed and presented at academic conferences and in journals. Some of these articles are listed below: A formal, scalable approach to semantic interoperability Jim Davies, James Welch, David Milward, Steve Harris Science of Computer Programming 192, Elsevier. June 2020. Engineering Agile Big\u2212Data Systems Kevin Feeney\u201a Jim Davies\u201a James Welch\u201a Sebastian Hellmann\u201a Christian Dirschl\u201a Andreas Koller\u201a Pieter Francois and Arkadiusz Marciniak River Publishers; Illustrated edition. December 2018. Domain\u2212Specific Modelling for Clinical Research Jim Davies\u201a Jeremy Gibbons\u201a Adam Milward\u201a David Milward\u201a Seyyed Shah\u201a Monika Solanki and James Welch In SPLASH Workshop on Domain\u2212Specific Modelling . October 2015. The CancerGrid Experience: Metadata\u2212Based Model\u2212Driven Engineering for Clinical Trials Jim Davies\u201a Jeremy Gibbons\u201a Steve Harris and Charles Crichton In Science of Computer Programming . Vol. 89B. Pages 126\u2212143. September 2014. Form Follows Function: Model\u2212Driven Engineering for Clinical Trials Jim Davies\u201a Jeremy Gibbons\u201a Radu Calinescu\u201a Charles Crichton\u201a Steve Harris and Andrew Tsui In International Symposium on Foundations of Health Information Engineering and Systems . Vol. 7151 of LNCS. Pages 21\u221238. Springer. August, 2011. Models for Forms Daniel Abler\u201a Charles Crichton\u201a Jim Davies\u201a Steve Harris and James Welch In Proceedings of the 11 th Workshop on Domain-Specific Modeling , 2011. Semantic Interoperability in Practice Jim Davies\u201a Steve Harris and Aadya Shukla In HICSS (Electronic Government Track). 2010. Metadata\u2212Driven Software for Clinical Trials Charles Crichton\u201a Jim Davies\u201a Jeremy Gibbons\u201a Steve Harris\u201a Andrew Tsui and James Brenton In ICSE Workshop on Software Engineering and Health Care . May 2009.","title":"Research"},{"location":"community/build-plugins/","text":"","title":"Build plugins"},{"location":"community/contribute/","text":"Mauro Data Mapper is an open source tool, supported by a small core team of developers who welcome contributions from the user community. There are many ways to contribute, but before doing so we recommend that you join our community through our Zulip organisation so that you can understand the priorities and see which tasks are already in progress. We welcome any contributions, however small, and will ensure any contribution is credited appropriately. Our immediate general priorities are in bug fixes, documentation, and the creation of plugins. In each case, we have tooling or templates to help get started and the core development team are happy to provide additional support for contributing activities. Before contributing to any of our repositories, please have a read of our Contributor License Agreement . Any submissions to our repositories will be under this agreement to ensure that our code stays free of any further restrictions. Documentation \u00b6 New members of the community and those with little experience collaborating with an Open Source community may like to start here. Our documentation is under constant evaluation and improvement: screenshots and instructions need updating; API documentation needs further elaboration and examples while user guides to new features need writing and updating. We're particularly interested in real-world examples of usage - experience reports that can be turned into 'how-to' guides for others wanting to use Mauro in the future. Many of the examples we have are based on health data, so we're also keen to get examples from other domains too. The documentation can be found in the 'docs' repository in the Mauro Github organisation; clone the repository into a suitable location. The whole documentation website can be built using the MkDocs tool . Follow the instructions to install it locally and run mkdocs serve to build and host a version on your local machine. This will automatically update as you make changes to the source files. Please create a new branch with your changes and submit a pull request with your changes when you're ready to submit them. Before accepting, we will check for the following properties: That there are no conflicts with the existing 'develop' branch of the documentation That there are no spelling mistakes or grammatical errors That there are no broken or invalid links in the new text We may also ask some community members to review changes before they are published. Please add comments to your pull request to guide us on how best to accept your changes. Once your pull request has been accepted, the changes will be uploaded to the Github help pages as part of the next documentation 'release' . Bug fixes \u00b6 For developers getting started with our code base, fixing bugs is a great place to start. Lists of open issues are available on our Youtrack and GitHub sites, or you may have found your own issue you'd like to dive in and fix. All our repositories use the GitFlow model for managing code branches. New work should be created in a feature branch. Our core repositories act on a 'pull request' system and each repository will have criteria to meet before committing code. For the Core and UI repositories, more information is given below. With all contributions to the source code, we recommend you engage with the community first, to make sure no work is duplicated, and to understand the current priorities. Core \u00b6 The Core code is built using Gradle and Grails. Instructions for getting the code running locally are provided in the repository's README file. Before accepting any pull requests, the core team will: Checkout the relevant branch and check that any new functionality works correctly Run all integration and unit tests (where this hasn't already been done by our build servers) Run lint tools over the code to ensure new code meets existing quality checks Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch. Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly. UI \u00b6 The UI project is build using npm and ng and instructions for getting the code running locally are provided in the repository's README file. In order to build against a working back-end server, you might like to first check-out the Core code and get that running, or use our Docker installation to bring up a back-end which your copy of the web interface can communicate with. Before approving a pull request in the UI repository, the core team will: Checkout the relevant branch and check that any new functionality works correctly run ng test to ensure that all tests pass correctly run ng lint and npm run eslint to check that any code changes are sensible check through the diff of the current branch to look for any unintended changes Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch. Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly. Plugin development \u00b6 Back-end plugins can be written to provide functionality for specific use cases. They can make use of existing code 'hooks' , such as importers, exporters, and profiles or they may simply provide additional API endpoints for particular purposes. The Mauro Data Mapper Plugins Github organisation provides template plugins for these use cases, which can be cloned and used as a starting point for the development of a new plugin. If your plugin has universal appeal, we'd be happy to host it within our organisation so that other users can find it. Otherwise we can advertise it through these pages, or keep a pinned list within Zulip. Share your models \u00b6 The development team would be pleased to receive details of any models that we might be able to re-use - either for sharing with the community, or example models that can be used for demo purposes. For real-world models that will be useful to other members of the community, we intend to use these pages to advertise either downloadable files, or URLs that can be used to access publicly available models. Please pay particular care to any copyright or licensing information on the model itself before sharing.","title":"Contribute"},{"location":"community/contribute/#documentation","text":"New members of the community and those with little experience collaborating with an Open Source community may like to start here. Our documentation is under constant evaluation and improvement: screenshots and instructions need updating; API documentation needs further elaboration and examples while user guides to new features need writing and updating. We're particularly interested in real-world examples of usage - experience reports that can be turned into 'how-to' guides for others wanting to use Mauro in the future. Many of the examples we have are based on health data, so we're also keen to get examples from other domains too. The documentation can be found in the 'docs' repository in the Mauro Github organisation; clone the repository into a suitable location. The whole documentation website can be built using the MkDocs tool . Follow the instructions to install it locally and run mkdocs serve to build and host a version on your local machine. This will automatically update as you make changes to the source files. Please create a new branch with your changes and submit a pull request with your changes when you're ready to submit them. Before accepting, we will check for the following properties: That there are no conflicts with the existing 'develop' branch of the documentation That there are no spelling mistakes or grammatical errors That there are no broken or invalid links in the new text We may also ask some community members to review changes before they are published. Please add comments to your pull request to guide us on how best to accept your changes. Once your pull request has been accepted, the changes will be uploaded to the Github help pages as part of the next documentation 'release' .","title":"Documentation"},{"location":"community/contribute/#bug-fixes","text":"For developers getting started with our code base, fixing bugs is a great place to start. Lists of open issues are available on our Youtrack and GitHub sites, or you may have found your own issue you'd like to dive in and fix. All our repositories use the GitFlow model for managing code branches. New work should be created in a feature branch. Our core repositories act on a 'pull request' system and each repository will have criteria to meet before committing code. For the Core and UI repositories, more information is given below. With all contributions to the source code, we recommend you engage with the community first, to make sure no work is duplicated, and to understand the current priorities.","title":"Bug fixes"},{"location":"community/contribute/#core","text":"The Core code is built using Gradle and Grails. Instructions for getting the code running locally are provided in the repository's README file. Before accepting any pull requests, the core team will: Checkout the relevant branch and check that any new functionality works correctly Run all integration and unit tests (where this hasn't already been done by our build servers) Run lint tools over the code to ensure new code meets existing quality checks Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch. Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly.","title":"Core"},{"location":"community/contribute/#ui","text":"The UI project is build using npm and ng and instructions for getting the code running locally are provided in the repository's README file. In order to build against a working back-end server, you might like to first check-out the Core code and get that running, or use our Docker installation to bring up a back-end which your copy of the web interface can communicate with. Before approving a pull request in the UI repository, the core team will: Checkout the relevant branch and check that any new functionality works correctly run ng test to ensure that all tests pass correctly run ng lint and npm run eslint to check that any code changes are sensible check through the diff of the current branch to look for any unintended changes Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch. Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly.","title":"UI"},{"location":"community/contribute/#plugin-development","text":"Back-end plugins can be written to provide functionality for specific use cases. They can make use of existing code 'hooks' , such as importers, exporters, and profiles or they may simply provide additional API endpoints for particular purposes. The Mauro Data Mapper Plugins Github organisation provides template plugins for these use cases, which can be cloned and used as a starting point for the development of a new plugin. If your plugin has universal appeal, we'd be happy to host it within our organisation so that other users can find it. Otherwise we can advertise it through these pages, or keep a pinned list within Zulip.","title":"Plugin development"},{"location":"community/contribute/#share-your-models","text":"The development team would be pleased to receive details of any models that we might be able to re-use - either for sharing with the community, or example models that can be used for demo purposes. For real-world models that will be useful to other members of the community, we intend to use these pages to advertise either downloadable files, or URLs that can be used to access publicly available models. Please pay particular care to any copyright or licensing information on the model itself before sharing.","title":"Share your models"},{"location":"community/support/","text":"We have an active and ever growing community of Mauro Data Mapper users in a variety of different application domains. As an open source project, we encourage all our users to interact and help solve problems, share experience or offer advice. The main channel for this is our new Zulip organisation which we encourage all potential users to visit. In addition to active users and contributors, the core development team are also active on Zulip and able to help with any technical questions. You can log into our Zulip organisation using your GitHub credentials, or alternatively ask an existing member to send you an invite. Note that although our Slack channels are still open for the core development team, we will be migrating all previous channels / conversations to Zulip where appropriate. Reporting Issues \u00b6 The Mauro team are keen to hear of any bugs, performance problems or other issues that may be affecting your experience of using the tools. The core development team run an instance of JetBrains YouTrack for managing issues. You can view current issues or submit new ones through hosted instances of the Web Interface. Click the 'Report Issue' link in the footer of the page. When an error is encountered through the web interface, there is also an option to submit a new issue directly. This will include any debug details necessary for the development team to investigate. Please first check to see if your issue has been previously reported. If in doubt, submit a new issue, but in many cases it may be more helpful to add additional context or examples to an existing issue rather than creating another. When submitting issues in this way, please record your name in the 'Reporters Name' field. This will allow the development team to follow up with further questions or reports on progress. Please also do include any additional information that will allow the development team to better understand your issue. There are many online guides to help you write good bug reports - one such example is here . As well as YouTrack, you can also submit issues to our GitHub organisations. Please try to report issues against the correct repository. Back-end issues should be submitted against mdm-core while user interface issues should be submitted against mdm-ui and so on. Requesting Features \u00b6 The core team are very pleased to receive suggestions for new features or improvements to the tools - especially those that will benefit a wide audience or encourage adoption of the tools. Our Development Roadmap page shows a few items that are either in progress or planned for development in the near future. The team may also be engaged in other funded projects with other deliverables not reported on that page, so do please discuss with the team if there are features that you would find useful. In particular, early feedback can ensure we cater for as many use-cases as possible when building new functionality and we can let you know directly once support is in progress, ready for testing, or available as part of a tagged code release. Visit our Zulip organisation to discuss new features and get in touch with the development team directly.","title":"Support"},{"location":"community/support/#reporting-issues","text":"The Mauro team are keen to hear of any bugs, performance problems or other issues that may be affecting your experience of using the tools. The core development team run an instance of JetBrains YouTrack for managing issues. You can view current issues or submit new ones through hosted instances of the Web Interface. Click the 'Report Issue' link in the footer of the page. When an error is encountered through the web interface, there is also an option to submit a new issue directly. This will include any debug details necessary for the development team to investigate. Please first check to see if your issue has been previously reported. If in doubt, submit a new issue, but in many cases it may be more helpful to add additional context or examples to an existing issue rather than creating another. When submitting issues in this way, please record your name in the 'Reporters Name' field. This will allow the development team to follow up with further questions or reports on progress. Please also do include any additional information that will allow the development team to better understand your issue. There are many online guides to help you write good bug reports - one such example is here . As well as YouTrack, you can also submit issues to our GitHub organisations. Please try to report issues against the correct repository. Back-end issues should be submitted against mdm-core while user interface issues should be submitted against mdm-ui and so on.","title":"Reporting Issues"},{"location":"community/support/#requesting-features","text":"The core team are very pleased to receive suggestions for new features or improvements to the tools - especially those that will benefit a wide audience or encourage adoption of the tools. Our Development Roadmap page shows a few items that are either in progress or planned for development in the near future. The team may also be engaged in other funded projects with other deliverables not reported on that page, so do please discuss with the team if there are features that you would find useful. In particular, early feedback can ensure we cater for as many use-cases as possible when building new functionality and we can let you know directly once support is in progress, ready for testing, or available as part of a tagged code release. Visit our Zulip organisation to discuss new features and get in touch with the development team directly.","title":"Requesting Features"},{"location":"glossary/glossary/","text":"A \u00b6 Aliases D \u00b6 Data Asset Data Class Data Element Dataflow Data Model Data Standard Data Type E \u00b6 Enumeration Data Type L \u00b6 Label M \u00b6 Multiplicity P \u00b6 Primitive Data Type R \u00b6 Reference Data Type S \u00b6 Semantic links T \u00b6 Terminology Data Type","title":"Glossary"},{"location":"glossary/glossary/#a","text":"Aliases","title":"A"},{"location":"glossary/glossary/#d","text":"Data Asset Data Class Data Element Dataflow Data Model Data Standard Data Type","title":"D"},{"location":"glossary/glossary/#e","text":"Enumeration Data Type","title":"E"},{"location":"glossary/glossary/#l","text":"Label","title":"L"},{"location":"glossary/glossary/#m","text":"Multiplicity","title":"M"},{"location":"glossary/glossary/#p","text":"Primitive Data Type","title":"P"},{"location":"glossary/glossary/#r","text":"Reference Data Type","title":"R"},{"location":"glossary/glossary/#s","text":"Semantic links","title":"S"},{"location":"glossary/glossary/#t","text":"Terminology Data Type","title":"T"},{"location":"glossary/aliases/aliases/","text":"What is an Alias? \u00b6 An Alias is an alternative name for a catalogue item which helps to locate it when searched for. Data Models , Data Classes and Data Elements must have one primary Label , but can have many Aliases . How are Aliases used? \u00b6 Aliases appear in the first row of the details panel when an item is selected in the Model Tree . An example of some suitable Aliases for the Data Model labelled \u2018Head and Neck Cancer Audit (HANA)\u2019 could be \u2018Neck Cancer\u2019 , \u2018Head Cancer\u2019 , \u2018Head and Neck Cancer\u2019 and \u2018HANA\u2019 . Therefore, if one of these items is searched for, the \u2018Head and Neck Cancer Audit (HANA)\u2019 Data Model will appear in the search results. This helps users access the catalogue item they need, without having to know the exact Label . How do you edit an Alias? \u00b6 To add or remove Aliases click the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. This will allow you to edit the 'Aliases' row at the top of the details panel. Type the name of the new Alias and click the green '+' sign on the right to add it. To delete an Alias , click the red 'x' sign to the right of the Alias you wish to remove. Once you have finished editing, click 'Save changes' at the bottom right of the details panel. A green box should appear at the bottom right of the screen, confirming that the catalogue item has been successfully updated.","title":"Aliases"},{"location":"glossary/aliases/aliases/#what-is-an-alias","text":"An Alias is an alternative name for a catalogue item which helps to locate it when searched for. Data Models , Data Classes and Data Elements must have one primary Label , but can have many Aliases .","title":"What is an Alias?"},{"location":"glossary/aliases/aliases/#how-are-aliases-used","text":"Aliases appear in the first row of the details panel when an item is selected in the Model Tree . An example of some suitable Aliases for the Data Model labelled \u2018Head and Neck Cancer Audit (HANA)\u2019 could be \u2018Neck Cancer\u2019 , \u2018Head Cancer\u2019 , \u2018Head and Neck Cancer\u2019 and \u2018HANA\u2019 . Therefore, if one of these items is searched for, the \u2018Head and Neck Cancer Audit (HANA)\u2019 Data Model will appear in the search results. This helps users access the catalogue item they need, without having to know the exact Label .","title":"How are Aliases used?"},{"location":"glossary/aliases/aliases/#how-do-you-edit-an-alias","text":"To add or remove Aliases click the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. This will allow you to edit the 'Aliases' row at the top of the details panel. Type the name of the new Alias and click the green '+' sign on the right to add it. To delete an Alias , click the red 'x' sign to the right of the Alias you wish to remove. Once you have finished editing, click 'Save changes' at the bottom right of the details panel. A green box should appear at the bottom right of the screen, confirming that the catalogue item has been successfully updated.","title":"How do you edit an Alias?"},{"location":"glossary/data-asset/data-asset/","text":"What is a Data Asset? \u00b6 There are two types of Data Models within Mauro Data Mapper : Data Asset Data Standard A Data Asset contains existing data. This can be in the form of a database, dataset or a number of completed forms. How are Data Assets used? \u00b6 A Data Model which is a Data Asset is represented by a database icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree . Data Assets may also include summary metadata within its properties and its data can also be populated from other Data Assets via a Dataflow . Selecting a Data Model type \u00b6 You will need to assign a Data Model type whenever you are adding or importing a Data Model . When adding a Data Model , select the type from the dropdown menu on the 'Data Model Details form' . For further information on this, visit our 'Create a Data Model user guide' . When importing a Data Model using Excel, you will need to specify the type in the relevant column of the Data Model listing sheet . For further information on this, please see our 'Import a Data Model from Excel user guide' .","title":"Data Asset"},{"location":"glossary/data-asset/data-asset/#what-is-a-data-asset","text":"There are two types of Data Models within Mauro Data Mapper : Data Asset Data Standard A Data Asset contains existing data. This can be in the form of a database, dataset or a number of completed forms.","title":"What is a Data Asset?"},{"location":"glossary/data-asset/data-asset/#how-are-data-assets-used","text":"A Data Model which is a Data Asset is represented by a database icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree . Data Assets may also include summary metadata within its properties and its data can also be populated from other Data Assets via a Dataflow .","title":"How are Data Assets used?"},{"location":"glossary/data-asset/data-asset/#selecting-a-data-model-type","text":"You will need to assign a Data Model type whenever you are adding or importing a Data Model . When adding a Data Model , select the type from the dropdown menu on the 'Data Model Details form' . For further information on this, visit our 'Create a Data Model user guide' . When importing a Data Model using Excel, you will need to specify the type in the relevant column of the Data Model listing sheet . For further information on this, please see our 'Import a Data Model from Excel user guide' .","title":"Selecting a Data Model type"},{"location":"glossary/data-class/data-class/","text":"What is a Data Class? \u00b6 A Data Class is a collection of data, also known as Data Elements , that are related to each other in some way. For example, each Data Element could appear in the same table of a database, or the same section within a form. How are Data Classes used? \u00b6 Data Classes are the building blocks of a Data Model . Within each Data Class lies several Data Elements and these are the descriptions of an individual field, variable, column or property. You can also have a Data Class within a Data Class , known as a Nested Data Class , which can be a useful way of managing complex sets of data. There is no limit on the number of Nested Data Classes you can include. For example, in a webform, there may be a section called 'Contact details' , which would be one Data Class . Within that section however, there may be another Data Class labelled 'Correspondence Address' . This would be a Nested Data Class . Each Data Class has a: Label This is the name of the Data Class which has to be unique within the Data Model or parent Data Class . Aliases Alternative names that can help locate the Data Class when searched for. Description A definition either written in HTML, Markdown, or plain text which explains the types of data items that are grouped together within the Data Class , as well as any contextual details. Parent Hierarchy The parent of a Data Class can either be the Data Model itself, in which case it is described as a \u2018top level data class\u2019 . Or, if it is a Nested Data Class , its parent Data Class . Multiplicity This specifies the minimum and maximum number of times the Data Class appears within its parent. Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally). Classifications These are effectively tags that you can apply to the Data Class . The above are all shown within the details panel, when the Data Class is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Class is selected in the Model Tree . Content This refers to the various Data Elements and Nested Data Classes within the selected Data Class . Properties Arbitrary additional metadata about this Data Class . Comments Any relevant comments or notes. Links Semantic links between relevant Data Classes . Summary Further metadata information on the nature of the Data Elements within the Data Class . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Attachments Files can be added to provide additional information and context.","title":"Data Class"},{"location":"glossary/data-class/data-class/#what-is-a-data-class","text":"A Data Class is a collection of data, also known as Data Elements , that are related to each other in some way. For example, each Data Element could appear in the same table of a database, or the same section within a form.","title":"What is a Data Class?"},{"location":"glossary/data-class/data-class/#how-are-data-classes-used","text":"Data Classes are the building blocks of a Data Model . Within each Data Class lies several Data Elements and these are the descriptions of an individual field, variable, column or property. You can also have a Data Class within a Data Class , known as a Nested Data Class , which can be a useful way of managing complex sets of data. There is no limit on the number of Nested Data Classes you can include. For example, in a webform, there may be a section called 'Contact details' , which would be one Data Class . Within that section however, there may be another Data Class labelled 'Correspondence Address' . This would be a Nested Data Class . Each Data Class has a: Label This is the name of the Data Class which has to be unique within the Data Model or parent Data Class . Aliases Alternative names that can help locate the Data Class when searched for. Description A definition either written in HTML, Markdown, or plain text which explains the types of data items that are grouped together within the Data Class , as well as any contextual details. Parent Hierarchy The parent of a Data Class can either be the Data Model itself, in which case it is described as a \u2018top level data class\u2019 . Or, if it is a Nested Data Class , its parent Data Class . Multiplicity This specifies the minimum and maximum number of times the Data Class appears within its parent. Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally). Classifications These are effectively tags that you can apply to the Data Class . The above are all shown within the details panel, when the Data Class is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Class is selected in the Model Tree . Content This refers to the various Data Elements and Nested Data Classes within the selected Data Class . Properties Arbitrary additional metadata about this Data Class . Comments Any relevant comments or notes. Links Semantic links between relevant Data Classes . Summary Further metadata information on the nature of the Data Elements within the Data Class . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Attachments Files can be added to provide additional information and context.","title":"How are Data Classes used?"},{"location":"glossary/data-element/data-element/","text":"What is a Data Element? \u00b6 A Data Element is a description of an individual field, variable, column or property of a data item. Each Data Element has a name and a Data Type . How are Data Elements used? \u00b6 Data Elements that are related to each other in some way are grouped together in Data Classes . These Data Classes are the building blocks of Data Models . For example, a Data Element could be an individual field such as \u2018Postcode\u2019 within a webform. Each Data Element has a: Label This is the name of the Data Element which has to be unique within its parent Data Class . Aliases Alternative names that can help locate the Data Element when searched for. Description A definition either written in HTML, Markdown, or plain text which explains any contextual details relating to the Data Element . Data Type The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. Parent Hierarchy Details the parent Data Class and Data Model of the Data Element . Multiplicity This specifies the minimum and maximum number of times the Data Element appears within its parent. Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally). Classifications These are effectively tags that you can apply to the Data Element . The above are all shown within the details panel, when the Data Element is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Element is selected in the Model Tree . Properties Additional metadata about this Data Element . This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility. Comments Any relevant comments or notes. Links Semantic links between relevant Data Elements . Summary Further metadata information on the nature of the Data Elements . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Attachments Files can be added to provide additional information and context.","title":"Data Element"},{"location":"glossary/data-element/data-element/#what-is-a-data-element","text":"A Data Element is a description of an individual field, variable, column or property of a data item. Each Data Element has a name and a Data Type .","title":"What is a Data Element?"},{"location":"glossary/data-element/data-element/#how-are-data-elements-used","text":"Data Elements that are related to each other in some way are grouped together in Data Classes . These Data Classes are the building blocks of Data Models . For example, a Data Element could be an individual field such as \u2018Postcode\u2019 within a webform. Each Data Element has a: Label This is the name of the Data Element which has to be unique within its parent Data Class . Aliases Alternative names that can help locate the Data Element when searched for. Description A definition either written in HTML, Markdown, or plain text which explains any contextual details relating to the Data Element . Data Type The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. Parent Hierarchy Details the parent Data Class and Data Model of the Data Element . Multiplicity This specifies the minimum and maximum number of times the Data Element appears within its parent. Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally). Classifications These are effectively tags that you can apply to the Data Element . The above are all shown within the details panel, when the Data Element is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Element is selected in the Model Tree . Properties Additional metadata about this Data Element . This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility. Comments Any relevant comments or notes. Links Semantic links between relevant Data Elements . Summary Further metadata information on the nature of the Data Elements . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Attachments Files can be added to provide additional information and context.","title":"How are Data Elements used?"},{"location":"glossary/data-model/data-model/","text":"What is a Data Model? \u00b6 A Data Model is a description of an existing collection of metadata, or a specification of data that is to be collected. Data Models which contain existing data are known as a Data Asset , while models which contain templates for data collection are known as a Data Standard . Both are called models as they are effectively representations of the data that they describe. How are Data Models used? \u00b6 Data Models make the connection between the names of columns, fields or variables and our understanding of how the corresponding data is acquired, managed and interpreted. Mauro Data Mapper acts as a directory for these Data Models and allows us to create, search and share these data descriptions. Within each Data Model lies several Data Classes which are groups of data that are related in some way. Data Classes contain Data Elements which are the descriptions of an individual field or variable. For example, a webform where patients enter their details would be a Data Model . The 'Personal details' and 'Contact details' sections within the webform would each be a Data Class . While the individual entries such as 'First Name' , 'Last Name' , 'Date of Birth' etc, would each be a Data Element . The 'Correspondence Address' section is within the 'Contact details' section and would therefore be a Nested Data Class . Each Data Model has a: Label This is the unique name of the Data Model . Aliases Alternative names that can help locate the Data Model when searched for. Organisation Details of who is responsible for creating the Data Model . Description A definition either written in html or plain text which explains the types of data items that are grouped together within the Data Model , as well as any contextual details. Type This defines whether the Data Model is a Data Asset , which contains existing data, or a Data Standard , which contains templates for data collection. Classifications These are effectively tags that you can apply to the Data Class . The above are all shown within the details panel, when the Data Model is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Model is selected in the Model Tree . Data Classes This is a list of all the Data Classes within the Data Model . Types The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. Properties Additional metadata about this Data Model . This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility. Summary Further metadata information on the nature of the Data Classes within the Data Model . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Comments Any relevant comments or notes. History A detailed record of user, date, time and description of all the changes made to the Data Model . Diagram A UML diagram which is a graphical way of summarising the Data Classes and Data Elements within the Data Model . Links Semantic links between relevant Data Models . Attachments Files can be added to provide additional information and context. Dataflow A diagram illustrating where the data within the Data Model has come from and how it has moved across different databases and organisations. This gives users valuable information on the history of each data point and how it has been manipulated.","title":"Data Model"},{"location":"glossary/data-model/data-model/#what-is-a-data-model","text":"A Data Model is a description of an existing collection of metadata, or a specification of data that is to be collected. Data Models which contain existing data are known as a Data Asset , while models which contain templates for data collection are known as a Data Standard . Both are called models as they are effectively representations of the data that they describe.","title":"What is a Data Model?"},{"location":"glossary/data-model/data-model/#how-are-data-models-used","text":"Data Models make the connection between the names of columns, fields or variables and our understanding of how the corresponding data is acquired, managed and interpreted. Mauro Data Mapper acts as a directory for these Data Models and allows us to create, search and share these data descriptions. Within each Data Model lies several Data Classes which are groups of data that are related in some way. Data Classes contain Data Elements which are the descriptions of an individual field or variable. For example, a webform where patients enter their details would be a Data Model . The 'Personal details' and 'Contact details' sections within the webform would each be a Data Class . While the individual entries such as 'First Name' , 'Last Name' , 'Date of Birth' etc, would each be a Data Element . The 'Correspondence Address' section is within the 'Contact details' section and would therefore be a Nested Data Class . Each Data Model has a: Label This is the unique name of the Data Model . Aliases Alternative names that can help locate the Data Model when searched for. Organisation Details of who is responsible for creating the Data Model . Description A definition either written in html or plain text which explains the types of data items that are grouped together within the Data Model , as well as any contextual details. Type This defines whether the Data Model is a Data Asset , which contains existing data, or a Data Standard , which contains templates for data collection. Classifications These are effectively tags that you can apply to the Data Class . The above are all shown within the details panel, when the Data Model is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Model is selected in the Model Tree . Data Classes This is a list of all the Data Classes within the Data Model . Types The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. Properties Additional metadata about this Data Model . This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility. Summary Further metadata information on the nature of the Data Classes within the Data Model . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Comments Any relevant comments or notes. History A detailed record of user, date, time and description of all the changes made to the Data Model . Diagram A UML diagram which is a graphical way of summarising the Data Classes and Data Elements within the Data Model . Links Semantic links between relevant Data Models . Attachments Files can be added to provide additional information and context. Dataflow A diagram illustrating where the data within the Data Model has come from and how it has moved across different databases and organisations. This gives users valuable information on the history of each data point and how it has been manipulated.","title":"How are Data Models used?"},{"location":"glossary/data-standard/data-standard/","text":"What is a Data Standard? \u00b6 There are two types of Data Models within Mauro Data Mapper : Data Standard Data Asset A Data Standard is essentially a template for collecting new data. This can be a form, schema or a specification for distributed data collection. How are Data Standards used? \u00b6 A Data Model which is a Data Standard is represented by a document icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree . As a Data Standard does not contain any collected data, there will be no summary metadata properties or Dataflows associated with this type of model. Selecting a Data Model type \u00b6 You will need to assign a Data Model type whenever you are adding or importing a Data Model . When adding a Data Model , select the type from the dropdown menu on the 'Data Model Details form' . For further information on how to do this, visit our 'Create a Data Model user guide' . When importing a Data Model using Excel, you will need to specify the type in the relevant column on the Data Model listing sheet . For further information on this, please see our 'Import a Data Model from Excel user guide' .","title":"Data Standard"},{"location":"glossary/data-standard/data-standard/#what-is-a-data-standard","text":"There are two types of Data Models within Mauro Data Mapper : Data Standard Data Asset A Data Standard is essentially a template for collecting new data. This can be a form, schema or a specification for distributed data collection.","title":"What is a Data Standard?"},{"location":"glossary/data-standard/data-standard/#how-are-data-standards-used","text":"A Data Model which is a Data Standard is represented by a document icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree . As a Data Standard does not contain any collected data, there will be no summary metadata properties or Dataflows associated with this type of model.","title":"How are Data Standards used?"},{"location":"glossary/data-standard/data-standard/#selecting-a-data-model-type","text":"You will need to assign a Data Model type whenever you are adding or importing a Data Model . When adding a Data Model , select the type from the dropdown menu on the 'Data Model Details form' . For further information on how to do this, visit our 'Create a Data Model user guide' . When importing a Data Model using Excel, you will need to specify the type in the relevant column on the Data Model listing sheet . For further information on this, please see our 'Import a Data Model from Excel user guide' .","title":"Selecting a Data Model type"},{"location":"glossary/data-type/data-type/","text":"What is a Data Type? \u00b6 A Data Type describes the range of possible values that each Data Element may take. How are Data Types used? \u00b6 There are four different Data Types stored within Data Models of Mauro Data Mapper : Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. When adding a new Data Type to a Data Element , you will need to select the relevant Data Type from the dropdown menu on the 'Data Element Details' form. For more information on how to do this, go to step '5.1 Add Data Elements' on our Document a Health Dataset user guide' Each Data Type has a: Label This is the unique name of the Data Type . Aliases Alternative names that can help locate the Data Type when searched for. Description A definition written in either html or plain text which explains any contextual details relating to the Data Type . DataModel The Data Model that the Data Type belongs to. Type The Data Type ( Enumeration , Primitive , Reference or Terminology ). Classifications These are effectively tags that you can apply to the Data Type . Other characteristics are displayed in the tabs underneath the details panel, when the Data Type is selected in the Model Tree . Properties Arbitrary additional metadata about this Data Type . Data Elements The Data Elements that use the selected Data Type . Comments Any relevant comments or notes. Links Semantic links between relevant Data Types . Attachments Files can be added to provide additional information and context.","title":"Data Type"},{"location":"glossary/data-type/data-type/#what-is-a-data-type","text":"A Data Type describes the range of possible values that each Data Element may take.","title":"What is a Data Type?"},{"location":"glossary/data-type/data-type/#how-are-data-types-used","text":"There are four different Data Types stored within Data Models of Mauro Data Mapper : Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. When adding a new Data Type to a Data Element , you will need to select the relevant Data Type from the dropdown menu on the 'Data Element Details' form. For more information on how to do this, go to step '5.1 Add Data Elements' on our Document a Health Dataset user guide' Each Data Type has a: Label This is the unique name of the Data Type . Aliases Alternative names that can help locate the Data Type when searched for. Description A definition written in either html or plain text which explains any contextual details relating to the Data Type . DataModel The Data Model that the Data Type belongs to. Type The Data Type ( Enumeration , Primitive , Reference or Terminology ). Classifications These are effectively tags that you can apply to the Data Type . Other characteristics are displayed in the tabs underneath the details panel, when the Data Type is selected in the Model Tree . Properties Arbitrary additional metadata about this Data Type . Data Elements The Data Elements that use the selected Data Type . Comments Any relevant comments or notes. Links Semantic links between relevant Data Types . Attachments Files can be added to provide additional information and context.","title":"How are Data Types used?"},{"location":"glossary/dataflow/dataflow/","text":"What is a Dataflow? \u00b6 A Dataflow is a way of describing the meaning of data. It is a visual representation of how data flows from one Data Model , typically a Data Asset , to another. This helps users to understand where data has been extracted from, how it has been extracted and the series of transformations it has gone through. Each Dataflow has a source Data Model , which is where the data originated from, and a target Data Model , which is where the final data will be located. The transformations in between define how the data from one or more Data Elements within the source Data Model flow into one or more Data Elements within the target Data Model . How are Dataflows used? \u00b6 The Dataflows relating to each Data Asset can be found in the Dataflows tab at the bottom of the details panel, when the relevant Data Model is selected in the Model Tree . The Dataflows tab displays a diagram which consists of a series of annotated grey blocks connected by annotated arrows. This tab allows you to view the Dataflows at a Data Model level, a Data Class level and a Data Element level. Therefore, the grey blocks can represent Data Assets , Data Classes or Data Elements , depending on the selected view. The arrows represent the relevant transformations that have occurred and are annotated with human readable descriptions. Data Model view \u00b6 When you first click the Dataflows tab, the Data Model view will initially be displayed. This gives a high level overview of how data flows from one Data Asset to another. The grey blocks represent Data Assets which can be databases, lab systems or modules of data. Data Class view \u00b6 To find out information on the different components of a particular Dataflow and how tables of data flow from one place to another, you can access the Data Class view. To do this, hover over one of the transformation arrows until the hand icon appears and double click. The Data Class view will then be displayed with each Data Class represented by a grey rectangle. Data Element view \u00b6 To find out more details on the specific fields of a data product and how they have been derived, you can access the Data Element view. Again, hover over a transformation arrow until the hand icon appears and double click. Here, you will find the various fields grouped together and the relationships between them. This is particularly useful for end users as it explains how data has been manipulated. Zoom \u00b6 In all three views, you can zoom in and out by scrolling on your mouse, or by clicking the \u2018+ Zoom in' and \u2018- Zoom out' buttons at the top left of the Dataflows view and in the header bar. Reset \u00b6 To reset the view, select \u2018RESET\u2019 at the top left of the Dataflows view or click the circular arrow \u2018Reset zoom and canvas\u2019 button in the header bar. Fullscreen \u00b6 You can also view the Dataflow tab in fullscreen mode by clicking the 'Popup in full screen' icon on the top right of the header bar. Download image \u00b6 To download an image of the Dataflow select the 'Download as image' icon at the top right of the header bar. This will download a png image of the current view displayed in the Dataflow tab to your downloads folder. Navigating through Dataflows \u00b6 To navigate between the three different views, hover and double click over the transformation arrows in each view to access the next level. For example, to move from the Data Model view to the Data Class view. To navigate up a level of hierarchy, select the upwards arrow \u2018Move up a level of hierarchy\u2019 icon in the top left of the header bar. For example, to move from the Data Class view to the Data Model view. In all three views you can move the blocks and transformation arrows by clicking and holding the left mouse button which will allow you to drag them to the desired position. This can be helpful to visualise the Dataflows in a clearer manner.","title":"Dataflow"},{"location":"glossary/dataflow/dataflow/#what-is-a-dataflow","text":"A Dataflow is a way of describing the meaning of data. It is a visual representation of how data flows from one Data Model , typically a Data Asset , to another. This helps users to understand where data has been extracted from, how it has been extracted and the series of transformations it has gone through. Each Dataflow has a source Data Model , which is where the data originated from, and a target Data Model , which is where the final data will be located. The transformations in between define how the data from one or more Data Elements within the source Data Model flow into one or more Data Elements within the target Data Model .","title":"What is a Dataflow?"},{"location":"glossary/dataflow/dataflow/#how-are-dataflows-used","text":"The Dataflows relating to each Data Asset can be found in the Dataflows tab at the bottom of the details panel, when the relevant Data Model is selected in the Model Tree . The Dataflows tab displays a diagram which consists of a series of annotated grey blocks connected by annotated arrows. This tab allows you to view the Dataflows at a Data Model level, a Data Class level and a Data Element level. Therefore, the grey blocks can represent Data Assets , Data Classes or Data Elements , depending on the selected view. The arrows represent the relevant transformations that have occurred and are annotated with human readable descriptions.","title":"How are Dataflows used?"},{"location":"glossary/dataflow/dataflow/#data-model-view","text":"When you first click the Dataflows tab, the Data Model view will initially be displayed. This gives a high level overview of how data flows from one Data Asset to another. The grey blocks represent Data Assets which can be databases, lab systems or modules of data.","title":"Data Model view"},{"location":"glossary/dataflow/dataflow/#data-class-view","text":"To find out information on the different components of a particular Dataflow and how tables of data flow from one place to another, you can access the Data Class view. To do this, hover over one of the transformation arrows until the hand icon appears and double click. The Data Class view will then be displayed with each Data Class represented by a grey rectangle.","title":"Data Class view"},{"location":"glossary/dataflow/dataflow/#data-element-view","text":"To find out more details on the specific fields of a data product and how they have been derived, you can access the Data Element view. Again, hover over a transformation arrow until the hand icon appears and double click. Here, you will find the various fields grouped together and the relationships between them. This is particularly useful for end users as it explains how data has been manipulated.","title":"Data Element view"},{"location":"glossary/dataflow/dataflow/#zoom","text":"In all three views, you can zoom in and out by scrolling on your mouse, or by clicking the \u2018+ Zoom in' and \u2018- Zoom out' buttons at the top left of the Dataflows view and in the header bar.","title":"Zoom"},{"location":"glossary/dataflow/dataflow/#reset","text":"To reset the view, select \u2018RESET\u2019 at the top left of the Dataflows view or click the circular arrow \u2018Reset zoom and canvas\u2019 button in the header bar.","title":"Reset"},{"location":"glossary/dataflow/dataflow/#fullscreen","text":"You can also view the Dataflow tab in fullscreen mode by clicking the 'Popup in full screen' icon on the top right of the header bar.","title":"Fullscreen"},{"location":"glossary/dataflow/dataflow/#download-image","text":"To download an image of the Dataflow select the 'Download as image' icon at the top right of the header bar. This will download a png image of the current view displayed in the Dataflow tab to your downloads folder.","title":"Download image"},{"location":"glossary/dataflow/dataflow/#navigating-through-dataflows","text":"To navigate between the three different views, hover and double click over the transformation arrows in each view to access the next level. For example, to move from the Data Model view to the Data Class view. To navigate up a level of hierarchy, select the upwards arrow \u2018Move up a level of hierarchy\u2019 icon in the top left of the header bar. For example, to move from the Data Class view to the Data Model view. In all three views you can move the blocks and transformation arrows by clicking and holding the left mouse button which will allow you to drag them to the desired position. This can be helpful to visualise the Dataflows in a clearer manner.","title":"Navigating through Dataflows"},{"location":"glossary/enumeration-data-type/enumeration-data-type/","text":"What is an Enumeration Data Type? \u00b6 An Enumeration Data Type is one of the four possible Data Types within Mauro Data Mapper . Each Enumeration Data Type describes a constrained set of possible Enumeration values. Enumerations are typically used for describing simple lists of data. How are Enumeration Data Types used? \u00b6 Each Enumeration value has an associated label, or a coded key, and a textual description, or human-readable value. These key-value pairs can be used to describe a list of data. For example, one Enumeration Data Type could be 'Ethnic category' , where each Enumeration value describes a specific ethnicity. Further details of the particular Enumeration Data Type can be found in its details panel. The individual Enumeration values, along with their corresponding 'Key' and 'Value' are displayed in an \u2018Enumerations\u2019 table below the details panel. These can be edited by clicking the pencil icon to the right of each row. Enumerations can also be removed by clicking the red bin icon to the right of each row. To add an Enumeration, click \u2018+Add Enumeration\u2019 at the top right of the 'Enumerations' table. This will add a row to the bottom of the table, where you can then populate the \u2018Group\u2019 , \u2018Key\u2019 and \u2018Value\u2019 fields. Once completed, click the green tick to the right of the row, and the new Enumeration will be added. You can also view all the Data Elements that use that Enumeration Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , 'Name' and 'Description' of each Data Element .","title":"Enumeration Data Type"},{"location":"glossary/enumeration-data-type/enumeration-data-type/#what-is-an-enumeration-data-type","text":"An Enumeration Data Type is one of the four possible Data Types within Mauro Data Mapper . Each Enumeration Data Type describes a constrained set of possible Enumeration values. Enumerations are typically used for describing simple lists of data.","title":"What is an Enumeration Data Type?"},{"location":"glossary/enumeration-data-type/enumeration-data-type/#how-are-enumeration-data-types-used","text":"Each Enumeration value has an associated label, or a coded key, and a textual description, or human-readable value. These key-value pairs can be used to describe a list of data. For example, one Enumeration Data Type could be 'Ethnic category' , where each Enumeration value describes a specific ethnicity. Further details of the particular Enumeration Data Type can be found in its details panel. The individual Enumeration values, along with their corresponding 'Key' and 'Value' are displayed in an \u2018Enumerations\u2019 table below the details panel. These can be edited by clicking the pencil icon to the right of each row. Enumerations can also be removed by clicking the red bin icon to the right of each row. To add an Enumeration, click \u2018+Add Enumeration\u2019 at the top right of the 'Enumerations' table. This will add a row to the bottom of the table, where you can then populate the \u2018Group\u2019 , \u2018Key\u2019 and \u2018Value\u2019 fields. Once completed, click the green tick to the right of the row, and the new Enumeration will be added. You can also view all the Data Elements that use that Enumeration Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , 'Name' and 'Description' of each Data Element .","title":"How are Enumeration Data Types used?"},{"location":"glossary/label/label/","text":"What is a Label? \u00b6 A Label is a name that describes and uniquely identifies each item within Mauro Data Mapper . This Label will appear in the Model Tree on the left hand side of the catalogue and at the top of the page when the item is selected. The Label is also used to identify the item when searched for. How are Labels used? \u00b6 The Label of each item must be unique within its parent group so that no two items share the same Label . Therefore, each Data Model must have a unique Label . Each Data Class must have a unique Label within its parent Data Model . Each Data Element must have a unique Label within its parent Data Class . For example, there can only be one Data Class called \u2018Personal details\u2019 within a particular Data Model . Therefore, if you need to add a similar Data Class , include version information within the Label such as \u2018Personal details 2.0\u2019 to uniquely identify it. In some cases, two different Data Models could consist of a Data Class with the same Label , such as 'Personal details' . However, because these two Data Classes are each associated with their own unique parent Data Model , then this is acceptable. Only when two items are within the same parent must they each have a unique Label . How do you edit a Label? \u00b6 You can edit the Label of any item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the Label at the top of the details panel.","title":"Label"},{"location":"glossary/label/label/#what-is-a-label","text":"A Label is a name that describes and uniquely identifies each item within Mauro Data Mapper . This Label will appear in the Model Tree on the left hand side of the catalogue and at the top of the page when the item is selected. The Label is also used to identify the item when searched for.","title":"What is a Label?"},{"location":"glossary/label/label/#how-are-labels-used","text":"The Label of each item must be unique within its parent group so that no two items share the same Label . Therefore, each Data Model must have a unique Label . Each Data Class must have a unique Label within its parent Data Model . Each Data Element must have a unique Label within its parent Data Class . For example, there can only be one Data Class called \u2018Personal details\u2019 within a particular Data Model . Therefore, if you need to add a similar Data Class , include version information within the Label such as \u2018Personal details 2.0\u2019 to uniquely identify it. In some cases, two different Data Models could consist of a Data Class with the same Label , such as 'Personal details' . However, because these two Data Classes are each associated with their own unique parent Data Model , then this is acceptable. Only when two items are within the same parent must they each have a unique Label .","title":"How are Labels used?"},{"location":"glossary/label/label/#how-do-you-edit-a-label","text":"You can edit the Label of any item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the Label at the top of the details panel.","title":"How do you edit a Label?"},{"location":"glossary/multiplicity/multiplicity/","text":"What is Multiplicity? \u00b6 An item's Multiplicity refers to the minimum and maximum number of times it can appear within its parent in Mauro Data Mapper . For example, the number of instances a particular Data Element appears within a Data Class . How is Multiplicity used? \u00b6 Every Data Class and Data Element is assigned a Multiplicity . Typically, the Multiplicity is written in the form \u2018minmax \u2019, where min and max are integers representing the minimum and maximum number of times an item will appear within its parent. The symbol * represents an unbounded maximum. Using this notation: Mandatory data has a minimum Multiplicity of 1 and a maximum Multiplicity of a specific integer or if there is no upper bound, then '*' (which is represented by '-1' internally). Optional data has a minimum Multiplicity of 0 and a maximum Multiplicity of 1 or if there is no upper bound, *. For example, each person in a Data Model only has one date of birth and if you want to record this, then the corresponding Multiplicity will be 1..1. However, each person may have many prescription records, which may or may not be relevant. In this case, the Multiplicity would be 0..*. Furthermore, each person will only have one date of death, which again you may or may not want to record, so the Multiplicity for this would be 0..1. How do you edit an item's Multiplicity? \u00b6 The Multiplicity can be found in the details panel of Data Classes and Data Elements . It is a mandatory field when adding or importing a Data Class or Data Elements , as explained in the 'Document a Health Datatset user guide' and 'Import a Data Model from Excel user guide' . You can edit the Multiplicity of an item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the min and max values of that item's Multiplicity .","title":"Multiplicity"},{"location":"glossary/multiplicity/multiplicity/#what-is-multiplicity","text":"An item's Multiplicity refers to the minimum and maximum number of times it can appear within its parent in Mauro Data Mapper . For example, the number of instances a particular Data Element appears within a Data Class .","title":"What is Multiplicity?"},{"location":"glossary/multiplicity/multiplicity/#how-is-multiplicity-used","text":"Every Data Class and Data Element is assigned a Multiplicity . Typically, the Multiplicity is written in the form \u2018minmax \u2019, where min and max are integers representing the minimum and maximum number of times an item will appear within its parent. The symbol * represents an unbounded maximum. Using this notation: Mandatory data has a minimum Multiplicity of 1 and a maximum Multiplicity of a specific integer or if there is no upper bound, then '*' (which is represented by '-1' internally). Optional data has a minimum Multiplicity of 0 and a maximum Multiplicity of 1 or if there is no upper bound, *. For example, each person in a Data Model only has one date of birth and if you want to record this, then the corresponding Multiplicity will be 1..1. However, each person may have many prescription records, which may or may not be relevant. In this case, the Multiplicity would be 0..*. Furthermore, each person will only have one date of death, which again you may or may not want to record, so the Multiplicity for this would be 0..1.","title":"How is Multiplicity used?"},{"location":"glossary/multiplicity/multiplicity/#how-do-you-edit-an-items-multiplicity","text":"The Multiplicity can be found in the details panel of Data Classes and Data Elements . It is a mandatory field when adding or importing a Data Class or Data Elements , as explained in the 'Document a Health Datatset user guide' and 'Import a Data Model from Excel user guide' . You can edit the Multiplicity of an item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the min and max values of that item's Multiplicity .","title":"How do you edit an item's Multiplicity?"},{"location":"glossary/primitive-data-type/primitive-data-type/","text":"What is a Primitive Data Type? \u00b6 A Primitive Data Type is one of the four possible Data Types within Mauro Data Mapper . It describes data which has no further details about structure or referencing. For example, if a Data Element is a name, then it\u2019s Primitive Data Type would be \u2018String\u2019 . Whereas, if a Data Element is a number, then it\u2019s Primitive Data Type would be \u2018Integer\u2019 . How are Primitive Data Types used? \u00b6 Primitive Data Types are typically used for data which are strings or integers such as names, dates or times. Further details of the particular Primitive Data Type can be found in its details panel. You can also view all the Data Elements that use that Primitive Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , Name and Description of each Data Element .","title":"Primitive Data Type"},{"location":"glossary/primitive-data-type/primitive-data-type/#what-is-a-primitive-data-type","text":"A Primitive Data Type is one of the four possible Data Types within Mauro Data Mapper . It describes data which has no further details about structure or referencing. For example, if a Data Element is a name, then it\u2019s Primitive Data Type would be \u2018String\u2019 . Whereas, if a Data Element is a number, then it\u2019s Primitive Data Type would be \u2018Integer\u2019 .","title":"What is a Primitive Data Type?"},{"location":"glossary/primitive-data-type/primitive-data-type/#how-are-primitive-data-types-used","text":"Primitive Data Types are typically used for data which are strings or integers such as names, dates or times. Further details of the particular Primitive Data Type can be found in its details panel. You can also view all the Data Elements that use that Primitive Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , Name and Description of each Data Element .","title":"How are Primitive Data Types used?"},{"location":"glossary/reference-data-type/reference-data-type/","text":"What is a Reference Data Type? \u00b6 A Reference Data Type is one of the four possible Data Types within Mauro Data Mapper . It is used to describe relationships between different Data Classes within the same Data Model . Typically, Reference Data Types \u2018refer\u2019 and consequently link to another specified Data Class . How are Reference Data Types used? \u00b6 By using Reference Data Types to link similar Data Classes together, users don\u2019t have to repeatedly add in new Data Classes , if a similar version already exists. For example, consider a Data Model where you have a \u2018Patient\u2019 Data Class and a \u2018GP\u2019 Data Class . Within the \u2018Patient\u2019 Data Class , there may be a Data Element called \u2018registeredGP\u2019 . This Data Element refers to the \u2018GP\u2019 Data Class , and therefore it is a Reference Data Type . In the details panel of a Reference Data Type , a link to the Data Class it refers to is displayed in the \u2018Type\u2019 field. Clicking this link will navigate you to the details panel of that particular Data Class , where you can view all its associated Data Elements in the \u2018Content\u2019 tab below the details panel. You can also view all the Data Elements that use that Reference Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , Name and Description of each Data Element .","title":"Reference Data Type"},{"location":"glossary/reference-data-type/reference-data-type/#what-is-a-reference-data-type","text":"A Reference Data Type is one of the four possible Data Types within Mauro Data Mapper . It is used to describe relationships between different Data Classes within the same Data Model . Typically, Reference Data Types \u2018refer\u2019 and consequently link to another specified Data Class .","title":"What is a Reference Data Type?"},{"location":"glossary/reference-data-type/reference-data-type/#how-are-reference-data-types-used","text":"By using Reference Data Types to link similar Data Classes together, users don\u2019t have to repeatedly add in new Data Classes , if a similar version already exists. For example, consider a Data Model where you have a \u2018Patient\u2019 Data Class and a \u2018GP\u2019 Data Class . Within the \u2018Patient\u2019 Data Class , there may be a Data Element called \u2018registeredGP\u2019 . This Data Element refers to the \u2018GP\u2019 Data Class , and therefore it is a Reference Data Type . In the details panel of a Reference Data Type , a link to the Data Class it refers to is displayed in the \u2018Type\u2019 field. Clicking this link will navigate you to the details panel of that particular Data Class , where you can view all its associated Data Elements in the \u2018Content\u2019 tab below the details panel. You can also view all the Data Elements that use that Reference Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , Name and Description of each Data Element .","title":"How are Reference Data Types used?"},{"location":"glossary/semantic-links/semantic-links/","text":"What is a Semantic link? \u00b6 The term semantics refers to the meaning of data. It is essential that whenever data is produced in one system and used in another, it\u2019s meaning remains consistent between the two systems. This is known as semantic interoperability. Therefore, a Semantic link indicates that there is some sort of relationship between two descriptions of data. How are Semantic links used? \u00b6 There are two types of Semantic links used within Mauro Data Mapper . Refines \u00b6 The most common type of semantic relationship is a \u2018Refines\u2019 link. Refine means to improve and in this context it signifies improving the quality and amount of information provided. Consequently, when one description refines another, it means that everything that is true about one description is also true about the other description, whilst often adding more information or context. For example, if a description of a column in a database specification Refines an item in a Data Standard , then everything the Data Standard says about that data item applies to the column. The rest of the database specification may contain more information about that column, such as specific conditions or constraints, but the description in the Data Standard still applies. This type of Semantic link can be created between Data Models , Data Classes and Data Elements . Does not refine \u00b6 Similarly, you can also create a \u2018does not refine\u2019 link which is used to indicate that the present definition is not intended as a refinement of another. If a Data Model , Data Class or Data Element contain any Semantic links , these are summarised in a 'Links' table below the details panel when the relevant data is selected in the Model Tree . This table displays a hyperlink to the target data the Semantic link refers to, as well as the type and total number of links. Links can be edited, created or removed in the 'Links' tab below the details panel. To edit the type of link or the target, click the 'Edit' pencil icon to the right. To add a link click '+ Add Link' and you will be able to specify the type of link and select the target. Click the green tick to confirm and save your changes.","title":"Semantic links"},{"location":"glossary/semantic-links/semantic-links/#what-is-a-semantic-link","text":"The term semantics refers to the meaning of data. It is essential that whenever data is produced in one system and used in another, it\u2019s meaning remains consistent between the two systems. This is known as semantic interoperability. Therefore, a Semantic link indicates that there is some sort of relationship between two descriptions of data.","title":"What is a Semantic link?"},{"location":"glossary/semantic-links/semantic-links/#how-are-semantic-links-used","text":"There are two types of Semantic links used within Mauro Data Mapper .","title":"How are Semantic links used?"},{"location":"glossary/semantic-links/semantic-links/#refines","text":"The most common type of semantic relationship is a \u2018Refines\u2019 link. Refine means to improve and in this context it signifies improving the quality and amount of information provided. Consequently, when one description refines another, it means that everything that is true about one description is also true about the other description, whilst often adding more information or context. For example, if a description of a column in a database specification Refines an item in a Data Standard , then everything the Data Standard says about that data item applies to the column. The rest of the database specification may contain more information about that column, such as specific conditions or constraints, but the description in the Data Standard still applies. This type of Semantic link can be created between Data Models , Data Classes and Data Elements .","title":"Refines"},{"location":"glossary/semantic-links/semantic-links/#does-not-refine","text":"Similarly, you can also create a \u2018does not refine\u2019 link which is used to indicate that the present definition is not intended as a refinement of another. If a Data Model , Data Class or Data Element contain any Semantic links , these are summarised in a 'Links' table below the details panel when the relevant data is selected in the Model Tree . This table displays a hyperlink to the target data the Semantic link refers to, as well as the type and total number of links. Links can be edited, created or removed in the 'Links' tab below the details panel. To edit the type of link or the target, click the 'Edit' pencil icon to the right. To add a link click '+ Add Link' and you will be able to specify the type of link and select the target. Click the green tick to confirm and save your changes.","title":"Does not refine"},{"location":"glossary/terminology-data-type/terminology-data-type/","text":"What is a Terminology Data Type? \u00b6 A Terminology Data Type is one of the four possible Data Types within Mauro Data Mapper . It is used to describe a structured collection of Enumerated Values which have relationships between different data terms. How are Terminology Data Types used? \u00b6 Terminology Data Types have now been expanded in Mauro Data Mapper to \u2018Model Reference\u2019 . Model References can now point to a Terminology , a CodeSet or a ReferenceDataModel . Terminology \u00b6 A Terminology is a vocabulary, or a collection of allowable Terms as well as any relationships between them. A Term typically has a coded key and a human-readable value, along with some other information. Any relationships between pairs of Terms can be defined, including the relationship stating that one Term has a broader or narrower meaning than another Term . For example, a hierarchy of Terms denoting patient diagnoses might include a general Term to indicate some form of Diabetes. This may be related to more specific Terms to indicate a particular form of Diabetes such as Type 1 Diabetes. Terminologies are represented by a book icon in Mauro Data Mapper and you can browse through several different databases by selecting \u2018Healthcare Terminologies\u2019 in the Model Tree . Further details of the particular Terminology can be found in its details panel when selected. To view information relating to a specific Term , select the relevant Term in the Model Tree and its corresponding details panel will be displayed. Each Term has a: Label This is the unique name of the Term . Aliases Alternative names that can help locate the Term when searched for. Terminology The Terminology that this Term is associated with. Description A definition either written in html or plain text which explains any contextual details relating to the Term . URL A URL to the original definition of the Term . Classifications These are effectively tags that you can apply to the Term . Below the details panel, is a list of Relationships between the selected Term and other Terms within the specified Terminology . CodeSet \u00b6 A CodeSet is a selection of terms, which may be taken from one or more Terminologies . For example, a CodeSet would be all the Terms that describe the different variants of Diabetes. ReferenceDataModel \u00b6 ReferenceDataModels are similar to a large database containing lots of detailed information and properties that would be too difficult to accurately manage in an Enumeration list. For example, consider the NHS organisation codes. This list includes all the organisation codes that represent each hospital, GP surgery and other practices. Alongside this organisation code could also be a name, an address and the details of the main contact at each hospital or surgery. Therefore, each data item has many different properties associated with it.","title":"Terminology Data Type"},{"location":"glossary/terminology-data-type/terminology-data-type/#what-is-a-terminology-data-type","text":"A Terminology Data Type is one of the four possible Data Types within Mauro Data Mapper . It is used to describe a structured collection of Enumerated Values which have relationships between different data terms.","title":"What is a Terminology Data Type?"},{"location":"glossary/terminology-data-type/terminology-data-type/#how-are-terminology-data-types-used","text":"Terminology Data Types have now been expanded in Mauro Data Mapper to \u2018Model Reference\u2019 . Model References can now point to a Terminology , a CodeSet or a ReferenceDataModel .","title":"How are Terminology Data Types used?"},{"location":"glossary/terminology-data-type/terminology-data-type/#terminology","text":"A Terminology is a vocabulary, or a collection of allowable Terms as well as any relationships between them. A Term typically has a coded key and a human-readable value, along with some other information. Any relationships between pairs of Terms can be defined, including the relationship stating that one Term has a broader or narrower meaning than another Term . For example, a hierarchy of Terms denoting patient diagnoses might include a general Term to indicate some form of Diabetes. This may be related to more specific Terms to indicate a particular form of Diabetes such as Type 1 Diabetes. Terminologies are represented by a book icon in Mauro Data Mapper and you can browse through several different databases by selecting \u2018Healthcare Terminologies\u2019 in the Model Tree . Further details of the particular Terminology can be found in its details panel when selected. To view information relating to a specific Term , select the relevant Term in the Model Tree and its corresponding details panel will be displayed. Each Term has a: Label This is the unique name of the Term . Aliases Alternative names that can help locate the Term when searched for. Terminology The Terminology that this Term is associated with. Description A definition either written in html or plain text which explains any contextual details relating to the Term . URL A URL to the original definition of the Term . Classifications These are effectively tags that you can apply to the Term . Below the details panel, is a list of Relationships between the selected Term and other Terms within the specified Terminology .","title":"Terminology"},{"location":"glossary/terminology-data-type/terminology-data-type/#codeset","text":"A CodeSet is a selection of terms, which may be taken from one or more Terminologies . For example, a CodeSet would be all the Terms that describe the different variants of Diabetes.","title":"CodeSet"},{"location":"glossary/terminology-data-type/terminology-data-type/#referencedatamodel","text":"ReferenceDataModels are similar to a large database containing lots of detailed information and properties that would be too difficult to accurately manage in an Enumeration list. For example, consider the NHS organisation codes. This list includes all the organisation codes that represent each hospital, GP surgery and other practices. Alongside this organisation code could also be a name, an address and the details of the main contact at each hospital or surgery. Therefore, each data item has many different properties associated with it.","title":"ReferenceDataModel"},{"location":"installing/administration/","text":"Checking version information \u00b6 Inside the web interface, the Plugins and Modules tab on the administrator dashboard provides information about the loaded components of the Mauro installation. The first list, Plugins , provides a list of names and versions of components which integrate with known plugin hooks. For example Importers , Exporters and Email Providers . The Modules list below provides a more comprehensive account of all Java / Grails components installed, including system libraries. Each Module may include one or more Plugins . To provide automated reporting, the API call behind this dashboard may be called separately. The GET urls below will return lists of plugins, for each named type: /api/admin/providers/importers /api/admin/providers/exporters /api/admin/providers/dataLoaders /api/admin/providers/emailers The endpoint below will return the list of modules: /api/admin/modules Updating to the latest version \u00b6 The Docker installation repository provides an ./update script for updating to the latest version of Mauro: 1 2 3 4 5 6 7 # Update an already built system ./update Usage ./update [ -b COMMIT_BRANCH ] [ -f COMMIT_BRANCH ] -b, --back-end COMMIT_BRANCH : The commit or branch to checkout and build for the back-end from mdm-core -f, --front-end COMMIT_BRANCH : The commit or branch to checkout and build for the front-end from mdm-ui When running the Docker repository from the main branch, this will update the running instance to the latest tagged release. When running the develop branch of the Docker repository, this command will update to the latest develop branch of Core and UI components. For example, when in the Git repository, you can run: 1 git pull Followed by: 1 ./update This will rebuild just the Mauro Data Mapper image with the latest version. The script will restart the container using that version. Occasionally, database migrations are required when updating to a new version. These run automatically when the application restarts, making use of the Flyway versioning system. No manual steps are required from the user. Backing up the database \u00b6 In order to back up the data from a running Mauro system, it is usually sufficient to take a simple backup of the underlying Postgres database, which can be achieved through standard Postgres utilities (for example, using pg_dump ). Within the Docker repository, a simple script in postgres/bin/snapshot-data.sh can be used within the docker container to take a copy of the underlying postgres database. This creates a file in a new folder /database on the container which can be copied back out to the host machine. Alternatively, you can run an exec command directly from the host machine. For example the command listed below: 1 docker-compose exec postgres pg_dump -U postgres maurodatamapper | gzip -9 > db-backup- $( date +%d-%m-%y ) .sql.gz This will execute the pg_dump command on the postgres container, connecting to the maurodatamapper database. The result will be zipped using the gzip command, creating a file with today's timestamp on it. Backup requirements vary, but a typical use-case is to combine one of the backup commands listed above with a script to manage regular backups at timed intervals and deleting old backups once a certain period has passed. Example scripts which may be adapted can be found on the official Postgres Wiki here . Re-building the search index \u00b6 The search index improves the performance of searching and this content is stored in-memory (and persisted to files on disk at suitable intervals). In some places in the Core , it may also be used to speed up access to particular model contents. The index is built using Apache Lucene but managed in the code through Hibernate. This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary. For example if searching provides incorrect or inconsistent results. Administrators may rebuild the Lucene index through the user interface. To do this, click the white arrow by your user profile to the right of the menu header. Select 'Configuration' from the dropdown menu. This will take you to configuration page where you can click the 'Lucene' tab and then select 'Rebuild Index' . Please do not leave the page whilst reindexing is in progress. The time required is dependent on the number of models saved in the system, but may take between 5 and 10 minutes for a large system. Alternatively, an API call may be made: see here for details. This POST call may be made with an existing session cookie, by passing username / password parameters as part of the call, or by passing an API Key. Only those with system administrator role may perform this action. The contents of the search index can be hard to inspect for debugging purposes! We use a tool called Marple but be sure to use a compatible version. Docker administration \u00b6 Cleaning up docker \u00b6 Continually building docker images will leave a lot of loose snapshot images floating around, occasionally make use of: Clean up stopped containers - docker rm $(docker ps -a -q) Clean up untagged images - docker rmi $(docker images | grep \"^<none>\" | awk \"{print $3}\") Clean up dangling volumes - docker volume rm $(docker volume ls -qf dangling=true) You can make life easier by adding the following to the appropriate bash profile file: 1 2 3 alias docker-rmi = 'docker rmi $(docker images -q --filter \"dangling=true\")' alias docker-rm = 'docker rm $(docker ps -a -q)' alias docker-rmv = 'docker volume rm $(docker volume ls -qf dangling=true)' Remove all stopped containers first then remove all tagged images. A useful tool is Dockviz , ever since docker did away with docker images --tree you can't see all the layers of images and therefore how many disconneected images you have. Add the following to the appropriate bash profile file: 1 alias dockviz = \"docker run --privileged -it --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz\" Then in a new terminal you can run dockviz images -t to see the tree. The program also does dot notation files for a graphical view as well. Multiple compose files \u00b6 When you supply multiple files, docker-compose combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their successors. 1 2 3 4 5 # Apply the .dev yml file, create and start the containers in the background $ docker-compose -f docker-compose.yml -f docker-compose.dev.yml -d <COMMAND> # Apply the .prod yml file, create and start the containers in the background $ docker-compose -f docker-compose.yml -f docker-compose.prod.yml -d <COMMAND> We recommend adding the following lines to the appropriate bash profile file: 1 2 alias docker-compose-dev = \"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\" alias docker-compose-prod = \"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\" This will allow you to start compose in dev mode without all the extra file definitions. Debugging and advanced configuration \u00b6 Here we present some useful hints for extending or customising the Docker setup for different configurations or use cases: Development file override The docker-compose.dev.yml can be used to override the standard docker-compose.yml file for development. In its initial configuarion, it opens up the ports in the Postgres container for manual connection from the host machine. This .dev compose file rebuilds all of the images, whereas the standard compose file and .prod versions do not build new images. Make use of the wait_scripts While -links and depends_on make sure the services a service requires are brought up first Docker only waits till they are running NOT till they are actually ready. The wait scripts provided test responses on given ports to make sure that a given service is actually available and ready to interact. Use COPY over ADD Docker recommends using COPY instead of ADD unless the source is a URL or a tar file which ADD can retrieve and/or unzip. Use of ENTRYPOINT & CMD If not requiring any dependencies then just set CMD [\"arg1\", ...] and the args will be passed to the ENTRYPOINT If requiring dependencies then set the ENTRYPOINT to the wait script and the CMD to CMD [\"process\", \"arg1\", ...] Try to keep images as small as possible As a general rule, we try to use the base images (e.g. of Tomcat, Postgres) and install additional packages at runtime or start-up. This increases portability and cuts down on disk usage when deploying updates. Exposing ports Exposing ports to other services must be carefully considered, to avoid unnecessary security vulnerabilities If the port only needs to be available to other docker services then use expose . If the port needs to be open to the host machine or beyond, then use ports . If the ports option is used this opens the port from the service to the outside world, it does not affect exposed ports between services, so if a service (e.g. postgres with port 5432) exposes a port then any service which used link to postgres will be able to find the database at postgresql://postgres:5432","title":"Administration"},{"location":"installing/administration/#checking-version-information","text":"Inside the web interface, the Plugins and Modules tab on the administrator dashboard provides information about the loaded components of the Mauro installation. The first list, Plugins , provides a list of names and versions of components which integrate with known plugin hooks. For example Importers , Exporters and Email Providers . The Modules list below provides a more comprehensive account of all Java / Grails components installed, including system libraries. Each Module may include one or more Plugins . To provide automated reporting, the API call behind this dashboard may be called separately. The GET urls below will return lists of plugins, for each named type: /api/admin/providers/importers /api/admin/providers/exporters /api/admin/providers/dataLoaders /api/admin/providers/emailers The endpoint below will return the list of modules: /api/admin/modules","title":"Checking version information"},{"location":"installing/administration/#updating-to-the-latest-version","text":"The Docker installation repository provides an ./update script for updating to the latest version of Mauro: 1 2 3 4 5 6 7 # Update an already built system ./update Usage ./update [ -b COMMIT_BRANCH ] [ -f COMMIT_BRANCH ] -b, --back-end COMMIT_BRANCH : The commit or branch to checkout and build for the back-end from mdm-core -f, --front-end COMMIT_BRANCH : The commit or branch to checkout and build for the front-end from mdm-ui When running the Docker repository from the main branch, this will update the running instance to the latest tagged release. When running the develop branch of the Docker repository, this command will update to the latest develop branch of Core and UI components. For example, when in the Git repository, you can run: 1 git pull Followed by: 1 ./update This will rebuild just the Mauro Data Mapper image with the latest version. The script will restart the container using that version. Occasionally, database migrations are required when updating to a new version. These run automatically when the application restarts, making use of the Flyway versioning system. No manual steps are required from the user.","title":"Updating to the latest version"},{"location":"installing/administration/#backing-up-the-database","text":"In order to back up the data from a running Mauro system, it is usually sufficient to take a simple backup of the underlying Postgres database, which can be achieved through standard Postgres utilities (for example, using pg_dump ). Within the Docker repository, a simple script in postgres/bin/snapshot-data.sh can be used within the docker container to take a copy of the underlying postgres database. This creates a file in a new folder /database on the container which can be copied back out to the host machine. Alternatively, you can run an exec command directly from the host machine. For example the command listed below: 1 docker-compose exec postgres pg_dump -U postgres maurodatamapper | gzip -9 > db-backup- $( date +%d-%m-%y ) .sql.gz This will execute the pg_dump command on the postgres container, connecting to the maurodatamapper database. The result will be zipped using the gzip command, creating a file with today's timestamp on it. Backup requirements vary, but a typical use-case is to combine one of the backup commands listed above with a script to manage regular backups at timed intervals and deleting old backups once a certain period has passed. Example scripts which may be adapted can be found on the official Postgres Wiki here .","title":"Backing up the database"},{"location":"installing/administration/#re-building-the-search-index","text":"The search index improves the performance of searching and this content is stored in-memory (and persisted to files on disk at suitable intervals). In some places in the Core , it may also be used to speed up access to particular model contents. The index is built using Apache Lucene but managed in the code through Hibernate. This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary. For example if searching provides incorrect or inconsistent results. Administrators may rebuild the Lucene index through the user interface. To do this, click the white arrow by your user profile to the right of the menu header. Select 'Configuration' from the dropdown menu. This will take you to configuration page where you can click the 'Lucene' tab and then select 'Rebuild Index' . Please do not leave the page whilst reindexing is in progress. The time required is dependent on the number of models saved in the system, but may take between 5 and 10 minutes for a large system. Alternatively, an API call may be made: see here for details. This POST call may be made with an existing session cookie, by passing username / password parameters as part of the call, or by passing an API Key. Only those with system administrator role may perform this action. The contents of the search index can be hard to inspect for debugging purposes! We use a tool called Marple but be sure to use a compatible version.","title":"Re-building the search index"},{"location":"installing/administration/#docker-administration","text":"","title":"Docker administration"},{"location":"installing/administration/#cleaning-up-docker","text":"Continually building docker images will leave a lot of loose snapshot images floating around, occasionally make use of: Clean up stopped containers - docker rm $(docker ps -a -q) Clean up untagged images - docker rmi $(docker images | grep \"^<none>\" | awk \"{print $3}\") Clean up dangling volumes - docker volume rm $(docker volume ls -qf dangling=true) You can make life easier by adding the following to the appropriate bash profile file: 1 2 3 alias docker-rmi = 'docker rmi $(docker images -q --filter \"dangling=true\")' alias docker-rm = 'docker rm $(docker ps -a -q)' alias docker-rmv = 'docker volume rm $(docker volume ls -qf dangling=true)' Remove all stopped containers first then remove all tagged images. A useful tool is Dockviz , ever since docker did away with docker images --tree you can't see all the layers of images and therefore how many disconneected images you have. Add the following to the appropriate bash profile file: 1 alias dockviz = \"docker run --privileged -it --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz\" Then in a new terminal you can run dockviz images -t to see the tree. The program also does dot notation files for a graphical view as well.","title":"Cleaning up docker"},{"location":"installing/administration/#multiple-compose-files","text":"When you supply multiple files, docker-compose combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their successors. 1 2 3 4 5 # Apply the .dev yml file, create and start the containers in the background $ docker-compose -f docker-compose.yml -f docker-compose.dev.yml -d <COMMAND> # Apply the .prod yml file, create and start the containers in the background $ docker-compose -f docker-compose.yml -f docker-compose.prod.yml -d <COMMAND> We recommend adding the following lines to the appropriate bash profile file: 1 2 alias docker-compose-dev = \"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\" alias docker-compose-prod = \"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\" This will allow you to start compose in dev mode without all the extra file definitions.","title":"Multiple compose files"},{"location":"installing/administration/#debugging-and-advanced-configuration","text":"Here we present some useful hints for extending or customising the Docker setup for different configurations or use cases: Development file override The docker-compose.dev.yml can be used to override the standard docker-compose.yml file for development. In its initial configuarion, it opens up the ports in the Postgres container for manual connection from the host machine. This .dev compose file rebuilds all of the images, whereas the standard compose file and .prod versions do not build new images. Make use of the wait_scripts While -links and depends_on make sure the services a service requires are brought up first Docker only waits till they are running NOT till they are actually ready. The wait scripts provided test responses on given ports to make sure that a given service is actually available and ready to interact. Use COPY over ADD Docker recommends using COPY instead of ADD unless the source is a URL or a tar file which ADD can retrieve and/or unzip. Use of ENTRYPOINT & CMD If not requiring any dependencies then just set CMD [\"arg1\", ...] and the args will be passed to the ENTRYPOINT If requiring dependencies then set the ENTRYPOINT to the wait script and the CMD to CMD [\"process\", \"arg1\", ...] Try to keep images as small as possible As a general rule, we try to use the base images (e.g. of Tomcat, Postgres) and install additional packages at runtime or start-up. This increases portability and cuts down on disk usage when deploying updates. Exposing ports Exposing ports to other services must be carefully considered, to avoid unnecessary security vulnerabilities If the port only needs to be available to other docker services then use expose . If the port needs to be open to the host machine or beyond, then use ports . If the ports option is used this opens the port from the service to the outside world, it does not affect exposed ports between services, so if a service (e.g. postgres with port 5432) exposes a port then any service which used link to postgres will be able to find the database at postgresql://postgres:5432","title":"Debugging and advanced configuration"},{"location":"installing/docker/","text":"System requirements \u00b6 The simplest installation method is to run our preconfigured application using Docker . Any operating system, on a server or desktop, running Docker can run Mauro Data Mapper , but please note that some organisations may restrict the use of Docker on virtual machines. Installing Docker and Docker Compose \u00b6 You will need to install Docker and Docker Compose . Docker Compose is included as part of the standard 'Docker Desktop' for Windows and MacOS. To run Mauro Data Mapper , the minimum versions are as follows: Docker Engine: 19.03.8 Docker Compose: 1.25.5 Warning If you're running on Ubuntu the default version of docker-compose installed with apt-get is currently 1.17.1, and you might get the error message: 1 2 Building docker compose ERROR: Need service name for --build-arg option In this case, you should uninstall docker-compose and re-install directly from Docker, following the instructions here . Docker Machine configuration \u00b6 The default docker-machine in a Windows or Mac OS X environment is configured to make use of one CPU and 1GB RAM. This is not enough RAM to reliably run the Mauro Data Mapper system and so should be increased. On Linux the docker machine is the host machine so there is no need to build or remove anything. Native Docker \u00b6 If using the Native Docker then edit the preferences of the Docker application and increase the RAM to at least 4GB. You will probably need to restart Docker after doing this. Docker toolbox \u00b6 If using the Docker Toolbox then you will need to perform the following in a 'docker' terminal: 1 2 3 4 5 6 7 8 # Stop the default docker machine $ docker-machine stop default # Remove the default machine $ docker-machine rm default # Replace with a more powerful machine (4096 is the minimum recommended RAM, if you can give it more then do so) $ docker-machine create --driver virtualbox --virtualbox-cpu-count \"-1\" --virtualbox-memory \"4096\" default Use the default Docker Machine \u00b6 When controlling using Docker Machine via your terminal shell it is useful to set the default docker machine. Type the following at the command line, or add it to the appropriate bash profile file: 1 eval \" $( docker-machine env default ) \" If not you may see the following error: Cannot connect to the Docker daemon. Is the docker daemon running on this host? Git repository \u00b6 Depending on the operating system of the server you are running on, you may first need to install git to checkout the Mauro application. You can read more about installing git on different operating systems here: Getting Started - Installing Git The Mauro Docker configuration repository can be found here: https://github.com/MauroDataMapper/mdm-docker . Where you clone it is up to you, but on a *nix system we recommend cloning into /opt/ (for optional software packages) Different branches provide different configurations. We recommend checking out the main branch which will provide the latest releases of back-end and front-end. Alternatively, you can check out a specific tag to install a specific front-end / back-end combination. Tagged releases of Docker take the form Ba.b.c_Fx.y.z where a.b.c is the tagged version of the back-end and x.y.z is the tagged version of the front-end. Information If you're running on an internal server with SSH access forbidden by a firewall, you can use the following link to access the repository via HTTPS: SSH over HTTPS document . Overview \u00b6 The Docker Compose configuration defines two interacting containers: Postgres 12 [ postgres ] - Postgres Database Mauro Data Mapper [ maurodatamapper ] - Mauro Data Mapper The first of these is a standard Postgres container with an external volume for persistent storage. The second builds on the standard Apache Tomcat container, which hosts built versions of the Mauro application. The Postgres container must be running whenever the Mauro application starts. The Mauro container persists logs and Lucene indexes to shared folders which can be found in the docker repository folder. Building \u00b6 At present, the Mauro application must be checked out, compiled and deployed within the Mauro / Tomcat container. Some shell scripts have been provided to run this process on a *nix system and are described below. We plan to streamline this process in the near future. Make / Update \u00b6 1 2 3 4 5 6 7 # Build the entire system ./make Usage ./make [ -b COMMIT_BRANCH ] [ -f COMMIT_BRANCH ] -b, --back-end COMMIT_BRANCH : The commit or branch to checkout and build for the back-end from mdm-core -f, --front-end COMMIT_BRANCH : The commit or branch to checkout and build for the front-end from mdm-ui The make command will build all the necessary base images and then perform a docker-compose build to complete the build. It is preconfigured within each branch of the Mauro Docker repository, so the -b and -f options only need specifying if you wish to configure a different build configuration. This script runs the following: Build an updated OS version of tomcat which is where the application will run mdm/tomcat:9.0.27-jdk12-adoptopenjdk-openj9 Build the base SDK image for building the application in mdm/sdk_base:grails-4.0.6-adoptopenjdk-12-jdk-openj9 Build an initial image with the code checked out and dependencies installed mdm/mdm_base:develop Once these 3 images are built the main docker-compose service will be able to build without the use of the make file. An update script can be used to update an existing, running instance to the latest version. See Updating to the latest version for more details. Users should ordinarily use the ./make script once only, and then afterwards only use ./update . However, the ./make script can be run again in order to update the base Tomcat build image - for example when security fixes have been released. There is an additional argument for the ./make script that will do exactly that: 1 ./make -u which performs a clean build of just the Tomcat image. Once the ./make script has been run once, the commit/branch choice can be altered by changing the build args in the docker-compose.yml file. 1 2 3 4 5 6 7 mauro-data-mapper : build : context : mauro-data-mapper args : MDM_APPLICATION_COMMIT : develop MDM_UI_COMMIT : develop ... Default username / password \u00b6 The docker installation is empty on initialisation - it comes with one pre-configured user: with the username admin@maurodatamapper.com and the password password . We strongly recommend changing this password on first login, and then setting up personal user accounts for individual users. Additional backend Plugins \u00b6 Additional plugins can be found at the Mauro Data Mapper Plugins organisation page. Each of these can be added as dependencies by adding them to the ADDITIONAL_PLUGINS build argument within the docker-compose.yml file. These dependencies should be provided in a semi-colon separated list in the gradle style. They will be split and each will be added as a runtimeOnly dependency. For example: 1 2 3 4 5 mauro-data-mapper : build : context : mauro-data-mapper args : ADDITIONAL_PLUGINS : \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-keycloak:1.0.1\" Will add the keycloak plugin to the dependencies.gradle file: 1 runtimeOnly uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-keycloak:1.0.1 Theme \u00b6 Mauro comes with a default user interface theme - with the standard blue branding, and default text on the home page. This can be overridden in the docker-compose.yml file, with instructions provided in the Branding guide . The default theme is called default and can be set with: 1 MDM_UI_THEME_NAME : \"default\" Running multiple instances \u00b6 If running multiple docker-compose instances then they will all make use of the same initial images, therefore you only need to run the ./make script once per server. SSH firewalled servers \u00b6 Some servers have the 22 SSH port firewalled for external connections. If this is the case you can change the base_images/sdk_base/ssh/config file: Comment out the Hostname field that's currently active Uncomment both commented out Hostname and Port fields, this will allow git to work using the 443 port which will not be blocked Run environment \u00b6 Please see mauro-data-mapper/Dockerfile for all defaults. Required to be overridden \u00b6 The following variables need to be overridden/set when starting up a new mauro-data-mapper image. Usually this is done in the docker-compose.yml file. It should not be done in the Dockerfile as each instance which starts up may use different values. MDM_FQ_HOSTNAME The fully-qualified domain name (FQDN) of the server where the catalogue will be accessed MDM_PORT The port used to access the catalogue MDM_AUTHORITY_URL The full URL to the location of the catalogue. This is considered a unique identifier to distinguish any instance from another and therefore no 2 instances should use the same URL MDM_AUTHORITY_NAME A unique name used to distinguish a running MDM instance and boostrap an initial Mauro Authority PGPASSWORD This should be the password for the postgres instance being connected. When using the docker-compose.yml file and the configured postgres instance this should be left alone EMAIL_USERNAME To allow the catalogue to send emails this needs to be a valid username for the EMAIL_HOST EMAIL_PASSWORD To allow the catalogue to send emails this needs to be a valid password for the EMAIL_HOST and EMAIL_USERNAME EMAIL_HOST This is the FQDN of the mail server to use when sending emails Optional \u00b6 CATALINA_OPTS Java options to be passed to the Tomcat application server DATABASE_HOST The host of the database. If using docker-compose this should be left as postgres or changed to the name of the database service DATABASE_PORT The port of the database DATABASE_NAME The name of the database which the catalogue data will be stored in DATABASE_USERNAME Username to connect to the database DATABASE_PASSWORD Password to connect to the database EMAIL_PORT The port to use when sending emails EMAIL_TRANSPORTSTRATEGY The transport strategy to use when sending emails SEARCH_INDEX_BASE The directory to store the lucene index files in EMAILSERVICE_URL The url to the special email service, this will result in the alternative email system being used EMAILSERVICE_USERNAME The username for the email service needs to be valid for EMAIL_SERVICE_URL EMAILSERVICE_PASSWORD The password for the email service needs to be valid for EMAIL_SERVICE_URL Environment Notes \u00b6 Database The system is designed to use the postgres service provided in the docker-compose file, therefore there should be no need to alter any of these settings. Only make alterations if running postgres as a separate service outside docker-compose. MDM_FQ_HOSTNAME & MDM_PORT The provided values will be used to define the CORS allowed origins. The port will be used to define http or https (443), if its not 80 or 443 then it will be added to the url generated. The host must be the host used in the web url when accessing the catalogue in a web browser. Email The standard email properties will allow emails to be sent to a specific SMTP server. The emailservice properties override this and send the email to the specified email service which will then forward it onto our email SMTP server. Docker Reference \u00b6 Running \u00b6 Before running please read the parameters section first. With docker and docker-compose installed, run the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Build all the images $ docker-compose-dev build # Start all the components up $ docker-compose up -d # To only start 1 service # This will also start up any of the services the named service depends on (defined by `links` or `depends_on`) $ docker-compose up [ SERVICE ] # To push all the output to the background add `-d` $ docker-compose up -d [ SERVICE ] # Stop background running and remove the containers $ docker-compose down # To update an already running service $ docker-compose-dev build [ SERVICE ] $ docker-compose up -d --no-deps [ SERVICE ] # To run in production mode $ docker-compose-prod up -d [ SERVICE ] If you run everything in the background use Kitematic to see the individual container logs. You can do this if running in the foreground and its easier as it splits each of the containers up. If only starting a service when you stop, the service docker will not stop the dependencies that were started to allow the named service to start. The default compose file will pull the correct version images from Bintray, or a locally defined docker repository. For more information about administration of your running Docker instance, please see the Administration guide","title":"Docker Install"},{"location":"installing/docker/#system-requirements","text":"The simplest installation method is to run our preconfigured application using Docker . Any operating system, on a server or desktop, running Docker can run Mauro Data Mapper , but please note that some organisations may restrict the use of Docker on virtual machines.","title":"System requirements"},{"location":"installing/docker/#installing-docker-and-docker-compose","text":"You will need to install Docker and Docker Compose . Docker Compose is included as part of the standard 'Docker Desktop' for Windows and MacOS. To run Mauro Data Mapper , the minimum versions are as follows: Docker Engine: 19.03.8 Docker Compose: 1.25.5 Warning If you're running on Ubuntu the default version of docker-compose installed with apt-get is currently 1.17.1, and you might get the error message: 1 2 Building docker compose ERROR: Need service name for --build-arg option In this case, you should uninstall docker-compose and re-install directly from Docker, following the instructions here .","title":"Installing Docker and Docker Compose"},{"location":"installing/docker/#docker-machine-configuration","text":"The default docker-machine in a Windows or Mac OS X environment is configured to make use of one CPU and 1GB RAM. This is not enough RAM to reliably run the Mauro Data Mapper system and so should be increased. On Linux the docker machine is the host machine so there is no need to build or remove anything.","title":"Docker Machine configuration"},{"location":"installing/docker/#native-docker","text":"If using the Native Docker then edit the preferences of the Docker application and increase the RAM to at least 4GB. You will probably need to restart Docker after doing this.","title":"Native Docker"},{"location":"installing/docker/#docker-toolbox","text":"If using the Docker Toolbox then you will need to perform the following in a 'docker' terminal: 1 2 3 4 5 6 7 8 # Stop the default docker machine $ docker-machine stop default # Remove the default machine $ docker-machine rm default # Replace with a more powerful machine (4096 is the minimum recommended RAM, if you can give it more then do so) $ docker-machine create --driver virtualbox --virtualbox-cpu-count \"-1\" --virtualbox-memory \"4096\" default","title":"Docker toolbox"},{"location":"installing/docker/#use-the-default-docker-machine","text":"When controlling using Docker Machine via your terminal shell it is useful to set the default docker machine. Type the following at the command line, or add it to the appropriate bash profile file: 1 eval \" $( docker-machine env default ) \" If not you may see the following error: Cannot connect to the Docker daemon. Is the docker daemon running on this host?","title":"Use the default Docker Machine"},{"location":"installing/docker/#git-repository","text":"Depending on the operating system of the server you are running on, you may first need to install git to checkout the Mauro application. You can read more about installing git on different operating systems here: Getting Started - Installing Git The Mauro Docker configuration repository can be found here: https://github.com/MauroDataMapper/mdm-docker . Where you clone it is up to you, but on a *nix system we recommend cloning into /opt/ (for optional software packages) Different branches provide different configurations. We recommend checking out the main branch which will provide the latest releases of back-end and front-end. Alternatively, you can check out a specific tag to install a specific front-end / back-end combination. Tagged releases of Docker take the form Ba.b.c_Fx.y.z where a.b.c is the tagged version of the back-end and x.y.z is the tagged version of the front-end. Information If you're running on an internal server with SSH access forbidden by a firewall, you can use the following link to access the repository via HTTPS: SSH over HTTPS document .","title":"Git repository"},{"location":"installing/docker/#overview","text":"The Docker Compose configuration defines two interacting containers: Postgres 12 [ postgres ] - Postgres Database Mauro Data Mapper [ maurodatamapper ] - Mauro Data Mapper The first of these is a standard Postgres container with an external volume for persistent storage. The second builds on the standard Apache Tomcat container, which hosts built versions of the Mauro application. The Postgres container must be running whenever the Mauro application starts. The Mauro container persists logs and Lucene indexes to shared folders which can be found in the docker repository folder.","title":"Overview"},{"location":"installing/docker/#building","text":"At present, the Mauro application must be checked out, compiled and deployed within the Mauro / Tomcat container. Some shell scripts have been provided to run this process on a *nix system and are described below. We plan to streamline this process in the near future.","title":"Building"},{"location":"installing/docker/#make-update","text":"1 2 3 4 5 6 7 # Build the entire system ./make Usage ./make [ -b COMMIT_BRANCH ] [ -f COMMIT_BRANCH ] -b, --back-end COMMIT_BRANCH : The commit or branch to checkout and build for the back-end from mdm-core -f, --front-end COMMIT_BRANCH : The commit or branch to checkout and build for the front-end from mdm-ui The make command will build all the necessary base images and then perform a docker-compose build to complete the build. It is preconfigured within each branch of the Mauro Docker repository, so the -b and -f options only need specifying if you wish to configure a different build configuration. This script runs the following: Build an updated OS version of tomcat which is where the application will run mdm/tomcat:9.0.27-jdk12-adoptopenjdk-openj9 Build the base SDK image for building the application in mdm/sdk_base:grails-4.0.6-adoptopenjdk-12-jdk-openj9 Build an initial image with the code checked out and dependencies installed mdm/mdm_base:develop Once these 3 images are built the main docker-compose service will be able to build without the use of the make file. An update script can be used to update an existing, running instance to the latest version. See Updating to the latest version for more details. Users should ordinarily use the ./make script once only, and then afterwards only use ./update . However, the ./make script can be run again in order to update the base Tomcat build image - for example when security fixes have been released. There is an additional argument for the ./make script that will do exactly that: 1 ./make -u which performs a clean build of just the Tomcat image. Once the ./make script has been run once, the commit/branch choice can be altered by changing the build args in the docker-compose.yml file. 1 2 3 4 5 6 7 mauro-data-mapper : build : context : mauro-data-mapper args : MDM_APPLICATION_COMMIT : develop MDM_UI_COMMIT : develop ...","title":"Make / Update"},{"location":"installing/docker/#default-username-password","text":"The docker installation is empty on initialisation - it comes with one pre-configured user: with the username admin@maurodatamapper.com and the password password . We strongly recommend changing this password on first login, and then setting up personal user accounts for individual users.","title":"Default username / password"},{"location":"installing/docker/#additional-backend-plugins","text":"Additional plugins can be found at the Mauro Data Mapper Plugins organisation page. Each of these can be added as dependencies by adding them to the ADDITIONAL_PLUGINS build argument within the docker-compose.yml file. These dependencies should be provided in a semi-colon separated list in the gradle style. They will be split and each will be added as a runtimeOnly dependency. For example: 1 2 3 4 5 mauro-data-mapper : build : context : mauro-data-mapper args : ADDITIONAL_PLUGINS : \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-keycloak:1.0.1\" Will add the keycloak plugin to the dependencies.gradle file: 1 runtimeOnly uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-keycloak:1.0.1","title":"Additional backend Plugins"},{"location":"installing/docker/#theme","text":"Mauro comes with a default user interface theme - with the standard blue branding, and default text on the home page. This can be overridden in the docker-compose.yml file, with instructions provided in the Branding guide . The default theme is called default and can be set with: 1 MDM_UI_THEME_NAME : \"default\"","title":"Theme"},{"location":"installing/docker/#running-multiple-instances","text":"If running multiple docker-compose instances then they will all make use of the same initial images, therefore you only need to run the ./make script once per server.","title":"Running multiple instances"},{"location":"installing/docker/#ssh-firewalled-servers","text":"Some servers have the 22 SSH port firewalled for external connections. If this is the case you can change the base_images/sdk_base/ssh/config file: Comment out the Hostname field that's currently active Uncomment both commented out Hostname and Port fields, this will allow git to work using the 443 port which will not be blocked","title":"SSH firewalled servers"},{"location":"installing/docker/#run-environment","text":"Please see mauro-data-mapper/Dockerfile for all defaults.","title":"Run environment"},{"location":"installing/docker/#required-to-be-overridden","text":"The following variables need to be overridden/set when starting up a new mauro-data-mapper image. Usually this is done in the docker-compose.yml file. It should not be done in the Dockerfile as each instance which starts up may use different values. MDM_FQ_HOSTNAME The fully-qualified domain name (FQDN) of the server where the catalogue will be accessed MDM_PORT The port used to access the catalogue MDM_AUTHORITY_URL The full URL to the location of the catalogue. This is considered a unique identifier to distinguish any instance from another and therefore no 2 instances should use the same URL MDM_AUTHORITY_NAME A unique name used to distinguish a running MDM instance and boostrap an initial Mauro Authority PGPASSWORD This should be the password for the postgres instance being connected. When using the docker-compose.yml file and the configured postgres instance this should be left alone EMAIL_USERNAME To allow the catalogue to send emails this needs to be a valid username for the EMAIL_HOST EMAIL_PASSWORD To allow the catalogue to send emails this needs to be a valid password for the EMAIL_HOST and EMAIL_USERNAME EMAIL_HOST This is the FQDN of the mail server to use when sending emails","title":"Required to be overridden"},{"location":"installing/docker/#optional","text":"CATALINA_OPTS Java options to be passed to the Tomcat application server DATABASE_HOST The host of the database. If using docker-compose this should be left as postgres or changed to the name of the database service DATABASE_PORT The port of the database DATABASE_NAME The name of the database which the catalogue data will be stored in DATABASE_USERNAME Username to connect to the database DATABASE_PASSWORD Password to connect to the database EMAIL_PORT The port to use when sending emails EMAIL_TRANSPORTSTRATEGY The transport strategy to use when sending emails SEARCH_INDEX_BASE The directory to store the lucene index files in EMAILSERVICE_URL The url to the special email service, this will result in the alternative email system being used EMAILSERVICE_USERNAME The username for the email service needs to be valid for EMAIL_SERVICE_URL EMAILSERVICE_PASSWORD The password for the email service needs to be valid for EMAIL_SERVICE_URL","title":"Optional"},{"location":"installing/docker/#environment-notes","text":"Database The system is designed to use the postgres service provided in the docker-compose file, therefore there should be no need to alter any of these settings. Only make alterations if running postgres as a separate service outside docker-compose. MDM_FQ_HOSTNAME & MDM_PORT The provided values will be used to define the CORS allowed origins. The port will be used to define http or https (443), if its not 80 or 443 then it will be added to the url generated. The host must be the host used in the web url when accessing the catalogue in a web browser. Email The standard email properties will allow emails to be sent to a specific SMTP server. The emailservice properties override this and send the email to the specified email service which will then forward it onto our email SMTP server.","title":"Environment Notes"},{"location":"installing/docker/#docker-reference","text":"","title":"Docker Reference"},{"location":"installing/docker/#running","text":"Before running please read the parameters section first. With docker and docker-compose installed, run the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Build all the images $ docker-compose-dev build # Start all the components up $ docker-compose up -d # To only start 1 service # This will also start up any of the services the named service depends on (defined by `links` or `depends_on`) $ docker-compose up [ SERVICE ] # To push all the output to the background add `-d` $ docker-compose up -d [ SERVICE ] # Stop background running and remove the containers $ docker-compose down # To update an already running service $ docker-compose-dev build [ SERVICE ] $ docker-compose up -d --no-deps [ SERVICE ] # To run in production mode $ docker-compose-prod up -d [ SERVICE ] If you run everything in the background use Kitematic to see the individual container logs. You can do this if running in the foreground and its easier as it splits each of the containers up. If only starting a service when you stop, the service docker will not stop the dependencies that were started to allow the named service to start. The default compose file will pull the correct version images from Bintray, or a locally defined docker repository. For more information about administration of your running Docker instance, please see the Administration guide","title":"Running"},{"location":"installing/migration/","text":"A tool has been built to migrate data from old instances of Metadata Catalogue to Mauro Data Mapper. This is a docker-based tool which applies a number of SQL scripts to move data into a new database schema and transform the data into a new format suitable for Mauro. The GitHub repository for the migration is here: https://github.com/MauroDataMapper/mc-to-mdm-migration . Manual Process \u00b6 Please see the documents in the guide folder for further information. Automated Process \u00b6 You can use one of the two available scripts in the top of the repository to run the migration from start to finish. You have two options as with the manual process documents, depending on if you're running Metadata Catalogue and Mauro Data Mapper inside or outside Docker. The scripts execute the stages described in the guide, so if you're unsure as to what parameters you should feed in then please read the guides to find out more. The scripts all execute using the defaults as if you have built the system using Docker. Docker Based PostgreSQL \u00b6 1 2 3 4 # Help/Usage ./run-complete-migration-docker.sh --help # Default parameters run ./run-complete-migration-docker.sh Remote/Local PostgreSQL \u00b6 1 2 3 4 # Help/Usage ./run-complete-migration-remote.sh --help # Default parameters run ./run-complete-migration-remote.sh Notes \u00b6 Please be aware that DataFlows are currently not migrated using this system, and may need to be manually migrated, or exported / imported.","title":"Migrating Old Data"},{"location":"installing/migration/#manual-process","text":"Please see the documents in the guide folder for further information.","title":"Manual Process"},{"location":"installing/migration/#automated-process","text":"You can use one of the two available scripts in the top of the repository to run the migration from start to finish. You have two options as with the manual process documents, depending on if you're running Metadata Catalogue and Mauro Data Mapper inside or outside Docker. The scripts execute the stages described in the guide, so if you're unsure as to what parameters you should feed in then please read the guides to find out more. The scripts all execute using the defaults as if you have built the system using Docker.","title":"Automated Process"},{"location":"installing/migration/#docker-based-postgresql","text":"1 2 3 4 # Help/Usage ./run-complete-migration-docker.sh --help # Default parameters run ./run-complete-migration-docker.sh","title":"Docker Based PostgreSQL"},{"location":"installing/migration/#remotelocal-postgresql","text":"1 2 3 4 # Help/Usage ./run-complete-migration-remote.sh --help # Default parameters run ./run-complete-migration-remote.sh","title":"Remote/Local PostgreSQL"},{"location":"installing/migration/#notes","text":"Please be aware that DataFlows are currently not migrated using this system, and may need to be manually migrated, or exported / imported.","title":"Notes"},{"location":"installing/plugins/","text":"We host a number of community plugins, the code for which is available in our GitHub 'Plugins' organisation . Commits to master branches kick off a build process on our continuous integration server and successfully-built artefacts are hosted on our instance of Artifactory . Below is a list of all the available plugins, along with their latest version number, release date and any dependencies each has. More details about the changes in each release can be found on the Release Notes page. To install a plugin, use the 'artefact name' as directed in the Installing page. Importers / Exporters \u00b6 Plugin name Version Release date Artefact name Dependencies art-decor 1.0.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-artdecor:1.0.0 Core >= 4.2.0 AWS Glue 1.3.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-awsglue:1.3.0 Core >= 4.2.0 CSV 3.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-csv:3.0.0 Core >= 4.5.0 Excel 3.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:3.0.0 Core >= 4.5.0 FHIR 1.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-fhir:1.0.0 Core >= 4.5.0 MS SQL 3.2.0 2021-05-14 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-sqlserver:3.2.0 Core >= 4.2.0 MySQL 2.2.0 2021-05-14 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-mysql:2.2.0 Core >= 4.2.0 Oracle SQL 3.2.0 2021-05-14 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-oracle:3.2.0 Core >= 4.2.0 PostgreSQL 3.2.0 2021-05-14 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-postgresql:3.2.0 Core >= 4.2.0 Security \u00b6 Plugin name Version Release date Artefact name Dependencies Keycloak Integrated Authentication 2.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-keycloak:2.0.0 Core >= 4.5.0 Technical / Other \u00b6 Plugin name Version Release date Artefact name Dependencies Freemarker Templating 1.1.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-freemarker:1.1.0 Core >= 4.2.0 SPARQL 1.1.1 2021-03-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-sparql:1.1.1 Core >= 4.2.0","title":"Plugins"},{"location":"installing/plugins/#importers-exporters","text":"Plugin name Version Release date Artefact name Dependencies art-decor 1.0.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-artdecor:1.0.0 Core >= 4.2.0 AWS Glue 1.3.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-awsglue:1.3.0 Core >= 4.2.0 CSV 3.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-csv:3.0.0 Core >= 4.5.0 Excel 3.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:3.0.0 Core >= 4.5.0 FHIR 1.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-fhir:1.0.0 Core >= 4.5.0 MS SQL 3.2.0 2021-05-14 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-sqlserver:3.2.0 Core >= 4.2.0 MySQL 2.2.0 2021-05-14 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-mysql:2.2.0 Core >= 4.2.0 Oracle SQL 3.2.0 2021-05-14 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-oracle:3.2.0 Core >= 4.2.0 PostgreSQL 3.2.0 2021-05-14 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-postgresql:3.2.0 Core >= 4.2.0","title":"Importers / Exporters"},{"location":"installing/plugins/#security","text":"Plugin name Version Release date Artefact name Dependencies Keycloak Integrated Authentication 2.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-keycloak:2.0.0 Core >= 4.5.0","title":"Security"},{"location":"installing/plugins/#technical-other","text":"Plugin name Version Release date Artefact name Dependencies Freemarker Templating 1.1.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-freemarker:1.1.0 Core >= 4.2.0 SPARQL 1.1.1 2021-03-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-sparql:1.1.1 Core >= 4.2.0","title":"Technical / Other"},{"location":"installing/branding/branding/","text":"Introduction \u00b6 The Mauro Data Mapper Web interface can be customised to match an organisation's brand or styling. Creating a new theme requires developer effort, the steps of which are detailed below. Angular Material \u00b6 The Angular Material framework, which the UI uses for layout and controls, can support multiple themes (colour palettes and typography) as well as dynamic switching of themes. However, the themes must be precompiled into the application as part of the stylesheets for the site. This means that themes must be prepared by a developer. See also: Angular Material theming guide Theming your components Themes \u00b6 Overview \u00b6 Themes in Mauro Data Mapper are organised in the project by name, and uses folder and naming conventions to identify where files and CSS selectors are. The conventions used are: SASS files : src/style/themes/{name}.scss CSS theme class : .{name}-theme Asset files : src/assets/themes/{name}/*.* So, as an example, a custom theme called nhs-digital would be: SASS files : src/style/themes/nhs-digital.scss CSS theme class : .nhs-digital-theme Asset files : src/assets/themes/nhs-digital/*.* Switching Themes \u00b6 To determine the theme to use for the site, update the src/environments/environment.ts : 1 2 3 4 5 export const environment = { // ... themeName : 'nhs-digital' // ... }; Note If the theme name is not provided, this will fall back to default . An alternative for switching the theme for production UI builds is to set an environment variable called MDM_UI_THEME_NAME . This allows the theme setting to be defined as part of a wider build process, such as creating a Docker image. Importing Themes \u00b6 Each theme requires a SASS file to define the colour palette and typography for the Angular Material theming system to use. This SASS file can also override any other CSS selectors that do not affect the Material controls. The src/style/styles.scss file is the place to import all theme files. The key parts are: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @import \"~@angular/material/theming\" ; // Include the common styles for Angular Material. Only required to include this once! @include mat-core (); /* Themes Import more theme SASS files here to make them available to the app */ @import \"style/themes/default.scss\" ; @import \"style/themes/nhs-digital.scss\" ; // ...etc // More SASS files imported here... Note Remember that Angular Material requires all themes to be precompiled, so all SASS files must be imported to include them as options. Only one theme will appear at a time though. Creating Themes \u00b6 To create a new theme it is advised to copy src/style/themes/default.scss and make the necessary adjustments. Then @import the new theme file in src/style/styles.scss . The specifics of the Angular Material theme system should be referenced for better explanation. However, in summary, you will need to define: The colour palettes required to represent: Primary colours Accented colours Warning colours The typography settings to use for text/font The Material mixins should then be included within a top-level CSS selector named after the theme. This CSS selector will be applied to the body of the HTML page and therefore affect all DOM elements below it. The theme file should effectively include the following steps: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // Required for Angular Material @import \"~@angular/material/theming\" ; // Required for custom theming of MDM components @import \"../components/custom\" ; /* Define colour platte maps here ... */ $nhs-digital-theme-primary : mat-palette ( ... ); $nhs-digital-theme-accent : mat-palette ( ... ); $nhs-digital-theme-warn : mat-palette ( ... ); /* Create the colour theme */ $nhs-digital-theme : mat-light-theme ( $nhs-digital-theme-primary , $nhs-digital-theme-accent , $nhs-digital-theme-warn ); /* Define the typography settings to use... */ $nhs-digital-typography : mat-typography-config ( ... ); /* Define the top-level CSS class name. Name this in the format \".{name}-theme\" so the application can automatically use it for the page */ .nhs-digital-theme { // Use the created theme variables to configure Angular Material @include angular-material-theme ( $nhs-digital-theme ); @include angular-material-typography ( $nhs-digital-typography ); // Some MDM components have also been added to the Angular Material theme system. Include this mixin to have these match the same theme @include mdm-custom-components-theme ( $nhs-digital-theme ); /* Add any further CSS class overrides here... */ } Assets \u00b6 Themeable assets, such as logos and images, should be stored under src/assets/themes under the specific sub-folder named after the theme. The file names used should be consistent across all themes e.g. logo.png . To consistently reference the correct asset path to use, the ThemingService can be injected into your component/service and use the getAssetPath() function to get the correct path for an asset according to the current theme. Edit page content \u00b6 Some of the static content may be adjusted by an administrator to allow the end users to make content edits instead of relying on developers. Follow the steps below to edit page content. Sign in and click the white arrow by your user profile on the right of the menu header. Select 'Configuration' from the dropdown menu. Click the 'Properties' tab to view a list of properties, with the cog icon indicating a system property. To edit or delete a property, click the three vetical dots to the right of the relevant row and choose either 'Edit' or 'Delete' from the dropdown menu. To add a new property click the '+Add' button at the top right of the page which will take you to an 'Add Property' form. Firstly, select a property you want to add from the dropdown menu. Then enter a 'Key' , 'Category' and 'Value' . Tick the 'Publicly visible' box if it applies. Once the form is completed, click 'Add property' and a green notifciaiton box should appear at the bottom right of your screen, confirming the changes. Note It may be necessary to manually adjust the HTML source to set correct styling/markup for the content. This can be done by clicking on the `` 'Change Mode' toolbar button in each form edit field. There are configuration properties available to modify the following: Homepage Logo Footer Home Page \u00b6 The homepage of Mauro Data Mapper displays a summary of the tool as well as the most important links to help users easily navigate to the main sections. The homepage can be modified to include custom content to suit your organisation. The homepage is split into two columns and four main sections which can each be modified. Note It is recommended to include correct HTML source and styling in these sections. Introduction - content.home.intro.left This is usually some introduction text on the left column of the starting page Introduction image - content.home.intro.right This is usually an image or illustration on the right column of the starting page that compliments the introduction text Features heading - content.home.detail.heading This is usually a heading for the 'Features' section underneath the introduction Feature boxes - content.home.detail.column1 - 3 These are three feature boxes that usually appear in the 'Features' section underneath the introduction Logo \u00b6 A static asset should also be provided for the logo in the navbar component as a default, however an image URL may also be provided if the logo is hosted in another location e.g. CDN. To set a logo, modify the theme.logo.url property. The theme.logo.width property is also provided to adjust the size of the logo in the space provided. Note that: The value entered for theme.logo.width must be supported by CSS e.g. 20px , 1.2em , etc The maximum width of the logo is 120px . If the image is larger than this then it will be scaled down to fit Footer \u00b6 The copyright notice can be altered in the footer by changing the content.footer.copyright property. Defaults \u00b6 If no property values are provided for the above, then suitable defaults are used instead based on the current theme. To revert back to default values for any property, simply delete the property from the configuration table.","title":"Branding"},{"location":"installing/branding/branding/#introduction","text":"The Mauro Data Mapper Web interface can be customised to match an organisation's brand or styling. Creating a new theme requires developer effort, the steps of which are detailed below.","title":"Introduction"},{"location":"installing/branding/branding/#angular-material","text":"The Angular Material framework, which the UI uses for layout and controls, can support multiple themes (colour palettes and typography) as well as dynamic switching of themes. However, the themes must be precompiled into the application as part of the stylesheets for the site. This means that themes must be prepared by a developer. See also: Angular Material theming guide Theming your components","title":"Angular Material"},{"location":"installing/branding/branding/#themes","text":"","title":"Themes"},{"location":"installing/branding/branding/#overview","text":"Themes in Mauro Data Mapper are organised in the project by name, and uses folder and naming conventions to identify where files and CSS selectors are. The conventions used are: SASS files : src/style/themes/{name}.scss CSS theme class : .{name}-theme Asset files : src/assets/themes/{name}/*.* So, as an example, a custom theme called nhs-digital would be: SASS files : src/style/themes/nhs-digital.scss CSS theme class : .nhs-digital-theme Asset files : src/assets/themes/nhs-digital/*.*","title":"Overview"},{"location":"installing/branding/branding/#switching-themes","text":"To determine the theme to use for the site, update the src/environments/environment.ts : 1 2 3 4 5 export const environment = { // ... themeName : 'nhs-digital' // ... }; Note If the theme name is not provided, this will fall back to default . An alternative for switching the theme for production UI builds is to set an environment variable called MDM_UI_THEME_NAME . This allows the theme setting to be defined as part of a wider build process, such as creating a Docker image.","title":"Switching Themes"},{"location":"installing/branding/branding/#importing-themes","text":"Each theme requires a SASS file to define the colour palette and typography for the Angular Material theming system to use. This SASS file can also override any other CSS selectors that do not affect the Material controls. The src/style/styles.scss file is the place to import all theme files. The key parts are: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @import \"~@angular/material/theming\" ; // Include the common styles for Angular Material. Only required to include this once! @include mat-core (); /* Themes Import more theme SASS files here to make them available to the app */ @import \"style/themes/default.scss\" ; @import \"style/themes/nhs-digital.scss\" ; // ...etc // More SASS files imported here... Note Remember that Angular Material requires all themes to be precompiled, so all SASS files must be imported to include them as options. Only one theme will appear at a time though.","title":"Importing Themes"},{"location":"installing/branding/branding/#creating-themes","text":"To create a new theme it is advised to copy src/style/themes/default.scss and make the necessary adjustments. Then @import the new theme file in src/style/styles.scss . The specifics of the Angular Material theme system should be referenced for better explanation. However, in summary, you will need to define: The colour palettes required to represent: Primary colours Accented colours Warning colours The typography settings to use for text/font The Material mixins should then be included within a top-level CSS selector named after the theme. This CSS selector will be applied to the body of the HTML page and therefore affect all DOM elements below it. The theme file should effectively include the following steps: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // Required for Angular Material @import \"~@angular/material/theming\" ; // Required for custom theming of MDM components @import \"../components/custom\" ; /* Define colour platte maps here ... */ $nhs-digital-theme-primary : mat-palette ( ... ); $nhs-digital-theme-accent : mat-palette ( ... ); $nhs-digital-theme-warn : mat-palette ( ... ); /* Create the colour theme */ $nhs-digital-theme : mat-light-theme ( $nhs-digital-theme-primary , $nhs-digital-theme-accent , $nhs-digital-theme-warn ); /* Define the typography settings to use... */ $nhs-digital-typography : mat-typography-config ( ... ); /* Define the top-level CSS class name. Name this in the format \".{name}-theme\" so the application can automatically use it for the page */ .nhs-digital-theme { // Use the created theme variables to configure Angular Material @include angular-material-theme ( $nhs-digital-theme ); @include angular-material-typography ( $nhs-digital-typography ); // Some MDM components have also been added to the Angular Material theme system. Include this mixin to have these match the same theme @include mdm-custom-components-theme ( $nhs-digital-theme ); /* Add any further CSS class overrides here... */ }","title":"Creating Themes"},{"location":"installing/branding/branding/#assets","text":"Themeable assets, such as logos and images, should be stored under src/assets/themes under the specific sub-folder named after the theme. The file names used should be consistent across all themes e.g. logo.png . To consistently reference the correct asset path to use, the ThemingService can be injected into your component/service and use the getAssetPath() function to get the correct path for an asset according to the current theme.","title":"Assets"},{"location":"installing/branding/branding/#edit-page-content","text":"Some of the static content may be adjusted by an administrator to allow the end users to make content edits instead of relying on developers. Follow the steps below to edit page content. Sign in and click the white arrow by your user profile on the right of the menu header. Select 'Configuration' from the dropdown menu. Click the 'Properties' tab to view a list of properties, with the cog icon indicating a system property. To edit or delete a property, click the three vetical dots to the right of the relevant row and choose either 'Edit' or 'Delete' from the dropdown menu. To add a new property click the '+Add' button at the top right of the page which will take you to an 'Add Property' form. Firstly, select a property you want to add from the dropdown menu. Then enter a 'Key' , 'Category' and 'Value' . Tick the 'Publicly visible' box if it applies. Once the form is completed, click 'Add property' and a green notifciaiton box should appear at the bottom right of your screen, confirming the changes. Note It may be necessary to manually adjust the HTML source to set correct styling/markup for the content. This can be done by clicking on the `` 'Change Mode' toolbar button in each form edit field. There are configuration properties available to modify the following: Homepage Logo Footer","title":"Edit page content"},{"location":"installing/branding/branding/#home-page","text":"The homepage of Mauro Data Mapper displays a summary of the tool as well as the most important links to help users easily navigate to the main sections. The homepage can be modified to include custom content to suit your organisation. The homepage is split into two columns and four main sections which can each be modified. Note It is recommended to include correct HTML source and styling in these sections. Introduction - content.home.intro.left This is usually some introduction text on the left column of the starting page Introduction image - content.home.intro.right This is usually an image or illustration on the right column of the starting page that compliments the introduction text Features heading - content.home.detail.heading This is usually a heading for the 'Features' section underneath the introduction Feature boxes - content.home.detail.column1 - 3 These are three feature boxes that usually appear in the 'Features' section underneath the introduction","title":"Home Page"},{"location":"installing/branding/branding/#logo","text":"A static asset should also be provided for the logo in the navbar component as a default, however an image URL may also be provided if the logo is hosted in another location e.g. CDN. To set a logo, modify the theme.logo.url property. The theme.logo.width property is also provided to adjust the size of the logo in the space provided. Note that: The value entered for theme.logo.width must be supported by CSS e.g. 20px , 1.2em , etc The maximum width of the logo is 120px . If the image is larger than this then it will be scaled down to fit","title":"Logo"},{"location":"installing/branding/branding/#footer","text":"The copyright notice can be altered in the footer by changing the content.footer.copyright property.","title":"Footer"},{"location":"installing/branding/branding/#defaults","text":"If no property values are provided for the above, then suitable defaults are used instead based on the current theme. To revert back to default values for any property, simply delete the property from the configuration table.","title":"Defaults"},{"location":"model/glossary/","text":"The Metadata Catalogue supports a variety of concepts which are not new, but are frequently referred to in different tools or applications by other names, or with different semantics. In this article we give definitions to the terms we use, and describe their intended usage. This page may also serve as a lightweight guide to the underlying data model - the UML diagrams provide guidance to those wishing to understand the underlying data structures and relationships. Data Models \u00b6 A Data Model is a description of an existing collection of data, or the specification of data that is to be collected. We use the same notation for both types, so that they may easily be compared or linked, and we annotate the Data Model to describe them as either a Data Asset or a Data Standard respectively. A Data Model has a label, which is used to uniquely identify it within the catalogue. It may also have a description, a number of alternate names (aliases), an author, and an organisation. A Data Model contains a number of Data Classes : groupings or collections of data points that share some common context: for example appearing in the same table of a database, or the same section in a form. A data class has a name, a description, some aliases, and may contain further (sub-) data classes. It has a maximum and minimum multiplicity, determining how many times data in this class may appear. For example, optional data may have a minimum multiplicity of 0 and a maximum multiplicity of 1; mandatory data may have a minimum multiplicity of 1; data which may occur any number of times is given a multiplicity of '*' (represented as -1 internally). The name of a data class is unique within its context (parent Data Model, or Data Class). A data class contains a number of Data Elements : the description of an individual field, variable, column, or property. A data element has a label, which must be unique within its containing Data Class. It has a description, some alternate names, and a multiplicity. The range of possible values that it may take are described within its data type: a reference to a Data Type stored within this model. A Data Type may be one of three sorts: a Primitive Type such as a String, a Date or an Integer. an Enumerated Type : a constrained set of values such as you might see for a gender or an ethnicity. Each Enumeration Type defines a number of Enumeration Values : a coded key, and a human-readable value. a Reference Type referring to another class within the same model - used to describe relationships between Data Classes within a model. Terminologies \u00b6 A Terminology can represent a complex ontology, a structured collection of enumerated values, or something inbetween. A terminology is stored in the catalogue in a similar manner to a Data Model: it has a unique label, a description, an author, an organisation, and some alternate names. A Terminology contains a number of Terms which themselves have a code, a human-readable definition, a URL which can be used to point to a definitive definition, and a description, which may be more verbose than the stated definition. Pairs of terms inside a terminology may be related: within the Terminology we store a number of Term Relationships - each of which has a source and target term. The Term Relationship is annotated with the Relationship Type - one of a set of types which are defined separately for each terminology. Codesets \u00b6 A CodeSet is a named collection of terms which may be pulled from multiple terminologies. It has similar fields to a Data Model: A label (which must be unique within the system), a description, an author, organisation, and alternate names. Folders \u00b6 A Folder is the principle mechanism for classifying and organising structures within the Metadata Catalogue. Folders may contain sub-folders, and each folder may contain Data Models, Terminologies or Code Sets. A folder has a label, which must be unique within its container (parent folder, or within the top-level of the folder hierarchy). Data Flows \u00b6 Currently undocumented Additional features \u00b6 All items in the Metadata Catalogue conform to a simple notion of Catalogue Item which has additional properties. These are outlined in the following subsections. Metadata / Properties \u00b6 Each Catalogue Item holds an extensible list of Properties (historically called Metadata), which can store arbitrary additional information about the Catalogue Item. Each property is identified by a namespace, and a key, and stored a value. Attachments \u00b6 Each Catalogue Item can also hold Attachments : files relating to the item in question - potentially providing additional documentation or context. Each attachment has a title, a description, and the file itself. Comments \u00b6 Any Catalogue Item may have Comments attached. Each comment has a title and some text, and these comments may themselves have further comments (replies in the sense of a forum or messageboard). Classifiers \u00b6 Classifiers provide a means for further classifying or tagging items within the catalogue. Classifiers contain a label and a description, and may hold further sub-classifiers. These classifiers may be managed separately by users, having their own read/write permissions. Semantic Links \u00b6 Currently undocumented Summary Metadata \u00b6 Currently undocumented","title":"Glossary"},{"location":"model/glossary/#data-models","text":"A Data Model is a description of an existing collection of data, or the specification of data that is to be collected. We use the same notation for both types, so that they may easily be compared or linked, and we annotate the Data Model to describe them as either a Data Asset or a Data Standard respectively. A Data Model has a label, which is used to uniquely identify it within the catalogue. It may also have a description, a number of alternate names (aliases), an author, and an organisation. A Data Model contains a number of Data Classes : groupings or collections of data points that share some common context: for example appearing in the same table of a database, or the same section in a form. A data class has a name, a description, some aliases, and may contain further (sub-) data classes. It has a maximum and minimum multiplicity, determining how many times data in this class may appear. For example, optional data may have a minimum multiplicity of 0 and a maximum multiplicity of 1; mandatory data may have a minimum multiplicity of 1; data which may occur any number of times is given a multiplicity of '*' (represented as -1 internally). The name of a data class is unique within its context (parent Data Model, or Data Class). A data class contains a number of Data Elements : the description of an individual field, variable, column, or property. A data element has a label, which must be unique within its containing Data Class. It has a description, some alternate names, and a multiplicity. The range of possible values that it may take are described within its data type: a reference to a Data Type stored within this model. A Data Type may be one of three sorts: a Primitive Type such as a String, a Date or an Integer. an Enumerated Type : a constrained set of values such as you might see for a gender or an ethnicity. Each Enumeration Type defines a number of Enumeration Values : a coded key, and a human-readable value. a Reference Type referring to another class within the same model - used to describe relationships between Data Classes within a model.","title":"Data Models"},{"location":"model/glossary/#terminologies","text":"A Terminology can represent a complex ontology, a structured collection of enumerated values, or something inbetween. A terminology is stored in the catalogue in a similar manner to a Data Model: it has a unique label, a description, an author, an organisation, and some alternate names. A Terminology contains a number of Terms which themselves have a code, a human-readable definition, a URL which can be used to point to a definitive definition, and a description, which may be more verbose than the stated definition. Pairs of terms inside a terminology may be related: within the Terminology we store a number of Term Relationships - each of which has a source and target term. The Term Relationship is annotated with the Relationship Type - one of a set of types which are defined separately for each terminology.","title":"Terminologies"},{"location":"model/glossary/#codesets","text":"A CodeSet is a named collection of terms which may be pulled from multiple terminologies. It has similar fields to a Data Model: A label (which must be unique within the system), a description, an author, organisation, and alternate names.","title":"Codesets"},{"location":"model/glossary/#folders","text":"A Folder is the principle mechanism for classifying and organising structures within the Metadata Catalogue. Folders may contain sub-folders, and each folder may contain Data Models, Terminologies or Code Sets. A folder has a label, which must be unique within its container (parent folder, or within the top-level of the folder hierarchy).","title":"Folders"},{"location":"model/glossary/#data-flows","text":"Currently undocumented","title":"Data Flows"},{"location":"model/glossary/#additional-features","text":"All items in the Metadata Catalogue conform to a simple notion of Catalogue Item which has additional properties. These are outlined in the following subsections.","title":"Additional features"},{"location":"model/glossary/#metadata-properties","text":"Each Catalogue Item holds an extensible list of Properties (historically called Metadata), which can store arbitrary additional information about the Catalogue Item. Each property is identified by a namespace, and a key, and stored a value.","title":"Metadata / Properties"},{"location":"model/glossary/#attachments","text":"Each Catalogue Item can also hold Attachments : files relating to the item in question - potentially providing additional documentation or context. Each attachment has a title, a description, and the file itself.","title":"Attachments"},{"location":"model/glossary/#comments","text":"Any Catalogue Item may have Comments attached. Each comment has a title and some text, and these comments may themselves have further comments (replies in the sense of a forum or messageboard).","title":"Comments"},{"location":"model/glossary/#classifiers","text":"Classifiers provide a means for further classifying or tagging items within the catalogue. Classifiers contain a label and a description, and may hold further sub-classifiers. These classifiers may be managed separately by users, having their own read/write permissions.","title":"Classifiers"},{"location":"model/glossary/#semantic-links","text":"Currently undocumented","title":"Semantic Links"},{"location":"model/glossary/#summary-metadata","text":"Currently undocumented","title":"Summary Metadata"},{"location":"plugins/freemarker/","text":"","title":"Freemarker Templating"},{"location":"plugins/sparql/","text":"Introduction \u00b6 The SPARQL plugin provides a compliant endpoint for executing SPARQL queries against an RDF rendering of the data. The underlying data store of Mauro Data Mapper is Postgres, and this plugin uses the D2RQ library for extracting the information as RDF triples. The REST API ensures that the mapping is configured with correct access constraints so that users may only see a subset of the triples corresponding to their access privileges. In the most basic cases, a system administrator can query across the whole data corpus in triple form; an unauthenticated user on an instance with no publicly available models will not be able to see any triples. Warning This plugin allows users to execute their own queries against the data store. A malicious user may write arbitrarily complex queries which could cause Mauro to slow down or become unresponsive. If you install this plugin, you may wish to ensure the Mauro instance is behind a firewall, and ensure that users know what they are doing! API Endpoints \u00b6 The plugin provides two (equivalent) endpoints which accept a SPARQL query as part of the request body and return results in a variety of formats. /api/sparql /api/sparql The Accept header determines which format is returned, according to the table below: Accept Header Value Response Format application/sparql-results+xml xml application/xml XML text/csv csv CSV application/sparql-results+json json application/json JSON The default format is for results to be returned in JSON format. In future, it may be possible to extend this plugin to support RDF/XML and other formats. Response format \u00b6 For the request body given in Example 1 below, the response body will have one of the following formats: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \"head\" : { \"vars\" : [ \"s\" , \"p\" , \"o\" ] }, \"results\" : { \"bindings\" : [ { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5\" }, \"p\" : { \"type\" : \"uri\" , \"value\" : \"http://metadata-catalogue.org/label\" }, \"o\" : { \"type\" : \"literal\" , \"value\" : \"Example Data Model\" } }, ... ] } } Response body (XML) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 <?xml version=\"1.0\"?> <sparql xmlns= \"http://www.w3.org/2005/sparql-results#\" > <head> <variable name= \"s\" /> <variable name= \"p\" /> <variable name= \"o\" /> </head> <results> <result> <binding name= \"s\" > <uri> http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5 </uri> </binding> <binding name= \"p\" > <uri> http://metadata-catalogue.org/label </uri> </binding> <binding name= \"o\" > <literal> Example Data Model </literal> </binding> </result> ... </results> </sparql> Response body (CSV) 1 2 3 s,p,o http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5,http://metadata-catalogue.org/label,Example Data Model ... Example Queries \u00b6 This page is not intended to provide a tutorial to writing SPARQL queries - some other tutorials are available online and linked below. These examples serve as a starting point for exploring the triple space. Example 1: Arbitrary triples \u00b6 Select the first 20 triples from the entire graph: 1 2 3 SELECT ?s ?p ?o WHERE { ?s ?p ?o } limit 20 Example 2: Restricting types \u00b6 Select the labels of all Data Models : 1 2 3 4 5 6 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?o WHERE { ?s mdm : label ?o . ?s a mdm : datamodel } Example 3: Relating entities \u00b6 Find the id of the classes which belong to a model called \"Complex Test DataModel\": 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?dcl WHERE { ?dm mdm : label \"Complex Test DataModel\" . ?dm a mdm : datamodel . ?dc mdm : child_class_of ?dm . ?dc mdm : id ?dcl } Example 4: Relations, multiple results \u00b6 Find the Enumeration Values which have a 'Value' of \"Not known\" . Find their keys and the label of the containing data type: 1 2 3 4 5 6 7 8 9 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?dtl ?evk WHERE { ?ev a mdm : enumerationvalue . ?ev mdm : value \"Not known\" . ?ev mdm : key ?evk . ?dt mdm : component_value ?ev . ?dt mdm : label ?dtl } Example 5: Finding metadata \u00b6 Find the schema.org abstract property from a Data Model : 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> PREFIX so : <http://metadata-catalogue.org/schema.org/> SELECT ?a WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dm so : abstract ?a . } Example 6: Transitive searches \u00b6 The first query finds the immediate child classes of a model: 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?l WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dc mdm : child_class_of ?dm . ?dc mdm : label ?l } Compare this with the second, which transitively follows the child_class_of relationship to find child classes of that class: 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?l WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dc mdm : child_class_of + ?dm . ?dc mdm : label ?l } Example 7: Transitive Searches on Terminologies \u00b6 This search recursively finds all terms narrower than another, and provides their code and definition: 1 2 3 4 5 6 7 8 9 10 11 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?code ?definition WHERE { ?ut mdm : code \"N20-N23\" . ?tm mdm : label \"International Classification of Diseases (ICD) Version 10 Edition 5\" . ?t mdm : term_of ?tm . ?ut mdm : term_of ?tm . ?t mdm : narrowerThan + ?ut . ?t mdm : code ?code . ?t mdm : definition ?definition . } ORDER BY ASC ( ?code ) Links to SPARQL Tutorials \u00b6 There are many good tutorials available online - this is not intended to represet a comprehensive list. But here are some some that we've found useful in the past: Apache Jena: SPARQL Tutorial Stardog: Learn SPARQL W3C: SPARQL By Example","title":"SPARQL"},{"location":"plugins/sparql/#introduction","text":"The SPARQL plugin provides a compliant endpoint for executing SPARQL queries against an RDF rendering of the data. The underlying data store of Mauro Data Mapper is Postgres, and this plugin uses the D2RQ library for extracting the information as RDF triples. The REST API ensures that the mapping is configured with correct access constraints so that users may only see a subset of the triples corresponding to their access privileges. In the most basic cases, a system administrator can query across the whole data corpus in triple form; an unauthenticated user on an instance with no publicly available models will not be able to see any triples. Warning This plugin allows users to execute their own queries against the data store. A malicious user may write arbitrarily complex queries which could cause Mauro to slow down or become unresponsive. If you install this plugin, you may wish to ensure the Mauro instance is behind a firewall, and ensure that users know what they are doing!","title":"Introduction"},{"location":"plugins/sparql/#api-endpoints","text":"The plugin provides two (equivalent) endpoints which accept a SPARQL query as part of the request body and return results in a variety of formats. /api/sparql /api/sparql The Accept header determines which format is returned, according to the table below: Accept Header Value Response Format application/sparql-results+xml xml application/xml XML text/csv csv CSV application/sparql-results+json json application/json JSON The default format is for results to be returned in JSON format. In future, it may be possible to extend this plugin to support RDF/XML and other formats.","title":"API Endpoints"},{"location":"plugins/sparql/#response-format","text":"For the request body given in Example 1 below, the response body will have one of the following formats: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \"head\" : { \"vars\" : [ \"s\" , \"p\" , \"o\" ] }, \"results\" : { \"bindings\" : [ { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5\" }, \"p\" : { \"type\" : \"uri\" , \"value\" : \"http://metadata-catalogue.org/label\" }, \"o\" : { \"type\" : \"literal\" , \"value\" : \"Example Data Model\" } }, ... ] } } Response body (XML) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 <?xml version=\"1.0\"?> <sparql xmlns= \"http://www.w3.org/2005/sparql-results#\" > <head> <variable name= \"s\" /> <variable name= \"p\" /> <variable name= \"o\" /> </head> <results> <result> <binding name= \"s\" > <uri> http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5 </uri> </binding> <binding name= \"p\" > <uri> http://metadata-catalogue.org/label </uri> </binding> <binding name= \"o\" > <literal> Example Data Model </literal> </binding> </result> ... </results> </sparql> Response body (CSV) 1 2 3 s,p,o http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5,http://metadata-catalogue.org/label,Example Data Model ...","title":"Response format"},{"location":"plugins/sparql/#example-queries","text":"This page is not intended to provide a tutorial to writing SPARQL queries - some other tutorials are available online and linked below. These examples serve as a starting point for exploring the triple space.","title":"Example Queries"},{"location":"plugins/sparql/#example-1-arbitrary-triples","text":"Select the first 20 triples from the entire graph: 1 2 3 SELECT ?s ?p ?o WHERE { ?s ?p ?o } limit 20","title":"Example 1: Arbitrary triples"},{"location":"plugins/sparql/#example-2-restricting-types","text":"Select the labels of all Data Models : 1 2 3 4 5 6 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?o WHERE { ?s mdm : label ?o . ?s a mdm : datamodel }","title":"Example 2: Restricting types"},{"location":"plugins/sparql/#example-3-relating-entities","text":"Find the id of the classes which belong to a model called \"Complex Test DataModel\": 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?dcl WHERE { ?dm mdm : label \"Complex Test DataModel\" . ?dm a mdm : datamodel . ?dc mdm : child_class_of ?dm . ?dc mdm : id ?dcl }","title":"Example 3: Relating entities"},{"location":"plugins/sparql/#example-4-relations-multiple-results","text":"Find the Enumeration Values which have a 'Value' of \"Not known\" . Find their keys and the label of the containing data type: 1 2 3 4 5 6 7 8 9 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?dtl ?evk WHERE { ?ev a mdm : enumerationvalue . ?ev mdm : value \"Not known\" . ?ev mdm : key ?evk . ?dt mdm : component_value ?ev . ?dt mdm : label ?dtl }","title":"Example 4: Relations, multiple results"},{"location":"plugins/sparql/#example-5-finding-metadata","text":"Find the schema.org abstract property from a Data Model : 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> PREFIX so : <http://metadata-catalogue.org/schema.org/> SELECT ?a WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dm so : abstract ?a . }","title":"Example 5: Finding metadata"},{"location":"plugins/sparql/#example-6-transitive-searches","text":"The first query finds the immediate child classes of a model: 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?l WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dc mdm : child_class_of ?dm . ?dc mdm : label ?l } Compare this with the second, which transitively follows the child_class_of relationship to find child classes of that class: 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?l WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dc mdm : child_class_of + ?dm . ?dc mdm : label ?l }","title":"Example 6: Transitive searches"},{"location":"plugins/sparql/#example-7-transitive-searches-on-terminologies","text":"This search recursively finds all terms narrower than another, and provides their code and definition: 1 2 3 4 5 6 7 8 9 10 11 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?code ?definition WHERE { ?ut mdm : code \"N20-N23\" . ?tm mdm : label \"International Classification of Diseases (ICD) Version 10 Edition 5\" . ?t mdm : term_of ?tm . ?ut mdm : term_of ?tm . ?t mdm : narrowerThan + ?ut . ?t mdm : code ?code . ?t mdm : definition ?definition . } ORDER BY ASC ( ?code )","title":"Example 7: Transitive Searches on Terminologies"},{"location":"plugins/sparql/#links-to-sparql-tutorials","text":"There are many good tutorials available online - this is not intended to represet a comprehensive list. But here are some some that we've found useful in the past: Apache Jena: SPARQL Tutorial Stardog: Learn SPARQL W3C: SPARQL By Example","title":"Links to SPARQL Tutorials"},{"location":"resources/architecture/","text":"Overview \u00b6 Mauro Data Mapper is built using a fairly common layered design. At the heart is a standard, relational database where all data is primarily stored. All interaction with the database is controlled through business logic within the ' Core ' layer. Interfaces around the core allow interaction at different levels of abstraction Relational Database \u00b6 Mauro Data Mapper has been built on top of PostgreSQL and in particular is tried and tested against PostgreSQL version 12. However, we've taken care not to use any clever features or plugins so that the majority of the code should run against any recent version of PostgreSQL . Furthermore, since the interaction with the relational database is based upon the Hibernate ORM it could even be possible to rebuild against other database implementations, but we've yet to try. The only exception is during integration testing, in which an in-memory h2 database is used in order to speed up testing. System administrators with access to the database can access the data directly, and this is the preferred route for taking backups . However, editing or interpreting the data directly through the database is not recommended, as this will bypass the business logic in the core, with potential loss of system integrity. Core \u00b6 The Core component is built using Grails (version 4), which is a Java-based Model-View-Controller framework. Code is typically written in Groovy , which itself compiles down to Java. Much of the Grails framework is built on top of the widely-used Spring components. The Core codebase defines the object-oriented domain model, which specifies the structure and constraints on the underlying model. All program logic is contained within Services and Controllers , with Views defining the structure of any outputs to procedures or requests. REST API \u00b6 The REST API is a logical layer, defined completely within the Core component and is the standard way of interacting with the platform. A standard REST -style interface makes use of standard HTTP commands, for example GET , POST , PUT , and DELETE . Each REST endpoint is defined by a Controller and View within the Grails Core . Some endpoints are aliased for ease of use, or backwards compatibility, and there is genericity built in to make programming against the API easier. Plugins may extend the API with new endpoints. Each endpoint typically receives and responds in JSON; some can use XML but this is less well tested. Custom data formats apply in particular circumstances - for example when dealing with file attachments. Programming APIs \u00b6 The programming APIs wrap REST commands in programming constructs to make it easier for programmers to interact with Mauro Data Mapper without being concerned with the technical details of the REST API . Of the three current APIs, the Java library is most mature and is able to re-use components of the Grails Core for a faithful representation of the underlying object model. The Java API is suitable for programming complex import and export routines and has built in support for a number of batch operations that are not easily achieved through the web user interface. The Java client also supports connections to multiple instances, making it a good tool for implementing more sophisticated federation mechanisms. The Typescript API is a much simpler wrapper around those endpoints used by the web interface. It is hosted as a separate component and can be installed using npm , the standard package manager for javascript applications. User Interface \u00b6 The web-based user interface is defined in Angular 9 using the Angular Material library for look-and-feel. It is a self-contained, single-page web application which makes use of some additional typescript libraries (for rendering diagrams, providing notifications, etc) on top of the standard ones provided by Angular. It only uses the REST API (via the typescript client library) to communicate with the Core , and is built in a modular fashion to allow easy extensibility. Many of the components can be easily re-used in the creation of other web interfaces. Grails Plugins \u00b6 The Grails Core provides an easy mechanism for extension, through standard Grails plugins. A number of pre-defined extension points are available. For example, to implement new importers, exporters, profiles, or authentication mechanisms. However plugins may also arbitrarily extend the REST API with custom functionality, making use of the services and controllers defined within the core. A number of plugins are defined and made available through the central GitHub plugins organisation; developers may feel free to use and adapt those, or write their own, sharing if they wish. Note that some technical plugins such as the SPARQL or Apache Freemarker plugins allow users to perform their own queries against the database and arbitrarily complex queries may affect the performance of the server. The Core component is itself made up of a number of plugins, and can be disassembled for particular use cases. For example, it is possible to disable support for API Keys by compiling a version of the Core with that plugin removed. Search Index \u00b6 The search index improves the performance of searching, this content is stored in-memory (and persisted to files on disk at suitable intervals). In some places in the Core , it may also be used to speed up access to particular model contents. The index is built using Apache Lucene but managed in the code through Hibernate. This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary. The contents of the search index can be hard to inspect for debugging purposes! We use a tool called Marple but be sure to use a compatible version!","title":"Technical Architecture"},{"location":"resources/architecture/#overview","text":"Mauro Data Mapper is built using a fairly common layered design. At the heart is a standard, relational database where all data is primarily stored. All interaction with the database is controlled through business logic within the ' Core ' layer. Interfaces around the core allow interaction at different levels of abstraction","title":"Overview"},{"location":"resources/architecture/#relational-database","text":"Mauro Data Mapper has been built on top of PostgreSQL and in particular is tried and tested against PostgreSQL version 12. However, we've taken care not to use any clever features or plugins so that the majority of the code should run against any recent version of PostgreSQL . Furthermore, since the interaction with the relational database is based upon the Hibernate ORM it could even be possible to rebuild against other database implementations, but we've yet to try. The only exception is during integration testing, in which an in-memory h2 database is used in order to speed up testing. System administrators with access to the database can access the data directly, and this is the preferred route for taking backups . However, editing or interpreting the data directly through the database is not recommended, as this will bypass the business logic in the core, with potential loss of system integrity.","title":"Relational Database"},{"location":"resources/architecture/#core","text":"The Core component is built using Grails (version 4), which is a Java-based Model-View-Controller framework. Code is typically written in Groovy , which itself compiles down to Java. Much of the Grails framework is built on top of the widely-used Spring components. The Core codebase defines the object-oriented domain model, which specifies the structure and constraints on the underlying model. All program logic is contained within Services and Controllers , with Views defining the structure of any outputs to procedures or requests.","title":"Core"},{"location":"resources/architecture/#rest-api","text":"The REST API is a logical layer, defined completely within the Core component and is the standard way of interacting with the platform. A standard REST -style interface makes use of standard HTTP commands, for example GET , POST , PUT , and DELETE . Each REST endpoint is defined by a Controller and View within the Grails Core . Some endpoints are aliased for ease of use, or backwards compatibility, and there is genericity built in to make programming against the API easier. Plugins may extend the API with new endpoints. Each endpoint typically receives and responds in JSON; some can use XML but this is less well tested. Custom data formats apply in particular circumstances - for example when dealing with file attachments.","title":"REST API"},{"location":"resources/architecture/#programming-apis","text":"The programming APIs wrap REST commands in programming constructs to make it easier for programmers to interact with Mauro Data Mapper without being concerned with the technical details of the REST API . Of the three current APIs, the Java library is most mature and is able to re-use components of the Grails Core for a faithful representation of the underlying object model. The Java API is suitable for programming complex import and export routines and has built in support for a number of batch operations that are not easily achieved through the web user interface. The Java client also supports connections to multiple instances, making it a good tool for implementing more sophisticated federation mechanisms. The Typescript API is a much simpler wrapper around those endpoints used by the web interface. It is hosted as a separate component and can be installed using npm , the standard package manager for javascript applications.","title":"Programming APIs"},{"location":"resources/architecture/#user-interface","text":"The web-based user interface is defined in Angular 9 using the Angular Material library for look-and-feel. It is a self-contained, single-page web application which makes use of some additional typescript libraries (for rendering diagrams, providing notifications, etc) on top of the standard ones provided by Angular. It only uses the REST API (via the typescript client library) to communicate with the Core , and is built in a modular fashion to allow easy extensibility. Many of the components can be easily re-used in the creation of other web interfaces.","title":"User Interface"},{"location":"resources/architecture/#grails-plugins","text":"The Grails Core provides an easy mechanism for extension, through standard Grails plugins. A number of pre-defined extension points are available. For example, to implement new importers, exporters, profiles, or authentication mechanisms. However plugins may also arbitrarily extend the REST API with custom functionality, making use of the services and controllers defined within the core. A number of plugins are defined and made available through the central GitHub plugins organisation; developers may feel free to use and adapt those, or write their own, sharing if they wish. Note that some technical plugins such as the SPARQL or Apache Freemarker plugins allow users to perform their own queries against the database and arbitrarily complex queries may affect the performance of the server. The Core component is itself made up of a number of plugins, and can be disassembled for particular use cases. For example, it is possible to disable support for API Keys by compiling a version of the Core with that plugin removed.","title":"Grails Plugins"},{"location":"resources/architecture/#search-index","text":"The search index improves the performance of searching, this content is stored in-memory (and persisted to files on disk at suitable intervals). In some places in the Core , it may also be used to speed up access to particular model contents. The index is built using Apache Lucene but managed in the code through Hibernate. This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary. The contents of the search index can be hard to inspect for debugging purposes! We use a tool called Marple but be sure to use a compatible version!","title":"Search Index"},{"location":"resources/client/java/","text":"Introduction \u00b6 The Java / Groovy client library wraps the REST API in Java methods to make it easy for Java developers to interact with a Mauro instance. The library makes use of the mdm-core grails application, essentially loading a local, in-memory copy of the Mauro Core to take advantage of the services and controllers from Core, as well as re-using the domain model, and in-built validation. The REST API library can connect to a remote Mauro instance, and is typically used to perform bulk operations such as importing and exporting models, such as when scripting a complex import. This is often the easiest way to experiment with importing before building a Grails Plugin . The code can also be used to connect to multiple Mauro instances, for example to copy models from one instance to another. API documentation \u00b6 The API is documented using GroovyDoc and the complete documentation can be found here . Add dependency \u00b6 In order to include the Mauro client library in your Java / Groovy project, use the following dependency in Gradle or Maven: Gradle 1 2 3 4 5 6 7 8 9 10 repositories { ... maven {url \"https://jenkins.cs.ox.ac.uk/artifactory/libs-release\"} ... } dependencies { ... compile \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-api-java-restful:{version}\" } Maven 1 2 3 4 5 6 7 8 9 10 11 12 13 14 <distributionManagement> <snapshotRepository> <id> snapshots </id> <name> jenkins.cs.ox.ac.uk-snapshots </name> <url> http://jenkins.cs.ox.ac.uk/artifactory/libs-snapshot-local </url> </snapshotRepository> ... </distributionManagement> ... <dependency> <groupId> uk.ac.ox.softeng.maurodatamapper.plugins </groupId> <artifactId> mdm-api-java-restful </artifactId> <version> {version} </version> </dependency> Look on our Release Notes page to find the latest version number Getting started \u00b6 Information The examples given here are in Groovy. Conversion to equivalent Java is a fairly simple task. The simplest way to get started is by creating a client manually - using the url of the server that the instance is hosted on, and a username / password. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient class TestConnection { static void main ( String [] args ) { String baseUrl = \"http://localhost:8080\" String username = \"...\" String password = \"...\" new DataMapperClient ( baseUrl , username , password ). withCloseable { client -> // client is now connected with a session to a Mauro instance // Now we can do things with our client client . createFolder ( \"Test Folder\" ) } } } The BindingMauroDataMapperClient creates a connection to a Mauro instance, and maintains a session where necessary. The class implements the Closeable interface which means that any session will be closed at the end of the withCloseable closure. Authentication and passing arguments \u00b6 As in the example above, you can connect to the Mauro instance using a username and password . In this case, any session cookie returned will be stored and used for future calls automatically. You can also connect using an API Key and this will be passed in the parameters for every call. It's usually more convenient to pass arguments such as usernames, passwords or API keys in as parameters, rather than hard-coding them into the application. We take advantage of PicoCli to provide options for passing these parameters to an application. In the most basic case, consider the following application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient class TestConnection2 extends MdmCommandLineTool < MdmConnectionOptions > { static void main ( String [] args ) { TestConnection2 testConnection2 = new TestConnection2 ( args ) TestConnection2 . doStuff () } void doStuff () { options . getMauroDataMapperClient (). withCloseable { client -> // Do something with our client here... } } } The class extends MdmCommandLineTool , which provides an object of type MdmConnectionOptions , representing the options passed in on the command line. This class contains definitions for the following options: -U , --clientBaseUrl , --client.baseUrl The base url of the Mauro instance to connect to - for example http://www.example.com/mauro/ . Any trailing /api will be added automatically. -u , --clientUsername , --client.username The username for logging into this Mauro instance. -p , --clientPassword , --client.password The password for logging into this Mauro instance. -a , --clientApiKey , --client.apiKey The API key for logging into this Mauro instance. -h , --help Displays a help message describing these options -v , --verbose Runs the application in 'verbose' mode, giving additional logging for debug purposes -D , --debug Provide advanced debug information as logs -P , --properties Provides further parameters via a properties file - for example: --properties ./config.properties . See below for more details. The final option allows these properties to be passed in a standard java properties file. Properties should be provided using the format provided in the final options above - for example: 1 2 client.baseUrl = http://localhost:8080/ client.apiKey = 767c6e02-4ad6-4480-8b42-a36160143a24 The MdmConnectionOptions class provides a default mechanism to create a new BindingMauroDataMapperClient from the provided values. The class can be extended to provide additional, application-specific options. To provide specific functionality for these options, you can also extend the MdmCommandLineTool class. Dealing with multiple connections \u00b6 Occasionally, it's useful to deal with multiple connections to a single catalogue, or connections to more than one catalogue instance. The client can store multiple named connections and all methods have an optional final parameter to choose which connection to use. For example, the following code connects to two instances of Mauro, naming the connections as 'source' and 'target': 1 2 3 BindingMauroDataMapperClient bindingMauroDataMapperClient = new BindingMauroDataMapperClient () bindingMauroDataMapperClient . openConnection ( \"source\" , sourceProperties ) bindingMauroDataMapperClient . openConnection ( \"target\" , targetProperties ) Subsequent method calls can pass the name of the connection as a final argument - for example the following code copies a DataModel from the \"source\" instance of Mauro to the \"target\": 1 2 3 4 DataModel dataModel = bindingMauroDataMapperClient . exportAndBindDataModelById ( dataModelId , \"source\" ) bindingMauroDataMapperClient . importDataModel ( dataModel , folderId , dataModelName , finalised , importAsNewDocumentationVersion , \"target\" ) In every method, if no connection is specified by name, the default connection (internally named \"_default\") is used. Binding vs. non-binding clients \u00b6 The library provides two different clients: the first is the simpler MauroDataMapperClient . This provides a number of methods for interacting with the Mauro REST API in a more native form: dealing with responses in Map form. The alternative is the more complex BindingMauroDataMapperClient which extends MauroDataMapperClient with additional methods for binding responses into the appropriate Mauro domain types. For example, compare the following methods: Map exportDataModel(UUID id, String connectionName = defaultConnectionName) and DataModel exportAndBindDataModelById(UUID id, String connectionName = defaultConnectionName) The two methods access the same REST endpoint: the former returns the response JSON in map form; the latter takes that response and binds it to an object of class DataModel . The latter is obviously easier to process, but the former provides a faster response. The former variant also provides an important advantage: when a Map is bound to a Data Model within grails, it is associated with the internal, in-memory database and validated. At this point, any identifiers, or 'last modified' dates associated with any component of that DataModel will be dropped in favour of local variants. So, for example, if you wanted to re-use the identifiers of the DataClasses contained within that DataModel (for example, in order to update their descriptions individually in the remote instance), then you should use the non-binding version of the method. In general, since the BindingMauroDataMapperClient extends the MauroDataMapperClient class, the binding version is all that is required. The binding client has additional methods for manually binding results of calls after intermediate processing.","title":"Groovy / Java"},{"location":"resources/client/java/#introduction","text":"The Java / Groovy client library wraps the REST API in Java methods to make it easy for Java developers to interact with a Mauro instance. The library makes use of the mdm-core grails application, essentially loading a local, in-memory copy of the Mauro Core to take advantage of the services and controllers from Core, as well as re-using the domain model, and in-built validation. The REST API library can connect to a remote Mauro instance, and is typically used to perform bulk operations such as importing and exporting models, such as when scripting a complex import. This is often the easiest way to experiment with importing before building a Grails Plugin . The code can also be used to connect to multiple Mauro instances, for example to copy models from one instance to another.","title":"Introduction"},{"location":"resources/client/java/#api-documentation","text":"The API is documented using GroovyDoc and the complete documentation can be found here .","title":"API documentation"},{"location":"resources/client/java/#add-dependency","text":"In order to include the Mauro client library in your Java / Groovy project, use the following dependency in Gradle or Maven: Gradle 1 2 3 4 5 6 7 8 9 10 repositories { ... maven {url \"https://jenkins.cs.ox.ac.uk/artifactory/libs-release\"} ... } dependencies { ... compile \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-api-java-restful:{version}\" } Maven 1 2 3 4 5 6 7 8 9 10 11 12 13 14 <distributionManagement> <snapshotRepository> <id> snapshots </id> <name> jenkins.cs.ox.ac.uk-snapshots </name> <url> http://jenkins.cs.ox.ac.uk/artifactory/libs-snapshot-local </url> </snapshotRepository> ... </distributionManagement> ... <dependency> <groupId> uk.ac.ox.softeng.maurodatamapper.plugins </groupId> <artifactId> mdm-api-java-restful </artifactId> <version> {version} </version> </dependency> Look on our Release Notes page to find the latest version number","title":"Add dependency"},{"location":"resources/client/java/#getting-started","text":"Information The examples given here are in Groovy. Conversion to equivalent Java is a fairly simple task. The simplest way to get started is by creating a client manually - using the url of the server that the instance is hosted on, and a username / password. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient class TestConnection { static void main ( String [] args ) { String baseUrl = \"http://localhost:8080\" String username = \"...\" String password = \"...\" new DataMapperClient ( baseUrl , username , password ). withCloseable { client -> // client is now connected with a session to a Mauro instance // Now we can do things with our client client . createFolder ( \"Test Folder\" ) } } } The BindingMauroDataMapperClient creates a connection to a Mauro instance, and maintains a session where necessary. The class implements the Closeable interface which means that any session will be closed at the end of the withCloseable closure.","title":"Getting started"},{"location":"resources/client/java/#authentication-and-passing-arguments","text":"As in the example above, you can connect to the Mauro instance using a username and password . In this case, any session cookie returned will be stored and used for future calls automatically. You can also connect using an API Key and this will be passed in the parameters for every call. It's usually more convenient to pass arguments such as usernames, passwords or API keys in as parameters, rather than hard-coding them into the application. We take advantage of PicoCli to provide options for passing these parameters to an application. In the most basic case, consider the following application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient class TestConnection2 extends MdmCommandLineTool < MdmConnectionOptions > { static void main ( String [] args ) { TestConnection2 testConnection2 = new TestConnection2 ( args ) TestConnection2 . doStuff () } void doStuff () { options . getMauroDataMapperClient (). withCloseable { client -> // Do something with our client here... } } } The class extends MdmCommandLineTool , which provides an object of type MdmConnectionOptions , representing the options passed in on the command line. This class contains definitions for the following options: -U , --clientBaseUrl , --client.baseUrl The base url of the Mauro instance to connect to - for example http://www.example.com/mauro/ . Any trailing /api will be added automatically. -u , --clientUsername , --client.username The username for logging into this Mauro instance. -p , --clientPassword , --client.password The password for logging into this Mauro instance. -a , --clientApiKey , --client.apiKey The API key for logging into this Mauro instance. -h , --help Displays a help message describing these options -v , --verbose Runs the application in 'verbose' mode, giving additional logging for debug purposes -D , --debug Provide advanced debug information as logs -P , --properties Provides further parameters via a properties file - for example: --properties ./config.properties . See below for more details. The final option allows these properties to be passed in a standard java properties file. Properties should be provided using the format provided in the final options above - for example: 1 2 client.baseUrl = http://localhost:8080/ client.apiKey = 767c6e02-4ad6-4480-8b42-a36160143a24 The MdmConnectionOptions class provides a default mechanism to create a new BindingMauroDataMapperClient from the provided values. The class can be extended to provide additional, application-specific options. To provide specific functionality for these options, you can also extend the MdmCommandLineTool class.","title":"Authentication and passing arguments"},{"location":"resources/client/java/#dealing-with-multiple-connections","text":"Occasionally, it's useful to deal with multiple connections to a single catalogue, or connections to more than one catalogue instance. The client can store multiple named connections and all methods have an optional final parameter to choose which connection to use. For example, the following code connects to two instances of Mauro, naming the connections as 'source' and 'target': 1 2 3 BindingMauroDataMapperClient bindingMauroDataMapperClient = new BindingMauroDataMapperClient () bindingMauroDataMapperClient . openConnection ( \"source\" , sourceProperties ) bindingMauroDataMapperClient . openConnection ( \"target\" , targetProperties ) Subsequent method calls can pass the name of the connection as a final argument - for example the following code copies a DataModel from the \"source\" instance of Mauro to the \"target\": 1 2 3 4 DataModel dataModel = bindingMauroDataMapperClient . exportAndBindDataModelById ( dataModelId , \"source\" ) bindingMauroDataMapperClient . importDataModel ( dataModel , folderId , dataModelName , finalised , importAsNewDocumentationVersion , \"target\" ) In every method, if no connection is specified by name, the default connection (internally named \"_default\") is used.","title":"Dealing with multiple connections"},{"location":"resources/client/java/#binding-vs-non-binding-clients","text":"The library provides two different clients: the first is the simpler MauroDataMapperClient . This provides a number of methods for interacting with the Mauro REST API in a more native form: dealing with responses in Map form. The alternative is the more complex BindingMauroDataMapperClient which extends MauroDataMapperClient with additional methods for binding responses into the appropriate Mauro domain types. For example, compare the following methods: Map exportDataModel(UUID id, String connectionName = defaultConnectionName) and DataModel exportAndBindDataModelById(UUID id, String connectionName = defaultConnectionName) The two methods access the same REST endpoint: the former returns the response JSON in map form; the latter takes that response and binds it to an object of class DataModel . The latter is obviously easier to process, but the former provides a faster response. The former variant also provides an important advantage: when a Map is bound to a Data Model within grails, it is associated with the internal, in-memory database and validated. At this point, any identifiers, or 'last modified' dates associated with any component of that DataModel will be dropped in favour of local variants. So, for example, if you wanted to re-use the identifiers of the DataClasses contained within that DataModel (for example, in order to update their descriptions individually in the remote instance), then you should use the non-binding version of the method. In general, since the BindingMauroDataMapperClient extends the MauroDataMapperClient class, the binding version is all that is required. The binding client has additional methods for manually binding results of calls after intermediate processing.","title":"Binding vs. non-binding clients"},{"location":"resources/client/net/","text":"The .Net client library wraps the REST API in .net methods to make it easy for .Net developers to interact with a Mauro instance. The library makes use of the .Net Core API application with Controller & Model to call API methods The REST API library can connect to a remote Mauro instance, and is typically used to perform bulk operations such as importing and exporting models. The code can also be used to connect to multiple Mauro instances, for example to copy models from one instance to another. API documentation \u00b6 The API documentation is built using DocFx and is available here : Adding as a dependency \u00b6 In order to include the Mauro client library in your .Net project, you will need to include the following dependencies: CsvHelper Newtonsoft.Json System.Text.Json A .dll file built from the code can be referenced as mdm-api-dotnet-restful API Client \u00b6 MauroDataMapperClient provides constructors to login either by UserId, Password, BaseUrl & Connection Name Properties Object with Properties - UserName, Password & BaseUrl If you connect to an instance using a username and password, any session cookie returned will be stored and used for future calls automatically. It's usually more convenient to pass arguments such as usernames, passwords as command-line parameters, rather than hard-coding them into the application. API Client Methods \u00b6 The methods of the API Client make use of the System.Net.Http package. This library is described in more detail here . Each accepts an object of the class HttpRequestMessage , and each returns an instance of HttpResponseMessage .Net Console Application \u00b6 The mdm-api-dotnet-console application has been built as a Windows-friendly tool for bulk CSV import. In order to include this as part of an application, you need to add a reference to the mdm-api-dotnet-restful .dll file. This application has its own configuration settings: Folder & File location Whether to create each CSV definition as a DataClass or a DataModel The server address & user credentials are read from the app.config file. Once files are read, and their definitions uploaded to Mauro, they are moved to the 'Archive' folder.","title":".NET"},{"location":"resources/client/net/#api-documentation","text":"The API documentation is built using DocFx and is available here :","title":"API documentation"},{"location":"resources/client/net/#adding-as-a-dependency","text":"In order to include the Mauro client library in your .Net project, you will need to include the following dependencies: CsvHelper Newtonsoft.Json System.Text.Json A .dll file built from the code can be referenced as mdm-api-dotnet-restful","title":"Adding as a dependency"},{"location":"resources/client/net/#api-client","text":"MauroDataMapperClient provides constructors to login either by UserId, Password, BaseUrl & Connection Name Properties Object with Properties - UserName, Password & BaseUrl If you connect to an instance using a username and password, any session cookie returned will be stored and used for future calls automatically. It's usually more convenient to pass arguments such as usernames, passwords as command-line parameters, rather than hard-coding them into the application.","title":"API Client"},{"location":"resources/client/net/#api-client-methods","text":"The methods of the API Client make use of the System.Net.Http package. This library is described in more detail here . Each accepts an object of the class HttpRequestMessage , and each returns an instance of HttpResponseMessage","title":"API Client Methods"},{"location":"resources/client/net/#net-console-application","text":"The mdm-api-dotnet-console application has been built as a Windows-friendly tool for bulk CSV import. In order to include this as part of an application, you need to add a reference to the mdm-api-dotnet-restful .dll file. This application has its own configuration settings: Folder & File location Whether to create each CSV definition as a DataClass or a DataModel The server address & user credentials are read from the app.config file. Once files are read, and their definitions uploaded to Mauro, they are moved to the 'Archive' folder.","title":".Net Console Application"},{"location":"resources/client/typescript/","text":"Introduction \u00b6 The TypeScript client library wraps the REST API in TypeScript classes/functions to make it easy for JavaScript and TypeScript developers to interact with a Mauro instance. The TypeScript library that implements communication with the back-end server is available as a stand alone repository for incorporation into other applications. For example other web interfaces, or back-end functionality using node.js . This is in fact the client library that the Mauro Data Mapper user interface uses. The GitHub repository is called mdm-resources and is available within the Mauro Data Mapper organisation . API documentation \u00b6 The API is documented using TypeDoc and the complete documentation can be found here . Layout \u00b6 Methods to call API functions are roughly broken down by resource type, with filenames conforming to the pattern: mdm-{resourceType}.resource.ts There are additional utility functions available in mdm-resource.ts . There are also type definitions to assist with requests and responses, which can be found in filenames of the format mdm-{resourceType}.model.ts . An index.ts file lists all files for inclusion. Resources \u00b6 Each mdm-{resourceType}.resource.ts file defines a new class extending the super class MdmResource , and provides methods for each endpoint. These make use of the simpleGet() , simplePost() , etc methods defined in the super class. Every class that extends MdmResource can optionally provide these in the constructor : MdmResourcesConfiguration - object to define configuration options for every HTTP request. MdmRestHandler - object to the REST handler that will process the requests. If not provided, the DefaultMdmRestHandler will be used - see the REST Handlers section for further details. Including in applications \u00b6 If you are using NPM or Yarn , then you need the following line in your .npmrc or .yarnrc file: 1 @maurodatamapper:registry=https://npm.pkg.github.com` You can then add a line such as the following to your package.json file: 1 2 3 4 \"dependencies\" : { ... \"@maurodatamapper/mdm-resources\" : \"github:MauroDataMapper/mdm-resources#{version}\" } Where {version} refers to a git tag or branch name. Within a TypeScript file, you can then add an import statement such as the following: 1 import { MdmResourcesConfiguration } from '@maurodatamapper/mdm-resources' ; Or, as illustrated in the Mauro UI application, create a custom service to pull all the classes into a single location (see mdm-resources.service.ts within the mdm-ui project). REST Handlers \u00b6 mdm-resources provides a default implementation of the MdmRestHandler called DefaultMdmRestHandler . This implementation uses the fetch API to complete HTTP requests and return promises on each response. This default implementation is usually sufficient for most scenarios, but it is also possible to replace this with your own implementation. Reasons why you might want to do this are: To use something other than fetch() . For example, Angular applications tend to use the built-in HTTP Client to return observable streams instead of promises. To intercept any Mauro HTTP requests/responses to perform some custom operations, such as error handling on failed responses. To use a custom REST handler, follow the steps below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Define the class that implements `IMdmRestHandler` export class CustomMdmRestHandler implements IMdmRestHandler { process ( url : string , options : IMdmRestHandlerOptions ) { // (Optional) pre-process step (e.g. logging) const response = /* Send HTTP request */ // (Optional) post-process step (e.g. logging, error handling) return response ; } } // For every MDM resource created, pass in the custom REST handler instance instead const dataModelsResource = new MdmDataModelResource ( null , new CustomMdmRestHandler ()); Handling Responses \u00b6 All endpoints have type definitions that explicitly state their inputs for requests but generalise their response outputs. This is due to the ability to customise the REST handler , which may return different wrapper objects around the core response definitions. Nonetheless, mdm-resources does provide type definitions for responses and will include them in the documentation comments and type reference documentation for the use of the downstream developer. As an example, given this endpoint function type definition: 1 2 3 4 5 export class MdmDataModelResource extends MdmResource { get ( dataModelId : string , query? : QueryParameters , options? : RequestSettings ) : any { //... } } Response types can be explicitly added to return results so that further type checking can be performed, as in these examples: fetch 1 2 3 4 5 6 7 8 9 10 11 const dataModels = new MdmDataModelResource (); dataModels . get ( '679d582c-9f6c-4ce5-99c7-7fad79374637' ) . then (( response : DataModelDetailResponse ) => { // MdmDataModelResponse.get() states it will return `any` but will actually return // Promise<DataModelDetailResponse>. // Adding explicit type information will resolve further code that uses the response. // Do something here with the response... }) Angular 1 2 3 4 5 6 7 8 9 10 11 const dataModels = new MdmDataModelResource (); dataModels . get ( '679d582c-9f6c-4ce5-99c7-7fad79374637' ) . subscribe (( response : DataModelDetailResponse ) => { // MdmDataModelResponse.get() states it will return `any` but will actually return // Observable<DataModelDetailResponse>. // Adding explicit type information will resolve further code that uses the response. // Do something here with the response... })","title":"TypeScript"},{"location":"resources/client/typescript/#introduction","text":"The TypeScript client library wraps the REST API in TypeScript classes/functions to make it easy for JavaScript and TypeScript developers to interact with a Mauro instance. The TypeScript library that implements communication with the back-end server is available as a stand alone repository for incorporation into other applications. For example other web interfaces, or back-end functionality using node.js . This is in fact the client library that the Mauro Data Mapper user interface uses. The GitHub repository is called mdm-resources and is available within the Mauro Data Mapper organisation .","title":"Introduction"},{"location":"resources/client/typescript/#api-documentation","text":"The API is documented using TypeDoc and the complete documentation can be found here .","title":"API documentation"},{"location":"resources/client/typescript/#layout","text":"Methods to call API functions are roughly broken down by resource type, with filenames conforming to the pattern: mdm-{resourceType}.resource.ts There are additional utility functions available in mdm-resource.ts . There are also type definitions to assist with requests and responses, which can be found in filenames of the format mdm-{resourceType}.model.ts . An index.ts file lists all files for inclusion.","title":"Layout"},{"location":"resources/client/typescript/#resources","text":"Each mdm-{resourceType}.resource.ts file defines a new class extending the super class MdmResource , and provides methods for each endpoint. These make use of the simpleGet() , simplePost() , etc methods defined in the super class. Every class that extends MdmResource can optionally provide these in the constructor : MdmResourcesConfiguration - object to define configuration options for every HTTP request. MdmRestHandler - object to the REST handler that will process the requests. If not provided, the DefaultMdmRestHandler will be used - see the REST Handlers section for further details.","title":"Resources"},{"location":"resources/client/typescript/#including-in-applications","text":"If you are using NPM or Yarn , then you need the following line in your .npmrc or .yarnrc file: 1 @maurodatamapper:registry=https://npm.pkg.github.com` You can then add a line such as the following to your package.json file: 1 2 3 4 \"dependencies\" : { ... \"@maurodatamapper/mdm-resources\" : \"github:MauroDataMapper/mdm-resources#{version}\" } Where {version} refers to a git tag or branch name. Within a TypeScript file, you can then add an import statement such as the following: 1 import { MdmResourcesConfiguration } from '@maurodatamapper/mdm-resources' ; Or, as illustrated in the Mauro UI application, create a custom service to pull all the classes into a single location (see mdm-resources.service.ts within the mdm-ui project).","title":"Including in applications"},{"location":"resources/client/typescript/#rest-handlers","text":"mdm-resources provides a default implementation of the MdmRestHandler called DefaultMdmRestHandler . This implementation uses the fetch API to complete HTTP requests and return promises on each response. This default implementation is usually sufficient for most scenarios, but it is also possible to replace this with your own implementation. Reasons why you might want to do this are: To use something other than fetch() . For example, Angular applications tend to use the built-in HTTP Client to return observable streams instead of promises. To intercept any Mauro HTTP requests/responses to perform some custom operations, such as error handling on failed responses. To use a custom REST handler, follow the steps below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Define the class that implements `IMdmRestHandler` export class CustomMdmRestHandler implements IMdmRestHandler { process ( url : string , options : IMdmRestHandlerOptions ) { // (Optional) pre-process step (e.g. logging) const response = /* Send HTTP request */ // (Optional) post-process step (e.g. logging, error handling) return response ; } } // For every MDM resource created, pass in the custom REST handler instance instead const dataModelsResource = new MdmDataModelResource ( null , new CustomMdmRestHandler ());","title":"REST Handlers"},{"location":"resources/client/typescript/#handling-responses","text":"All endpoints have type definitions that explicitly state their inputs for requests but generalise their response outputs. This is due to the ability to customise the REST handler , which may return different wrapper objects around the core response definitions. Nonetheless, mdm-resources does provide type definitions for responses and will include them in the documentation comments and type reference documentation for the use of the downstream developer. As an example, given this endpoint function type definition: 1 2 3 4 5 export class MdmDataModelResource extends MdmResource { get ( dataModelId : string , query? : QueryParameters , options? : RequestSettings ) : any { //... } } Response types can be explicitly added to return results so that further type checking can be performed, as in these examples: fetch 1 2 3 4 5 6 7 8 9 10 11 const dataModels = new MdmDataModelResource (); dataModels . get ( '679d582c-9f6c-4ce5-99c7-7fad79374637' ) . then (( response : DataModelDetailResponse ) => { // MdmDataModelResponse.get() states it will return `any` but will actually return // Promise<DataModelDetailResponse>. // Adding explicit type information will resolve further code that uses the response. // Do something here with the response... }) Angular 1 2 3 4 5 6 7 8 9 10 11 const dataModels = new MdmDataModelResource (); dataModels . get ( '679d582c-9f6c-4ce5-99c7-7fad79374637' ) . subscribe (( response : DataModelDetailResponse ) => { // MdmDataModelResponse.get() states it will return `any` but will actually return // Observable<DataModelDetailResponse>. // Adding explicit type information will resolve further code that uses the response. // Do something here with the response... })","title":"Handling Responses"},{"location":"rest-api/admin/","text":"There are a number of endpoints which are specific to administrators: understanding the configuration of the particular instance; discovering the avaiable plugins, etc. Currently logged in users \u00b6 /api/admin/activeSessions This endpoint returns a list of all logged in users. /api/admin/activeSessions If called in post mode, you can pass in user credentials, rather than basing on an existing session. Configuration \u00b6 To find out more about the current instance of the catalogue: what version is running; the version of Java that's runing; the JDBC drivers currently available; call the following endpoint: /api/admin/status Modules \u00b6 To find out which modules are installed, call the following endpoint: /api/admin/modules The post version of the endpoint can be called in order to pass authentication credentials at the same time: /api/admin/modules Plugins \u00b6 To find out which plugins are currently installed, use one of the following endpoints: /api/admin/plugins/exporters /api/admin/plugins/emailers /api/admin/plugins/dataLoaders /api/admin/plugins/importers System actions \u00b6 /api/admin/rebuildLuceneIndexes This endpoint forces the rebuild of the Lucene indexes. This is only necessary when synchronisation between database and indexes is lost; when the search functionality is not returning correct results. Authentication credentials can be passed as part of the request body. Properties \u00b6 There are a number of system-wide properties that can be updated by administrators, such as the text of any emails sent and the email address from which catalogue emails appear to be sent. Properties are composed of keys and values . Keys can be any string with the following restrictions: Must be lowercase alpha characters No spaces are allowed May include periods ('.') and/or underscores ('_') Must be unique Getting properties \u00b6 Properties can be viewed at the following endpoint: /api/admin/properties If successful, the response body will list the available properties: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"count\" : X , \"items\" : [ { \"id\" : \"c7de1358-a4ce-4d72-abca-04013f7f4acc\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : false , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-10T15:17:05.459Z\" }, { \"id\" : \"76becaa3-da04-40d5-a433-51ed203c77b4\" , \"key\" : \"test.property.public\" , \"value\" : \"Public test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : true , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-10T15:17:05.558Z\" } ] } Notice that properties contain a publiclyVisible flag. This is because properties can be created to either be public or restricted to administrators/systems (the default being false ). Only an authenticated session can use the endpoint above, however an anonymous session may use this endpoint to list all publicly available properties: /api/properties To access a single property, this endpoint is provided: /api/admin/properties/ {propertyId} Created, updating and deleting \u00b6 Properties can be created as follows: /api/admin/properties Request body (JSON) 1 2 3 4 5 6 { \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"publiclyVisible\" : false , \"category\" : \"Test\" } If successful, the new property is returned in the response body including the new property id . Response body (JSON) 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : false , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-11T17:46:47.654Z\" } The property can then be updated with the put endpoint: /api/admin/properties Request body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"publiclyVisible\" : false , \"category\" : \"Test\" } And deleted with the delete endpoint: /api/admin/properties/ {propertyId} Data Models \u00b6 The following endpoints provide paginated lists of Data Models (for cleaning / monitoring processes). They list those models which have been deleted, superseded by a new model, and superseded by new documentation, respectively: /api/admin/dataModels/deleted /api/admin/dataModels/modelSuperseded /api/admin/dataModels/documentSuperseded Emails \u00b6 Retrieve the list of emails (recipient, message, date/time) sent by the system: /api/admin/emails","title":"Admin functions"},{"location":"rest-api/admin/#currently-logged-in-users","text":"/api/admin/activeSessions This endpoint returns a list of all logged in users. /api/admin/activeSessions If called in post mode, you can pass in user credentials, rather than basing on an existing session.","title":"Currently logged in users"},{"location":"rest-api/admin/#configuration","text":"To find out more about the current instance of the catalogue: what version is running; the version of Java that's runing; the JDBC drivers currently available; call the following endpoint: /api/admin/status","title":"Configuration"},{"location":"rest-api/admin/#modules","text":"To find out which modules are installed, call the following endpoint: /api/admin/modules The post version of the endpoint can be called in order to pass authentication credentials at the same time: /api/admin/modules","title":"Modules"},{"location":"rest-api/admin/#plugins","text":"To find out which plugins are currently installed, use one of the following endpoints: /api/admin/plugins/exporters /api/admin/plugins/emailers /api/admin/plugins/dataLoaders /api/admin/plugins/importers","title":"Plugins"},{"location":"rest-api/admin/#system-actions","text":"/api/admin/rebuildLuceneIndexes This endpoint forces the rebuild of the Lucene indexes. This is only necessary when synchronisation between database and indexes is lost; when the search functionality is not returning correct results. Authentication credentials can be passed as part of the request body.","title":"System actions"},{"location":"rest-api/admin/#properties","text":"There are a number of system-wide properties that can be updated by administrators, such as the text of any emails sent and the email address from which catalogue emails appear to be sent. Properties are composed of keys and values . Keys can be any string with the following restrictions: Must be lowercase alpha characters No spaces are allowed May include periods ('.') and/or underscores ('_') Must be unique","title":"Properties"},{"location":"rest-api/admin/#getting-properties","text":"Properties can be viewed at the following endpoint: /api/admin/properties If successful, the response body will list the available properties: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"count\" : X , \"items\" : [ { \"id\" : \"c7de1358-a4ce-4d72-abca-04013f7f4acc\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : false , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-10T15:17:05.459Z\" }, { \"id\" : \"76becaa3-da04-40d5-a433-51ed203c77b4\" , \"key\" : \"test.property.public\" , \"value\" : \"Public test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : true , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-10T15:17:05.558Z\" } ] } Notice that properties contain a publiclyVisible flag. This is because properties can be created to either be public or restricted to administrators/systems (the default being false ). Only an authenticated session can use the endpoint above, however an anonymous session may use this endpoint to list all publicly available properties: /api/properties To access a single property, this endpoint is provided: /api/admin/properties/ {propertyId}","title":"Getting properties"},{"location":"rest-api/admin/#created-updating-and-deleting","text":"Properties can be created as follows: /api/admin/properties Request body (JSON) 1 2 3 4 5 6 { \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"publiclyVisible\" : false , \"category\" : \"Test\" } If successful, the new property is returned in the response body including the new property id . Response body (JSON) 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : false , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-11T17:46:47.654Z\" } The property can then be updated with the put endpoint: /api/admin/properties Request body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"publiclyVisible\" : false , \"category\" : \"Test\" } And deleted with the delete endpoint: /api/admin/properties/ {propertyId}","title":"Created, updating and deleting"},{"location":"rest-api/admin/#data-models","text":"The following endpoints provide paginated lists of Data Models (for cleaning / monitoring processes). They list those models which have been deleted, superseded by a new model, and superseded by new documentation, respectively: /api/admin/dataModels/deleted /api/admin/dataModels/modelSuperseded /api/admin/dataModels/documentSuperseded","title":"Data Models"},{"location":"rest-api/admin/#emails","text":"Retrieve the list of emails (recipient, message, date/time) sent by the system: /api/admin/emails","title":"Emails"},{"location":"rest-api/apikeys/","text":"API keys offer an alternative way to authenticate to the Mauro Data Mapper REST API instead of logging in with a username and password and saving session cookies. This is the recommended method for authenticating when you: Have long-running processing scripts which could cause sessions to timeout between calls Need to store authentication details in clear text for an external application to use Each user can create multiple API keys, and so when sharing with multiple applications, can disable access individually. API keys are also configured with a default expiry date for additional security. Creating an API Key \u00b6 API keys may be set up through the web interface or via the API. To generate a first API key, the user must be logged in using a username and password - either through the web interface, or through the REST API. On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu. This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys. For each API belonging to the user, the list displays: Name This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys used for different purposes. Key This is the key itself, which is a UUID, unique to this user. Keys can be copied to your clipboard by clicking 'Copy' on the right of the key box. Expiry date This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be 'refreshed' before expiry. Refreshable Specifies whether an API key can be refreshed once it has expired. Status Details whether an API key is 'Active' or 'Disabled' . To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete. Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes. Using an API Key \u00b6 To use an API Key, simply add it into the headers of any REST API call. Info If you use API keys to authenticate, the session cookies are not used to persist identity and so the key should be passed with every call. The header key should be apiKey and the value should be the UUID value of the API key itself. Using Postman \u00b6 If you are using Postman as a client, there are two ways to configure the API key for a request, which both have the same result. Firstly, select the 'Authorization' tab which will display several fields that you need to complete. From the 'TYPE' dropdown menu, select 'API Key' . In the 'Key' box on the right hand side type apiKey . Enter the value of the API key in the 'Value' field and select 'Header' from the 'Add to' dropdown menu. The API key must be passed in the headers, not in the query parameters, which is the second method. This method sets the headers automatically, although you can also set them manually. Select the 'Headers' tab which will display a list of Keys and Values . Again, set the 'Key' field to apiKey and the 'Value' field to the API key value. Refreshing an expired API key \u00b6 When an API key has expired and it has previously been marked as 'Refreshable' , then it may be refreshed with a new expiry date. To do this, navigate to the list of 'API keys' via your user profile. Identify which API keys have 'API Key expired' in the 'Expiry date' column. Click the three vertical dots to the right of the relevant API key and you will now have the option to 'Refresh API Key' in the dropdown menu. Select this option and then enter a new number of days for expiry. Revoking an API key \u00b6 To revoke a particular API key, you can mark it as 'Disabled' . Navigate to the 'API Keys' list and click the three vertical dots to the right of the relevant API key. Select 'Disable' from the dropdown menu. The same option will allow you to re-enable the key if necessary. Info It is good practise to set up different API keys for each application. In this way it is easy to revoke access to a single application without having to recreate all other keys and update other application settings. Managing keys through the REST API \u00b6 Info Note that API Keys can only be managed by the user that they belong to. Once authenticated , the endpoint for listing existing API keys is: /api/catalogueUsers/ {catalogueUserId} /apiKeys This returns a paginated list of API keys as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"count\" : X , \"items\" : [ { \"id\" : \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\" , \"apiKey\" : \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\" , \"name\" : \"Test API Key\" , \"expiryDate\" : \"2022-02-03\" , \"expired\" : false , \"disabled\" : true , \"refreshable\" : false , \"createdDate\" : \"2021-02-03\" }, ... ] } The parameters are as described above. The id field is the global primary key identifer for the key. To create a new API key, post to the following endpoint: /api/catalogueUsers/ {userId} /apiKeys The body of the post method should be structured as follows: Request body (JSON) 1 2 3 4 5 { \"name\" : \"My Name\" , \"expiresInDays\" : 365 , \"refreshable\" : true } Where the parameters are as described above. To enable an existing, disabled API key, you can use it's ID (as described above), with the following endpoint: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /enable Similarly, to disable an existing, enabled API key, use the following: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /disable To refresh an API key, provide the number of days before the next expiry with the following endpoint: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /refresh/ {expiresInDays} Finally, to delete an API key identified by a particular UUID: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {id}","title":"API Keys"},{"location":"rest-api/apikeys/#creating-an-api-key","text":"API keys may be set up through the web interface or via the API. To generate a first API key, the user must be logged in using a username and password - either through the web interface, or through the REST API. On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu. This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys. For each API belonging to the user, the list displays: Name This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys used for different purposes. Key This is the key itself, which is a UUID, unique to this user. Keys can be copied to your clipboard by clicking 'Copy' on the right of the key box. Expiry date This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be 'refreshed' before expiry. Refreshable Specifies whether an API key can be refreshed once it has expired. Status Details whether an API key is 'Active' or 'Disabled' . To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete. Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes.","title":"Creating an API Key"},{"location":"rest-api/apikeys/#using-an-api-key","text":"To use an API Key, simply add it into the headers of any REST API call. Info If you use API keys to authenticate, the session cookies are not used to persist identity and so the key should be passed with every call. The header key should be apiKey and the value should be the UUID value of the API key itself.","title":"Using an API Key"},{"location":"rest-api/apikeys/#using-postman","text":"If you are using Postman as a client, there are two ways to configure the API key for a request, which both have the same result. Firstly, select the 'Authorization' tab which will display several fields that you need to complete. From the 'TYPE' dropdown menu, select 'API Key' . In the 'Key' box on the right hand side type apiKey . Enter the value of the API key in the 'Value' field and select 'Header' from the 'Add to' dropdown menu. The API key must be passed in the headers, not in the query parameters, which is the second method. This method sets the headers automatically, although you can also set them manually. Select the 'Headers' tab which will display a list of Keys and Values . Again, set the 'Key' field to apiKey and the 'Value' field to the API key value.","title":"Using Postman"},{"location":"rest-api/apikeys/#refreshing-an-expired-api-key","text":"When an API key has expired and it has previously been marked as 'Refreshable' , then it may be refreshed with a new expiry date. To do this, navigate to the list of 'API keys' via your user profile. Identify which API keys have 'API Key expired' in the 'Expiry date' column. Click the three vertical dots to the right of the relevant API key and you will now have the option to 'Refresh API Key' in the dropdown menu. Select this option and then enter a new number of days for expiry.","title":"Refreshing an expired API key"},{"location":"rest-api/apikeys/#revoking-an-api-key","text":"To revoke a particular API key, you can mark it as 'Disabled' . Navigate to the 'API Keys' list and click the three vertical dots to the right of the relevant API key. Select 'Disable' from the dropdown menu. The same option will allow you to re-enable the key if necessary. Info It is good practise to set up different API keys for each application. In this way it is easy to revoke access to a single application without having to recreate all other keys and update other application settings.","title":"Revoking an API key"},{"location":"rest-api/apikeys/#managing-keys-through-the-rest-api","text":"Info Note that API Keys can only be managed by the user that they belong to. Once authenticated , the endpoint for listing existing API keys is: /api/catalogueUsers/ {catalogueUserId} /apiKeys This returns a paginated list of API keys as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"count\" : X , \"items\" : [ { \"id\" : \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\" , \"apiKey\" : \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\" , \"name\" : \"Test API Key\" , \"expiryDate\" : \"2022-02-03\" , \"expired\" : false , \"disabled\" : true , \"refreshable\" : false , \"createdDate\" : \"2021-02-03\" }, ... ] } The parameters are as described above. The id field is the global primary key identifer for the key. To create a new API key, post to the following endpoint: /api/catalogueUsers/ {userId} /apiKeys The body of the post method should be structured as follows: Request body (JSON) 1 2 3 4 5 { \"name\" : \"My Name\" , \"expiresInDays\" : 365 , \"refreshable\" : true } Where the parameters are as described above. To enable an existing, disabled API key, you can use it's ID (as described above), with the following endpoint: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /enable Similarly, to disable an existing, enabled API key, use the following: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /disable To refresh an API key, provide the number of days before the next expiry with the following endpoint: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /refresh/ {expiresInDays} Finally, to delete an API key identified by a particular UUID: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {id}","title":"Managing keys through the REST API"},{"location":"rest-api/authentication/","text":"Mauro Data Mapper stores content that may be either publicly accessible, or have access restricted to particular users or groups. Therefore the majority of API requests can be made as an 'anonymous user' (by passing no session information in the request header), or as a 'logged in' user (by passing a valid session key in the request headers). A request to the login will result in a new session token being generated. This is typically 32 hexadecimal characters in length, and uniquely identifies the current session. These tokens should not be shared, and will automatically expire with 30mins of inactivity. Sessions can be manually terminated through a call to the logout resource. At any point the validity of a session may be checked against the server. Login \u00b6 To login to the server, POST to the following API endpoint: /api/authentication/login The request body should contain the username, and the password. The username is not case-sensitive: Request body (JSON) 1 2 3 4 { \"username\" : \"joe.bloggs@test.com\" , \"password\" : \"pa55w0rd\" } Request body (XML) 1 2 3 4 <user> <username> joe.bloggs@test.com </username> <password> pa55w0rd\" </password> </user> If successful, the response body will contain the user's id , email address, first and last names, and whether or not that user's account has been disabled (typically false in the case of a successful login). Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"id\" : \"01234567-0123-0123-0123-01234567\" , \"emailAddress\" : \"joe.bloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"pending\" : false , \"disabled\" : false , \"createdBy\" : \"admin@test.com\" } One of the response headers will also contain an identifier for the new session. The header is of the form: 1 Set-Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B; Path=/; HttpOnly The session id (JSESSIONID) can be passed to any subsequent request to ensure that the user's credentials are used. To supply the cookie, it should be placed in the Cookie request header: 1 Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B Further requests without the session cookie will be treated as anonymous requests. Session validation \u00b6 In order to validate whether a session is currently active, or has expired (by logging out, or timed-out due to inactivity): /api/session/isAuthenticated No request body or parameters are required for this request. The response will include a true or false value depending on the validity of the session whose JSESSIONID is passed in as part of the request headers. Response body (JSON) 1 2 3 { \"authenticatedSession\" : true } Administration validation \u00b6 In order to validate whether a session is an administrative role: /api/session/isApplicationAdministration No request body or parameters are required for this request. The response will include a true or false value depending on the validity of the session whose JSESSIONID is passed in as part of the request headers. Response body (JSON) 1 2 3 { \"applicationAdministrationSession\" : true } Logout \u00b6 Every session should ideally be closed manually, rather than leaving it to expire through inactivity. In order to close a user session, you should call the logout endpoint, again including the JSESSIONID cookie as part of the request headers: /api/authentication/logout The response should include the status 204: No Content and the successful response will be empty.","title":"Authentication"},{"location":"rest-api/authentication/#login","text":"To login to the server, POST to the following API endpoint: /api/authentication/login The request body should contain the username, and the password. The username is not case-sensitive: Request body (JSON) 1 2 3 4 { \"username\" : \"joe.bloggs@test.com\" , \"password\" : \"pa55w0rd\" } Request body (XML) 1 2 3 4 <user> <username> joe.bloggs@test.com </username> <password> pa55w0rd\" </password> </user> If successful, the response body will contain the user's id , email address, first and last names, and whether or not that user's account has been disabled (typically false in the case of a successful login). Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"id\" : \"01234567-0123-0123-0123-01234567\" , \"emailAddress\" : \"joe.bloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"pending\" : false , \"disabled\" : false , \"createdBy\" : \"admin@test.com\" } One of the response headers will also contain an identifier for the new session. The header is of the form: 1 Set-Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B; Path=/; HttpOnly The session id (JSESSIONID) can be passed to any subsequent request to ensure that the user's credentials are used. To supply the cookie, it should be placed in the Cookie request header: 1 Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B Further requests without the session cookie will be treated as anonymous requests.","title":"Login"},{"location":"rest-api/authentication/#session-validation","text":"In order to validate whether a session is currently active, or has expired (by logging out, or timed-out due to inactivity): /api/session/isAuthenticated No request body or parameters are required for this request. The response will include a true or false value depending on the validity of the session whose JSESSIONID is passed in as part of the request headers. Response body (JSON) 1 2 3 { \"authenticatedSession\" : true }","title":"Session validation"},{"location":"rest-api/authentication/#administration-validation","text":"In order to validate whether a session is an administrative role: /api/session/isApplicationAdministration No request body or parameters are required for this request. The response will include a true or false value depending on the validity of the session whose JSESSIONID is passed in as part of the request headers. Response body (JSON) 1 2 3 { \"applicationAdministrationSession\" : true }","title":"Administration validation"},{"location":"rest-api/authentication/#logout","text":"Every session should ideally be closed manually, rather than leaving it to expire through inactivity. In order to close a user session, you should call the logout endpoint, again including the JSESSIONID cookie as part of the request headers: /api/authentication/logout The response should include the status 204: No Content and the successful response will be empty.","title":"Logout"},{"location":"rest-api/errors/","text":"The Mauro Data Mapper API uses standard HTTP response codes to indicate the success or failure of an API request. In addition, some requests also return additional status information relating to the reasons for any error or failure that has occurred. In general, codes of the form 2XX indicate success, codes of the form 4XX indicate an error with the request, and codes of the form 5XX indicate that an error occurred with the server during the processing of a potentially valid request. Hopefully those in the last category are rare! Further details for each of the common error codes are shown in the tables below. Error code tables \u00b6 Code Meaning Description 200 OK The response succeeded as expected 201 Created The POST method was successful and a new resource was created 204 No Content The server successfully processed the request, but no content was returned - for example when deleting a resource Code Meaning Description 400 Bad Request The server cannot process the request - either a required parameter was missing, or the body was badly formatted 401 Unauthorized The requested resource requires authentication , but none was provided as part of the header information 403 Forbidden The server refused to process the request because the authenticated user does not have the correct permissions 404 Not Found The resource requested could not be found. This may be because the URL is malformed, or because the HTTP method was not permitted for this particular URL - for example PUT on a resource which may not be edited 408 Request Timeout The server gave up waiting for a request. This code may occasionally be seen when the upload of a file takes longer than the server is prepared to wait 409 Conflict The server could not process the request because of some conflict in the current state of the resource. Most commonly this occurs when a user tried to log in, despite already being logged in with a valid session Code Meaning Description 500 Internal Server Error This is a catch-all error message, when the request appears valid but the server was unable to process it. This may well be caused by a bug in the software; such error messages may be reported through our issue-tracking software 502 Bad Gateway This is a system error relating to the server. It may be that the Metadata Catalogue is configured incorrectly, or is otherwise not installed correctly 503 Service Unavailable The API server is currently unavailable. It may have been taken down for maintenance, or is otherwise not running 504 Gateway Timeout See 502 - the server may be badly configured or is otherwise unavailable","title":"Errors"},{"location":"rest-api/errors/#error-code-tables","text":"Code Meaning Description 200 OK The response succeeded as expected 201 Created The POST method was successful and a new resource was created 204 No Content The server successfully processed the request, but no content was returned - for example when deleting a resource Code Meaning Description 400 Bad Request The server cannot process the request - either a required parameter was missing, or the body was badly formatted 401 Unauthorized The requested resource requires authentication , but none was provided as part of the header information 403 Forbidden The server refused to process the request because the authenticated user does not have the correct permissions 404 Not Found The resource requested could not be found. This may be because the URL is malformed, or because the HTTP method was not permitted for this particular URL - for example PUT on a resource which may not be edited 408 Request Timeout The server gave up waiting for a request. This code may occasionally be seen when the upload of a file takes longer than the server is prepared to wait 409 Conflict The server could not process the request because of some conflict in the current state of the resource. Most commonly this occurs when a user tried to log in, despite already being logged in with a valid session Code Meaning Description 500 Internal Server Error This is a catch-all error message, when the request appears valid but the server was unable to process it. This may well be caused by a bug in the software; such error messages may be reported through our issue-tracking software 502 Bad Gateway This is a system error relating to the server. It may be that the Metadata Catalogue is configured incorrectly, or is otherwise not installed correctly 503 Service Unavailable The API server is currently unavailable. It may have been taken down for maintenance, or is otherwise not running 504 Gateway Timeout See 502 - the server may be badly configured or is otherwise unavailable","title":"Error code tables"},{"location":"rest-api/importexport/","text":"A number of plugins exist for importing and exporting Data Models, Data Flows, and Terminologies. The endpoint for each import / export contains the details of the plugin to be used, which includes the namespace, the name, and the version number. Data Model \u00b6 Import \u00b6 The endpoint for importing one or more models is as follows: /api/dataModels/import/ {importerNamespace} / {importerName} / {importerVersion} Each importer defines its own set of parameters, relating to the method of import. For example, an XML or JSON import will require a file upload; a SQL import will require a list of connection parameters in order to connect to a relational database. As the import parameters may involve file attachements, standard practise is to provide upload parameters as multipart/form-data format, which allows the attachment of files. The standard importers available with a default installation are as follows: Namespace Name Version ox.softeng.metadatacatalogue.plugins.excel ExcelDataModelImporterService 1.0.0 ox.softeng.metadatacatalogue.core.spi.xml XmlImporterService 2.2 ox.softeng.metadatacatalogue.core.spi.json JsonImporterService 1.1 ox.softeng.metadatacatalogue.plugins.database.postgres PostgresDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.oracle OracleDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.sqlserver SqlServerDatabaseImporterService 2.0.0 These fall into two basic categories - simple file-based importers, or simple database-connection importers. The parameters for each of these types are detailed in the sections below. Simple file-based importers \u00b6 The simple file-based importers include the Excel, XML and JSON importers. These take the following parameters: Parameter Name Description folderId The UUID identifier for the folder that the new model is to be uploaded to. This is mandatory. finalised A mandatory boolean value determining whether the new model is to be marked as finalised. This determines whether the resulting model can be further edited within the interface. importAsNewDocumentationVersion A mandatory boolean value. If this option is selected, then any models with the same name will be superseded. If this option is not set, then the importer will produce an error if there are existing models with the same label. importFile The file containing the data to be imported - for example an XML file, JSON file, or Excel spreadsheet. All fields are mandatory. Simple database-connection importers (example) \u00b6 In order to connect to a database, fields are required to build the connection string, as well as handle the resulting generated model. Each SQL importer is slightly different, but the SQL Server importer serves as an adequate example: Parameter Name Description folderId The UUID identifier for the folder that the new model is to be uploaded to. This is mandatory. finalised A mandatory boolean value determining whether the new model is to be marked as finalised. This determines whether the resulting model can be further edited within the interface. importAsNewDocumentationVersion A mandatory boolean value. If this option is selected, then any models with the same name will be superseded. If this option is not set, then the importer will produce an error if there are existing models with the same label. databaseHost The hostname of the server that is running the database databasePort The port that the database is accessed through. If none is set, then the default port for the specified database type will be used. databaseNames A comma-separated list of database names that are to be analysed and imported. If multiple databases are specified, the same username and password will be used for all. databaseUsername The usesrname used to connect to the database databasePassword The password used to connect to the database domain The User Domain name for SQL Server. This should be used rather than prefixing the username with <DOMAIN>/<username> databasesSSL Whether SSL should be used to connect to the database. The default is false. useNtlmv2 Whether to use NLTMv2 when connecting to the database. The default is false. dataModelName If a single database is imported, this field can be used to override its name schemaNames A comma-separated list of the schema names to import. If not supplied, then all schemas other than 'sys' and 'INFORMATION_SCHEMA' will be imported Other database-connecting import plugins provide a similar list of parameters, to be documented later. Export \u00b6 Data Flow \u00b6 Terminology \u00b6","title":"Import / Export"},{"location":"rest-api/importexport/#data-model","text":"","title":"Data Model"},{"location":"rest-api/importexport/#import","text":"The endpoint for importing one or more models is as follows: /api/dataModels/import/ {importerNamespace} / {importerName} / {importerVersion} Each importer defines its own set of parameters, relating to the method of import. For example, an XML or JSON import will require a file upload; a SQL import will require a list of connection parameters in order to connect to a relational database. As the import parameters may involve file attachements, standard practise is to provide upload parameters as multipart/form-data format, which allows the attachment of files. The standard importers available with a default installation are as follows: Namespace Name Version ox.softeng.metadatacatalogue.plugins.excel ExcelDataModelImporterService 1.0.0 ox.softeng.metadatacatalogue.core.spi.xml XmlImporterService 2.2 ox.softeng.metadatacatalogue.core.spi.json JsonImporterService 1.1 ox.softeng.metadatacatalogue.plugins.database.postgres PostgresDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.oracle OracleDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.sqlserver SqlServerDatabaseImporterService 2.0.0 These fall into two basic categories - simple file-based importers, or simple database-connection importers. The parameters for each of these types are detailed in the sections below.","title":"Import"},{"location":"rest-api/importexport/#simple-file-based-importers","text":"The simple file-based importers include the Excel, XML and JSON importers. These take the following parameters: Parameter Name Description folderId The UUID identifier for the folder that the new model is to be uploaded to. This is mandatory. finalised A mandatory boolean value determining whether the new model is to be marked as finalised. This determines whether the resulting model can be further edited within the interface. importAsNewDocumentationVersion A mandatory boolean value. If this option is selected, then any models with the same name will be superseded. If this option is not set, then the importer will produce an error if there are existing models with the same label. importFile The file containing the data to be imported - for example an XML file, JSON file, or Excel spreadsheet. All fields are mandatory.","title":"Simple file-based importers"},{"location":"rest-api/importexport/#simple-database-connection-importers-example","text":"In order to connect to a database, fields are required to build the connection string, as well as handle the resulting generated model. Each SQL importer is slightly different, but the SQL Server importer serves as an adequate example: Parameter Name Description folderId The UUID identifier for the folder that the new model is to be uploaded to. This is mandatory. finalised A mandatory boolean value determining whether the new model is to be marked as finalised. This determines whether the resulting model can be further edited within the interface. importAsNewDocumentationVersion A mandatory boolean value. If this option is selected, then any models with the same name will be superseded. If this option is not set, then the importer will produce an error if there are existing models with the same label. databaseHost The hostname of the server that is running the database databasePort The port that the database is accessed through. If none is set, then the default port for the specified database type will be used. databaseNames A comma-separated list of database names that are to be analysed and imported. If multiple databases are specified, the same username and password will be used for all. databaseUsername The usesrname used to connect to the database databasePassword The password used to connect to the database domain The User Domain name for SQL Server. This should be used rather than prefixing the username with <DOMAIN>/<username> databasesSSL Whether SSL should be used to connect to the database. The default is false. useNtlmv2 Whether to use NLTMv2 when connecting to the database. The default is false. dataModelName If a single database is imported, this field can be used to override its name schemaNames A comma-separated list of the schema names to import. If not supplied, then all schemas other than 'sys' and 'INFORMATION_SCHEMA' will be imported Other database-connecting import plugins provide a similar list of parameters, to be documented later.","title":"Simple database-connection importers (example)"},{"location":"rest-api/importexport/#export","text":"","title":"Export"},{"location":"rest-api/importexport/#data-flow","text":"","title":"Data Flow"},{"location":"rest-api/importexport/#terminology","text":"","title":"Terminology"},{"location":"rest-api/introduction/","text":"The Mauro Data Mapper API conforms to standard REST principles. The API has resource-oriented URLs, accepts XML and JSON body content (or form-encoded parameters where applicable), and can return data in XML or JSON formats. Each call uses standard HTTP response codes, authentication, and verbs. Requests \u00b6 To make a REST API request, you combine: The HTTP method: GET , POST , PUT , PATCH or DELETE The URL to the API service - for example http://modelcatalogue.cs.ox.ac.uk/demo/api The URL to a resource to query, update or delete One or more HTTP request headers , for example the identifier of any session token, or a request to save data back in XML or JSON format Most calls may also require a JSON or XML body representing any new or updated data, or query parameters to filter or restrict the response. HTTP request headers \u00b6 The commonly-used HTTP request headers used are: Accept \u00b6 This header determines the format of the response body for those requests with structured output. The syntax is: 1 Accept: application/<format> Where <format> can be either xml or json . By default, the format of the response body will match that of the request body, where applicable. Content-Type \u00b6 This header specifies the format of the request body, where applicable. The syntax is: 1 Content-Type: application/<format> Where <format> can be either xml or json . By default, the request body is assumed to be JSON unless otherwise specified. Cookie \u00b6 This header stores the session identifier which persists a login between calls. For example, having received a session cookie during login , the token can be used to validate the user. 1 Cookie: JSESSIONID=<sessionid> Typically, a session identifier is 32 characters long and uses hexadecimal characters 0-9 , A-F . Tools \u00b6 We use Postman for testing API calls during development. It has an intuitive interface that lets you set parameters, headers, and message bodies, and preview structured responses. It can also be used as part of an automated testing or debugging requests. If you're looking for a more lightweight solution, curl is a suitable command-line tool which can be easily configured to make complex REST API requests. In this set of documentation, requests are illustrated with the appropriate curl command. Testing \u00b6 There is a test API resource which will show whether the server API is running correctly, and whether the client has been correctly configured. To test this using curl , run the following command: 1 curl -X GET http://localhost:8080/api/test This will return the following JSON: Response body (JSON) 1 2 3 4 5 6 7 { \"message\" : \"Not Found\" , \"error\" : 404 , \"path\" : null , \"object\" : null , \"id\" : null }","title":"Introduction"},{"location":"rest-api/introduction/#requests","text":"To make a REST API request, you combine: The HTTP method: GET , POST , PUT , PATCH or DELETE The URL to the API service - for example http://modelcatalogue.cs.ox.ac.uk/demo/api The URL to a resource to query, update or delete One or more HTTP request headers , for example the identifier of any session token, or a request to save data back in XML or JSON format Most calls may also require a JSON or XML body representing any new or updated data, or query parameters to filter or restrict the response.","title":"Requests"},{"location":"rest-api/introduction/#http-request-headers","text":"The commonly-used HTTP request headers used are:","title":"HTTP request headers"},{"location":"rest-api/introduction/#accept","text":"This header determines the format of the response body for those requests with structured output. The syntax is: 1 Accept: application/<format> Where <format> can be either xml or json . By default, the format of the response body will match that of the request body, where applicable.","title":"Accept"},{"location":"rest-api/introduction/#content-type","text":"This header specifies the format of the request body, where applicable. The syntax is: 1 Content-Type: application/<format> Where <format> can be either xml or json . By default, the request body is assumed to be JSON unless otherwise specified.","title":"Content-Type"},{"location":"rest-api/introduction/#cookie","text":"This header stores the session identifier which persists a login between calls. For example, having received a session cookie during login , the token can be used to validate the user. 1 Cookie: JSESSIONID=<sessionid> Typically, a session identifier is 32 characters long and uses hexadecimal characters 0-9 , A-F .","title":"Cookie"},{"location":"rest-api/introduction/#tools","text":"We use Postman for testing API calls during development. It has an intuitive interface that lets you set parameters, headers, and message bodies, and preview structured responses. It can also be used as part of an automated testing or debugging requests. If you're looking for a more lightweight solution, curl is a suitable command-line tool which can be easily configured to make complex REST API requests. In this set of documentation, requests are illustrated with the appropriate curl command.","title":"Tools"},{"location":"rest-api/introduction/#testing","text":"There is a test API resource which will show whether the server API is running correctly, and whether the client has been correctly configured. To test this using curl , run the following command: 1 curl -X GET http://localhost:8080/api/test This will return the following JSON: Response body (JSON) 1 2 3 4 5 6 7 { \"message\" : \"Not Found\" , \"error\" : 404 , \"path\" : null , \"object\" : null , \"id\" : null }","title":"Testing"},{"location":"rest-api/pagination/","text":"The majority of requests for multiple objects have parameters to manage pagination. By returning results in separate pages, we can minimise network traffic and reduce the load on the server. The size or limit ( max ) and starting position ( offset ) of each page can be passed in as a parameter to the query. The response will always return the total number of objects, along with a list of 'items' corresponding to the specified 'page' of results. Parameter format \u00b6 In these examples we consider the endpoint endpoint for listing all folders: /api/folders To manually specify the offset and max values, these should be passed as form parameters - for example the request: /api/folders?offset=10&max=5 Would return folders 10-14 inclusive in the overall list. To specify that all results should be returned, the boolean parameter all can be passed - for example the request: /api/folders?all=true Will return the complete list of visible folders. The all parameter is an alternative, and should not be specified at the same time as offset and max . Response format \u00b6 Again consider the endpoint endpoint for listing all folders described above. The response body would look something like: Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"count\" : n , \"items\" : [ { ... }, ... ] } Where n is the total number of folders available. The number of items returned will be at most max items. Default settings \u00b6 If no parameters are passed, the default values are: offset : 0 max : 10","title":"Pagination"},{"location":"rest-api/pagination/#parameter-format","text":"In these examples we consider the endpoint endpoint for listing all folders: /api/folders To manually specify the offset and max values, these should be passed as form parameters - for example the request: /api/folders?offset=10&max=5 Would return folders 10-14 inclusive in the overall list. To specify that all results should be returned, the boolean parameter all can be passed - for example the request: /api/folders?all=true Will return the complete list of visible folders. The all parameter is an alternative, and should not be specified at the same time as offset and max .","title":"Parameter format"},{"location":"rest-api/pagination/#response-format","text":"Again consider the endpoint endpoint for listing all folders described above. The response body would look something like: Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"count\" : n , \"items\" : [ { ... }, ... ] } Where n is the total number of folders available. The number of items returned will be at most max items.","title":"Response format"},{"location":"rest-api/pagination/#default-settings","text":"If no parameters are passed, the default values are: offset : 0 max : 10","title":"Default settings"},{"location":"rest-api/postman/","text":"The Postman app is a tool for working with external APIs. Originally a plugin for Google Chrome, it now comes as a desktop app for all operating systems, as well as providing a web version. Downloading \u00b6 The Mauro Postman repository provides definitions for Postman in order to test the Mauro APIs and contains sample environment configurations. To use, simply clone the repository into your local system, or download the files as follows: Navigate to the main branch of the GitHub repository Choose a folder to match the version of mdm-core that you intend to run against Download the two listed JSON files - one for the 'Collection' and another for the 'Environment' Using the collection \u00b6 Within the Postman app, choose 'File' -> 'Import...' and under the 'File' tab, choose 'Upload Files' . Select the collection JSON file, and click 'Import' to import the new collection. If you've previously imported an older version of this collection, you are asked whether you wish to import as a new copy, or overwrite the previous version. In the 'Collections' tab on the left hand side, you can now see a number of folders and subfolders containing configuration for Mauro API endpoints. By clicking on each you can view the endpoint, and optionally execute it against a given server. Parameters to the call, including the server name, are indicated by double braces {{ ... }} - e.g. {{base_url}} in the URL of the endpoint. To instantiate these parameters, you can either replace the text manually, or use an environment to provide consistent replacements across all endpoints. The Mauro Postman Environment provides some default values which can be customised. Using the environment \u00b6 To import the Mauro environment, choose 'File' -> 'Import...' and under the 'File' tab, choose 'Upload Files' . Select the environment JSON file, and click 'Import' to import the new environment. Once imported, the enviroment can be selected from the drop-down at the top-right of the screen. To edit the environment, click 'Manage Envronments' . A number of parameters have been pre-set and can be edited here; new parameters can be added to suit your own usage. Swagger, OpenAPI \u00b6 We are yet to create a description of our APIs compatible with Swagger , or OpenAPI . However, there are tools that should automate the conversion of Postman collections to these formats - albeit in a manner specific to a particular environment or use case. Some example tools include: APIMatic REST United APITransform The Mauro team have limited experience with these tools and so would welcome any feedback! Submitting changes \u00b6 The Postman library is not yet complete. There are endpoints undocumented, and plenty of improvements that could be made to the environment, or particular usage scenarios we've not yet catered for. If you've made changes to the Postman Library and think they would be of more general use, please do consider submitting a pull request, so we can make them more widely available.","title":"Postman Library"},{"location":"rest-api/postman/#downloading","text":"The Mauro Postman repository provides definitions for Postman in order to test the Mauro APIs and contains sample environment configurations. To use, simply clone the repository into your local system, or download the files as follows: Navigate to the main branch of the GitHub repository Choose a folder to match the version of mdm-core that you intend to run against Download the two listed JSON files - one for the 'Collection' and another for the 'Environment'","title":"Downloading"},{"location":"rest-api/postman/#using-the-collection","text":"Within the Postman app, choose 'File' -> 'Import...' and under the 'File' tab, choose 'Upload Files' . Select the collection JSON file, and click 'Import' to import the new collection. If you've previously imported an older version of this collection, you are asked whether you wish to import as a new copy, or overwrite the previous version. In the 'Collections' tab on the left hand side, you can now see a number of folders and subfolders containing configuration for Mauro API endpoints. By clicking on each you can view the endpoint, and optionally execute it against a given server. Parameters to the call, including the server name, are indicated by double braces {{ ... }} - e.g. {{base_url}} in the URL of the endpoint. To instantiate these parameters, you can either replace the text manually, or use an environment to provide consistent replacements across all endpoints. The Mauro Postman Environment provides some default values which can be customised.","title":"Using the collection"},{"location":"rest-api/postman/#using-the-environment","text":"To import the Mauro environment, choose 'File' -> 'Import...' and under the 'File' tab, choose 'Upload Files' . Select the environment JSON file, and click 'Import' to import the new environment. Once imported, the enviroment can be selected from the drop-down at the top-right of the screen. To edit the environment, click 'Manage Envronments' . A number of parameters have been pre-set and can be edited here; new parameters can be added to suit your own usage.","title":"Using the environment"},{"location":"rest-api/postman/#swagger-openapi","text":"We are yet to create a description of our APIs compatible with Swagger , or OpenAPI . However, there are tools that should automate the conversion of Postman collections to these formats - albeit in a manner specific to a particular environment or use case. Some example tools include: APIMatic REST United APITransform The Mauro team have limited experience with these tools and so would welcome any feedback!","title":"Swagger, OpenAPI"},{"location":"rest-api/postman/#submitting-changes","text":"The Postman library is not yet complete. There are endpoints undocumented, and plenty of improvements that could be made to the environment, or particular usage scenarios we've not yet catered for. If you've made changes to the Postman Library and think they would be of more general use, please do consider submitting a pull request, so we can make them more widely available.","title":"Submitting changes"},{"location":"rest-api/trees/","text":"","title":"Trees"},{"location":"rest-api/resources/catalogue-item/","text":"A Catalogue Item in the catalogue is an abstract class containing properties that are common to most objects in the catalogue - for example DataModels, DataClasses, DataElements, DataTypes, EnumerationValues, Terminologies, etc. These properties include metadata (properties) , summary metadata , permissions , annotations (comments) and so on. In some cases the url for each endpoint uses the word 'facet'; in others the data type (DataModel, DataClass, etc) are used. This page lists all the endpoints and describes the structure of each property. Metadata \u00b6 The metadata , or properties , of a Catalogue Item are extensible key/value pairs to store any further information about an object - including technical properies, or field conforming to an external model. A single item of metadata is structured as follows: Response body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"c9a36d30-2c6a-4dd0-a792-a337a2eca9c8\" , \"namespace\" : \"ox.softeng.metadatacatalogue.dataloaders.hdf\" , \"key\" : \"Volumes\" , \"value\" : \"Varies annually: in 2013/14, 18.2m finished consultant episodes (FCEs) and 15.5m Finished Admission Episodes (FAEs)\" , \"lastUpdated\" : \"2019-10-03T09:15:12.082Z\" } The fields are as follows: id (UUID): The unique identifier of this property namespace (String): a namespace used to group particular properties - and can be used to filter properties for particular uses key (String): the title or label of this property. The combination of namespace and key should be unique for this object. value (String): the value that this property holds. This field may take HTML or MarkDown syntax, and may include links to other objects in the catalogue. lastUpdated (DateTime): The date/time when this Metadata property was last modified The endpoints for using metadata properties are listed below. To retrieve all the properties for a particular object, use the following endpoint. The metadata properties are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata Where {catalogueItemDomainType} can be one of: folders , dataModels , dataClasses , dataTypes , terminologies , terms , or referenceDataModels To get a specific property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} To add a new property to an object you have write-access to, post a structure similar to the one displayed above (ignoring id and lastUpdated fields, which will be automatically set to the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata To edit an existing property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} To delete an existing property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} The following endpoint returns all known namespaces for a particular object (given by the id field). To find all namespaces across the whole catalogue, the final component of the URL can be left off. /api/metadata/namespaces/ {id} ? Permissions \u00b6 Logged in users may query to discover who is able to read or write a particular object (that they themselves have read-access to). The structure of a response is as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"readableByEveryone\" : false , \"readableByAuthenticated\" : true , \"readableByGroups\" : [], \"writeableByGroups\" : [ { \"id\" : \"cb1b7f4e-6955-41ba-8f91-2ca92b97c189\" , \"label\" : \"Test Group\" , \"createdBy\" : { \"id\" : \"dc7a7c25-5622-4cb0-869f-6d0e688b490f\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } } ], \"readableByUsers\" : [ { \"id\" : \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } ], \"writeableByUsers\" : [ { \"id\" : \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } ] } The fields are as follows: readableByEveryone (Boolean): whether the object in question is publicly available - i.e. can be read by any un-authenticated user of the system readableByAuthenticated (Boolean): whether the object in question can be read by any authenticated (logged-in) user of the system readableByGroups (Set(Group)): the set of groups who have permission to read a particular object. The group has a label, an identifier, and the details of the user responsible for creating that group writeableByGroups (Set(Group)): the set of groups who have permission to edit a particular object. Note that this set of groups is always a subset of the groups listed in readableByGroups readableByUsers (Set(User)): the set of users who have permission to read a particular object. The user has an identifier, first name, last name, email address and flag indicating whether their access is currently valid or disabled writeableByGroups (Set(Group)): the set of users who have permission to edit a particular object. Note that this set of users is always a subset of the users listed in readableByUsers Note Note that read/write permissions are propagated through folders and sub-folders, and the list of permissions given is the inferred list. So changes to that list may not always have an affect if they are contradicted by another assertion further up the tree. The endpoint for getting the permissions each of DataModel, ReferenceDataModel, Folder, CodeSet and Classifier are listed below. The details for updating permissions are listed on their respective pages. /api/dataModels/ {dataModelId} /permissions /api/referenceDataModels/ {referenceDataModelId} /permissions /api/folders/ {folderId} /permissions /api/codeSets/ {codeSetId} /permissions /api/classifiers/ {classifierId} /permissions Annotations \u00b6 Annotations, or comments, can be attached to any item in the catalogue. The structure is as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \"count\" : 2 , \"items\" : [ { \"id\" : \"da3d6229-b152-4cbb-8667-eede523c7eb1\" , \"description\" : \"DataModel finalised by Joe Bloggs on 2018-09-28T20:21:35.995Z\" , \"createdBy\" : { \"id\" : \"5b96991a-d350-4470-958a-29bfac557ed0\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false }, \"lastUpdated\" : \"2018-09-28T20:21:37.655Z\" , \"label\" : \"Finalised Model\" }, { \"id\" : \"670e7c31-00fd-425f-903f-6d024845e63e\" , \"createdBy\" : { \"id\" : \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false }, \"lastUpdated\" : \"2018-07-17T15:51:45.643Z\" , \"label\" : \"Is this model is ready for finalisation?\" } ] } Listing annotations \u00b6 To get all the annotations for a particular object, use the following endpoint. The results are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations Where {catalogueItemDomainType} can be one of: folders , dataModels , dataClasses , dataTypes , terminologies , terms , or referenceDataModels To get the details of a particular annotation / comment, whose id is known, use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {id} To create a new top-level annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and lastUpdated fields) /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations To delete an annotation / comment whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {id} Child annotations (responses) \u00b6 Comments can have child comments (or replies). To get all the child comments for a particular comment, use the following endpoint. The results are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations To get the details of a particular child annotation / comment, whose id is known, use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations/ {id} To create a new child annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and lastUpdated fields) /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations To delete a child annotation / comment whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations/ {id} Searching \u00b6 An advanced search is powered by Lucene. The parameters for an advanced search can be provided as query parameters of a get request, as follows: Parameter Description searchTerm (String): the search string - this can take a number of standard search operators - for example \"smoking + pregnancy\" limit (Integer): the number of results returned in a paginated list offset (Integer): the index of the first result returned in a paginated list domainTypes (Set(String)): the catalogue object types that should be searched. These can include: DataModel , DataClass , DataElement , DataType , EnumerationType . labelOnly (Boolean): whether the search should only query the label of all objects dataModelTypes (Set(String)): the types of data model that should be searched - for example Data Asset , Data Standard classifiers (Set(String)): a set of classifier labels, such that all results must be classified by one of those tags lastUpdatedAfter (DateTime): Only include objects in the search results if they have been modified more recently than the given date lastUpdatedBefore (DateTime): Only include objects in the search results if they have been modified earlier than the given date createdAfter (DateTime): Only include objects in the search results if they were created more recently than the given date createdBefore (DateTime): Only include objects in the search results if they were created earlier than the given date The response will be a paginated list of items, where each item has the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\" : \"127bdf61-cbfe-47dc-9854-fdce276f13bf\" , \"domainType\" : \"DataElement\" , \"label\" : \"AGE AT ONSET OF SYMPTOMS (CHILDREN TEENAGERS AND YOUNG ADULTS CANCER)\" , \"description\" : \"AGE BAND AT SMOKING QUIT DATE is derived as the number of completed years between the PERSON BIRTH DATE of the PERSON and the SMOKING QUIT DATE of the Person Stop Smoking Episode. Permitted National Codes: 01 Under 18 years of age 02 18 to 34 years of age 03 35 - 44 years of age 04 45 - 59 years of age 05 60 and over years of age\" , \"breadcrumbs\" : [ { \"id\" : \"078955c7-6c0f-4fc2-a30e-55629a85b9da\" , \"label\" : \"NHS Data Dictionary\" , \"domainType\" : \"DataModel\" , \"finalised\" : true }, { \"id\" : \"012e8dd5-b4b1-4d26-82aa-17430baf2e2b\" , \"label\" : \"All Data Elements\" , \"domainType\" : \"DataClass\" } ] } where the fields are defined as follows: id (UUID): The identifier of the returned object domainType (String): The type of the returned object - for example \"DataModel\", \"DataClass\", \"DataElement\" label (String): The name of the returned object description (String): The description of the returned object. This may include formatting specified in HTML, or MarkDown. breadcrumbs (List(Breadcrumb)): An ordered list of, e.g. DataModels and DataClasses to show the location of an object in the hierarchy of a model. This will include, for each component of the breadcrumb, an id , a label and a domainType . To search across the whole catalogue, use the following endpoint, optionally passing the above query parameters: /api/tree/folders/search Similarly, to search within a particular data model, use the following: /api/dataModels/ {dataModelId} /search Finally, to search within a specific Data Class, use the following: /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /search Item history \u00b6 The edit history for various catalogue items can be retrieved using the endpoints listed below. The format of a response is a paginated list of edits, with the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 { \"dateCreated\" : \"2018-07-17T15:53:17.276Z\" , \"createdBy\" : { \"id\" : \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\" , \"emailAddress\" : \"ollie.freeman@gmail.com\" , \"firstName\" : \"Oliver\" , \"lastName\" : \"Freeman\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } \"description\" : \"[Data Standard:HIC: Hepatitis v2.0.0] changed properties [folder]\" } The fields have the following definition: dateCreated (DateTime): the date and time when this modification was made createdBy (User): the user responsible for making the edit. This will include their id , emailAddress , firstName , lastName **, **userRole and whether the user account is currently disabled . It may also include the profile image of the user in question description (String): The human-readable description of the edit made. These descriptions are automatically generated by the catalogue The endpoints for getting the edit history for each of DataModel, Terminology, Folder, CodeSet, Classifier and UserGroup are listed below. /api/dataModels/ {dataModelId} /edits /api/terminologies/ {terminologyId} /edits /api/folders/ {folderId} /edits /api/codeSets/ {codeSetId} /edits /api/classifiers/ {classifierId} /edits /api/userGroups/ {userGroupId} /edits Reference files \u00b6 Reference files (or attachments) can be stored alongside various catalogue items to supplement information about the catalogue item. Reference files have the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 { \"id\" : \"eea67c19-1833-4125-9934-b06f45844c20\" , \"domainType\" : \"ReferenceFile\" , \"fileName\" : \"uploadedFile.png\" , \"fileSize\" : 80899 , \"fileType\" : \"image/png\" , \"lastUpdated\" : \"2021-05-13T12:50:37.523Z\" } The fields have the following definition: id (UUID): The identifier of the returned object domainType (String): The type of the returned object - in this case, \"ReferenceFile\" fileName (String): The name of the uploaded file fileSize (Number): The size of the uploaded reference file, in bytes fileType (String): The MIME type of the uploaded reference file lastUpdated (DateTime): the date and time when this modification was made To upload and attach a new reference file to a catalogue item, use the following endpoint and request payload: /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles Request body (JSON) 1 2 3 4 5 6 7 8 { \"fileName\" : \"uploadedFile.png\" , \"fileSize\" : 80899 , \"fileType\" : \"image/png\" , \"fileContents\" : [ // Array of bytes ] } Where {catalogueItemDomainType} can be one of: dataModels , terminologies , codeSets , or referenceDataModels To get either a paginated list of reference files for a catalogue item, or an individual reference file known by {id} : /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles/ {id} To delete a reference file from a catalogue item whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles/ {id} Summary Metadata \u00b6 /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} Summary Metadata Reports \u00b6 /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id}","title":"Catalogue item"},{"location":"rest-api/resources/catalogue-item/#metadata","text":"The metadata , or properties , of a Catalogue Item are extensible key/value pairs to store any further information about an object - including technical properies, or field conforming to an external model. A single item of metadata is structured as follows: Response body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"c9a36d30-2c6a-4dd0-a792-a337a2eca9c8\" , \"namespace\" : \"ox.softeng.metadatacatalogue.dataloaders.hdf\" , \"key\" : \"Volumes\" , \"value\" : \"Varies annually: in 2013/14, 18.2m finished consultant episodes (FCEs) and 15.5m Finished Admission Episodes (FAEs)\" , \"lastUpdated\" : \"2019-10-03T09:15:12.082Z\" } The fields are as follows: id (UUID): The unique identifier of this property namespace (String): a namespace used to group particular properties - and can be used to filter properties for particular uses key (String): the title or label of this property. The combination of namespace and key should be unique for this object. value (String): the value that this property holds. This field may take HTML or MarkDown syntax, and may include links to other objects in the catalogue. lastUpdated (DateTime): The date/time when this Metadata property was last modified The endpoints for using metadata properties are listed below. To retrieve all the properties for a particular object, use the following endpoint. The metadata properties are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata Where {catalogueItemDomainType} can be one of: folders , dataModels , dataClasses , dataTypes , terminologies , terms , or referenceDataModels To get a specific property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} To add a new property to an object you have write-access to, post a structure similar to the one displayed above (ignoring id and lastUpdated fields, which will be automatically set to the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata To edit an existing property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} To delete an existing property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} The following endpoint returns all known namespaces for a particular object (given by the id field). To find all namespaces across the whole catalogue, the final component of the URL can be left off. /api/metadata/namespaces/ {id} ?","title":"Metadata"},{"location":"rest-api/resources/catalogue-item/#permissions","text":"Logged in users may query to discover who is able to read or write a particular object (that they themselves have read-access to). The structure of a response is as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"readableByEveryone\" : false , \"readableByAuthenticated\" : true , \"readableByGroups\" : [], \"writeableByGroups\" : [ { \"id\" : \"cb1b7f4e-6955-41ba-8f91-2ca92b97c189\" , \"label\" : \"Test Group\" , \"createdBy\" : { \"id\" : \"dc7a7c25-5622-4cb0-869f-6d0e688b490f\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } } ], \"readableByUsers\" : [ { \"id\" : \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } ], \"writeableByUsers\" : [ { \"id\" : \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } ] } The fields are as follows: readableByEveryone (Boolean): whether the object in question is publicly available - i.e. can be read by any un-authenticated user of the system readableByAuthenticated (Boolean): whether the object in question can be read by any authenticated (logged-in) user of the system readableByGroups (Set(Group)): the set of groups who have permission to read a particular object. The group has a label, an identifier, and the details of the user responsible for creating that group writeableByGroups (Set(Group)): the set of groups who have permission to edit a particular object. Note that this set of groups is always a subset of the groups listed in readableByGroups readableByUsers (Set(User)): the set of users who have permission to read a particular object. The user has an identifier, first name, last name, email address and flag indicating whether their access is currently valid or disabled writeableByGroups (Set(Group)): the set of users who have permission to edit a particular object. Note that this set of users is always a subset of the users listed in readableByUsers Note Note that read/write permissions are propagated through folders and sub-folders, and the list of permissions given is the inferred list. So changes to that list may not always have an affect if they are contradicted by another assertion further up the tree. The endpoint for getting the permissions each of DataModel, ReferenceDataModel, Folder, CodeSet and Classifier are listed below. The details for updating permissions are listed on their respective pages. /api/dataModels/ {dataModelId} /permissions /api/referenceDataModels/ {referenceDataModelId} /permissions /api/folders/ {folderId} /permissions /api/codeSets/ {codeSetId} /permissions /api/classifiers/ {classifierId} /permissions","title":"Permissions"},{"location":"rest-api/resources/catalogue-item/#annotations","text":"Annotations, or comments, can be attached to any item in the catalogue. The structure is as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \"count\" : 2 , \"items\" : [ { \"id\" : \"da3d6229-b152-4cbb-8667-eede523c7eb1\" , \"description\" : \"DataModel finalised by Joe Bloggs on 2018-09-28T20:21:35.995Z\" , \"createdBy\" : { \"id\" : \"5b96991a-d350-4470-958a-29bfac557ed0\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false }, \"lastUpdated\" : \"2018-09-28T20:21:37.655Z\" , \"label\" : \"Finalised Model\" }, { \"id\" : \"670e7c31-00fd-425f-903f-6d024845e63e\" , \"createdBy\" : { \"id\" : \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false }, \"lastUpdated\" : \"2018-07-17T15:51:45.643Z\" , \"label\" : \"Is this model is ready for finalisation?\" } ] }","title":"Annotations"},{"location":"rest-api/resources/catalogue-item/#listing-annotations","text":"To get all the annotations for a particular object, use the following endpoint. The results are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations Where {catalogueItemDomainType} can be one of: folders , dataModels , dataClasses , dataTypes , terminologies , terms , or referenceDataModels To get the details of a particular annotation / comment, whose id is known, use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {id} To create a new top-level annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and lastUpdated fields) /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations To delete an annotation / comment whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {id}","title":"Listing annotations"},{"location":"rest-api/resources/catalogue-item/#child-annotations-responses","text":"Comments can have child comments (or replies). To get all the child comments for a particular comment, use the following endpoint. The results are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations To get the details of a particular child annotation / comment, whose id is known, use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations/ {id} To create a new child annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and lastUpdated fields) /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations To delete a child annotation / comment whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations/ {id}","title":"Child annotations (responses)"},{"location":"rest-api/resources/catalogue-item/#searching","text":"An advanced search is powered by Lucene. The parameters for an advanced search can be provided as query parameters of a get request, as follows: Parameter Description searchTerm (String): the search string - this can take a number of standard search operators - for example \"smoking + pregnancy\" limit (Integer): the number of results returned in a paginated list offset (Integer): the index of the first result returned in a paginated list domainTypes (Set(String)): the catalogue object types that should be searched. These can include: DataModel , DataClass , DataElement , DataType , EnumerationType . labelOnly (Boolean): whether the search should only query the label of all objects dataModelTypes (Set(String)): the types of data model that should be searched - for example Data Asset , Data Standard classifiers (Set(String)): a set of classifier labels, such that all results must be classified by one of those tags lastUpdatedAfter (DateTime): Only include objects in the search results if they have been modified more recently than the given date lastUpdatedBefore (DateTime): Only include objects in the search results if they have been modified earlier than the given date createdAfter (DateTime): Only include objects in the search results if they were created more recently than the given date createdBefore (DateTime): Only include objects in the search results if they were created earlier than the given date The response will be a paginated list of items, where each item has the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\" : \"127bdf61-cbfe-47dc-9854-fdce276f13bf\" , \"domainType\" : \"DataElement\" , \"label\" : \"AGE AT ONSET OF SYMPTOMS (CHILDREN TEENAGERS AND YOUNG ADULTS CANCER)\" , \"description\" : \"AGE BAND AT SMOKING QUIT DATE is derived as the number of completed years between the PERSON BIRTH DATE of the PERSON and the SMOKING QUIT DATE of the Person Stop Smoking Episode. Permitted National Codes: 01 Under 18 years of age 02 18 to 34 years of age 03 35 - 44 years of age 04 45 - 59 years of age 05 60 and over years of age\" , \"breadcrumbs\" : [ { \"id\" : \"078955c7-6c0f-4fc2-a30e-55629a85b9da\" , \"label\" : \"NHS Data Dictionary\" , \"domainType\" : \"DataModel\" , \"finalised\" : true }, { \"id\" : \"012e8dd5-b4b1-4d26-82aa-17430baf2e2b\" , \"label\" : \"All Data Elements\" , \"domainType\" : \"DataClass\" } ] } where the fields are defined as follows: id (UUID): The identifier of the returned object domainType (String): The type of the returned object - for example \"DataModel\", \"DataClass\", \"DataElement\" label (String): The name of the returned object description (String): The description of the returned object. This may include formatting specified in HTML, or MarkDown. breadcrumbs (List(Breadcrumb)): An ordered list of, e.g. DataModels and DataClasses to show the location of an object in the hierarchy of a model. This will include, for each component of the breadcrumb, an id , a label and a domainType . To search across the whole catalogue, use the following endpoint, optionally passing the above query parameters: /api/tree/folders/search Similarly, to search within a particular data model, use the following: /api/dataModels/ {dataModelId} /search Finally, to search within a specific Data Class, use the following: /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /search","title":"Searching"},{"location":"rest-api/resources/catalogue-item/#item-history","text":"The edit history for various catalogue items can be retrieved using the endpoints listed below. The format of a response is a paginated list of edits, with the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 { \"dateCreated\" : \"2018-07-17T15:53:17.276Z\" , \"createdBy\" : { \"id\" : \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\" , \"emailAddress\" : \"ollie.freeman@gmail.com\" , \"firstName\" : \"Oliver\" , \"lastName\" : \"Freeman\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } \"description\" : \"[Data Standard:HIC: Hepatitis v2.0.0] changed properties [folder]\" } The fields have the following definition: dateCreated (DateTime): the date and time when this modification was made createdBy (User): the user responsible for making the edit. This will include their id , emailAddress , firstName , lastName **, **userRole and whether the user account is currently disabled . It may also include the profile image of the user in question description (String): The human-readable description of the edit made. These descriptions are automatically generated by the catalogue The endpoints for getting the edit history for each of DataModel, Terminology, Folder, CodeSet, Classifier and UserGroup are listed below. /api/dataModels/ {dataModelId} /edits /api/terminologies/ {terminologyId} /edits /api/folders/ {folderId} /edits /api/codeSets/ {codeSetId} /edits /api/classifiers/ {classifierId} /edits /api/userGroups/ {userGroupId} /edits","title":"Item history"},{"location":"rest-api/resources/catalogue-item/#reference-files","text":"Reference files (or attachments) can be stored alongside various catalogue items to supplement information about the catalogue item. Reference files have the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 { \"id\" : \"eea67c19-1833-4125-9934-b06f45844c20\" , \"domainType\" : \"ReferenceFile\" , \"fileName\" : \"uploadedFile.png\" , \"fileSize\" : 80899 , \"fileType\" : \"image/png\" , \"lastUpdated\" : \"2021-05-13T12:50:37.523Z\" } The fields have the following definition: id (UUID): The identifier of the returned object domainType (String): The type of the returned object - in this case, \"ReferenceFile\" fileName (String): The name of the uploaded file fileSize (Number): The size of the uploaded reference file, in bytes fileType (String): The MIME type of the uploaded reference file lastUpdated (DateTime): the date and time when this modification was made To upload and attach a new reference file to a catalogue item, use the following endpoint and request payload: /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles Request body (JSON) 1 2 3 4 5 6 7 8 { \"fileName\" : \"uploadedFile.png\" , \"fileSize\" : 80899 , \"fileType\" : \"image/png\" , \"fileContents\" : [ // Array of bytes ] } Where {catalogueItemDomainType} can be one of: dataModels , terminologies , codeSets , or referenceDataModels To get either a paginated list of reference files for a catalogue item, or an individual reference file known by {id} : /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles/ {id} To delete a reference file from a catalogue item whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles/ {id}","title":"Reference files"},{"location":"rest-api/resources/catalogue-item/#summary-metadata","text":"/api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id}","title":"Summary Metadata"},{"location":"rest-api/resources/catalogue-item/#summary-metadata-reports","text":"/api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id}","title":"Summary Metadata Reports"},{"location":"rest-api/resources/classifier/","text":"A Classifier is a container type and can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"Classification\" , \"label\" : \"classifier\" , \"description\" : \"Represents a classifier.\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" } The fields are as follows: id (UUID): The unique identifier of this classifier domainType (Type): The domain type of this catalogue object. Will always be Classification in this case. label (String): The human-readable identifier of this classifier. description (String): A long description of the classifier, and any important characteristics of the data. This field may include HTML, or MarkDown. lastUpdated (DateTime): The date/time when this Classifier was last modified As well as the endpoints listed below, a Classifier is also a CatalogueItem, and so a Classifier identifier can also be used as the parameter to any of those endpoints Child Classifiers \u00b6 A classifier may contain child classifiers. Endpoints are provided to differentiate between parent and child classifiers. Getting information \u00b6 The following endpoints returns a paginated list of all the Classifiers. The first requests all root classifiers in Mauro, the second requests the classifiers for a parent classifier. /api/classifiers /api/classifiers/ {classifierId} /classifiers These endpoints provide the detailed information about a particular Classifier; the first requests a root classifier in Mauro, the second requests a classifier from a parent classifier. /api/classifiers/ {id} /api/classifiers/ {classifierId} /classifiers/ {id} Finally, these endpoints request a list of catalogue items mapped to a classifier, and the inverse to list all classifiers mapped to a catalogue item, respectively. /api/classifiers/ {classifierId} /catalogueItems /api/ {catalogueItemDomainType} / {catalogueItemId} /classifiers Create / Update / Delete \u00b6 To create a new Classifier from scratch, use the following post endpoints, depending on whether to create one with or without a parent. /api/classifiers /api/classifiers/ {classifierId} /classifiers To edit the properties of a Classifier, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent. /api/classifiers/ {id} /api/classifiers/ {classifierId} /classifiers/ {id} To delete a Classifier, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/classifiers/ {id} ?permanent= {true/false} /api/classifiers/ {classifierId} /classifiers/ {id} ?permanent= {true/false} Security \u00b6 /api/classifiers/ {classifierId} /readByAuthenticated /api/classifiers/ {classifierId} /readByAuthenticated /api/classifiers/ {classifierId} /readByEveryone /api/classifiers/ {classifierId} /readByEveryone","title":"Classifier"},{"location":"rest-api/resources/classifier/#child-classifiers","text":"A classifier may contain child classifiers. Endpoints are provided to differentiate between parent and child classifiers.","title":"Child Classifiers"},{"location":"rest-api/resources/classifier/#getting-information","text":"The following endpoints returns a paginated list of all the Classifiers. The first requests all root classifiers in Mauro, the second requests the classifiers for a parent classifier. /api/classifiers /api/classifiers/ {classifierId} /classifiers These endpoints provide the detailed information about a particular Classifier; the first requests a root classifier in Mauro, the second requests a classifier from a parent classifier. /api/classifiers/ {id} /api/classifiers/ {classifierId} /classifiers/ {id} Finally, these endpoints request a list of catalogue items mapped to a classifier, and the inverse to list all classifiers mapped to a catalogue item, respectively. /api/classifiers/ {classifierId} /catalogueItems /api/ {catalogueItemDomainType} / {catalogueItemId} /classifiers","title":"Getting information"},{"location":"rest-api/resources/classifier/#create-update-delete","text":"To create a new Classifier from scratch, use the following post endpoints, depending on whether to create one with or without a parent. /api/classifiers /api/classifiers/ {classifierId} /classifiers To edit the properties of a Classifier, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent. /api/classifiers/ {id} /api/classifiers/ {classifierId} /classifiers/ {id} To delete a Classifier, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/classifiers/ {id} ?permanent= {true/false} /api/classifiers/ {classifierId} /classifiers/ {id} ?permanent= {true/false}","title":"Create / Update / Delete"},{"location":"rest-api/resources/classifier/#security","text":"/api/classifiers/ {classifierId} /readByAuthenticated /api/classifiers/ {classifierId} /readByAuthenticated /api/classifiers/ {classifierId} /readByEveryone /api/classifiers/ {classifierId} /readByEveryone","title":"Security"},{"location":"rest-api/resources/codeset/","text":"In its simplest form, a Code Set can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"CodeSet\" , \"label\" : \"Sample Codeset\" , \"aliases\" : [ \"sample\" ], \"description\" : \"Example of a Codeset\" , \"author\" : \"NHS Digital\" , \"organisation\" : \"NHS Digital\" , \"documentationVersion\" : \"2.0.0\" , \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"type\" : \"CodeSet\" , \"finalised\" : false , } The fields are as follows: id (UUID): The unique identifier of this code set domainType (Type): The domain type of this catalogue object - always CodeSet in this case label (String): The human-readable identifier of this code set. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this code set is known description (String): A long description of the code set, and any important characteristics of the data. This field may include HTML, or MarkDown. author (String): The names of those creating and maintaining this code set (not any underlying dataset itself) organisation (String): The name of the organisation holding the dataset documentationVersion (Version): The version of the description of an underlying dataset lastUpdated (DateTime): The date/time when this code set was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this code set (see classifiers ) type (CodeSet Type): Will always be defined as CodeSet . finalised (Boolean): Whether this code set has been 'finalised', or is in draft mode Endpoints which return multiple code sets typically include sufficient fields for generating links on the interface - a separate call to return the details of the Code Set is usually required. As well as the endpoints listed below, a Code Set is also a CatalogueItem, and so a Code Set identifier can also be used as the parameter to any of those endpoints List all code sets \u00b6 The following endpoint returns a paginated list of all code set objects readable by the current user: /api/codeSets This endpoint returns all the code sets within a particular folder; again, this result is paginated . /api/folders/ {folderId} /codeSets Get information about a particular code set \u00b6 This endpoint provides the default information about a code set, as per the JSON at the top of the page. /api/codeSets/ {id} Create code set \u00b6 To create a new code set from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/codeSets There are two ways of versioning code set in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/codeSets/ {codeSetId} /newModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/codeSets/ {codeSetId} /newDocumentationVersion By default, this will supersede the original data model. It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set: /api/codeSets/ {codeSetId} /newBranchModelVersion To create a fork of the original data model: /api/codeSets/ {codeSetId} /newForkModel Update code set \u00b6 To edit the primitive properties of a code set, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/codeSets/ {id} To move a code set from one folder to another, call the following, using the id fields for the code set, and the new folder: /api/folders/ {folderId} /codeSets/ {codeSetId} Alternatively, you can call this equivalent endpoint: /api/codeSets/ {codeSetId} /folder/ {folderId} To move a code set from a draft state to 'finalised', use the following endpoint: /api/codeSets/ {codeSetId} /finalise Sharing \u00b6 To allow a code set to be read by any authenticated user of the system, use the following endpoint: /api/codeSets/ {codeSetId} /readByAuthenticated ... and to remove this flag, use the following: /api/codeSets/ {codeSetId} /readByAuthenticated Similarly, to allow the code set to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/codeSets/ {codeSetId} /readByEveryone ... and the following to remove this flag: /api/codeSets/ {codeSetId} /readByEveryone Delete code set \u00b6 To delete a code set, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/codeSets/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/codeSets/ {id} /undoSoftDelete Import / export a code set \u00b6 To export a code set using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple code sets: /api/codeSets/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single code set, you can use the following endpoint with the id of the code sets specified: /api/codeSets/ {codeSetId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more code sets, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/codeSets/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Codeset"},{"location":"rest-api/resources/codeset/#list-all-code-sets","text":"The following endpoint returns a paginated list of all code set objects readable by the current user: /api/codeSets This endpoint returns all the code sets within a particular folder; again, this result is paginated . /api/folders/ {folderId} /codeSets","title":"List all code sets"},{"location":"rest-api/resources/codeset/#get-information-about-a-particular-code-set","text":"This endpoint provides the default information about a code set, as per the JSON at the top of the page. /api/codeSets/ {id}","title":"Get information about a particular code set"},{"location":"rest-api/resources/codeset/#create-code-set","text":"To create a new code set from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/codeSets There are two ways of versioning code set in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/codeSets/ {codeSetId} /newModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/codeSets/ {codeSetId} /newDocumentationVersion By default, this will supersede the original data model. It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set: /api/codeSets/ {codeSetId} /newBranchModelVersion To create a fork of the original data model: /api/codeSets/ {codeSetId} /newForkModel","title":"Create code set"},{"location":"rest-api/resources/codeset/#update-code-set","text":"To edit the primitive properties of a code set, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/codeSets/ {id} To move a code set from one folder to another, call the following, using the id fields for the code set, and the new folder: /api/folders/ {folderId} /codeSets/ {codeSetId} Alternatively, you can call this equivalent endpoint: /api/codeSets/ {codeSetId} /folder/ {folderId} To move a code set from a draft state to 'finalised', use the following endpoint: /api/codeSets/ {codeSetId} /finalise","title":"Update code set"},{"location":"rest-api/resources/codeset/#sharing","text":"To allow a code set to be read by any authenticated user of the system, use the following endpoint: /api/codeSets/ {codeSetId} /readByAuthenticated ... and to remove this flag, use the following: /api/codeSets/ {codeSetId} /readByAuthenticated Similarly, to allow the code set to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/codeSets/ {codeSetId} /readByEveryone ... and the following to remove this flag: /api/codeSets/ {codeSetId} /readByEveryone","title":"Sharing"},{"location":"rest-api/resources/codeset/#delete-code-set","text":"To delete a code set, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/codeSets/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/codeSets/ {id} /undoSoftDelete","title":"Delete code set"},{"location":"rest-api/resources/codeset/#import-export-a-code-set","text":"To export a code set using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple code sets: /api/codeSets/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single code set, you can use the following endpoint with the id of the code sets specified: /api/codeSets/ {codeSetId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more code sets, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/codeSets/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Import / export a code set"},{"location":"rest-api/resources/data-class/","text":"A DataClass can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"DataClass\" , \"label\" : \"parent\" , \"description\" : \"Represents a parent data class.\" , \"aliases\" : [ \"root\" ], \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" \"model\" : \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\" , \"parentDataClass\" : \"363c202b-e6d9-4098-a5bf-78194d57b70d\" , \"minMultipicity\" : 0 , \"maxMultiplicity\" : -1 , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], } The fields are as follows: id (UUID): The unique identifier of this data class domainType (Type): The domain type of this catalogue object. Will always be DataClass in this case. label (String): The human-readable identifier of this class. description (String): A long description of the data class, and any important characteristics of the data. This field may include HTML, or MarkDown. aliases (Set(String)): Any other names by which this data class is known lastUpdated (DateTime): The date/time when this DataClass was last modified model (UUID): The unique identifier of the owning data model parentDataClass (UUID): The unique identifier of the data class of which this is a child of. If the data class does not have a parent, this field is undefined/not provided. minMultiplicity (Number): Defines the minimum uses of this data class may be applied to a data model. See the multipicity . maxMultiplicity (Number): Defines the maximum uses of this data class may be applied to a data model. See the multipicity . classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this data class (see classifiers ) As well as the endpoints listed below, a DataClass is also a CatalogueItem, and so a DataClass identifier can also be used as the parameter to any of those endpoints Child Classes \u00b6 A DataClass may be composed of child classes to define more complex definitions for a DataModel. Endpoints are provided to differentiate between parent and child data classes. Multiplicity \u00b6 Each DataClass defines its multiplicity to state how many occurances of this class should be expected in a model. Multipicities are defined in the notation x..y , where: x represents the minimum multipicity. y represents the maximum multipicity. A minimum multipicity must be provided and be greater than or equal to 0 . A maximum multiplicity may be provided that is greater than or equal to 1 . To represent unbounded multipicity, the * symbol is used - numerically, for endpoints, this is represented as the integer -1 . Some examples of multipicities and what they represent: 0..* - an optional, unbounded data class. This may be present or not, and has not limit on how many are present. 1..* - a required, unbounded data class. Similar to above but with the added constraint that at least one must be present. 0..1 - an optional, singular data class. Either the class is present in the model or not. 1..1 - a required, singular data class. This represents that the class must be present in the model. Getting information \u00b6 The following endpoints returns a paginated list of all the DataTypes within a particular DataModel. The first requests the data classes for a data model, the second requests the data classes for a parent data class. /api/dataModels/ {dataModelId} /dataClasses /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses These endpoints provide the detailed information about a particular DataClass; the first requests a DataType under a particular DataModel, the second requests a DataClass from a parent DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id} Create / Update / Delete \u00b6 To create a new DataClass from scratch, use the following post endpoints, depending on whether to create one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses To edit the properties of a DataClass, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {id} To delete a DataClass, use the following endpoint, depending on whether to delete one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id} Copying \u00b6 Instead of creating a new DataClass from scratch, it is also possible to copy an existing DataClass from another DataModel. Use the following endpoints to accomplish this, depending on whether to copy one directly under a DataModel or under a parent DataClass respectively. The dataModelId and dataClassId refers to the target DataModel or parent DataClass to copy to; otherDataModelId and otherDataCLassId refers to the source DataModel/Class to copy from. /api/dataModels/ {dataModelId} /dataClasses/ {otherDataModelId} / {otherDataClassId} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {otherDataModelId} / {otherDataClassId}","title":"Data class"},{"location":"rest-api/resources/data-class/#child-classes","text":"A DataClass may be composed of child classes to define more complex definitions for a DataModel. Endpoints are provided to differentiate between parent and child data classes.","title":"Child Classes"},{"location":"rest-api/resources/data-class/#multiplicity","text":"Each DataClass defines its multiplicity to state how many occurances of this class should be expected in a model. Multipicities are defined in the notation x..y , where: x represents the minimum multipicity. y represents the maximum multipicity. A minimum multipicity must be provided and be greater than or equal to 0 . A maximum multiplicity may be provided that is greater than or equal to 1 . To represent unbounded multipicity, the * symbol is used - numerically, for endpoints, this is represented as the integer -1 . Some examples of multipicities and what they represent: 0..* - an optional, unbounded data class. This may be present or not, and has not limit on how many are present. 1..* - a required, unbounded data class. Similar to above but with the added constraint that at least one must be present. 0..1 - an optional, singular data class. Either the class is present in the model or not. 1..1 - a required, singular data class. This represents that the class must be present in the model.","title":"Multiplicity"},{"location":"rest-api/resources/data-class/#getting-information","text":"The following endpoints returns a paginated list of all the DataTypes within a particular DataModel. The first requests the data classes for a data model, the second requests the data classes for a parent data class. /api/dataModels/ {dataModelId} /dataClasses /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses These endpoints provide the detailed information about a particular DataClass; the first requests a DataType under a particular DataModel, the second requests a DataClass from a parent DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id}","title":"Getting information"},{"location":"rest-api/resources/data-class/#create-update-delete","text":"To create a new DataClass from scratch, use the following post endpoints, depending on whether to create one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses To edit the properties of a DataClass, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {id} To delete a DataClass, use the following endpoint, depending on whether to delete one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id}","title":"Create / Update / Delete"},{"location":"rest-api/resources/data-class/#copying","text":"Instead of creating a new DataClass from scratch, it is also possible to copy an existing DataClass from another DataModel. Use the following endpoints to accomplish this, depending on whether to copy one directly under a DataModel or under a parent DataClass respectively. The dataModelId and dataClassId refers to the target DataModel or parent DataClass to copy to; otherDataModelId and otherDataCLassId refers to the source DataModel/Class to copy from. /api/dataModels/ {dataModelId} /dataClasses/ {otherDataModelId} / {otherDataClassId} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {otherDataModelId} / {otherDataClassId}","title":"Copying"},{"location":"rest-api/resources/data-element/","text":"A DataElement can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"DataElement\" , \"label\" : \"element\" , \"description\" : \"Description of the Data Element.\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" \"model\" : \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\" , \"dataClass\" : \"afb3dcda-fd7d-40b9-857c-23fb5af8cbbf\" , \"dataType\" : { \"id\" : \"c85d78d3-cac8-449b-a22b-52da144b9a8f\" , \"domainType\" : \"PrimitiveType\" , \"label\" : \"integer\" } } The fields are as follows: id (UUID): The unique identifier of this data element domainType (Type): The domain type of this catalogue object. This is always DataElement in this case. label (String): The human-readable identifier of this element. description (String): A long description of the data element, and any important characteristics of the data. This field may include HTML, or MarkDown. lastUpdated (DateTime): The date/time when this DataElement was last modified model (UUID): The unique identifier of the parent data model dataClass (UUID): The unique identifier of the parent data class dataType (Object): The type definition of this data element. The object returned matches the JSON defined in Data type As well as the endpoints listed below, a DataElement is also a CatalogueItem, and so a DataElement identifier can also be used as the parameter to any of those endpoints Getting information \u00b6 The following endpoint returns a paginated list of all the DataElements within a particular DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements This endpoint provides the detailed information about a particular DataElement under a DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id} Create / Update / Delete \u00b6 To create a new DataElement from scratch, use the following post endpoint. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements To edit the properties of a DataElement, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id} To delete a DataElement, use the following endpoint. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id} Copying \u00b6 Instead of creating a new DataElement from scratch, it is also possible to copy an existing DataElement from another DataClass. Use the following endpoint to accomplish this. The dataModelId and dataClassId refers to the target DataClass to copy to; otherDataModelId , otherDataClassId and dataElementId refer to the source DataModel/Class/Element to copy from. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {otherDataModelId} / {otherDataClassId} / {dataElementId}","title":"Data element"},{"location":"rest-api/resources/data-element/#getting-information","text":"The following endpoint returns a paginated list of all the DataElements within a particular DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements This endpoint provides the detailed information about a particular DataElement under a DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id}","title":"Getting information"},{"location":"rest-api/resources/data-element/#create-update-delete","text":"To create a new DataElement from scratch, use the following post endpoint. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements To edit the properties of a DataElement, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id} To delete a DataElement, use the following endpoint. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id}","title":"Create / Update / Delete"},{"location":"rest-api/resources/data-element/#copying","text":"Instead of creating a new DataElement from scratch, it is also possible to copy an existing DataElement from another DataClass. Use the following endpoint to accomplish this. The dataModelId and dataClassId refers to the target DataClass to copy to; otherDataModelId , otherDataClassId and dataElementId refer to the source DataModel/Class/Element to copy from. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {otherDataModelId} / {otherDataClassId} / {dataElementId}","title":"Copying"},{"location":"rest-api/resources/data-flow-component/","text":"/api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataClassFlows/ {dataClassId} /dataFlowComponents /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {dataFlowComponentId} / {type} / {dataElementId} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {dataFlowComponentId} / {type} / {dataElementId}","title":"Data flow component"},{"location":"rest-api/resources/data-flow/","text":"/api/dataModels/ {dataModelId} /dataFlows/import/ {importerNamespace} / {importerName} / {importerVersion} /api/dataModels/ {dataModelId} /dataFlows/export/ {exporterNamespace} / {exporterName} / {exporterVersion} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataClassFlows /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /diagramLayout /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} /api/dataModels/ {dataModelId} /dataFlows /api/dataModels/ {dataModelId} /dataFlows /api/dataModels/ {dataModelId} /dataFlows/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {id}","title":"Data flow"},{"location":"rest-api/resources/data-model/","text":"DataModel object description \u00b6 In its simplest form, a DataModel can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"DataModel\" , \"label\" : \"Diagnostic Imaging Dataset\" , \"aliases\" : [ \"DID\" ], \"description\" : \"Central collection of detailed information about diagnostic imaging tests carried out on NHS patients (such as x-rays and MRI scans). Any organisation providing diagnostic imaging tests to NHS patients in England, i.e.:\\n* NHS (Foundation) trusts / hospitals\\n* NHS-funded activity with independent sector providers\\nNOT included are breast screening services or any other diagnostic imaging tests not typically recorded on the local provider's Radiology Information Systems.\\nDiagnostic Imaging Dataset (DID) does not store the images themselves, or the outcomes/diagnoses related to these images.\" , \"author\" : \"NHS Digital\" , \"organisation\" : \"NHS Digital\" , \"editable\" : true , \"documentationVersion\" : \"2.0.0\" , \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"type\" : \"Data Asset\" , \"finalised\" : false , } The fields are as follows: id (UUID): The unique identifier of this data model domainType (Type): The domain type of this catalogue object - always \"DataModel\" in this case label (String): The human-readable identifier of this model. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this datamodel is known description (String): A long description of the data model, and any important characteristics of the data. This field may include HTML, or MarkDown. author (String): The names of those creating and maintaining this Datamodel (not any underlying dataset itself) organisation (String): The name of the organisation holding the dataset editable (Boolean): Whether the current user (see authentication ) is allowed to edit this DataModel documentationVersion (Version): The version of the description of an underlying dataset lastUpdated (DateTime): The date/time when this DataModel was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this data model (see classifiers ) type (DataModel Type): Whether this DataModel is a \"Data Asset\", or a \"Data Standard\" finalised (Boolean): Whether this DataModel has been 'finalised', or is in draft mode Endpoints which return multiple models typically include sufficient fields for generating links on the interface - a separate call to return the details of the DataModel is usually required. As well as the endpoints listed below, a DataModel is also a CatalogueItem, and so a DataModel identifier can also be used as the parameter to any of those endpoints List all data models \u00b6 The following endpoint returns a paginated list of all DataModel objects readable by the current user: /api/dataModels This endpoint returns all the DataModels within a particular folder; again, this result is paginated . /api/folders/ {folderId} /dataModels Get information about a particular data model \u00b6 This endpoint provides the default information about a DataModel, as per the JSON at the top of the page. /api/dataModels/ {id} The 'hierarchy' endpoint provides a structured representation of the entire datamodel - child DataClasses, sub-DataClasses, and their child DataElements. Warning This call can take a long time for large data models /api/dataModels/ {dataModelId} /hierarchy The 'types' endpoint lists all the datatypes for a given datamodel. This returns primitive, reference, enumeration and terminology types owned by the data model, whether used or not. /api/dataModels/types Create data model \u00b6 To create a new data model from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/dataModels There are two ways of versioning Data Models in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/dataModels/ {dataModelId} /newModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/dataModels/ {dataModelId} /newDocumentationVersion By default, this will supersede the original data model. It is also possible to branch and fork Data Models to create drafts before finalising them. To create a new branch from an existing Data Model: /api/dataModels/ {dataModelId} /newBranchModelVersion To create a fork of the original data model: /api/dataModels/ {dataModelId} /newForkModel Update data model \u00b6 To edit the primitive properties of a data model, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {id} To move a data model from one folder to another, call the following, using the id fields for the data model, and the new folder: /api/folders/ {folderId} /dataModels/ {dataModelId} Alternatively, you can call this equivalent endpoint: /api/dataModels/ {dataModelId} /folder/ {folderId} To move a data model from a draft state to 'finalised', use the following endpoint: /api/dataModels/ {dataModelId} /finalise Sharing \u00b6 To allow a model to be read by any authenticated user of the system, use the following endpoint: /api/dataModels/ {dataModelId} /readByAuthenticated ... and to remove this flag, use the following: /api/dataModels/ {dataModelId} /readByAuthenticated Similarly, to allow the model to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/dataModels/ {dataModelId} /readByEveryone ... and the following to remove this flag: /api/dataModels/ {dataModelId} /readByEveryone Delete data model \u00b6 To delete a data model, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/dataModels/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/dataModels/ {id} /undoSoftDelete Import / export a data model \u00b6 To export a data model using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple data models: /api/dataModels/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single model, you can use the following endpoint with the id of the data model specified: /api/dataModels/ {dataModelId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more data models, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/dataModels/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Data model"},{"location":"rest-api/resources/data-model/#datamodel-object-description","text":"In its simplest form, a DataModel can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"DataModel\" , \"label\" : \"Diagnostic Imaging Dataset\" , \"aliases\" : [ \"DID\" ], \"description\" : \"Central collection of detailed information about diagnostic imaging tests carried out on NHS patients (such as x-rays and MRI scans). Any organisation providing diagnostic imaging tests to NHS patients in England, i.e.:\\n* NHS (Foundation) trusts / hospitals\\n* NHS-funded activity with independent sector providers\\nNOT included are breast screening services or any other diagnostic imaging tests not typically recorded on the local provider's Radiology Information Systems.\\nDiagnostic Imaging Dataset (DID) does not store the images themselves, or the outcomes/diagnoses related to these images.\" , \"author\" : \"NHS Digital\" , \"organisation\" : \"NHS Digital\" , \"editable\" : true , \"documentationVersion\" : \"2.0.0\" , \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"type\" : \"Data Asset\" , \"finalised\" : false , } The fields are as follows: id (UUID): The unique identifier of this data model domainType (Type): The domain type of this catalogue object - always \"DataModel\" in this case label (String): The human-readable identifier of this model. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this datamodel is known description (String): A long description of the data model, and any important characteristics of the data. This field may include HTML, or MarkDown. author (String): The names of those creating and maintaining this Datamodel (not any underlying dataset itself) organisation (String): The name of the organisation holding the dataset editable (Boolean): Whether the current user (see authentication ) is allowed to edit this DataModel documentationVersion (Version): The version of the description of an underlying dataset lastUpdated (DateTime): The date/time when this DataModel was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this data model (see classifiers ) type (DataModel Type): Whether this DataModel is a \"Data Asset\", or a \"Data Standard\" finalised (Boolean): Whether this DataModel has been 'finalised', or is in draft mode Endpoints which return multiple models typically include sufficient fields for generating links on the interface - a separate call to return the details of the DataModel is usually required. As well as the endpoints listed below, a DataModel is also a CatalogueItem, and so a DataModel identifier can also be used as the parameter to any of those endpoints","title":"DataModel object description"},{"location":"rest-api/resources/data-model/#list-all-data-models","text":"The following endpoint returns a paginated list of all DataModel objects readable by the current user: /api/dataModels This endpoint returns all the DataModels within a particular folder; again, this result is paginated . /api/folders/ {folderId} /dataModels","title":"List all data models"},{"location":"rest-api/resources/data-model/#get-information-about-a-particular-data-model","text":"This endpoint provides the default information about a DataModel, as per the JSON at the top of the page. /api/dataModels/ {id} The 'hierarchy' endpoint provides a structured representation of the entire datamodel - child DataClasses, sub-DataClasses, and their child DataElements. Warning This call can take a long time for large data models /api/dataModels/ {dataModelId} /hierarchy The 'types' endpoint lists all the datatypes for a given datamodel. This returns primitive, reference, enumeration and terminology types owned by the data model, whether used or not. /api/dataModels/types","title":"Get information about a particular data model"},{"location":"rest-api/resources/data-model/#create-data-model","text":"To create a new data model from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/dataModels There are two ways of versioning Data Models in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/dataModels/ {dataModelId} /newModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/dataModels/ {dataModelId} /newDocumentationVersion By default, this will supersede the original data model. It is also possible to branch and fork Data Models to create drafts before finalising them. To create a new branch from an existing Data Model: /api/dataModels/ {dataModelId} /newBranchModelVersion To create a fork of the original data model: /api/dataModels/ {dataModelId} /newForkModel","title":"Create data model"},{"location":"rest-api/resources/data-model/#update-data-model","text":"To edit the primitive properties of a data model, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {id} To move a data model from one folder to another, call the following, using the id fields for the data model, and the new folder: /api/folders/ {folderId} /dataModels/ {dataModelId} Alternatively, you can call this equivalent endpoint: /api/dataModels/ {dataModelId} /folder/ {folderId} To move a data model from a draft state to 'finalised', use the following endpoint: /api/dataModels/ {dataModelId} /finalise","title":"Update data model"},{"location":"rest-api/resources/data-model/#sharing","text":"To allow a model to be read by any authenticated user of the system, use the following endpoint: /api/dataModels/ {dataModelId} /readByAuthenticated ... and to remove this flag, use the following: /api/dataModels/ {dataModelId} /readByAuthenticated Similarly, to allow the model to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/dataModels/ {dataModelId} /readByEveryone ... and the following to remove this flag: /api/dataModels/ {dataModelId} /readByEveryone","title":"Sharing"},{"location":"rest-api/resources/data-model/#delete-data-model","text":"To delete a data model, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/dataModels/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/dataModels/ {id} /undoSoftDelete","title":"Delete data model"},{"location":"rest-api/resources/data-model/#import-export-a-data-model","text":"To export a data model using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple data models: /api/dataModels/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single model, you can use the following endpoint with the id of the data model specified: /api/dataModels/ {dataModelId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more data models, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/dataModels/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Import / export a data model"},{"location":"rest-api/resources/data-type/","text":"A DataType can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"PrimitiveType\" , \"label\" : \"integer\" , \"description\" : \"Represents a number.\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" \"model\" : \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\" , } The fields are as follows: id (UUID): The unique identifier of this data type domainType (Type): The domain type of this catalogue object. Could be PrimitiveType or EnumerationType . label (String): The human-readable identifier of this type. description (String): A long description of the data type, and any important characteristics of the data. This field may include HTML, or MarkDown. lastUpdated (DateTime): The date/time when this DataType was last modified model (UUID): The unique identifier of the parent data model As well as the endpoints listed below, a DataType is also a CatalogueItem, and so a DataType identifier can also be used as the parameter to any of those endpoints Default Data Types \u00b6 When creating DataModels , a default data type provider can be used to automatically define data types for a data model. To list all available data type providers: /api/dataModels/defaultDataTypeProviders Getting information \u00b6 The following endpoint returns a paginated list of all the DataTypes within a particular DataModel. /api/dataModels/ {dataModelId} /dataTypes This endpoint provides the detailed information about a particular DataType under a DataModel. /api/dataModels/ {dataModelId} /dataTypes/ {id} Create / Update / Delete \u00b6 To create a new DataType from scratch, use the following post endpoint. /api/dataModels/ {dataModelId} /dataTypes To edit the properties of a DataType, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {dataModelId} /dataTypes/ {id} To delete a DataType, use the following endpoint. /api/dataModels/ {dataModelId} /dataTypes/ {id} Copying \u00b6 Instead of creating a new DataType from scratch, it is also possible to copy an existing DataType from another DataModel. Use the following endpoint to accomplish this. The dataModelId refers to the target DataModel to copy to; otherDataModelId and dataTypeId refer to the source DataModel/Type to copy from. /api/dataModels/ {dataModelId} /dataTypes/ {otherDataModelId} / {dataTypeId}","title":"Data type"},{"location":"rest-api/resources/data-type/#default-data-types","text":"When creating DataModels , a default data type provider can be used to automatically define data types for a data model. To list all available data type providers: /api/dataModels/defaultDataTypeProviders","title":"Default Data Types"},{"location":"rest-api/resources/data-type/#getting-information","text":"The following endpoint returns a paginated list of all the DataTypes within a particular DataModel. /api/dataModels/ {dataModelId} /dataTypes This endpoint provides the detailed information about a particular DataType under a DataModel. /api/dataModels/ {dataModelId} /dataTypes/ {id}","title":"Getting information"},{"location":"rest-api/resources/data-type/#create-update-delete","text":"To create a new DataType from scratch, use the following post endpoint. /api/dataModels/ {dataModelId} /dataTypes To edit the properties of a DataType, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {dataModelId} /dataTypes/ {id} To delete a DataType, use the following endpoint. /api/dataModels/ {dataModelId} /dataTypes/ {id}","title":"Create / Update / Delete"},{"location":"rest-api/resources/data-type/#copying","text":"Instead of creating a new DataType from scratch, it is also possible to copy an existing DataType from another DataModel. Use the following endpoint to accomplish this. The dataModelId refers to the target DataModel to copy to; otherDataModelId and dataTypeId refer to the source DataModel/Type to copy from. /api/dataModels/ {dataModelId} /dataTypes/ {otherDataModelId} / {dataTypeId}","title":"Copying"},{"location":"rest-api/resources/enumeration-value/","text":"/api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues /api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues /api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues/ {id} /api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues/ {id} /api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues/ {id}","title":"Enumeration value"},{"location":"rest-api/resources/folder/","text":"A Folder is a container type and can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"Folder\" , \"label\" : \"folder\" , \"description\" : \"Represents a folder.\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" , \"hasChildFolders\" : true } The fields are as follows: id (UUID): The unique identifier of this folder domainType (Type): The domain type of this catalogue object. Will always be Folder in this case. label (String): The human-readable identifier of this folder. description (String): A long description of the folder, and any important characteristics of the data. This field may include HTML, or MarkDown. lastUpdated (DateTime): The date/time when this folder was last modified hasChildFolders (Boolean): Determines if this folder contains child folders. As well as the endpoints listed below, a Folder is also a CatalogueItem, and so a Folder identifier can also be used as the parameter to any of those endpoints Child Folders \u00b6 A folder may contain child folders. Endpoints are provided to differentiate between parent and child folders. Getting information \u00b6 The following endpoints returns a paginated list of all the folders. The first requests all root folders in Mauro, the second requests the folders for a parent folder. /api/folders /api/folders/ {folderId} /folders These endpoints provide the detailed information about a particular folder; the first requests a root folder in Mauro, the second requests a folder from a parent folder. /api/folders/ {id} /api/folders/ {folderId} /folders/ {id} Create / Update / Delete \u00b6 To create a new folder from scratch, use the following post endpoints, depending on whether to create one with or without a parent. /api/folders /api/folders/ {folderId} /folders To edit the properties of a folder, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent. /api/folders/ {id} /api/folders/ {folderId} /folders/ {id} To delete a folder, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/folders/ {id} ?permanent= {true/false} /api/folders/ {folderId} /folders/ {id} ?permanent= {true/false} Security \u00b6 /api/folders/ {folderId} /readByAuthenticated /api/folders/ {folderId} /readByAuthenticated /api/folders/ {folderId} /readByEveryone /api/folders/ {folderId} /readByEveryone","title":"Folder"},{"location":"rest-api/resources/folder/#child-folders","text":"A folder may contain child folders. Endpoints are provided to differentiate between parent and child folders.","title":"Child Folders"},{"location":"rest-api/resources/folder/#getting-information","text":"The following endpoints returns a paginated list of all the folders. The first requests all root folders in Mauro, the second requests the folders for a parent folder. /api/folders /api/folders/ {folderId} /folders These endpoints provide the detailed information about a particular folder; the first requests a root folder in Mauro, the second requests a folder from a parent folder. /api/folders/ {id} /api/folders/ {folderId} /folders/ {id}","title":"Getting information"},{"location":"rest-api/resources/folder/#create-update-delete","text":"To create a new folder from scratch, use the following post endpoints, depending on whether to create one with or without a parent. /api/folders /api/folders/ {folderId} /folders To edit the properties of a folder, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent. /api/folders/ {id} /api/folders/ {folderId} /folders/ {id} To delete a folder, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/folders/ {id} ?permanent= {true/false} /api/folders/ {folderId} /folders/ {id} ?permanent= {true/false}","title":"Create / Update / Delete"},{"location":"rest-api/resources/folder/#security","text":"/api/folders/ {folderId} /readByAuthenticated /api/folders/ {folderId} /readByAuthenticated /api/folders/ {folderId} /readByEveryone /api/folders/ {folderId} /readByEveryone","title":"Security"},{"location":"rest-api/resources/plugin/","text":"/api/public/plugins/dataFlowImporters /api/public/plugins/dataFlowExporters /api/public/plugins/dataModelImporters /api/public/plugins/dataModelExporters Importer \u00b6 /api/importer/parameters/ {ns} ?/ {name} ?/ {version} ?","title":"Plugin"},{"location":"rest-api/resources/plugin/#importer","text":"/api/importer/parameters/ {ns} ?/ {name} ?/ {version} ?","title":"Importer"},{"location":"rest-api/resources/semantic-link/","text":"/api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks /api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks /api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks/ {id} /api/catalogueItems/ {catalogueItemId} /semanticLinks /api/catalogueItems/ {catalogueItemId} /semanticLinks /api/catalogueItems/ {catalogueItemId} /semanticLinks/ {id} /api/catalogueItems/ {catalogueItemId} /semanticLinks/ {id} /api/catalogueItems/ {catalogueItemId} /semanticLinks/ {id}","title":"Semantic link"},{"location":"rest-api/resources/term-relationship/","text":"/api/terminologies/ {terminologyId} /termRelationshipTypes/ {termRelationshipTypeId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /termRelationshipTypes/ {termRelationshipTypeId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} Term relationship types \u00b6 /api/terminologies/ {terminologyId} /termRelationshipTypes /api/terminologies/ {terminologyId} /termRelationshipTypes/ {id}","title":"Term relationship"},{"location":"rest-api/resources/term-relationship/#term-relationship-types","text":"/api/terminologies/ {terminologyId} /termRelationshipTypes /api/terminologies/ {terminologyId} /termRelationshipTypes/ {id}","title":"Term relationship types"},{"location":"rest-api/resources/term/","text":"A Term is part a Terminology and can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"Term\" , \"model\" : \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\" , \"code\" : \"CT1\" , \"definition\" : \"Custom Term\" , \"label\" : \"CT1: Custom Term\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" } The fields are as follows: id (UUID): The unique identifier of this term domainType (Type): The domain type of this catalogue object. Will always be Term in this case. model (UUID): The unique identifier of the owning data model code (String): A unique code identifier for the term. definition (String): The definition/name of this term. label (String): The human-readable identifier of this term. This is the combination of code and definition . lastUpdated (DateTime): The date/time when this term was last modified As well as the endpoints listed below, a term is also a CatalogueItem, and so a term identifier can also be used as the parameter to any of those endpoints Getting information \u00b6 The following endpoint returns a paginated list of all the terms within a particular Terminology. /api/terminologies/ {terminologyId} /terms This endpoint provides the detailed information about a particular term. /api/terminologies/ {terminologyId} /terms/ {id} Create / Update / Delete \u00b6 To create a new term from scratch, use the following post endpoint with a JSON request body similar to above. /api/terminologies/ {terminologyId} /terms To edit the properties of a term, use the following endpoint, with a body similar to the JSON described at the top of this page. /api/terminologies/ {terminologyId} /terms/ {id} To delete a term, use the following endpoint. /api/terminologies/ {terminologyId} /terms/ {id} Relationships \u00b6 /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id}","title":"Term"},{"location":"rest-api/resources/term/#getting-information","text":"The following endpoint returns a paginated list of all the terms within a particular Terminology. /api/terminologies/ {terminologyId} /terms This endpoint provides the detailed information about a particular term. /api/terminologies/ {terminologyId} /terms/ {id}","title":"Getting information"},{"location":"rest-api/resources/term/#create-update-delete","text":"To create a new term from scratch, use the following post endpoint with a JSON request body similar to above. /api/terminologies/ {terminologyId} /terms To edit the properties of a term, use the following endpoint, with a body similar to the JSON described at the top of this page. /api/terminologies/ {terminologyId} /terms/ {id} To delete a term, use the following endpoint. /api/terminologies/ {terminologyId} /terms/ {id}","title":"Create / Update / Delete"},{"location":"rest-api/resources/term/#relationships","text":"/api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id}","title":"Relationships"},{"location":"rest-api/resources/terminology/","text":"In its simplest form, a Terminology can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"Terminology\" , \"label\" : \"Sample Terminology\" , \"aliases\" : [ \"sample\" ], \"description\" : \"Example of a Terminology\" , \"author\" : \"NHS Digital\" , \"organisation\" : \"NHS Digital\" , \"documentationVersion\" : \"2.0.0\" , \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"type\" : \"Terminology\" , \"finalised\" : false , } The fields are as follows: id (UUID): The unique identifier of this terminology domainType (Type): The domain type of this catalogue object - always Terminology in this case label (String): The human-readable identifier of this terminology. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this terminology is known description (String): A long description of the description, and any important characteristics of the data. This field may include HTML, or MarkDown. author (String): The names of those creating and maintaining this terminology (not any underlying dataset itself) organisation (String): The name of the organisation holding the terminology documentationVersion (Version): The version of the description of an underlying dataset lastUpdated (DateTime): The date/time when this terminology was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this terminology (see classifiers ) type (Terminology Type): Will always be defined as Terminology . finalised (Boolean): Whether this terminology has been 'finalised', or is in draft mode Endpoints which return multiple terminologies typically include sufficient fields for generating links on the interface - a separate call to return the details of the terminology is usually required. As well as the endpoints listed below, a terminology is also a CatalogueItem, and so a terminology identifier can also be used as the parameter to any of those endpoints List all terminologies \u00b6 The following endpoint returns a paginated list of all terminologies readable by the current user: /api/terminologies This endpoint returns all the terminologies within a particular folder; again, this result is paginated . /api/folders/ {folderId} /terminologies Get information about a particular terminology \u00b6 This endpoint provides the default information about a terminology, as per the JSON at the top of the page. /api/terminologies/ {id} Create terminologies \u00b6 There are two ways of versioning terminologies in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/terminology/ {terminologyId} /newModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/terminology/ {terminologyId} /newDocumentationVersion By default, this will supersede the original terminology. It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set: /api/terminology/ {terminologyId} /newBranchModelVersion To create a fork of the original terminology: /api/terminology/ {terminologyId} /newForkModel Update terminology \u00b6 To edit the primitive properties of a terminology, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/terminology/ {id} To move a terminology from one folder to another, call the following, using the id fields for the terminology, and the new folder: /api/folders/ {folderId} /terminologies/ {terminologyId} Alternatively, you can call this equivalent endpoint: /api/terminologies/ {terminologyId} /folder/ {folderId} To move a terminology from a draft state to 'finalised', use the following endpoint: /api/terminologies/ {terminologyId} /finalise Sharing \u00b6 To allow a terminology to be read by any authenticated user of the system, use the following endpoint: /api/terminologies/ {terminologyId} /readByAuthenticated ... and to remove this flag, use the following: /api/terminologies/ {terminologyId} /readByAuthenticated Similarly, to allow the terminology to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/terminologies/ {terminologyId} /readByEveryone ... and the following to remove this flag: /api/terminologies/ {terminologyId} /readByEveryone Delete terminology \u00b6 To delete a terminology, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/terminologies/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted terminology using the following endpoint: /api/admin/terminologies/ {id} /undoSoftDelete Import / export a terminology \u00b6 To export a terminology using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple code sets: /api/terminologies/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single terminology, you can use the following endpoint with the id of the terminologies specified: /api/terminologies/ {terminologyId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more terminologies, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/terminologies/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Terminology"},{"location":"rest-api/resources/terminology/#list-all-terminologies","text":"The following endpoint returns a paginated list of all terminologies readable by the current user: /api/terminologies This endpoint returns all the terminologies within a particular folder; again, this result is paginated . /api/folders/ {folderId} /terminologies","title":"List all terminologies"},{"location":"rest-api/resources/terminology/#get-information-about-a-particular-terminology","text":"This endpoint provides the default information about a terminology, as per the JSON at the top of the page. /api/terminologies/ {id}","title":"Get information about a particular terminology"},{"location":"rest-api/resources/terminology/#create-terminologies","text":"There are two ways of versioning terminologies in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/terminology/ {terminologyId} /newModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/terminology/ {terminologyId} /newDocumentationVersion By default, this will supersede the original terminology. It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set: /api/terminology/ {terminologyId} /newBranchModelVersion To create a fork of the original terminology: /api/terminology/ {terminologyId} /newForkModel","title":"Create terminologies"},{"location":"rest-api/resources/terminology/#update-terminology","text":"To edit the primitive properties of a terminology, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/terminology/ {id} To move a terminology from one folder to another, call the following, using the id fields for the terminology, and the new folder: /api/folders/ {folderId} /terminologies/ {terminologyId} Alternatively, you can call this equivalent endpoint: /api/terminologies/ {terminologyId} /folder/ {folderId} To move a terminology from a draft state to 'finalised', use the following endpoint: /api/terminologies/ {terminologyId} /finalise","title":"Update terminology"},{"location":"rest-api/resources/terminology/#sharing","text":"To allow a terminology to be read by any authenticated user of the system, use the following endpoint: /api/terminologies/ {terminologyId} /readByAuthenticated ... and to remove this flag, use the following: /api/terminologies/ {terminologyId} /readByAuthenticated Similarly, to allow the terminology to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/terminologies/ {terminologyId} /readByEveryone ... and the following to remove this flag: /api/terminologies/ {terminologyId} /readByEveryone","title":"Sharing"},{"location":"rest-api/resources/terminology/#delete-terminology","text":"To delete a terminology, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/terminologies/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted terminology using the following endpoint: /api/admin/terminologies/ {id} /undoSoftDelete","title":"Delete terminology"},{"location":"rest-api/resources/terminology/#import-export-a-terminology","text":"To export a terminology using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple code sets: /api/terminologies/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single terminology, you can use the following endpoint with the id of the terminologies specified: /api/terminologies/ {terminologyId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more terminologies, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/terminologies/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Import / export a terminology"},{"location":"rest-api/resources/user-group/","text":"/api/userGroups/ {userGroupId} /catalogueUsers /api/userGroups/ {userGroupId} /catalogueUsers/ {catalogueUserId} /api/userGroups/ {userGroupId} /catalogueUsers/ {catalogueUserId} /api/userGroups /api/userGroups /api/userGroups/ {id} /api/userGroups/ {id} /api/userGroups/ {id}","title":"User group"},{"location":"rest-api/resources/user/","text":"/api/catalogueUsers /api/catalogueUsers /api/catalogueUsers/ {id} /api/catalogueUsers/ {id} /api/catalogueUsers/ {id} Admin functionality \u00b6 /api/catalogueUsers/adminRegister /api/catalogueUsers/pending /api/catalogueUsers/userExists/ {emailAddress} /api/catalogueUsers/ {catalogueUserId} /adminPasswordReset /api/catalogueUsers/ {catalogueUserId} /resetPasswordLink /api/catalogueUsers/ {catalogueUserId} /rejectRegistration /api/catalogueUsers/ {catalogueUserId} /approveRegistration /api/catalogueUsers/ {catalogueUserId} /changePassword /api/catalogueUsers/ {catalogueUserId} /userPreferences /api/catalogueUsers/ {catalogueUserId} /userPreferences Search \u00b6 /api/catalogueUsers/search/ {searchTerm?} /api/catalogueUsers/search Profile images \u00b6 /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image","title":"User"},{"location":"rest-api/resources/user/#admin-functionality","text":"/api/catalogueUsers/adminRegister /api/catalogueUsers/pending /api/catalogueUsers/userExists/ {emailAddress} /api/catalogueUsers/ {catalogueUserId} /adminPasswordReset /api/catalogueUsers/ {catalogueUserId} /resetPasswordLink /api/catalogueUsers/ {catalogueUserId} /rejectRegistration /api/catalogueUsers/ {catalogueUserId} /approveRegistration /api/catalogueUsers/ {catalogueUserId} /changePassword /api/catalogueUsers/ {catalogueUserId} /userPreferences /api/catalogueUsers/ {catalogueUserId} /userPreferences","title":"Admin functionality"},{"location":"rest-api/resources/user/#search","text":"/api/catalogueUsers/search/ {searchTerm?} /api/catalogueUsers/search","title":"Search"},{"location":"rest-api/resources/user/#profile-images","text":"/api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image","title":"Profile images"},{"location":"start/creating-model/","text":"To create a new DataModel, you first need to create a folder To create the model right click on the folder you\u2019ve created and select Add data model. Creating a data model requires the completion of some initial data: Label The name of the data model, as it will appear in the Finder panel. This should include any asset version information, as no two models may be given the same label. Author The name(s) of the authors of the model, or, alternatively, of the underlying data asset or specification Organisation The name of the organisation responsible for the documentation, or the underlying data asset / specification. Description A long-text description of the asset: this should include any information for users of the catalogue unfamiliar with the data asset or specification being described. Type This will be either a Data Asset or a Data Standard. Classifier Select from a list of existing classifiers, or type a new name to create a new classifier. You may enter multiple classifiers at this point. All fields except description and classifications are mandatory; however we recommend completing all fields. Once all this information has been populated, you want to click on next. You will then have the option of selecting the default data types, once done click next. Once you\u2019ve added the relevant property\u2019s click submit, and your Data model will be added to your folder and will appear in the right-hand panel if the catalogue.","title":"Creating model"},{"location":"tutorials/introduction/","text":"These tutorials are intended to provide more information about particular aspects of functionality. Unlike the User Guides , which explain how to achieve things in Mauro Data Mapper , these Tutorials will explain why such functionality is available and how it is intended to be used. Our current Tutorials: Semantic Links In this tutorial we explain why Semantic links are important, and how they can be used to assist re-use of existing data.","title":"Introduction"},{"location":"tutorials/semantic-links/","text":"Introduction \u00b6 When defining the meaning of data, it can be helpful to point to definitions elsewhere that are already known or understood. For example, we might like to consider that the 'Date of Birth' field in one dataset might mean exactly the same as 'Birth Date' in another. In practice, however, we rarely find that two Data Elements share exactly the same context. Measurements might be taken using different apparatus; answers to a question on a form may differ depending on how the question is phrased or presented; the timing or ordering of data collection may alter the possible values. To provide a more practical approach, in Mauro Data Mapper we can link two definitions to indicate that one refines the other: it says everything that the other definition says, and possibly more. In the general case, this allows us to define abstract definitions of data with minimal context (for example as a dictionary or a data specification) and relate more concrete definitions (for example the design of a data collection form or the description of a data asset). We may go further and for any definitions A and B, indicate that both A refines B and B refines A. The two links together imply that the two fields really are identical and share the exact same context. In practice, we've found this to be an overly strong statement and use it very rarely. Example \u00b6 As an example, consider the three definitions given in the diagram below: Here, we assume that the name of the Data Element which could be 'Word Count' and the Data Type , which could be 'Positive Integer' is the same in each case, and we are focusing simply on the explanatory text. The refinement arrows represent assertions that the definitions to the left and right are both refinements of the definition at the centre. In this example, the description 'number of words in document' defines a more abstract notion; the descriptions 'number of words in document according to Microsoft' and 'number of words in document according to Apple' provide extra context about the means of calculation. Such assertions cannot be derived automatically from the explanatory text. These explanations may be subjective and will only give a partial account of the context. It may be that several people with expertise in how the data is captured, recorded and analysed may all need to be consulted to provide an accurate assertion about the relationship between two descriptions. Note that there may not be any link defined between two more concrete definitions. In this example, there is no direct relationship between the Microsoft and Apple interpretations of the value - only that they both share some common abstract interpretation. Of course there may be some mapping of values, or conversion algorithm, which allows data collected according to one definition to be converted into a form matching another definition. In this case, perhaps opening the document on another computer and re-running the word count. Simple mappings, for example when converting units of measurement, can be uncontroversial, but in general they will also be subjective or suitable only for a particular purpose. Interpretation \u00b6 There are two immediate practical applications of linking information. Considering the example above, first suppose that some data assets exist for each of the two concrete definitions above - some documents whose word count has been calculated by either Microsoft of Apple software. If the analysis only requires a value for the 'number of words in a document' , then both types of data will be equally applicable and can be included in the analysis. If, however, the requirement is for counts according to Microsoft, then those documents whose word count has been calculated by Apple may not be suitable. Another application might be in the provision of data according to some specification. If a data specification requires values according to our abstract description: 'the number of words in document' and if we already have values for those documents as calculated by Microsoft software, then those values are suitable to provide. If, however, the specification is more concrete, and requires those word counts to have been determined by Microsoft software, then any word counts calculated by other software, or, more importantly, any document word count whose provenance is not known , will not be suitable to provide. Linking in Mauro \u00b6 In Mauro Data Mapper , Semantic links can be recorded against any model components, but are typically used between two Data Elements in different Data Models . Or between an enumeration value in a model and a term in a Terminology . Other mappings, for example to indicate refinement between two Data Classes , may be much harder to interpret and may only be of use in particular circumstances. As described above, the assertion of Semantic links may require domain knowledge and cannot be automatically inferred from the text of a description. In Mauro Data Mapper , links are not created automatically during ingest of models, but may be manually asserted between items individually. However, Mauro includes a tool which will make suggestions based on the description text of the fields and also the name of the field, the name of the Data Type and containing Data Class , and any enumeration values. The user interface allows users to select the best match, or choose from other alternatives ranked by \u2018closest match\u2019 . Although this doesn't remove the need for a human to make the final decision, it can save time searching for the correct item within a target Data Model . Does not refine \u00b6 As our models may represent incomplete information about the artefacts they represent and the semantic relationships between them, we cannot infer that no refinement exists simply because there is no refines link present. To record the assertion that no refinement exists, we can use the ** does not refine **link. As a further example, consider the three attribute definitions given in the diagram below. Here, we have a more specific definition at the centre of the diagram: \u2018number of words in a document ignoring hyphens\u2019 . We have also an assertion that this refines the definition with the explanatory text \u2018number of words in a document\u2019 . If we accept this assertion, then any data collected against the new definition can be used in any situation in which the original definition was accepted. We have also an assertion that the definition \u2018number of words in a document according to Microsoft\u2019 is not a refinement of this new, more specific definition. The \u2018word count\u2019 feature in Microsoft's Word application treats hyphenated phrases as single words: it does not ignore hyphens. In contrast, the same feature in Apple's Pages application does ignore hyphens, treating them as if they were spaces. For example, Word will count the phrase \u2018strongly-connected\u2019 as one word, whereas Pages will count it as two words. In general, specifying 'does not refine' as a universal statement - suggesting that there are no circumstances where item A may be used according to definition B - is a strong statement whose use will be limited. However, there can be value in disambiguating or asserting a distinction between two similarly-defined items whose descriptions may otherwise cause confusion. To find out how to add a Semantic link between two descriptions of data, see our Semantic links user guide","title":"Semantic Links"},{"location":"tutorials/semantic-links/#introduction","text":"When defining the meaning of data, it can be helpful to point to definitions elsewhere that are already known or understood. For example, we might like to consider that the 'Date of Birth' field in one dataset might mean exactly the same as 'Birth Date' in another. In practice, however, we rarely find that two Data Elements share exactly the same context. Measurements might be taken using different apparatus; answers to a question on a form may differ depending on how the question is phrased or presented; the timing or ordering of data collection may alter the possible values. To provide a more practical approach, in Mauro Data Mapper we can link two definitions to indicate that one refines the other: it says everything that the other definition says, and possibly more. In the general case, this allows us to define abstract definitions of data with minimal context (for example as a dictionary or a data specification) and relate more concrete definitions (for example the design of a data collection form or the description of a data asset). We may go further and for any definitions A and B, indicate that both A refines B and B refines A. The two links together imply that the two fields really are identical and share the exact same context. In practice, we've found this to be an overly strong statement and use it very rarely.","title":"Introduction"},{"location":"tutorials/semantic-links/#example","text":"As an example, consider the three definitions given in the diagram below: Here, we assume that the name of the Data Element which could be 'Word Count' and the Data Type , which could be 'Positive Integer' is the same in each case, and we are focusing simply on the explanatory text. The refinement arrows represent assertions that the definitions to the left and right are both refinements of the definition at the centre. In this example, the description 'number of words in document' defines a more abstract notion; the descriptions 'number of words in document according to Microsoft' and 'number of words in document according to Apple' provide extra context about the means of calculation. Such assertions cannot be derived automatically from the explanatory text. These explanations may be subjective and will only give a partial account of the context. It may be that several people with expertise in how the data is captured, recorded and analysed may all need to be consulted to provide an accurate assertion about the relationship between two descriptions. Note that there may not be any link defined between two more concrete definitions. In this example, there is no direct relationship between the Microsoft and Apple interpretations of the value - only that they both share some common abstract interpretation. Of course there may be some mapping of values, or conversion algorithm, which allows data collected according to one definition to be converted into a form matching another definition. In this case, perhaps opening the document on another computer and re-running the word count. Simple mappings, for example when converting units of measurement, can be uncontroversial, but in general they will also be subjective or suitable only for a particular purpose.","title":"Example"},{"location":"tutorials/semantic-links/#interpretation","text":"There are two immediate practical applications of linking information. Considering the example above, first suppose that some data assets exist for each of the two concrete definitions above - some documents whose word count has been calculated by either Microsoft of Apple software. If the analysis only requires a value for the 'number of words in a document' , then both types of data will be equally applicable and can be included in the analysis. If, however, the requirement is for counts according to Microsoft, then those documents whose word count has been calculated by Apple may not be suitable. Another application might be in the provision of data according to some specification. If a data specification requires values according to our abstract description: 'the number of words in document' and if we already have values for those documents as calculated by Microsoft software, then those values are suitable to provide. If, however, the specification is more concrete, and requires those word counts to have been determined by Microsoft software, then any word counts calculated by other software, or, more importantly, any document word count whose provenance is not known , will not be suitable to provide.","title":"Interpretation"},{"location":"tutorials/semantic-links/#linking-in-mauro","text":"In Mauro Data Mapper , Semantic links can be recorded against any model components, but are typically used between two Data Elements in different Data Models . Or between an enumeration value in a model and a term in a Terminology . Other mappings, for example to indicate refinement between two Data Classes , may be much harder to interpret and may only be of use in particular circumstances. As described above, the assertion of Semantic links may require domain knowledge and cannot be automatically inferred from the text of a description. In Mauro Data Mapper , links are not created automatically during ingest of models, but may be manually asserted between items individually. However, Mauro includes a tool which will make suggestions based on the description text of the fields and also the name of the field, the name of the Data Type and containing Data Class , and any enumeration values. The user interface allows users to select the best match, or choose from other alternatives ranked by \u2018closest match\u2019 . Although this doesn't remove the need for a human to make the final decision, it can save time searching for the correct item within a target Data Model .","title":"Linking in Mauro"},{"location":"tutorials/semantic-links/#does-not-refine","text":"As our models may represent incomplete information about the artefacts they represent and the semantic relationships between them, we cannot infer that no refinement exists simply because there is no refines link present. To record the assertion that no refinement exists, we can use the ** does not refine **link. As a further example, consider the three attribute definitions given in the diagram below. Here, we have a more specific definition at the centre of the diagram: \u2018number of words in a document ignoring hyphens\u2019 . We have also an assertion that this refines the definition with the explanatory text \u2018number of words in a document\u2019 . If we accept this assertion, then any data collected against the new definition can be used in any situation in which the original definition was accepted. We have also an assertion that the definition \u2018number of words in a document according to Microsoft\u2019 is not a refinement of this new, more specific definition. The \u2018word count\u2019 feature in Microsoft's Word application treats hyphenated phrases as single words: it does not ignore hyphens. In contrast, the same feature in Apple's Pages application does ignore hyphens, treating them as if they were spaces. For example, Word will count the phrase \u2018strongly-connected\u2019 as one word, whereas Pages will count it as two words. In general, specifying 'does not refine' as a universal statement - suggesting that there are no circumstances where item A may be used according to definition B - is a strong statement whose use will be limited. However, there can be value in disambiguating or asserting a distinction between two similarly-defined items whose descriptions may otherwise cause confusion. To find out how to add a Semantic link between two descriptions of data, see our Semantic links user guide","title":"Does not refine"},{"location":"tutorials/document-assets/","text":"This document explains how to create a standard description for a health dataset using the metadata catalogue. Such a description will come in two parts: a description of the dataset as a whole descriptions of the individual data items within the dataset, and of the structural relationships between them There is no single, applicable standard for the first part. For the moment, pending the development of the gateway interface, we require only a minimal set of properties, sufficient to uniquely identify the dataset in question. A slightly longer list will be required once the gateway specification has been agreed. For the second part, we are able to adapt and extend an existing international standard for metadata registration: ISO/IEC 11179. We specify a list of properties that should be recorded for each item, and each relationship; this list will remain unchanged, although the information provided may be updated over time. If the dataset is held in a relational datastore, then we may be able to determine the name and type of each data item, and the structural relationships between them, automatically. We will not, however, be able to determine an adequate, human-readable explanation of each item ; this will need to be entered by hand and/or carefully extracted from existing, electronic documentation. Describing the dataset \u00b6 Each of the \u2018top level\u2019 descriptions in the catalogue is called a model . Models are stored in a familiar folder (or directory) structure . Folders may have sub-folders. The folder tree is shown to the left of the screen. To create a new model of a health dataset, first choose the folder in which you wish to store it. To create a new folder at the top-level, click the \u2018plus\u2019 button at the top of the tree view. To create a sub-folder of an existing folder, right-click the folder and choose \u2018Add folder\u2019. When creating a new folder, you must give it a name , and should enter a short description describing the purpose of the folder. To create a new model, right-click upon the folder in which you want it to appear, and choose \u2018Add Data Model\u2019: You will be presented with a short form to enter the details of your new model: Choose a label for your model. This should be enough to uniquely identify the dataset that you are describing. You will be able to add other names or labels later on. The author field should be used to record the names of those creating and maintaining this data model (not the dataset itself). The organisation field should be used to record the name of the organisation holding the dataset. In the description field, enter a short (e.g. 2 paragraphs), human-readable description of the data stored within that dataset, and any important characteristics of the data. For type , choose \u2018data asset\u2019. Once you have entered all mandatory fields, labelled with \u2018 * \u2019, you can click \u2018 Next \u2019 to choose a default set of data types to be imported into your model. For example, you may choose to import the default datatypes of a MS SQL Server database. If you are unsure at this stage, you can leave the field blank - you can always import these later on. Click \u2018 Submit \u2019 to finish creating the new data model. This will take you to the \u2018overview page\u2019 of your new model. You are welcome to record further characteristics of the dataset - this could help the gateway providers when the come to design their interface . If you wish to do this, Click the \u2018 Properties \u2019 tab on the panel below, and choose the \u2018 + \u2019 button to add each new property. namespace : This will be used to select the correct profile / property list. key : This is the property name - e.g. \u2018contact email\u2019. You should add a property for each of the names listed below, but may add further properties if you wish. value : This is the value of the given property - e.g. \u2018 enquiries-mydataset@hub.org \u2019. Describing the data items \u00b6 Data items are created and managed within data classes. If a dataset is managed as a collection of tables, then you may wish to create a class for each table. This is the default approach. Alternatively, you may wish to create a set of classes to provide a more abstract account of the data set - grouping and presenting the data items in a way that is quite different from the way in which they are stored and managed. To create a new class, select a data model from the model tree, choose the \u2018DataClasses\u2019 tab, and click the \u2018+\u2019 button: In documenting an existing data set, you will be creating rather than copying classes, so choose the first option and click \u2018 Next \u2019. You should then choose a name or label for your class. Again, you will be able to add further names later, as aliases, if you wish. The description of a class may explain what kind of data items are grouped together here; alternatively, it may explain some common context for the items it contains, to avoid the need to include that information in the description of each individual item. The multiplicity values specify the number of instances of that class that may appear in an instance of the model. For example, if a class were to correspond to a table in a relational database, the multiplicity values would be the minimum, and the maximum, number of rows allowed in the table. In a model of a dataset, there is usually no need to specify the multiplicity of a class. Once all mandatory fields have been completed, you may click \u2018 Submit \u2019 to create the new data class. This will take you to the page for the newly -created class. You can click the link back to the parent data model to continue adding further classes of data as necessary. You may also choose to add further \u2018child\u2019 classes to this class: choose the 'Content\u2019 tab on the DataClass page, and click the \u2018+\u2019 symbol to add a new data class. Data items are represented as \u2018data elements\u2019 within the model. To start adding data elements to a class, visit its DataClass page, and click the \u2018+\u2019 button on the \u2018Content\u2019 tab. This will give you two options - to create a new contained \u2018child\u2019 class, as above, or to create a new data element - choose the second option. As with creating a new class, this will give you the option to copy an existing data element from elsewhere in the model (or from another model which you have read access to). In creating a model of an existing dataset, you will almost certainly want to create a new data element. The next form that appears will ask for the details of the new data element: For label , enter the preferred name of the data item. This could be the name of a column in a relational database, for example. You can add further names as aliases later. In description , you should describe the data item and the values it may take. This may include information about provenance - the context of collection, and any subsequent processing, but should also say something about intended interpretation or possible use. This description should explain the data point in terms that might be useful to prospective data users. The multiplicity of a data element specifies the number of values that it may take at the same time. For example, the number of \u2018date of death \u2019 items contained within a \u2018patient\u2019 record might be at least 0, and at most 1. The min field should be a positive whole number; the max field should be a positive whole number no smaller than min, or may be \u2018 * \u2019 to indicate that no maximum number is set. If you are unsure, this field may be left blank. The data type field describes the values that this data item may take. A data type may be either: primitive : for example a String, an Integer, or a Date. enumerated : chosen from a given list of values, which may be described using codes or free-text. For example M = Male, F = Female, U = Unknown reference-valued : a reference to another class of data. For example, the \u2018Registered GP\u2019 data element of a \u2018Patient\u2019 may refer to a separate class of \u2018GP\u2019 (containing name, surgery, address, etc) When creating a new data element, you can choose to use an existing data type that belongs to the data model in question, or you can create a new data type (by providing its name, the list of values, or a class to reference, respectively). From the data model page it is also possible to import data types from another model, or some pre-defined sets of data types (such as those found in MS SQLServer databases, for example). Click \u2018 Submit \u2019 to create the new Data Element. Repeat this process to add the other data elements for each class. Entering this information offline \u00b6 While creating a model of an existing dataset, you may find it more convenient to enter and share the information above using an Excel spreadsheet - and then upload the contents of that spreadsheet into the catalogue. You can create more than one model using the same spreadsheet. A blank spreadsheet should be included with this document, together with an example of a completed spreadsheet, describing the Diagnostic Imaging Dataset. The first sheet o the spreadsheet (which must be called DataModels) should introduce one or more data models. Each subsequent sheet should describe the contents of one of these data models. In the DataModels sheet, there should be one row for each data model described, and the following columns can be completed: SHEET_KEY : the name of the sheet (in this spreadsheet) describing the contents of the data model Name : the name or label of the data model (as explained above) Description : a brief description of the dataset Author : the author of this data model of the dataset Organisation : the organisation holding the dataset Type : this should be \u2018Data Asset\u2019 In the blank spreadsheet supplied, there is a KEY_1 sheet with the column headings required for each of the subsequent sheets. You can rename or copy this sheet. Whatever name is chosen should be included in the list of data models presented in the opening \u2018DataModels\u2019 sheet (or the contents will not be uploaded). The following columns should be completed: DataClass Path (essential): this should be the path from the top level of the model to the data class described in the current row, or the class containing the item described in the current row; for a top-level class, it will be simply the class name; for child classes, it will be a list of class names, using \u201c|\u201d as a delimiter. DataElement Name (essential for data elements): if the row is describing a data element, rather than a data class, then the name of the element should be inserted here Description : an explanation of the intended interpretation (and perhaps also the context of collection or provenance) of the DataClass or DataElement Minimum Multiplicity (may be left blank): the minimum number of instances of the class or element, usually 0 or 1 Maximum Multiplicity (may be left blank): the maximum number of instances, with -1 (rather than *) used to indicate that there is no upper bound DataType Name (essential for data elements): the name of the data type of the data element being described DataType Description (needed only for the first time that the data type in question is mentioned): the description of the data type Reference to DataClass Path : if the data type is another class (if the data element is a reference to an instance of another class) then insert the path to that class here Enumeration Key and Enumeration Value : if the data type is an enumeration, then you may add several pairs of entries in these two columns, one for each key-value pair in the enumeration; in each case, the key is the text or string that may appear in a column of the dataset, and the value is its expansion or explanatory text. Once you have done this, you will need to select the corresponding cells in the DataClass Path column - the first column - and merge them. You may wish to merge the corresponding cells in the other columns as well, for a clearer presentation of the information: for example, A completed spreadsheet can be imported into the catalogue using the \u2018import\u2019 button, located on the toolbar at the top of the screen: Select the Excel importer. Having chosen to import from a spreadsheet, you will have the opportunity to provide some more information: Folder : this is the folder in which the newly created model(s) will reside. The drop-down menu lets you choose a folder which you have access to Finalised : we recommend that you keep models as \u2018draft\u2019 until the gateway presentation of model descriptions has been decided Import : as new documentation version check this option if you intend to replace the current (catalogue) version of an existing model description File : Choose the spreadsheet file for upload. You may drag/drop a file from your file browser into the box here. Press \u2018 Submit \u2019 to import the model. Excel files can be safely used to \"round-trip\" data model descriptions. You can export a model from the catalogue in spreadsheet form, edit the spreadsheet, and import the new version of the spreadsheet to produce an updated version of the model - in this case, you need to select \u201cNew Documentation Version\u201d. Extracting metadata from a relational database \u00b6 If the dataset to be described is held in a relational database, and you have direct access to that database, you can also use the catalogue to extract basic metadata - table names, column names, types, and structural relationships - from the database itself. The effort of manual data entry can then be focussed upon producing adequate accounts of the account of the intended interpretation of each data element. Clicking on the \u2018import\u2019 button as above, you can select an importer for most types of relational database. Having chosen to import from a relational database, the next set of options allow you to configure the import. Fields marked with a \u2018*\u2019 are mandatory. The fields should be completed as follows: Folder : this is the folder in which the newly created model(s) will reside. The drop-down menu lets you choose a folder which you have access to Data Model Name : this will be the name of the new data model. If no name is specified, the name of the database will be used. Finalised : if the newly-imported model is immediately \u2018finalised\u2019, you will no-longer be able to make changes to it. We recommend that you keep models as \u2018draft\u2019; otherwise if you wish to edit the descriptions you will need to create a new version. Import as new documentation version : this determines how the newly imported model supersedes any existing model with the same name. We recommend you check this option if you intend to overwrite an older version of the model. Database Name(s) : please enter the database name. If you\u2019d like to import multiple databases in one go, please enter the list of names , separated by a comma. Database Host : this is the IP address, or the server name, of the machine that the database is installed on. Username : this is the username which is used to connect to the database. Ideally, a user with read-only access will be used. This is not stored within the Metadata Catalogue application. Password : the password used by the user to connect to the database. This is not stored within the Metadata Catalogue application. Database port : the port which the database is communicating on - eg. 1433 for MS SQL Server. If no port is specified, the default port for the database will be used (MS SQL Server: 1433, Postgres: 5432, OracleDB: 1521) SSL : whether the database requires an encrypted connection (usually false). Other features \u00b6 The catalogue toolkit has a range of related functions for creating and updating models of datasets and data standards. As the features of the gateway interface are determined, we will update this documentation to address any additional metadata requirements, and to describe any additional functions that have become relevant. We will update it also to describe any improvements made to the functions described above.","title":"Index"},{"location":"tutorials/document-assets/#describing-the-dataset","text":"Each of the \u2018top level\u2019 descriptions in the catalogue is called a model . Models are stored in a familiar folder (or directory) structure . Folders may have sub-folders. The folder tree is shown to the left of the screen. To create a new model of a health dataset, first choose the folder in which you wish to store it. To create a new folder at the top-level, click the \u2018plus\u2019 button at the top of the tree view. To create a sub-folder of an existing folder, right-click the folder and choose \u2018Add folder\u2019. When creating a new folder, you must give it a name , and should enter a short description describing the purpose of the folder. To create a new model, right-click upon the folder in which you want it to appear, and choose \u2018Add Data Model\u2019: You will be presented with a short form to enter the details of your new model: Choose a label for your model. This should be enough to uniquely identify the dataset that you are describing. You will be able to add other names or labels later on. The author field should be used to record the names of those creating and maintaining this data model (not the dataset itself). The organisation field should be used to record the name of the organisation holding the dataset. In the description field, enter a short (e.g. 2 paragraphs), human-readable description of the data stored within that dataset, and any important characteristics of the data. For type , choose \u2018data asset\u2019. Once you have entered all mandatory fields, labelled with \u2018 * \u2019, you can click \u2018 Next \u2019 to choose a default set of data types to be imported into your model. For example, you may choose to import the default datatypes of a MS SQL Server database. If you are unsure at this stage, you can leave the field blank - you can always import these later on. Click \u2018 Submit \u2019 to finish creating the new data model. This will take you to the \u2018overview page\u2019 of your new model. You are welcome to record further characteristics of the dataset - this could help the gateway providers when the come to design their interface . If you wish to do this, Click the \u2018 Properties \u2019 tab on the panel below, and choose the \u2018 + \u2019 button to add each new property. namespace : This will be used to select the correct profile / property list. key : This is the property name - e.g. \u2018contact email\u2019. You should add a property for each of the names listed below, but may add further properties if you wish. value : This is the value of the given property - e.g. \u2018 enquiries-mydataset@hub.org \u2019.","title":"Describing the dataset"},{"location":"tutorials/document-assets/#describing-the-data-items","text":"Data items are created and managed within data classes. If a dataset is managed as a collection of tables, then you may wish to create a class for each table. This is the default approach. Alternatively, you may wish to create a set of classes to provide a more abstract account of the data set - grouping and presenting the data items in a way that is quite different from the way in which they are stored and managed. To create a new class, select a data model from the model tree, choose the \u2018DataClasses\u2019 tab, and click the \u2018+\u2019 button: In documenting an existing data set, you will be creating rather than copying classes, so choose the first option and click \u2018 Next \u2019. You should then choose a name or label for your class. Again, you will be able to add further names later, as aliases, if you wish. The description of a class may explain what kind of data items are grouped together here; alternatively, it may explain some common context for the items it contains, to avoid the need to include that information in the description of each individual item. The multiplicity values specify the number of instances of that class that may appear in an instance of the model. For example, if a class were to correspond to a table in a relational database, the multiplicity values would be the minimum, and the maximum, number of rows allowed in the table. In a model of a dataset, there is usually no need to specify the multiplicity of a class. Once all mandatory fields have been completed, you may click \u2018 Submit \u2019 to create the new data class. This will take you to the page for the newly -created class. You can click the link back to the parent data model to continue adding further classes of data as necessary. You may also choose to add further \u2018child\u2019 classes to this class: choose the 'Content\u2019 tab on the DataClass page, and click the \u2018+\u2019 symbol to add a new data class. Data items are represented as \u2018data elements\u2019 within the model. To start adding data elements to a class, visit its DataClass page, and click the \u2018+\u2019 button on the \u2018Content\u2019 tab. This will give you two options - to create a new contained \u2018child\u2019 class, as above, or to create a new data element - choose the second option. As with creating a new class, this will give you the option to copy an existing data element from elsewhere in the model (or from another model which you have read access to). In creating a model of an existing dataset, you will almost certainly want to create a new data element. The next form that appears will ask for the details of the new data element: For label , enter the preferred name of the data item. This could be the name of a column in a relational database, for example. You can add further names as aliases later. In description , you should describe the data item and the values it may take. This may include information about provenance - the context of collection, and any subsequent processing, but should also say something about intended interpretation or possible use. This description should explain the data point in terms that might be useful to prospective data users. The multiplicity of a data element specifies the number of values that it may take at the same time. For example, the number of \u2018date of death \u2019 items contained within a \u2018patient\u2019 record might be at least 0, and at most 1. The min field should be a positive whole number; the max field should be a positive whole number no smaller than min, or may be \u2018 * \u2019 to indicate that no maximum number is set. If you are unsure, this field may be left blank. The data type field describes the values that this data item may take. A data type may be either: primitive : for example a String, an Integer, or a Date. enumerated : chosen from a given list of values, which may be described using codes or free-text. For example M = Male, F = Female, U = Unknown reference-valued : a reference to another class of data. For example, the \u2018Registered GP\u2019 data element of a \u2018Patient\u2019 may refer to a separate class of \u2018GP\u2019 (containing name, surgery, address, etc) When creating a new data element, you can choose to use an existing data type that belongs to the data model in question, or you can create a new data type (by providing its name, the list of values, or a class to reference, respectively). From the data model page it is also possible to import data types from another model, or some pre-defined sets of data types (such as those found in MS SQLServer databases, for example). Click \u2018 Submit \u2019 to create the new Data Element. Repeat this process to add the other data elements for each class.","title":"Describing the data items"},{"location":"tutorials/document-assets/#entering-this-information-offline","text":"While creating a model of an existing dataset, you may find it more convenient to enter and share the information above using an Excel spreadsheet - and then upload the contents of that spreadsheet into the catalogue. You can create more than one model using the same spreadsheet. A blank spreadsheet should be included with this document, together with an example of a completed spreadsheet, describing the Diagnostic Imaging Dataset. The first sheet o the spreadsheet (which must be called DataModels) should introduce one or more data models. Each subsequent sheet should describe the contents of one of these data models. In the DataModels sheet, there should be one row for each data model described, and the following columns can be completed: SHEET_KEY : the name of the sheet (in this spreadsheet) describing the contents of the data model Name : the name or label of the data model (as explained above) Description : a brief description of the dataset Author : the author of this data model of the dataset Organisation : the organisation holding the dataset Type : this should be \u2018Data Asset\u2019 In the blank spreadsheet supplied, there is a KEY_1 sheet with the column headings required for each of the subsequent sheets. You can rename or copy this sheet. Whatever name is chosen should be included in the list of data models presented in the opening \u2018DataModels\u2019 sheet (or the contents will not be uploaded). The following columns should be completed: DataClass Path (essential): this should be the path from the top level of the model to the data class described in the current row, or the class containing the item described in the current row; for a top-level class, it will be simply the class name; for child classes, it will be a list of class names, using \u201c|\u201d as a delimiter. DataElement Name (essential for data elements): if the row is describing a data element, rather than a data class, then the name of the element should be inserted here Description : an explanation of the intended interpretation (and perhaps also the context of collection or provenance) of the DataClass or DataElement Minimum Multiplicity (may be left blank): the minimum number of instances of the class or element, usually 0 or 1 Maximum Multiplicity (may be left blank): the maximum number of instances, with -1 (rather than *) used to indicate that there is no upper bound DataType Name (essential for data elements): the name of the data type of the data element being described DataType Description (needed only for the first time that the data type in question is mentioned): the description of the data type Reference to DataClass Path : if the data type is another class (if the data element is a reference to an instance of another class) then insert the path to that class here Enumeration Key and Enumeration Value : if the data type is an enumeration, then you may add several pairs of entries in these two columns, one for each key-value pair in the enumeration; in each case, the key is the text or string that may appear in a column of the dataset, and the value is its expansion or explanatory text. Once you have done this, you will need to select the corresponding cells in the DataClass Path column - the first column - and merge them. You may wish to merge the corresponding cells in the other columns as well, for a clearer presentation of the information: for example, A completed spreadsheet can be imported into the catalogue using the \u2018import\u2019 button, located on the toolbar at the top of the screen: Select the Excel importer. Having chosen to import from a spreadsheet, you will have the opportunity to provide some more information: Folder : this is the folder in which the newly created model(s) will reside. The drop-down menu lets you choose a folder which you have access to Finalised : we recommend that you keep models as \u2018draft\u2019 until the gateway presentation of model descriptions has been decided Import : as new documentation version check this option if you intend to replace the current (catalogue) version of an existing model description File : Choose the spreadsheet file for upload. You may drag/drop a file from your file browser into the box here. Press \u2018 Submit \u2019 to import the model. Excel files can be safely used to \"round-trip\" data model descriptions. You can export a model from the catalogue in spreadsheet form, edit the spreadsheet, and import the new version of the spreadsheet to produce an updated version of the model - in this case, you need to select \u201cNew Documentation Version\u201d.","title":"Entering this information offline"},{"location":"tutorials/document-assets/#extracting-metadata-from-a-relational-database","text":"If the dataset to be described is held in a relational database, and you have direct access to that database, you can also use the catalogue to extract basic metadata - table names, column names, types, and structural relationships - from the database itself. The effort of manual data entry can then be focussed upon producing adequate accounts of the account of the intended interpretation of each data element. Clicking on the \u2018import\u2019 button as above, you can select an importer for most types of relational database. Having chosen to import from a relational database, the next set of options allow you to configure the import. Fields marked with a \u2018*\u2019 are mandatory. The fields should be completed as follows: Folder : this is the folder in which the newly created model(s) will reside. The drop-down menu lets you choose a folder which you have access to Data Model Name : this will be the name of the new data model. If no name is specified, the name of the database will be used. Finalised : if the newly-imported model is immediately \u2018finalised\u2019, you will no-longer be able to make changes to it. We recommend that you keep models as \u2018draft\u2019; otherwise if you wish to edit the descriptions you will need to create a new version. Import as new documentation version : this determines how the newly imported model supersedes any existing model with the same name. We recommend you check this option if you intend to overwrite an older version of the model. Database Name(s) : please enter the database name. If you\u2019d like to import multiple databases in one go, please enter the list of names , separated by a comma. Database Host : this is the IP address, or the server name, of the machine that the database is installed on. Username : this is the username which is used to connect to the database. Ideally, a user with read-only access will be used. This is not stored within the Metadata Catalogue application. Password : the password used by the user to connect to the database. This is not stored within the Metadata Catalogue application. Database port : the port which the database is communicating on - eg. 1433 for MS SQL Server. If no port is specified, the default port for the database will be used (MS SQL Server: 1433, Postgres: 5432, OracleDB: 1521) SSL : whether the database requires an encrypted connection (usually false).","title":"Extracting metadata from a relational database"},{"location":"tutorials/document-assets/#other-features","text":"The catalogue toolkit has a range of related functions for creating and updating models of datasets and data standards. As the features of the gateway interface are determined, we will update this documentation to address any additional metadata requirements, and to describe any additional functions that have become relevant. We will update it also to describe any improvements made to the functions described above.","title":"Other features"},{"location":"user/models/","text":"","title":"Models"},{"location":"user-guides/introduction/","text":"In this section you will find a variety of user guides which explain how to use Mauro Data Mapper . You don't need to follow them all in sequence - just dive into a guide that interests you! Our current User Guides: Create a Data Model In this user guide we explain how to use the web interface to create a new folder and a new Data Model from scratch, including selecting some default Data Types . Document a Health Dataset This longer guide explains how to add structure to your Data Model such as incorporating Data Classes and Data Elements . Exporting Data Models This user guide will explain how to export a Data Model using a variety of exporters including XML, JSON, XML Schema, Grails and Excel. Import a Data Model from Excel This guide presents the steps for uploading a Data Model from an Excel Spreadsheet, and explains how you can use the import / export capabilities to 'round-trip' documentation with external users. How to version and merge Data Models This guide explains how to create working versions of Data Models and how to merge changes into the main branch. This ensures changes can be made without adversely affecting other users. How to search In this guide we walk you through the searching capabilities of Mauro Data Mapper , including the different search types, the syntaxes used as well as tips on how to conduct specific searches. Semantic links This is a short guide which explains the steps required to add, edit and delete Semantic links . Publish/Subscribe This guide explains how to connect to another Mauro Data Mapper instance to consume its Atom feed of Federated Data Models . Each Federated Data Model can then be subscribed to for use in your own Mauro Data Mapper instance. User Profile This short guide explains how to update your User profile within the Mauro Data Mapper web interface and how to change your login password. Admin functionality This user guide will walk you through all the options and settings that are available to adminsitrators on Mauro Data Mapper .","title":"Introduction"},{"location":"user-guides/add-a-semantic-link/semantic-links/","text":"This user guide will explain the steps you need to follow to add a Semantic link between two descriptions of data. 1. Add a Semantic link \u00b6 Firstly, navigate to the source Data Model , Data Class or Data Element that you want to create the link from. Select the relevant data item in the Model Tree to display the details panel on the right. Any existing Semantic links are summarised in the 'Links' table below the details panel. To add, edit or remove a Semantic link , select the 'Links' tab, which will also display a list of existing Semantic links . Click the '+ Add Link' button at the top right of the 'Links' table which will add a new row. Complete the fields as described below: Source This is the Data Model , Data Class or Data Element which you are linking from. Link From the dropdown menu, select the type of Semantic link between 'Refines' or 'Does Not Refine' . A 'Refines' link is used when one description 'Refines' another. In other words, everything that is true about the target description is also true about the source description, whilst often adding more information or context. A 'Does Not Refine' link is used when the target description is not intended as a refinement of the source description. Target Select the target description that you want to link to. To do this, click 'Add target' which will open up a seperate box. Choose whether the target description is a Data Model or a Data Class and then select the relevant item from the Model Tree . This will automatically populate the 'Target' field and once completed, click the green tick to save and a green notification box should appear at the bottom right of your screen confirming that the 'Link saved successfuly' . 2. Delete a Semantic link \u00b6 To delete an existing Semantic link , navigate to the relevant link in the 'Links' tab underneath the details panel. Click the 'Delete' bin icon where you will then be asked to confirm the change. Click the green tick and a green notification box should appear at the bottom right of your screen confirming that the 'Link deleted successfuly' . 3. Edit a Semantic link \u00b6 To edit an existing Semantic link , navigate to the relevant link in the 'Links' tab underneath the details panel. Click the 'Edit' pencil icon which will allow you to change the 'Link' and 'Target' columns. Click the green tick to the right to confirm your changes and a green notification box should appear at the bottom right of your screen confirming that the 'Link updated successfuly' .","title":"Semantic links"},{"location":"user-guides/add-a-semantic-link/semantic-links/#1-add-a-semantic-link","text":"Firstly, navigate to the source Data Model , Data Class or Data Element that you want to create the link from. Select the relevant data item in the Model Tree to display the details panel on the right. Any existing Semantic links are summarised in the 'Links' table below the details panel. To add, edit or remove a Semantic link , select the 'Links' tab, which will also display a list of existing Semantic links . Click the '+ Add Link' button at the top right of the 'Links' table which will add a new row. Complete the fields as described below: Source This is the Data Model , Data Class or Data Element which you are linking from. Link From the dropdown menu, select the type of Semantic link between 'Refines' or 'Does Not Refine' . A 'Refines' link is used when one description 'Refines' another. In other words, everything that is true about the target description is also true about the source description, whilst often adding more information or context. A 'Does Not Refine' link is used when the target description is not intended as a refinement of the source description. Target Select the target description that you want to link to. To do this, click 'Add target' which will open up a seperate box. Choose whether the target description is a Data Model or a Data Class and then select the relevant item from the Model Tree . This will automatically populate the 'Target' field and once completed, click the green tick to save and a green notification box should appear at the bottom right of your screen confirming that the 'Link saved successfuly' .","title":"1. Add a Semantic link"},{"location":"user-guides/add-a-semantic-link/semantic-links/#2-delete-a-semantic-link","text":"To delete an existing Semantic link , navigate to the relevant link in the 'Links' tab underneath the details panel. Click the 'Delete' bin icon where you will then be asked to confirm the change. Click the green tick and a green notification box should appear at the bottom right of your screen confirming that the 'Link deleted successfuly' .","title":"2. Delete a Semantic link"},{"location":"user-guides/add-a-semantic-link/semantic-links/#3-edit-a-semantic-link","text":"To edit an existing Semantic link , navigate to the relevant link in the 'Links' tab underneath the details panel. Click the 'Edit' pencil icon which will allow you to change the 'Link' and 'Target' columns. Click the green tick to the right to confirm your changes and a green notification box should appear at the bottom right of your screen confirming that the 'Link updated successfuly' .","title":"3. Edit a Semantic link"},{"location":"user-guides/admin-functionality/admin-functionality/","text":"This user guide will walk you through all the options and settings that are available to adminsitrators on Mauro Data Mapper . 1. Dashboard \u00b6 To access your admin dashboard, log in to Mauro Data Mapper and click the white arrow by your user profile to the right of the menu header. From the dropdown menu under 'Admin settings' select 'Dashboard' . Your dashboard displays two tabs. One shows your 'Active sessions' and the other shows the 'Plugins and Modules' in the repository. 2. Model management \u00b6 As an administrator, you can delete several elements such as Data Models and Terminologies . To do this, select 'Model management' from the user profile dropdown menu. Select the relevant options to filter elements by type and status and the Model Tree displayed at the bottom of the page will automatically filter. Once you've found the element you wish to delete, click the checkbox until a green tick appears. You can do this for multiple elements and a summary list will be displayed on the right. Here, you will then have the option to either 'Delete Permanently' or 'Mark as Deleted' . 3. Emails \u00b6 To access your emails, select 'Emails' from the user profile dropdown menu. This will take you to your inbox where you can compose, edit and delete messages. 4. Manage users \u00b6 As an administrator you can add, activate and deactive users. Firstly, select 'Manage users' from the user profile dropdown menu. This will navigate you to a full list of all the users within the repository. Each user's full name, email, organisation, role, any groups they are associated with as well as the status of their account will be displayed. To order the list, you can click the small arrow to the right of each column heading. This will display the list in chronological order according to that category. To order from A-Z click the arrow until it points up. To order from Z-A click the arrow until it points down. You can also filter the user list, by clicking the filter symbol, to the right of 'repository' . This will display three boxes at the top of the list where you can then enter either a 'Name' , 'Email' , or 'Organisation' to filter the list by. 4.1 Add a new user \u00b6 To add a new user click the '+Add user' button at the top right of the 'Manage users' page. This will bring up an 'Add User' form which you will need to complete. Enter the first name, last name, email, organisation and role for the new user. You then need to choose whether the new user is an 'Administrator' or 'Editor' from the dropdown menu in the 'Role' field. You also need to specify the groups that the user is associated with by selecting the relevant groups from the dropdown menu. You can remove any pre-selected groups from the list by clicking the red cross to the right of the group you wish to delete. Once all the fields have been completed, click 'Add user' and a green notification box should appear at the bottom right of your screen confirming that the 'User saved successfully' . 4.2 Reset password \u00b6 To reset a user's password, click the 'Actions' button to the right of the relevant row and select 'Reset password' from the dropdown menu. A green notification box will appear at the bottom right of your screen, confirming that a 'Reset password email sent successfully' . The user can then follow the instructions in the email to reset their password. 4.3 Activate or Deactivate a user \u00b6 The details of whether a user's account is 'Active' or 'Disabled' is displayed in the 'Status' column. To activate a user's account, click the 'Actions' button to the right of the relevant user and select 'Activate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully' . The status of the user will now change to 'Active' . To deactivate a user's account, click the 'Actions' button to the right of the relevant user and select 'Deactivate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully' . The status of the user will now change to 'Disabled' . 5. Pending users \u00b6 To approve or reject users, select 'Pending users' from the user profile dropdown menu. This will navigate you to a list of all the users that are waiting for approval. To approve or reject a user, click the 'Actions' button to the right of the relevant user and select either 'Approve user' or 'Reject user' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming the change. 6. Manage groups \u00b6 As an administrator you can also add, edit and delete groups. To do this, select 'Manage groups' from the user profile dropdown menu. This will navigate you to a list of all the groups currently stored in the repository. To order the list, click the small arrow to the right of 'Name' . This will display the list in chronological order from A-Z when the arrow points up and from Z-A when the arrow points down. You can also filter the user list, by clicking the filter symbol to the right of 'repository' . This will display a 'Name' box at the top of the list where you can then enter a group name and the list will automatically filter. 6.1 Add a group \u00b6 To add a new group, click the '+Add' button at the top right of the 'Manage groups' page. Enter the 'Name' and 'Description' of the new group and click 'Save group' . A green notifiction box will appear at the bottom right of your screen confirming that the 'Group saved successfully' . 6.2 Edit a group \u00b6 To edit a group, click the 'Actions' button to the right of the relevant group and select 'Edit group' from the dropdown menu. This will take you to a summary page of the group, where you can amend the name and description. Make sure to click 'Save group' to save any changes. Also displayed is a list of members, which can be found under the 'Members' tab. Here, you can see the status, details and role of each member. You can aslo delete members by clicking the red bin icon to the right of the relevant row. Any past amendments to the group can be viewed under the 'History' tab. 6.2 Delete a group \u00b6 To delete a group, click the 'Actions' button to the right of the relevant row and select 'Delete group' . A green notification box will appear at the bottom right of your screen to confirm that the 'Group deleted successfully' . 7. Configuration \u00b6 As an administrator you can edit the various email templates associated with Mauro Data Mapper including: Admin confirm user registration email Admin registered user email User invited to edit email User invited to view email User self registered email Forgotten password email Reset password email To access these templates, select 'Configuration' from the user profile dropdown menu. If you make any changes remember to press 'Save email configuration' at the bottom of the page. You can also rebuild the Lucene Search Index by clicking the 'Lucene' tab and then the 'Rebuild index' button.","title":"Admin functionality"},{"location":"user-guides/admin-functionality/admin-functionality/#1-dashboard","text":"To access your admin dashboard, log in to Mauro Data Mapper and click the white arrow by your user profile to the right of the menu header. From the dropdown menu under 'Admin settings' select 'Dashboard' . Your dashboard displays two tabs. One shows your 'Active sessions' and the other shows the 'Plugins and Modules' in the repository.","title":"1. Dashboard"},{"location":"user-guides/admin-functionality/admin-functionality/#2-model-management","text":"As an administrator, you can delete several elements such as Data Models and Terminologies . To do this, select 'Model management' from the user profile dropdown menu. Select the relevant options to filter elements by type and status and the Model Tree displayed at the bottom of the page will automatically filter. Once you've found the element you wish to delete, click the checkbox until a green tick appears. You can do this for multiple elements and a summary list will be displayed on the right. Here, you will then have the option to either 'Delete Permanently' or 'Mark as Deleted' .","title":"2. Model management"},{"location":"user-guides/admin-functionality/admin-functionality/#3-emails","text":"To access your emails, select 'Emails' from the user profile dropdown menu. This will take you to your inbox where you can compose, edit and delete messages.","title":"3. Emails"},{"location":"user-guides/admin-functionality/admin-functionality/#4-manage-users","text":"As an administrator you can add, activate and deactive users. Firstly, select 'Manage users' from the user profile dropdown menu. This will navigate you to a full list of all the users within the repository. Each user's full name, email, organisation, role, any groups they are associated with as well as the status of their account will be displayed. To order the list, you can click the small arrow to the right of each column heading. This will display the list in chronological order according to that category. To order from A-Z click the arrow until it points up. To order from Z-A click the arrow until it points down. You can also filter the user list, by clicking the filter symbol, to the right of 'repository' . This will display three boxes at the top of the list where you can then enter either a 'Name' , 'Email' , or 'Organisation' to filter the list by.","title":"4. Manage users"},{"location":"user-guides/admin-functionality/admin-functionality/#41-add-a-new-user","text":"To add a new user click the '+Add user' button at the top right of the 'Manage users' page. This will bring up an 'Add User' form which you will need to complete. Enter the first name, last name, email, organisation and role for the new user. You then need to choose whether the new user is an 'Administrator' or 'Editor' from the dropdown menu in the 'Role' field. You also need to specify the groups that the user is associated with by selecting the relevant groups from the dropdown menu. You can remove any pre-selected groups from the list by clicking the red cross to the right of the group you wish to delete. Once all the fields have been completed, click 'Add user' and a green notification box should appear at the bottom right of your screen confirming that the 'User saved successfully' .","title":"4.1 Add a new user"},{"location":"user-guides/admin-functionality/admin-functionality/#42-reset-password","text":"To reset a user's password, click the 'Actions' button to the right of the relevant row and select 'Reset password' from the dropdown menu. A green notification box will appear at the bottom right of your screen, confirming that a 'Reset password email sent successfully' . The user can then follow the instructions in the email to reset their password.","title":"4.2 Reset password"},{"location":"user-guides/admin-functionality/admin-functionality/#43-activate-or-deactivate-a-user","text":"The details of whether a user's account is 'Active' or 'Disabled' is displayed in the 'Status' column. To activate a user's account, click the 'Actions' button to the right of the relevant user and select 'Activate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully' . The status of the user will now change to 'Active' . To deactivate a user's account, click the 'Actions' button to the right of the relevant user and select 'Deactivate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully' . The status of the user will now change to 'Disabled' .","title":"4.3 Activate or Deactivate a user"},{"location":"user-guides/admin-functionality/admin-functionality/#5-pending-users","text":"To approve or reject users, select 'Pending users' from the user profile dropdown menu. This will navigate you to a list of all the users that are waiting for approval. To approve or reject a user, click the 'Actions' button to the right of the relevant user and select either 'Approve user' or 'Reject user' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming the change.","title":"5. Pending users"},{"location":"user-guides/admin-functionality/admin-functionality/#6-manage-groups","text":"As an administrator you can also add, edit and delete groups. To do this, select 'Manage groups' from the user profile dropdown menu. This will navigate you to a list of all the groups currently stored in the repository. To order the list, click the small arrow to the right of 'Name' . This will display the list in chronological order from A-Z when the arrow points up and from Z-A when the arrow points down. You can also filter the user list, by clicking the filter symbol to the right of 'repository' . This will display a 'Name' box at the top of the list where you can then enter a group name and the list will automatically filter.","title":"6. Manage groups"},{"location":"user-guides/admin-functionality/admin-functionality/#61-add-a-group","text":"To add a new group, click the '+Add' button at the top right of the 'Manage groups' page. Enter the 'Name' and 'Description' of the new group and click 'Save group' . A green notifiction box will appear at the bottom right of your screen confirming that the 'Group saved successfully' .","title":"6.1 Add a group"},{"location":"user-guides/admin-functionality/admin-functionality/#62-edit-a-group","text":"To edit a group, click the 'Actions' button to the right of the relevant group and select 'Edit group' from the dropdown menu. This will take you to a summary page of the group, where you can amend the name and description. Make sure to click 'Save group' to save any changes. Also displayed is a list of members, which can be found under the 'Members' tab. Here, you can see the status, details and role of each member. You can aslo delete members by clicking the red bin icon to the right of the relevant row. Any past amendments to the group can be viewed under the 'History' tab.","title":"6.2 Edit a group"},{"location":"user-guides/admin-functionality/admin-functionality/#62-delete-a-group","text":"To delete a group, click the 'Actions' button to the right of the relevant row and select 'Delete group' . A green notification box will appear at the bottom right of your screen to confirm that the 'Group deleted successfully' .","title":"6.2 Delete a group"},{"location":"user-guides/admin-functionality/admin-functionality/#7-configuration","text":"As an administrator you can edit the various email templates associated with Mauro Data Mapper including: Admin confirm user registration email Admin registered user email User invited to edit email User invited to view email User self registered email Forgotten password email Reset password email To access these templates, select 'Configuration' from the user profile dropdown menu. If you make any changes remember to press 'Save email configuration' at the bottom of the page. You can also rebuild the Lucene Search Index by clicking the 'Lucene' tab and then the 'Rebuild index' button.","title":"7. Configuration"},{"location":"user-guides/create-a-data-model/create-a-data-model/","text":"This user guide will explain the steps you need to follow to create a new Data Model . 1. Create a new folder \u00b6 Data Models are stored in their own folders and subfolders which are displayed in the Model Tree . Therefore, to create a new Data Model , first you need to either create a new folder, or add a subfolder. To create a new folder click the 'Create a new Folder' icon at the top right of the Model Tree . Enter a 'Folder name' and click 'Add folder' . The new folder should now appear in the Model Tree . To create a new subfolder, right click on the existing folder and select 'Add Folder' . Enter a 'Folder name' and click 'Add folder' . The new subfolder should now appear in the Model Tree . To add a short description to the folder, select the folder in the Model Tree and its details will then be displayed on the right. Click the 'Edit' pencil button and enter a description into the box. Once completed, click 'Save changes' and the description will now display whenever the folder is selected. 2. Add Data Model \u00b6 To add a Data Model , right click the relevant folder and select 'Add Data Model' . A 'New Data Model' form will appear on the right. 3. Complete New Data Model form \u00b6 3.1 Complete Data Model Details \u00b6 Please complete both the mandatory and optional fields of the 'New Data Model' form. The defintions of each field are detailed below: Label Enter a unique name for the Data Model and include any version information, as two Data Models cannot share the same Label . Author Use this field to record the name(s) of the authors who are creating and maintaining this Data Model . Organisation Type the name of the organisation responsible for the Data Model , or the underlying data. Description Enter a detailed description in either plain text or html. Include any important contextual details relating to the Data Model . Select a Data Model Type Select whether the Data Model is a Data Asset or a Data Standard from the dropdown menu. A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema. Classifications Select any relevant Classifications (also known as tags) from the dropdown menu. You can select as many Classifications as you like. Once all the fields have been populated click 'Next Step' to complete the 'Default Data Types' section of the form. 3.2 Select Default Data Types \u00b6 Select the relevant set of 'Default Data Types' from the dropdown menu. These will be imported into your Data Model . You should select the category that includes all the Data Types that you will likely have within your Data Model , however if you are unsure at this stage, then leave this field blank and import them later on. 4. Submit Data Model \u00b6 Once completed, click 'Submit Data Model' and your new Data Model will be added. When selected in the Model Tree the details of the Data Model will now be displayed on the right.","title":"Create a Data Model"},{"location":"user-guides/create-a-data-model/create-a-data-model/#1-create-a-new-folder","text":"Data Models are stored in their own folders and subfolders which are displayed in the Model Tree . Therefore, to create a new Data Model , first you need to either create a new folder, or add a subfolder. To create a new folder click the 'Create a new Folder' icon at the top right of the Model Tree . Enter a 'Folder name' and click 'Add folder' . The new folder should now appear in the Model Tree . To create a new subfolder, right click on the existing folder and select 'Add Folder' . Enter a 'Folder name' and click 'Add folder' . The new subfolder should now appear in the Model Tree . To add a short description to the folder, select the folder in the Model Tree and its details will then be displayed on the right. Click the 'Edit' pencil button and enter a description into the box. Once completed, click 'Save changes' and the description will now display whenever the folder is selected.","title":"1. Create a new folder"},{"location":"user-guides/create-a-data-model/create-a-data-model/#2-add-data-model","text":"To add a Data Model , right click the relevant folder and select 'Add Data Model' . A 'New Data Model' form will appear on the right.","title":"2. Add Data Model"},{"location":"user-guides/create-a-data-model/create-a-data-model/#3-complete-new-data-model-form","text":"","title":" 3. Complete New Data Model form"},{"location":"user-guides/create-a-data-model/create-a-data-model/#31-complete-data-model-details","text":"Please complete both the mandatory and optional fields of the 'New Data Model' form. The defintions of each field are detailed below: Label Enter a unique name for the Data Model and include any version information, as two Data Models cannot share the same Label . Author Use this field to record the name(s) of the authors who are creating and maintaining this Data Model . Organisation Type the name of the organisation responsible for the Data Model , or the underlying data. Description Enter a detailed description in either plain text or html. Include any important contextual details relating to the Data Model . Select a Data Model Type Select whether the Data Model is a Data Asset or a Data Standard from the dropdown menu. A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema. Classifications Select any relevant Classifications (also known as tags) from the dropdown menu. You can select as many Classifications as you like. Once all the fields have been populated click 'Next Step' to complete the 'Default Data Types' section of the form.","title":"3.1 Complete Data Model Details"},{"location":"user-guides/create-a-data-model/create-a-data-model/#32-select-default-data-types","text":"Select the relevant set of 'Default Data Types' from the dropdown menu. These will be imported into your Data Model . You should select the category that includes all the Data Types that you will likely have within your Data Model , however if you are unsure at this stage, then leave this field blank and import them later on.","title":"3.2 Select Default Data Types"},{"location":"user-guides/create-a-data-model/create-a-data-model/#4-submit-data-model","text":"Once completed, click 'Submit Data Model' and your new Data Model will be added. When selected in the Model Tree the details of the Data Model will now be displayed on the right.","title":"4. Submit Data Model"},{"location":"user-guides/document-a-dataset/document-a-dataset/","text":"This user guide will explain the steps you need to follow to manually add a health dataset to Mauro Data Mapper . 1. Create a Data Model \u00b6 Datasets are stored in their own Data Models within Mauro Data Mapper . Therefore, you first need to create a new Data Model . To do this, follow the steps in the 'Create a Data Model user guide' . Once you have reached step 3 , 'Complete New Data Model form' you will need to select the 'Data Model Type' as Data Asset from the dropdown menu. Fill in the rest of the 'New Data Model' form and submit the Data Model as explained in steps 3.2 and 4 . 2. Add a property \u00b6 Once you've created your Data Model , it's important to record further characteristics of the corresponding dataset, particularly to help gateway providers when designing interfaces. To do this, select the Data Model in the Model Tree and then click the 'Properties' tab underneath the Data Model details panel. Click the '+ Add Property' button on the right to add a new row to the property table. Complete the details of the new property as follows: Namespace This will be used to select the correct profile / selection of properties. Key Where an existing namespace has been chosen, select a relevant property name such as 'contact email' . Otherwise, enter a new property name. Value This is the value of the given property, for example \u2018 enquiries-mydataset@hub.org \u2019 . You can also add a relevant element to the value of a property. Click '+ Add Element' in the 'Value' column and select the element type from the menu. Search for the element you require and once selected, it will automatically import into the 'Value' column of the properties table. Once you have filled in the details of the property, click the green 'Save' tick and the new property will be added to the table. 3. Create a Data Class \u00b6 Each Data Model is made up of several Data Classes which is where data items are both created and managed. If your dataset is a collection of tables, the conventional approach is to create a new class for each table. Alternatively, you can create a set of classes to provide a more abstract account of the data set, which group and present the data differently to how it is stored and managed. To create a Data Class , select the relevant Data Model in the Model Tree and click the 'Data Classes' tab on the panel below the Data Model details. Click the '+ Add' button on the right and a 'New Data Class' form will appear. There are two ways to import a Data Class into a Data Model . You can either create a new Data Class or copy a Data Class from an existing Data Model . 3.1 Create a New Data Class \u00b6 To create a new Data Class , select this option in the first section of the 'New Data Class' form and then click the 'Next step' button. Now you need to complete the 'Data Class Details' section of the form as follows: Label Enter a name for the new Data Class which has to be unique within the Data Model . Description Complete a description in either html or plain text which explains the types of data items grouped together within this Data Class . Also include contextual details which are common to the data items, to avoid having to add descriptions to each individual data item. Multiplicity The Multiplicity specifies the minimum and maximum number of times that the Data Class will appear within the Data Model . Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of * which represents -1 internally. Once you have completed the 'Data Class Details' form, click 'Submit Data Class' and the new Data Class will now be permanently displayed under the 'Data Classes' tab of the Data Model . You can add as many Data Classes as necessary. 3.2 Copy a Data Class \u00b6 To import Data Classes from an existing Data Model , select the 'Copy Data Classes(s) from...' option in the first section of the 'New Data Class' form. Select the relevant Data Model by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can then select the relevant Data Model . Once selected, click 'Next step' . The 'Data Class Details' section of the form will then appear, with a list of all the Data Classes within the selected Data Model . Select the Data Classes you wish to import and then click 'Submit Data Class' . The selected Data Classes will then be imported into your original Data Model , with the progress illustrated by a green loading bar at the bottom of the form. 4. Add a Nested Data Class \u00b6 A useful way of managing complex data sets is to use Nested Data Classes which are essentially a Data Class within a Data Class . For example, in a webform, there may be a section called 'Contact details' , which would be one Data Class . Within that section however, there may be another labelled 'Correspondence Address' , which would be a Nested Data Class . To add a Nested Data Class , click the relevant Data Class from the Model Tree and click the 'Content' tab on the panel below the model overview. Then click '+ Add' and select 'Add Data Class' from the dropdown menu. Complete the 'New Data Class' form as explained above in step '3. Create a Data Class' . 5. Add Data Elements \u00b6 Within each Data Class lies several Data Elements which are the descriptions of an individual field, variable, column or property of a data item. To create a Data Element you can use the same approach as creating a Data Class . Select the relevant Data Class in the Model Tree and click the 'Content' tab on the panel below the Data Class details. Click the '+ Add' button on the right and you will be given the choice to either add a Data Class or a Data Element . Select 'Add Data Element' and a 'New Data Element' form will appear. Similar to adding a Data Class , there are two ways to import a Data Element . You can either create a new Data Element or copy Data Elements from an existing Data Class . 5.1 Create a New Data Element \u00b6 Follow the steps in '3.1 Create a new Data Class' until you have completed the 'Label' , 'Description' and Multiplicity fields for the Data Element . Each Data Element then needs to be assigned a relevant Data Type . This can either be selected from an existing list, or you can add a new Data Type . 5.1.1 Select an existing Data Type \u00b6 Click the 'Search' box and a dropdown list of existing Data Types will appear. Select the relevant Data Type . You can then assign several Classifications to the Data Type by selecting them from the dropdown menu. Once all fields are complete, click 'Submit Data Element' to add the Data Element to the Data Class . Repeat this process to add other Data Elements . 5.1.2 Add a new Data Type \u00b6 To add a new Data Type , click 'Add a new Data Type' on the 'Data Element Details' form. Fill in the Label and Description fields and select the relevant Data Type . A Data Type can either be: Enumeration : A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value. If 'Enumeration' has been selected, an additional table will appear where you can add several Enumerations and specify a 'Group' , 'Key' and 'Value' . Primitive : Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference : Data which refers to another Data Class within the same Data Model . If Reference has been selected, the Reference Data Class can be selected from a dropdown menu. Terminology : A structured collection of Enumeration Values which have relationships between different data terms. Similarly, if Terminology has been selected, the relevant category can be chosen from a dropdown menu. You can then assign several 'Classifications' by selecting them from the dropdown menu. Once all fields are complete, click 'Submit Data Element' to add the new Data Element to the Data Class . Go back to step '5.1 Create a new Data Element' and repeat the process to add other Data Elements . 5.2 Copy a Data Element \u00b6 To import a Data Element from an existing Data Class , select the 'Copy Data Element(s) from...' option in the first section of the 'New Data Element' form. Select the relevant Data Class by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can select the relevant Data Model and Data Class . Once selected, click 'Next step' . The 'Data Element Details' section of the form will then appear, with a list of all the Data Elements within the selected Data Class . Select the Data Elements you wish to import by ticking the relevant boxes and these will then appear in a 'Summary of Selected Data Elements' table at the bottom of the form. Once you have checked this table is correct, click 'Submit Data Element' . The selected Data Elements will then be imported into your original Data Class , with the progress illustrated by a green loading bar at the bottom of the form.","title":"Document a Dataset"},{"location":"user-guides/document-a-dataset/document-a-dataset/#1-create-a-data-model","text":"Datasets are stored in their own Data Models within Mauro Data Mapper . Therefore, you first need to create a new Data Model . To do this, follow the steps in the 'Create a Data Model user guide' . Once you have reached step 3 , 'Complete New Data Model form' you will need to select the 'Data Model Type' as Data Asset from the dropdown menu. Fill in the rest of the 'New Data Model' form and submit the Data Model as explained in steps 3.2 and 4 .","title":"1. Create a Data Model"},{"location":"user-guides/document-a-dataset/document-a-dataset/#2-add-a-property","text":"Once you've created your Data Model , it's important to record further characteristics of the corresponding dataset, particularly to help gateway providers when designing interfaces. To do this, select the Data Model in the Model Tree and then click the 'Properties' tab underneath the Data Model details panel. Click the '+ Add Property' button on the right to add a new row to the property table. Complete the details of the new property as follows: Namespace This will be used to select the correct profile / selection of properties. Key Where an existing namespace has been chosen, select a relevant property name such as 'contact email' . Otherwise, enter a new property name. Value This is the value of the given property, for example \u2018 enquiries-mydataset@hub.org \u2019 . You can also add a relevant element to the value of a property. Click '+ Add Element' in the 'Value' column and select the element type from the menu. Search for the element you require and once selected, it will automatically import into the 'Value' column of the properties table. Once you have filled in the details of the property, click the green 'Save' tick and the new property will be added to the table.","title":"2. Add a property"},{"location":"user-guides/document-a-dataset/document-a-dataset/#3-create-a-data-class","text":"Each Data Model is made up of several Data Classes which is where data items are both created and managed. If your dataset is a collection of tables, the conventional approach is to create a new class for each table. Alternatively, you can create a set of classes to provide a more abstract account of the data set, which group and present the data differently to how it is stored and managed. To create a Data Class , select the relevant Data Model in the Model Tree and click the 'Data Classes' tab on the panel below the Data Model details. Click the '+ Add' button on the right and a 'New Data Class' form will appear. There are two ways to import a Data Class into a Data Model . You can either create a new Data Class or copy a Data Class from an existing Data Model .","title":" 3. Create a Data Class"},{"location":"user-guides/document-a-dataset/document-a-dataset/#31-create-a-new-data-class","text":"To create a new Data Class , select this option in the first section of the 'New Data Class' form and then click the 'Next step' button. Now you need to complete the 'Data Class Details' section of the form as follows: Label Enter a name for the new Data Class which has to be unique within the Data Model . Description Complete a description in either html or plain text which explains the types of data items grouped together within this Data Class . Also include contextual details which are common to the data items, to avoid having to add descriptions to each individual data item. Multiplicity The Multiplicity specifies the minimum and maximum number of times that the Data Class will appear within the Data Model . Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of * which represents -1 internally. Once you have completed the 'Data Class Details' form, click 'Submit Data Class' and the new Data Class will now be permanently displayed under the 'Data Classes' tab of the Data Model . You can add as many Data Classes as necessary.","title":" 3.1 Create a New Data Class"},{"location":"user-guides/document-a-dataset/document-a-dataset/#32-copy-a-data-class","text":"To import Data Classes from an existing Data Model , select the 'Copy Data Classes(s) from...' option in the first section of the 'New Data Class' form. Select the relevant Data Model by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can then select the relevant Data Model . Once selected, click 'Next step' . The 'Data Class Details' section of the form will then appear, with a list of all the Data Classes within the selected Data Model . Select the Data Classes you wish to import and then click 'Submit Data Class' . The selected Data Classes will then be imported into your original Data Model , with the progress illustrated by a green loading bar at the bottom of the form.","title":"3.2 Copy a Data Class"},{"location":"user-guides/document-a-dataset/document-a-dataset/#4-add-a-nested-data-class","text":"A useful way of managing complex data sets is to use Nested Data Classes which are essentially a Data Class within a Data Class . For example, in a webform, there may be a section called 'Contact details' , which would be one Data Class . Within that section however, there may be another labelled 'Correspondence Address' , which would be a Nested Data Class . To add a Nested Data Class , click the relevant Data Class from the Model Tree and click the 'Content' tab on the panel below the model overview. Then click '+ Add' and select 'Add Data Class' from the dropdown menu. Complete the 'New Data Class' form as explained above in step '3. Create a Data Class' .","title":"4. Add a Nested Data Class"},{"location":"user-guides/document-a-dataset/document-a-dataset/#5-add-data-elements","text":"Within each Data Class lies several Data Elements which are the descriptions of an individual field, variable, column or property of a data item. To create a Data Element you can use the same approach as creating a Data Class . Select the relevant Data Class in the Model Tree and click the 'Content' tab on the panel below the Data Class details. Click the '+ Add' button on the right and you will be given the choice to either add a Data Class or a Data Element . Select 'Add Data Element' and a 'New Data Element' form will appear. Similar to adding a Data Class , there are two ways to import a Data Element . You can either create a new Data Element or copy Data Elements from an existing Data Class .","title":"5. Add Data Elements"},{"location":"user-guides/document-a-dataset/document-a-dataset/#51-create-a-new-data-element","text":"Follow the steps in '3.1 Create a new Data Class' until you have completed the 'Label' , 'Description' and Multiplicity fields for the Data Element . Each Data Element then needs to be assigned a relevant Data Type . This can either be selected from an existing list, or you can add a new Data Type .","title":" 5.1 Create a New Data Element"},{"location":"user-guides/document-a-dataset/document-a-dataset/#511-select-an-existing-data-type","text":"Click the 'Search' box and a dropdown list of existing Data Types will appear. Select the relevant Data Type . You can then assign several Classifications to the Data Type by selecting them from the dropdown menu. Once all fields are complete, click 'Submit Data Element' to add the Data Element to the Data Class . Repeat this process to add other Data Elements .","title":"5.1.1 Select an existing Data Type"},{"location":"user-guides/document-a-dataset/document-a-dataset/#512-add-a-new-data-type","text":"To add a new Data Type , click 'Add a new Data Type' on the 'Data Element Details' form. Fill in the Label and Description fields and select the relevant Data Type . A Data Type can either be: Enumeration : A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value. If 'Enumeration' has been selected, an additional table will appear where you can add several Enumerations and specify a 'Group' , 'Key' and 'Value' . Primitive : Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference : Data which refers to another Data Class within the same Data Model . If Reference has been selected, the Reference Data Class can be selected from a dropdown menu. Terminology : A structured collection of Enumeration Values which have relationships between different data terms. Similarly, if Terminology has been selected, the relevant category can be chosen from a dropdown menu. You can then assign several 'Classifications' by selecting them from the dropdown menu. Once all fields are complete, click 'Submit Data Element' to add the new Data Element to the Data Class . Go back to step '5.1 Create a new Data Element' and repeat the process to add other Data Elements .","title":"5.1.2 Add a new Data Type"},{"location":"user-guides/document-a-dataset/document-a-dataset/#52-copy-a-data-element","text":"To import a Data Element from an existing Data Class , select the 'Copy Data Element(s) from...' option in the first section of the 'New Data Element' form. Select the relevant Data Class by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can select the relevant Data Model and Data Class . Once selected, click 'Next step' . The 'Data Element Details' section of the form will then appear, with a list of all the Data Elements within the selected Data Class . Select the Data Elements you wish to import by ticking the relevant boxes and these will then appear in a 'Summary of Selected Data Elements' table at the bottom of the form. Once you have checked this table is correct, click 'Submit Data Element' . The selected Data Elements will then be imported into your original Data Class , with the progress illustrated by a green loading bar at the bottom of the form.","title":"5.2 Copy a Data Element"},{"location":"user-guides/exporting-data-models/exporting-data-models/","text":"This user guide will explain how to export a Data Model using a variety of exporters including XML, JSON, XML Schema, Grails and Excel. 1. Export a Data Model \u00b6 Select the Data Model you wish to export in the Model Tree to display it's details panel. In the icon menu at the bottom right of the details panel click the 'Export as JSON, XML...' button. This will display a list of exporters. Select the exporter you wish to use. Once selected, the Data Model will automatically start exporting into the specificed format, with a green loading bar indicating the progress. Once exported, this progress bar will be replaced with a link to download the exported file. Click this link to download the file directly to your local machine. 2. Export multiple Data Models \u00b6 You can also export several Data Models together. To do this, click the 'Export DataModel(s)' icon at the top right of the Model Tree . A 'Data Model(s) Export' form will appear on the right hand side. To select the Data Models you wish to export, click the menu icon to the right of the 'Select Data Model...' box and the Model Tree will be displayed. Select the relevant Data Models and these will automatically appear in the 'Select your Data Model(s):' field. Once you have selected the relevant Data Models , you then need to choose an exporter. Note that only Excel (XLSX) Exporter and Simple Excel (XLSX) Exporter support multiple Data Model exports. Click the 'Select an exporter*' box and a dropdown list of all the available exporters will appear. Select the exporter you want and then click the 'Export Data Model(s)' button. A green loading bar will appear and once exported a green notification box will confirm that the 'Data Model(s) exported successfully' and a link to download the file will appear at the bottom of the form.","title":"Exporting Data Models"},{"location":"user-guides/exporting-data-models/exporting-data-models/#1-export-a-data-model","text":"Select the Data Model you wish to export in the Model Tree to display it's details panel. In the icon menu at the bottom right of the details panel click the 'Export as JSON, XML...' button. This will display a list of exporters. Select the exporter you wish to use. Once selected, the Data Model will automatically start exporting into the specificed format, with a green loading bar indicating the progress. Once exported, this progress bar will be replaced with a link to download the exported file. Click this link to download the file directly to your local machine.","title":"1. Export a Data Model"},{"location":"user-guides/exporting-data-models/exporting-data-models/#2-export-multiple-data-models","text":"You can also export several Data Models together. To do this, click the 'Export DataModel(s)' icon at the top right of the Model Tree . A 'Data Model(s) Export' form will appear on the right hand side. To select the Data Models you wish to export, click the menu icon to the right of the 'Select Data Model...' box and the Model Tree will be displayed. Select the relevant Data Models and these will automatically appear in the 'Select your Data Model(s):' field. Once you have selected the relevant Data Models , you then need to choose an exporter. Note that only Excel (XLSX) Exporter and Simple Excel (XLSX) Exporter support multiple Data Model exports. Click the 'Select an exporter*' box and a dropdown list of all the available exporters will appear. Select the exporter you want and then click the 'Export Data Model(s)' button. A green loading bar will appear and once exported a green notification box will confirm that the 'Data Model(s) exported successfully' and a link to download the file will appear at the bottom of the form.","title":"2. Export multiple Data Models"},{"location":"user-guides/how-to-search/how-to-search/","text":"The search function within Mauro Data Mapper is extremely powerful and allows you to search for datasets or specifications throughout the whole catalogue. This user guide will explain how to effectively search for an item as well as the methods Mauro Data Mapper uses for searching. 1. Types of search \u00b6 There are several different ways to search for an item within Mauro Data Mapper . 1.1 Search Models \u00b6 The first method of searching is to use the search box on the top left of the finder panel above the Model Tree . Here, you can search 'Models' , 'Classifications' and 'Favourites' by clicking the relevant tab and then entering the search term in the search box. As you type your search, the Model Tree will automatically filter, displaying only the relevant results, with the search term highlighted in yellow wherever it appears within the list. When searching Models , the results list will show Data Models and Data Classes which match the search term in any Label , description, comment or property. To refresh the results, click the 'Search' icon or the 'Reload Data Models Tree' icon in the menu to the right of the search box. You can also choose whether you want the results list to feature superseded or old Data Models by clicking the 'Filters' icon and then selecting the relevant options. To remove filtering click the small 'x' on the right of the search box. 1.2 Main search \u00b6 Another method of searching is to use the main search box which can be found under the 'Search' tab in the header. Once again, as you type your search, the results list will automatically filter, with the number of results displayed at the top of the list. You can then click on a result that you're interested in and the details panel will open up in a new tab in your browser. 1.3 Advanced search \u00b6 To conduct a more specific search you can use Advanced search . To the right of the main search box under the 'Search' tab in the header, click 'Advanced' . This will display a variety of different options to filter your search by. Restrict your search \u00b6 Firstly, enter the search term in the main search box and the results list will automtically filter at the bottom of the page. You can then restrict your search to a particular Folder , Data Model or Data Class by clicking the menu icon to the right of the ' Restrict your search to:' box. This will display the Model Tree , from which you can select the relevant item. Domain Types & Search Type \u00b6 You also have the option to filter by 'Domain Types' . This allows you to specify whether you want to search Data Models , Data Classes , Data Elements , Data Types or Enumeration Values by selecting the relevant boxes. You can also define the 'Search Type' . If you only want to search in the title of elements and not in the descriptions , metadata or aliases , then tick the \u2018Title only\u2019 box. Alternatively, select \u2018Exact match\u2019 to search for the exact search term throughout the entire catalogue. Date ranges \u00b6 Advanced search also allows you to specify a range of dates so that you can filter results by when they were 'Last Updated' or 'Created' . Enter the relevant date in the format of dd/mm/yyyy or click the \u2018Calendar\u2019 icon to the right of each date box. This will display a small calendar where you can select the relevant date which will automatically populate the date box. Classifiers \u00b6 In some cases you can also filter your search by Classifiers by clicking the \u2018Classifiers\u2019 box and selecting the relevant options from the dropdown menu. 2. How search works \u00b6 The search functionality in Mauro Data Mapper implements the Simple Query Strings of Hibernate search . This is essentially where a simple syntax is used to parse and split the provided query string into terms based on special operators. The query then analyses each term independently before returning matching results. 2.1 Search syntax \u00b6 The search syntaxes that Mauro Data Mapper supports are: AND using + OR using | NOT using \u2013 To search for different suffixes use * e.g. prefix* To search for an exact phrase use \u201c\u201d e.g. \u201cexample phrase\u201d To add precedence to search terms, use () e.g. (important) example To search for similar terms (fuzzy string searching) use ~2 e.g. smoke~2 will return smokes, smake etc To search for similar phrases use \u201c\u201d and ~2 e.g. \u201ccigarette smoke\u201d~3 For example, if you are interested in information relating to smoking. Searching \u2018smoking\u2019 will return a list of results. However, if you want to be more generic you can type \u2018smoke*\u2019 which will search for smoking, smoker, smoked etc. To look at information relating to smoking and pregnancy you can type \u2018smok*+pregnancy\u2019 which will return results that include both pregnancy and the different variants of smoking. 2.2 Search examples \u00b6 Below are some more examples to further illustrate how to use the search syntaxes effectively: storm~2 - will return results containing storms or sturm war + (peace | harmony) - will return results containing \"war and either peace or harmony\" storm tree - will return results containing the words storm or tree storm and tree - will return results containing exactly the phrase storm and tree 2.3 Label & Metadata Key \u00b6 The Label and Key entries are indexed using a WordDelimiter analyser . This essentially splits up the search term into individual words at the following points: Spaces Hypens Numbers Capital letters Full stops These individual words are referred to as \u2018Keys\u2019 and once the search term has been split up into its various keys, these are then saved and used to conduct the search. Therefore, the results will only match the keys and not the whole search phrase. For example: Datamodel for test - will be searchable by the keys datamodel , for and test Test DataModel - will be searchable by the keys test , data and model , it will not be searchable by the key datamodel because of the capital letter Test DataModel V1.0.1 - will be searchable by the keys test , data , model , v1 , 0 and 1 . It will not be searchable by the keys datamodel or v1.0.1 subject-34567 - will be searchable by the keys subject and 34567","title":"How to search"},{"location":"user-guides/how-to-search/how-to-search/#1-types-of-search","text":"There are several different ways to search for an item within Mauro Data Mapper .","title":"1. Types of search"},{"location":"user-guides/how-to-search/how-to-search/#11-search-models","text":"The first method of searching is to use the search box on the top left of the finder panel above the Model Tree . Here, you can search 'Models' , 'Classifications' and 'Favourites' by clicking the relevant tab and then entering the search term in the search box. As you type your search, the Model Tree will automatically filter, displaying only the relevant results, with the search term highlighted in yellow wherever it appears within the list. When searching Models , the results list will show Data Models and Data Classes which match the search term in any Label , description, comment or property. To refresh the results, click the 'Search' icon or the 'Reload Data Models Tree' icon in the menu to the right of the search box. You can also choose whether you want the results list to feature superseded or old Data Models by clicking the 'Filters' icon and then selecting the relevant options. To remove filtering click the small 'x' on the right of the search box.","title":"1.1 Search Models"},{"location":"user-guides/how-to-search/how-to-search/#12-main-search","text":"Another method of searching is to use the main search box which can be found under the 'Search' tab in the header. Once again, as you type your search, the results list will automatically filter, with the number of results displayed at the top of the list. You can then click on a result that you're interested in and the details panel will open up in a new tab in your browser.","title":"1.2 Main search"},{"location":"user-guides/how-to-search/how-to-search/#13-advanced-search","text":"To conduct a more specific search you can use Advanced search . To the right of the main search box under the 'Search' tab in the header, click 'Advanced' . This will display a variety of different options to filter your search by.","title":"1.3 Advanced search"},{"location":"user-guides/how-to-search/how-to-search/#restrict-your-search","text":"Firstly, enter the search term in the main search box and the results list will automtically filter at the bottom of the page. You can then restrict your search to a particular Folder , Data Model or Data Class by clicking the menu icon to the right of the ' Restrict your search to:' box. This will display the Model Tree , from which you can select the relevant item.","title":"Restrict your search"},{"location":"user-guides/how-to-search/how-to-search/#domain-types-search-type","text":"You also have the option to filter by 'Domain Types' . This allows you to specify whether you want to search Data Models , Data Classes , Data Elements , Data Types or Enumeration Values by selecting the relevant boxes. You can also define the 'Search Type' . If you only want to search in the title of elements and not in the descriptions , metadata or aliases , then tick the \u2018Title only\u2019 box. Alternatively, select \u2018Exact match\u2019 to search for the exact search term throughout the entire catalogue.","title":"Domain Types &amp; Search Type"},{"location":"user-guides/how-to-search/how-to-search/#date-ranges","text":"Advanced search also allows you to specify a range of dates so that you can filter results by when they were 'Last Updated' or 'Created' . Enter the relevant date in the format of dd/mm/yyyy or click the \u2018Calendar\u2019 icon to the right of each date box. This will display a small calendar where you can select the relevant date which will automatically populate the date box.","title":"Date ranges"},{"location":"user-guides/how-to-search/how-to-search/#classifiers","text":"In some cases you can also filter your search by Classifiers by clicking the \u2018Classifiers\u2019 box and selecting the relevant options from the dropdown menu.","title":"Classifiers"},{"location":"user-guides/how-to-search/how-to-search/#2-how-search-works","text":"The search functionality in Mauro Data Mapper implements the Simple Query Strings of Hibernate search . This is essentially where a simple syntax is used to parse and split the provided query string into terms based on special operators. The query then analyses each term independently before returning matching results.","title":"2. How search works"},{"location":"user-guides/how-to-search/how-to-search/#21-search-syntax","text":"The search syntaxes that Mauro Data Mapper supports are: AND using + OR using | NOT using \u2013 To search for different suffixes use * e.g. prefix* To search for an exact phrase use \u201c\u201d e.g. \u201cexample phrase\u201d To add precedence to search terms, use () e.g. (important) example To search for similar terms (fuzzy string searching) use ~2 e.g. smoke~2 will return smokes, smake etc To search for similar phrases use \u201c\u201d and ~2 e.g. \u201ccigarette smoke\u201d~3 For example, if you are interested in information relating to smoking. Searching \u2018smoking\u2019 will return a list of results. However, if you want to be more generic you can type \u2018smoke*\u2019 which will search for smoking, smoker, smoked etc. To look at information relating to smoking and pregnancy you can type \u2018smok*+pregnancy\u2019 which will return results that include both pregnancy and the different variants of smoking.","title":"2.1 Search syntax"},{"location":"user-guides/how-to-search/how-to-search/#22-search-examples","text":"Below are some more examples to further illustrate how to use the search syntaxes effectively: storm~2 - will return results containing storms or sturm war + (peace | harmony) - will return results containing \"war and either peace or harmony\" storm tree - will return results containing the words storm or tree storm and tree - will return results containing exactly the phrase storm and tree","title":"2.2 Search examples"},{"location":"user-guides/how-to-search/how-to-search/#23-label-metadata-key","text":"The Label and Key entries are indexed using a WordDelimiter analyser . This essentially splits up the search term into individual words at the following points: Spaces Hypens Numbers Capital letters Full stops These individual words are referred to as \u2018Keys\u2019 and once the search term has been split up into its various keys, these are then saved and used to conduct the search. Therefore, the results will only match the keys and not the whole search phrase. For example: Datamodel for test - will be searchable by the keys datamodel , for and test Test DataModel - will be searchable by the keys test , data and model , it will not be searchable by the key datamodel because of the capital letter Test DataModel V1.0.1 - will be searchable by the keys test , data , model , v1 , 0 and 1 . It will not be searchable by the keys datamodel or v1.0.1 subject-34567 - will be searchable by the keys subject and 34567","title":"2.3 Label &amp; Metadata Key"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/","text":"This user guide will explain the steps you need to follow to import a health dataset into Mauro Data Mapper using an Excel spreadsheet. To add an existing dataset to Mauro Data Mapper , you can either enter all the information online as explained in the 'Document a Health Dataset user guide' , or you may find it more convenient to import information automatically from an Excel spreadsheet. The importing functionality of Mauro Data Mapper allows you to import several Data Models using the same spreadsheet. 1. Create Data Model import file \u00b6 To ensure all the information is imported correctly, the dataset needs to be entered into a spreadsheet in a specific format. To help with this, you can download a zip file of the standard Data Model import file here . This standard spreadsheet contains two types of worksheets. Firstly, there is the Data Model listing sheet , titled \u2018DataModels\u2019 . This is effectively a contents page which lists the main details of each Data Model you wish to import. There must only ever be one Data Model listing sheet . The next sheet is the Data Model key sheet which is titled \u2018KEY_1\u2019 . This contains all the relevant details of one Data Model listed in the \u2018DataModels\u2019 sheet. Therefore, if you wish to import several Data Models , you will need to add a Data Model key sheet for each additional Data Model and title it \u2018KEY_2\u2019 , \u2018KEY_3\u2019 respectively. 1.1 Data Model listing sheet \u00b6 In the Data Model listing sheet , use one row for each Data Model . Enter the information according to the columns, which are explained below, along with any other properties or metadata which may be relevant. The following columns must be completed: SHEET_KEY The unique name of each Data Model key sheet such as \u2018KEY_1\u2019 , \u2018KEY_2\u2019 etc. Name The unique name or Label of the Data Model . Remember this should be different to all the existing Data Models within Mauro Data Mapper , unless you are updating an existing Data Model . Description Enter a description which explains the contextual details of the dataset within the Data Model . Author Record the name(s) of the authors who are creating and maintaining this Data Model . Organisation Type the name of the organisation responsible for the Data Model , or the underlying data. Type This is the type of Data Model , which can either be a Data Asset or a Data Standard . A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema. Adding properties Any other relevant properties or metadata relating to the Data Model can be included in additional columns to the right of the core columns. Metadata can have the following properties: Namespace This will be used to select the correct profile / property selection and should be entered in row 1 of the spreadsheet. If it is left blank, the default namespace of ox.softeng.metadatacatalogue.plugins.excel will be used. Key This is a relevant property name such as \u2018contact email\u2019 and must be entered in row 2 . If no key is supplied, then the value will not be assigned. Value This is the Value of the given property, for example \u2018 enquiries-mydataset@hub.org \u2019 and should be entered into the relevant row. If multiple rows are being imported and a Namespace and Key column is created, then the property will only be assigned if a Value is supplied. 1.2 Data Model key sheet \u00b6 There should be one Data Model key sheet for each Data Model , and its name should correspond to the relevant 'SHEET_KEY' on the Data Model listing sheet . Any Data Model listed without the correctly named key sheet will not be imported. Therefore, the best practise is to first copy the 'KEY_1' sheet in the standard excel template, rename it and then add the details of the relevant Data Model . Otherwise, formatting issues could occur, resulting in the data importing incorrectly. The following columns must be completed: DataClass Path This is the path from the top level of the Data Model to the Data Class . For a top level Data Class , only the class name should be entered in this field. However, if it is a Nested Data Class , then the name of the parent class along with the child class should be entered and be separated by \u201cI\u201d , for example: A nested class: 'parentIchild' A nested class within a nested class: 'grandparentIparentIchild' Another nested class within a nested class: 'grandparentIparentIanotherChild' DataElement Name If the row is describing a Data Element , instead of a Data Class , then the name of the Data Element should be entered here. If supplied, the remaining information in this row will be used to create the Data Element inside the Data Class provided in the 'DataClass Path' column. However, if left blank, the remaining information in this row will be assigned to the Data Class entered in the 'DataClass Path' column. Description Enter a description which explains any contextual details relating to the Data Element or Data Class . Minimum Multiplicity The minimum number of instances of the Data Class or Data Element within the Data Model . Optional data has a minimum Multiplicity of 0, whereas mandatory data has a minimum Multiplicity of 1. Maximum Multiplicity The maximum number of instances of the Data Class or Data Element within the Data Model . Optional data has a maximum Multiplicity of 1, whereas data which occurs any number of times and therefore has no upper bound has a maximum Multiplicity of * which represents -1 internally. DataType Name This is the name given to the Data Type of the Data Element being described and must be included when entering information for Data Elements . The Data Type describes the range of possible values that the Data Element may take. There are four different Data Types stored within a Data Model : Enumeration : A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value. Primitive : Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference : Data which refers to another Data Class within the same Data Model . Terminology : A structured collection of Enumeration Values which have relationships between different data terms. DataType Description This is a short description of the Data Type . If the same Data Type is used multiple times, then the first description entry will be used so subsequent description fields can be left blank. Reference to DataClass Path If the Data Element is a \u2018Reference\u2019 Data Type and therefore the Data Element is a reference to an instance of another Data Class , then the path to that Data Class should be entered here. This path must match a path provided in the 'DataClass Path' column using the same format with \u2018I\u2019 as the separator. This field cannot be used in conjunction with \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019 . Enumeration Key If the Data Element is an Enumeration Data Type , then the Key and Value of each Enumeration should be included in the \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019 columns respectively, with one Key and Value per row. The Enumeration Key is the text or string that may appear in a column of the dataset. Enumeration Value The Enumeration Value is the data that corresponds to the Key . For example, for a yes/no question on a webform, the Value is either \u2018Yes\u2019 or \u2018No\u2019. Each Value is assigned a Key , which in this case could be \u20181\u2019 and \u20180\u2019 respectively. An Enumeration Value must be provided if an Enumeration Key has been entered. Once all the Enumeration Key and Value pairs have been entered you will need to merge the corresponding cells in the 'DataClass Path' column, and you may wish to merge the other relevant cells in the other columns too for consistency. For example, if you were entering the information for a Data Element that was an Enumeration Data Type then the Data Model key sheet would look similar to the below: 2. Import Data Model \u00b6 Once the Data Model import file has been completed, click the 'Import a Data Model' icon at the top right of the Model Tree . The 'Import a new Data Model' form will then appear on the right. Click the 'Select an importer' box and select 'Excel (XLSX) importer' from the dropdown menu. This will automatically load the rest of the form that you need to complete. You then need to select the relevant folder that you wish to import the Data Model into. You can do this by either typing in the folder name, or clicking the menu icon to the right of the 'Folder location' box. This will display the Model Tree , from which you can then select the relevant folder. You then need to tick the 'Finalised' box to indicate whether the Data Model you are importing is finalised. Although, it is recommended to keep the model as a draft until the gateway presentation of model descriptions has been decided. If the Data Model you are importing is intended to replace an existing Data Model within the current version of Mauro Data Mapper , then tick the 'Import as New Documentation Version' box. This means that the imported model will supersede any Data Models with the same name and will be assigned the latest documentation version of the existing Data Model . If you are importing a new Data Model , then make sure the 'Name' field in the Data Model listing sheet is unique, otherwise this will cause an error when importing. In the 'Source' box, click 'Choose file' and your file explorer will open. Navigate to the relevant Data Model import file and then select 'Import Model(s)' . 3. Round Tripping Data Models \u00b6 Excel files can be safely used to 'Round trip' Data Model descriptions. This is essentially where: A Data Model is exported in the form of an Excel spreadsheet. This exported spreadsheet is edited or updated with new information. The new version of the spreadsheet is re-imported into Mauro Data Mapper and therefore automatically updates the existing Data Model . Note: 'Import as New Documentation Version' should be ticked when re-importing. This method is a quick and easy way to update existing Data Models as well as adding extra information. This can be particularly useful when Data Models have previously been imported using an alternative method, and therefore may only have the basic layout.","title":"Import a Data Model from Excel"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#1-create-data-model-import-file","text":"To ensure all the information is imported correctly, the dataset needs to be entered into a spreadsheet in a specific format. To help with this, you can download a zip file of the standard Data Model import file here . This standard spreadsheet contains two types of worksheets. Firstly, there is the Data Model listing sheet , titled \u2018DataModels\u2019 . This is effectively a contents page which lists the main details of each Data Model you wish to import. There must only ever be one Data Model listing sheet . The next sheet is the Data Model key sheet which is titled \u2018KEY_1\u2019 . This contains all the relevant details of one Data Model listed in the \u2018DataModels\u2019 sheet. Therefore, if you wish to import several Data Models , you will need to add a Data Model key sheet for each additional Data Model and title it \u2018KEY_2\u2019 , \u2018KEY_3\u2019 respectively.","title":"1. Create Data Model import file"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#11-data-model-listing-sheet","text":"In the Data Model listing sheet , use one row for each Data Model . Enter the information according to the columns, which are explained below, along with any other properties or metadata which may be relevant. The following columns must be completed: SHEET_KEY The unique name of each Data Model key sheet such as \u2018KEY_1\u2019 , \u2018KEY_2\u2019 etc. Name The unique name or Label of the Data Model . Remember this should be different to all the existing Data Models within Mauro Data Mapper , unless you are updating an existing Data Model . Description Enter a description which explains the contextual details of the dataset within the Data Model . Author Record the name(s) of the authors who are creating and maintaining this Data Model . Organisation Type the name of the organisation responsible for the Data Model , or the underlying data. Type This is the type of Data Model , which can either be a Data Asset or a Data Standard . A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema. Adding properties Any other relevant properties or metadata relating to the Data Model can be included in additional columns to the right of the core columns. Metadata can have the following properties: Namespace This will be used to select the correct profile / property selection and should be entered in row 1 of the spreadsheet. If it is left blank, the default namespace of ox.softeng.metadatacatalogue.plugins.excel will be used. Key This is a relevant property name such as \u2018contact email\u2019 and must be entered in row 2 . If no key is supplied, then the value will not be assigned. Value This is the Value of the given property, for example \u2018 enquiries-mydataset@hub.org \u2019 and should be entered into the relevant row. If multiple rows are being imported and a Namespace and Key column is created, then the property will only be assigned if a Value is supplied.","title":" 1.1 Data Model listing sheet"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#12-data-model-key-sheet","text":"There should be one Data Model key sheet for each Data Model , and its name should correspond to the relevant 'SHEET_KEY' on the Data Model listing sheet . Any Data Model listed without the correctly named key sheet will not be imported. Therefore, the best practise is to first copy the 'KEY_1' sheet in the standard excel template, rename it and then add the details of the relevant Data Model . Otherwise, formatting issues could occur, resulting in the data importing incorrectly. The following columns must be completed: DataClass Path This is the path from the top level of the Data Model to the Data Class . For a top level Data Class , only the class name should be entered in this field. However, if it is a Nested Data Class , then the name of the parent class along with the child class should be entered and be separated by \u201cI\u201d , for example: A nested class: 'parentIchild' A nested class within a nested class: 'grandparentIparentIchild' Another nested class within a nested class: 'grandparentIparentIanotherChild' DataElement Name If the row is describing a Data Element , instead of a Data Class , then the name of the Data Element should be entered here. If supplied, the remaining information in this row will be used to create the Data Element inside the Data Class provided in the 'DataClass Path' column. However, if left blank, the remaining information in this row will be assigned to the Data Class entered in the 'DataClass Path' column. Description Enter a description which explains any contextual details relating to the Data Element or Data Class . Minimum Multiplicity The minimum number of instances of the Data Class or Data Element within the Data Model . Optional data has a minimum Multiplicity of 0, whereas mandatory data has a minimum Multiplicity of 1. Maximum Multiplicity The maximum number of instances of the Data Class or Data Element within the Data Model . Optional data has a maximum Multiplicity of 1, whereas data which occurs any number of times and therefore has no upper bound has a maximum Multiplicity of * which represents -1 internally. DataType Name This is the name given to the Data Type of the Data Element being described and must be included when entering information for Data Elements . The Data Type describes the range of possible values that the Data Element may take. There are four different Data Types stored within a Data Model : Enumeration : A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value. Primitive : Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference : Data which refers to another Data Class within the same Data Model . Terminology : A structured collection of Enumeration Values which have relationships between different data terms. DataType Description This is a short description of the Data Type . If the same Data Type is used multiple times, then the first description entry will be used so subsequent description fields can be left blank. Reference to DataClass Path If the Data Element is a \u2018Reference\u2019 Data Type and therefore the Data Element is a reference to an instance of another Data Class , then the path to that Data Class should be entered here. This path must match a path provided in the 'DataClass Path' column using the same format with \u2018I\u2019 as the separator. This field cannot be used in conjunction with \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019 . Enumeration Key If the Data Element is an Enumeration Data Type , then the Key and Value of each Enumeration should be included in the \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019 columns respectively, with one Key and Value per row. The Enumeration Key is the text or string that may appear in a column of the dataset. Enumeration Value The Enumeration Value is the data that corresponds to the Key . For example, for a yes/no question on a webform, the Value is either \u2018Yes\u2019 or \u2018No\u2019. Each Value is assigned a Key , which in this case could be \u20181\u2019 and \u20180\u2019 respectively. An Enumeration Value must be provided if an Enumeration Key has been entered. Once all the Enumeration Key and Value pairs have been entered you will need to merge the corresponding cells in the 'DataClass Path' column, and you may wish to merge the other relevant cells in the other columns too for consistency. For example, if you were entering the information for a Data Element that was an Enumeration Data Type then the Data Model key sheet would look similar to the below:","title":" 1.2 Data Model key sheet"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#2-import-data-model","text":"Once the Data Model import file has been completed, click the 'Import a Data Model' icon at the top right of the Model Tree . The 'Import a new Data Model' form will then appear on the right. Click the 'Select an importer' box and select 'Excel (XLSX) importer' from the dropdown menu. This will automatically load the rest of the form that you need to complete. You then need to select the relevant folder that you wish to import the Data Model into. You can do this by either typing in the folder name, or clicking the menu icon to the right of the 'Folder location' box. This will display the Model Tree , from which you can then select the relevant folder. You then need to tick the 'Finalised' box to indicate whether the Data Model you are importing is finalised. Although, it is recommended to keep the model as a draft until the gateway presentation of model descriptions has been decided. If the Data Model you are importing is intended to replace an existing Data Model within the current version of Mauro Data Mapper , then tick the 'Import as New Documentation Version' box. This means that the imported model will supersede any Data Models with the same name and will be assigned the latest documentation version of the existing Data Model . If you are importing a new Data Model , then make sure the 'Name' field in the Data Model listing sheet is unique, otherwise this will cause an error when importing. In the 'Source' box, click 'Choose file' and your file explorer will open. Navigate to the relevant Data Model import file and then select 'Import Model(s)' .","title":"2. Import Data Model"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#3-round-tripping-data-models","text":"Excel files can be safely used to 'Round trip' Data Model descriptions. This is essentially where: A Data Model is exported in the form of an Excel spreadsheet. This exported spreadsheet is edited or updated with new information. The new version of the spreadsheet is re-imported into Mauro Data Mapper and therefore automatically updates the existing Data Model . Note: 'Import as New Documentation Version' should be ticked when re-importing. This method is a quick and easy way to update existing Data Models as well as adding extra information. This can be particularly useful when Data Models have previously been imported using an alternative method, and therefore may only have the basic layout.","title":"3. Round Tripping Data Models"},{"location":"user-guides/publish-subscribe/publish-subscribe/","text":"Mauro Data Mapper instances are able to provide an Atom feed that publishes all finalised versions of Data Models in the catalogue. This enables other Mauro Data Mapper instances to subscribe to these feeds and view the Federated Data Models available from external catalogues. Furthermore, a Mauro Data Mapper instance has the ability to subscribe to one or more of these Federated Data Models to import them into their own catalogues. This user guide will explain how. 1. Feeds and API Keys \u00b6 All Mauro Data Mapper server instances expose the URL api/feeds/all which returns the Atom syndication data. For another server instance to view all available data in that feed, each server instance must generate an API key and provide it to the required external parties. 1.1 Create or copy an API Key \u00b6 API keys may be set up through the web interface or via the API. To create an API key, you must be logged in with a username and password. On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu. This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys. For each API belonging to the user, the list displays: Name This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys used for different purposes. Key This is the key itself, which is a UUID, unique to this user. Expiry date This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be 'refreshed' before expiry. Refreshable Specifies whether an API key can be refreshed once expired. Status Details whether an API key is 'Active' or 'Disabled' . You can either copy an existing key value or create a new one to provide to an external party. To copy the key value to your clipboard click the 'Copy' button in the relevant 'Key' box. To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete. Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes. For more information about creating and managing API Keys, please see the API Keys page in the REST API documentation 2. Add a Subscribed Catalogue \u00b6 Note: The following can only be carried out by an administrator. Administrators can manage the set of Subscribed Catalogues a Mauro Data Mapper instance connects to. To manage the catalogues, click the white arrow next to your user profile and select 'Subscribed catalogues' from the dropdown menu. This will take you to a list of existing catalogues. Here you can test, edit and delete catalogues by clicking the three vertical dots to the right of each item in the list which will display a dropdown menu. For each existing catalogue, the list displays: Label A unique label to help identify the subscribed catalogue. Description A description to help explain what the subscribed catalogue is for. URL The URL to the server instance to connect to. API Key The generated key value to authenticate against the server instance. Refresh period State how often, in days, this server instance should refresh the subscribed catalogue feed to check for new data. If not provided, a suitable default will be used instead. To add a Subscribed Catalogue , you will need: The URL to the Mauro Data Mapper server instance to connect to. The API key to authenticate on that server instance. Note: The URL only needs to refer to the instance and not the absolute URL to the Atom feed. Click the '+Add' button at the top right of the 'Catalogues' list. This will open an 'Add Subscribed Catalogue' form which you will need to complete. Enter the label, description, URL, API Key and refresh period as described above. Once completed, click 'Add subscription' and a green notification box should appear at the bottom right of your screen confirming that the 'subscribed catalogue saved successfully' . 3. Federated Data Models \u00b6 When at least one subscribed catalogue has been configured, any signed in user may be able to view the Federated Data Models from those catalogues in the Model Tree . The Model Tree will be updated to list two new root nodes: This catalogue Lists all the folders and Data Models contained within this Mauro Data Mapper instance. External catalogues Lists each subscribed catalogue created for this Mauro Data Mapper instance. Expanding each subscribed catalogue will list all readable Federated Data Models that can be individually subscribed to. 3.1 Subscribe to a Federated Data Model \u00b6 You can view the subscription status of a Federated Data Model by selecting the relevant model in the Model Tree which will display it's details panel. To subscribe to a Federated Data Model so that it is included in the current catalogue, click 'Subscribe' and a 'Subscribe to Data Model' dialogue box will appear. Click the menu icon to the right of the box to display the Model Tree . Select the folder you wish to import the Federated Data Model into and click '+Subscribe' . The status of this model will now change to 'Subscribed' in the details panel, meaning that this catalogue will automatically maintain this subscription until the user chooses to unsubscribe. 3.2 Imported Federated Data Models \u00b6 Once subscribed to, each Federated Data Model will be automatically imported into the current Mauro Data Mapper catalogue at the chosen target folder, including the links between model versions. It is possible to manually synchronise the subscribed catalogues. To do this, select the subscribed catalogue in the Model Tree . In the relevant details panel, click the 'Synchronise' button. Each subscribed catalogue will also list version updates when the current Mauro Data Mapper instance refreshes the Atom feed, the frequency of which is determined during configuration. New versions of a Data Model will appear with their updated versions listed for you to also subscribe to, which is a manual operation.","title":"Publish / Subscribe"},{"location":"user-guides/publish-subscribe/publish-subscribe/#1-feeds-and-api-keys","text":"All Mauro Data Mapper server instances expose the URL api/feeds/all which returns the Atom syndication data. For another server instance to view all available data in that feed, each server instance must generate an API key and provide it to the required external parties.","title":"1. Feeds and API Keys"},{"location":"user-guides/publish-subscribe/publish-subscribe/#11-create-or-copy-an-api-key","text":"API keys may be set up through the web interface or via the API. To create an API key, you must be logged in with a username and password. On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu. This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys. For each API belonging to the user, the list displays: Name This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys used for different purposes. Key This is the key itself, which is a UUID, unique to this user. Expiry date This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be 'refreshed' before expiry. Refreshable Specifies whether an API key can be refreshed once expired. Status Details whether an API key is 'Active' or 'Disabled' . You can either copy an existing key value or create a new one to provide to an external party. To copy the key value to your clipboard click the 'Copy' button in the relevant 'Key' box. To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete. Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes. For more information about creating and managing API Keys, please see the API Keys page in the REST API documentation","title":"1.1 Create or copy an API Key"},{"location":"user-guides/publish-subscribe/publish-subscribe/#2-add-a-subscribed-catalogue","text":"Note: The following can only be carried out by an administrator. Administrators can manage the set of Subscribed Catalogues a Mauro Data Mapper instance connects to. To manage the catalogues, click the white arrow next to your user profile and select 'Subscribed catalogues' from the dropdown menu. This will take you to a list of existing catalogues. Here you can test, edit and delete catalogues by clicking the three vertical dots to the right of each item in the list which will display a dropdown menu. For each existing catalogue, the list displays: Label A unique label to help identify the subscribed catalogue. Description A description to help explain what the subscribed catalogue is for. URL The URL to the server instance to connect to. API Key The generated key value to authenticate against the server instance. Refresh period State how often, in days, this server instance should refresh the subscribed catalogue feed to check for new data. If not provided, a suitable default will be used instead. To add a Subscribed Catalogue , you will need: The URL to the Mauro Data Mapper server instance to connect to. The API key to authenticate on that server instance. Note: The URL only needs to refer to the instance and not the absolute URL to the Atom feed. Click the '+Add' button at the top right of the 'Catalogues' list. This will open an 'Add Subscribed Catalogue' form which you will need to complete. Enter the label, description, URL, API Key and refresh period as described above. Once completed, click 'Add subscription' and a green notification box should appear at the bottom right of your screen confirming that the 'subscribed catalogue saved successfully' .","title":"2. Add a Subscribed Catalogue"},{"location":"user-guides/publish-subscribe/publish-subscribe/#3-federated-data-models","text":"When at least one subscribed catalogue has been configured, any signed in user may be able to view the Federated Data Models from those catalogues in the Model Tree . The Model Tree will be updated to list two new root nodes: This catalogue Lists all the folders and Data Models contained within this Mauro Data Mapper instance. External catalogues Lists each subscribed catalogue created for this Mauro Data Mapper instance. Expanding each subscribed catalogue will list all readable Federated Data Models that can be individually subscribed to.","title":"3. Federated Data Models"},{"location":"user-guides/publish-subscribe/publish-subscribe/#31-subscribe-to-a-federated-data-model","text":"You can view the subscription status of a Federated Data Model by selecting the relevant model in the Model Tree which will display it's details panel. To subscribe to a Federated Data Model so that it is included in the current catalogue, click 'Subscribe' and a 'Subscribe to Data Model' dialogue box will appear. Click the menu icon to the right of the box to display the Model Tree . Select the folder you wish to import the Federated Data Model into and click '+Subscribe' . The status of this model will now change to 'Subscribed' in the details panel, meaning that this catalogue will automatically maintain this subscription until the user chooses to unsubscribe.","title":"3.1 Subscribe to a Federated Data Model"},{"location":"user-guides/publish-subscribe/publish-subscribe/#32-imported-federated-data-models","text":"Once subscribed to, each Federated Data Model will be automatically imported into the current Mauro Data Mapper catalogue at the chosen target folder, including the links between model versions. It is possible to manually synchronise the subscribed catalogues. To do this, select the subscribed catalogue in the Model Tree . In the relevant details panel, click the 'Synchronise' button. Each subscribed catalogue will also list version updates when the current Mauro Data Mapper instance refreshes the Atom feed, the frequency of which is determined during configuration. New versions of a Data Model will appear with their updated versions listed for you to also subscribe to, which is a manual operation.","title":"3.2 Imported Federated Data Models"},{"location":"user-guides/user-profile/user-profile/","text":"This user guide will show you how to add and edit information in your user profile. 1. Edit your profile \u00b6 When logged in to Mauro Data Mapper , your user profile is displayed on the top right of the menu header. Click the white arrow and a dropdown menu will appear. The options available in this menu will depend on whether you are an 'Editor' or an 'Administrator' . To edit your profile, select the 'My profile' option from the dropdown menu. This will take you to a page displaying your details, with the compulsory fields indicated by a *. These will already be completed according to the information entered when you were registered. Complete the fields below: First and last name This is used for identification. Organisation This allows other Mauro Data Mapper users to know which organisation you belong to. Role in Organisation This allows other Mauro Data Mapper users to know your role within your organisation. Once you have amended your details, click 'Save profile' and your changes will be updated. 2. Change your profile picture \u00b6 To add or edit an image to your profile picture, navigate to the 'My profile' page and click the pencil icon on your profile picture. A dropdown menu will appear with the option to either 'Change image' or 'Remove image' . To change the image for your profile picture, select 'Change image' and the file explorer on your computer will open. Navigate to the image you would like to use and then click 'Open' . This image will then be imported into your 'My profile' page, with a preview displayed on the right. You can then resize the image by hovering over the corners of the highlighted box until an arrow appears. Click and hold the left mouse button and drag the box until it is the desired size. To drag the entire highlighted box, hover your cursor inside the box until the 4 headed arrow appears. Click and hold the left mouse button until the box is in the desired location. Once you have finished adjusting your profile picture, click 'Update profile image' and a notification will appear on the bottom right of your screen to confirm the update. Then click 'Save profile' to save all changes. To remove a profile picture, click the pencil icon and select 'Remove image' from the dropdown menu. Again, a notification will appear at the bottom right of your screen to confirm the change. Then click 'Save profile' . 3. Update preferences \u00b6 You can also specify some interface preferences that are permanently saved to your user profile. To do this, navigate to the 'Preferences' page on your user profile and choose the options that suit you. Number of records per table By default this is set to 20. However depending on screen size, you may wish to view 5, 10 or 50. Select the option that you prefer. Always expand descriptions The next option allows descriptions to be expanded automatically, regardless of the character count. This is unchecked by default. However, those with larger screens may prefer to always expand descriptions in tables. If so, check this box. Include Superseded Document Models in Data Model Tree The final option determines whether you want the Finder Panel to show superseded document models within the Data Model Tree view. This is unchecked by default as typically these models represent invalid or outdated descriptions. However, feel free to check this box to change this. Once you have selected all your preferences, click 'Save preferences' and these will now be saved for future sessions. 4. Change your password \u00b6 To change your password, go to the 'Password' page under 'Account settings' . Enter your old password in the first box and your new password in the second box. The colour bar underneath will automatically change from red to green when you start typing. The width of the green bar will then change according to the strength of your chosen password. You can use both lower and upper case letters as well as special characters and numbers. If the colour bar changes to dark blue, then the password is too long and won't be accepted. To view or hide your passwords, click the eye icon to the right of each box. Once you have finalised your password, re-enter it into the 'Confirm password' box and then click 'Change password' to confirm the change.","title":"User profile"},{"location":"user-guides/user-profile/user-profile/#1-edit-your-profile","text":"When logged in to Mauro Data Mapper , your user profile is displayed on the top right of the menu header. Click the white arrow and a dropdown menu will appear. The options available in this menu will depend on whether you are an 'Editor' or an 'Administrator' . To edit your profile, select the 'My profile' option from the dropdown menu. This will take you to a page displaying your details, with the compulsory fields indicated by a *. These will already be completed according to the information entered when you were registered. Complete the fields below: First and last name This is used for identification. Organisation This allows other Mauro Data Mapper users to know which organisation you belong to. Role in Organisation This allows other Mauro Data Mapper users to know your role within your organisation. Once you have amended your details, click 'Save profile' and your changes will be updated.","title":"1. Edit your profile"},{"location":"user-guides/user-profile/user-profile/#2-change-your-profile-picture","text":"To add or edit an image to your profile picture, navigate to the 'My profile' page and click the pencil icon on your profile picture. A dropdown menu will appear with the option to either 'Change image' or 'Remove image' . To change the image for your profile picture, select 'Change image' and the file explorer on your computer will open. Navigate to the image you would like to use and then click 'Open' . This image will then be imported into your 'My profile' page, with a preview displayed on the right. You can then resize the image by hovering over the corners of the highlighted box until an arrow appears. Click and hold the left mouse button and drag the box until it is the desired size. To drag the entire highlighted box, hover your cursor inside the box until the 4 headed arrow appears. Click and hold the left mouse button until the box is in the desired location. Once you have finished adjusting your profile picture, click 'Update profile image' and a notification will appear on the bottom right of your screen to confirm the update. Then click 'Save profile' to save all changes. To remove a profile picture, click the pencil icon and select 'Remove image' from the dropdown menu. Again, a notification will appear at the bottom right of your screen to confirm the change. Then click 'Save profile' .","title":"2. Change your profile picture"},{"location":"user-guides/user-profile/user-profile/#3-update-preferences","text":"You can also specify some interface preferences that are permanently saved to your user profile. To do this, navigate to the 'Preferences' page on your user profile and choose the options that suit you. Number of records per table By default this is set to 20. However depending on screen size, you may wish to view 5, 10 or 50. Select the option that you prefer. Always expand descriptions The next option allows descriptions to be expanded automatically, regardless of the character count. This is unchecked by default. However, those with larger screens may prefer to always expand descriptions in tables. If so, check this box. Include Superseded Document Models in Data Model Tree The final option determines whether you want the Finder Panel to show superseded document models within the Data Model Tree view. This is unchecked by default as typically these models represent invalid or outdated descriptions. However, feel free to check this box to change this. Once you have selected all your preferences, click 'Save preferences' and these will now be saved for future sessions.","title":"3. Update preferences"},{"location":"user-guides/user-profile/user-profile/#4-change-your-password","text":"To change your password, go to the 'Password' page under 'Account settings' . Enter your old password in the first box and your new password in the second box. The colour bar underneath will automatically change from red to green when you start typing. The width of the green bar will then change according to the strength of your chosen password. You can use both lower and upper case letters as well as special characters and numbers. If the colour bar changes to dark blue, then the password is too long and won't be accepted. To view or hide your passwords, click the eye icon to the right of each box. Once you have finalised your password, re-enter it into the 'Confirm password' box and then click 'Change password' to confirm the change.","title":"4. Change your password"},{"location":"user-guides/version-data-models/version-data-models/","text":"This user guide will show you how to create working versions of Data Models as well as how to merge completed Data Models into the main branch. This ensures that you can make changes without adversely affecting other users of Mauro Data Mapper . 1. Versioning \u00b6 Data Models can only be versioned if they meet the below criteria: The Data Model must be finalised The Data Model must have a version number If your Data Model satisfies both of these conditions, then you will be able to create a new version. To do this, highlight the relevant Data Model in the Model Tree and click the three vertical dots at the top right of the details panel. Select 'Create a New Version' from the dropdown menu. You will then be presented with three options: New Fork This will create a copy of the Data Model with a new name and a new 'main' branch. Use this option if you are planning on taking this model in a new direction, or under a new authority. New Version This will create a copy of the Data Model under the 'main' branch. Use this option if you want to create the next iteration of this model. New Branch This will create a copy of the Data Model in a new branch, which you can choose the name for. Use this option if you want to make some changes that you subsequently wish to merge back into 'main' . 1.1 New Branch \u00b6 For this user guide, we will select 'New Branch' . A text box will then appear where you can enter a 'Branch name' . This will distinguish your Data Model from the others in your Data Model family. Once completed, click 'Add Branch' . This will create a copy of the Data Model for you to edit. If this is the first branch, then a 'main' branch will also be created. This will be the branch that changes from other branches will be merged into. 2. Merging \u00b6 To merge completed changes of a Data Model into the main branch, click the three vertical dots at the top right of the details panel. Select 'Merge' from the dropdown menu and then click 'Merge Model' . This will take you to a merge screen which is split into three columns. The left column shows the source branch which you are merging from. The middle column displays the items to be merged and the right column shows the items currently on the main branch. The right and left columns will display a tree which represents the Data Model and its changes. There are three types of possible changes which are represented by different colours: Addition - Green This is a change present on the source branch but isn\u2019t currently on the target branch. These types of changes will usually be auto merged and therefore won\u2019t need selecting. Modification - Yellow This is a change to an item which is present on both the source and target branches. These changes aren\u2019t normally auto merged and therefore will require selection by the user. Deletion - Red These changes will remove items from the main branch. These are never auto merged and will require selection by the user. As changes can be made to different items within the Data Model , it is recommended that you check the tree in the left column to ensure that all the changes you wish to merge are selected. 2.1 Property changes \u00b6 If required changes can be selected from the trees in both the left and right columns, a new editor window will appear. This allows you to make custom changes and then commit these changes to the main branch. 2.2 Check In \u00b6 Once all the relevant changes have been selected and are highlighted by a green tick, click 'Commit Changes' . A new dialogue box will appear where you can enter a check in comment. This is not required but can be useful for future auditing. You also have the option to delete the source branch by selecting the tick box to the left of the 'Delete Source Branch' option. To migrate all the changes to the main branch click 'Commit' and a green notification box should appear at the bottom right of your screen, confirming that the commit was successful. If the commit was unsuccessful, then a red notification box will appear at the bottom right of your screen. Correct the errors and try again.","title":"How to version and merge Data Models"},{"location":"user-guides/version-data-models/version-data-models/#1-versioning","text":"Data Models can only be versioned if they meet the below criteria: The Data Model must be finalised The Data Model must have a version number If your Data Model satisfies both of these conditions, then you will be able to create a new version. To do this, highlight the relevant Data Model in the Model Tree and click the three vertical dots at the top right of the details panel. Select 'Create a New Version' from the dropdown menu. You will then be presented with three options: New Fork This will create a copy of the Data Model with a new name and a new 'main' branch. Use this option if you are planning on taking this model in a new direction, or under a new authority. New Version This will create a copy of the Data Model under the 'main' branch. Use this option if you want to create the next iteration of this model. New Branch This will create a copy of the Data Model in a new branch, which you can choose the name for. Use this option if you want to make some changes that you subsequently wish to merge back into 'main' .","title":"1. Versioning"},{"location":"user-guides/version-data-models/version-data-models/#11-new-branch","text":"For this user guide, we will select 'New Branch' . A text box will then appear where you can enter a 'Branch name' . This will distinguish your Data Model from the others in your Data Model family. Once completed, click 'Add Branch' . This will create a copy of the Data Model for you to edit. If this is the first branch, then a 'main' branch will also be created. This will be the branch that changes from other branches will be merged into.","title":"1.1 New Branch"},{"location":"user-guides/version-data-models/version-data-models/#2-merging","text":"To merge completed changes of a Data Model into the main branch, click the three vertical dots at the top right of the details panel. Select 'Merge' from the dropdown menu and then click 'Merge Model' . This will take you to a merge screen which is split into three columns. The left column shows the source branch which you are merging from. The middle column displays the items to be merged and the right column shows the items currently on the main branch. The right and left columns will display a tree which represents the Data Model and its changes. There are three types of possible changes which are represented by different colours: Addition - Green This is a change present on the source branch but isn\u2019t currently on the target branch. These types of changes will usually be auto merged and therefore won\u2019t need selecting. Modification - Yellow This is a change to an item which is present on both the source and target branches. These changes aren\u2019t normally auto merged and therefore will require selection by the user. Deletion - Red These changes will remove items from the main branch. These are never auto merged and will require selection by the user. As changes can be made to different items within the Data Model , it is recommended that you check the tree in the left column to ensure that all the changes you wish to merge are selected.","title":"2. Merging"},{"location":"user-guides/version-data-models/version-data-models/#21-property-changes","text":"If required changes can be selected from the trees in both the left and right columns, a new editor window will appear. This allows you to make custom changes and then commit these changes to the main branch.","title":"2.1 Property changes"},{"location":"user-guides/version-data-models/version-data-models/#22-check-in","text":"Once all the relevant changes have been selected and are highlighted by a green tick, click 'Commit Changes' . A new dialogue box will appear where you can enter a check in comment. This is not required but can be useful for future auditing. You also have the option to delete the source branch by selecting the tick box to the left of the 'Delete Source Branch' option. To migrate all the changes to the main branch click 'Commit' and a green notification box should appear at the bottom right of your screen, confirming that the commit was successful. If the commit was unsuccessful, then a red notification box will appear at the bottom right of your screen. Correct the errors and try again.","title":"2.2 Check In"}]}