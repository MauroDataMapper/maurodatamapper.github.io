{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mauro Data Mapper: Documentation \u00b6 Welcome to the Mauro Data Mapper documentation site. Here you will find everything you need to understand, use and navigate the Mauro Data Mapper tool as well as API documentation for software developers. To browse through the documentation, use the menu on the left. Alternatively, click the buttons below to go straight to our most popular pages. About Discover more about Mauro Data Mapper, its history, and the plans for the future Glossary Understand the terminology and how it's used User Guides Learn how to use Mauro Data Mapper yourself Tutorials Study the full capabilities of Mauro Data Mapper and how it can benefit your work API Documentation Find out how to write scripts and applications to interact with Mauro Data Mapper Community Find support and contribute to our code and documentation","title":"Home"},{"location":"#mauro-data-mapper-documentation","text":"Welcome to the Mauro Data Mapper documentation site. Here you will find everything you need to understand, use and navigate the Mauro Data Mapper tool as well as API documentation for software developers. To browse through the documentation, use the menu on the left. Alternatively, click the buttons below to go straight to our most popular pages. About Discover more about Mauro Data Mapper, its history, and the plans for the future Glossary Understand the terminology and how it's used User Guides Learn how to use Mauro Data Mapper yourself Tutorials Study the full capabilities of Mauro Data Mapper and how it can benefit your work API Documentation Find out how to write scripts and applications to interact with Mauro Data Mapper Community Find support and contribute to our code and documentation","title":"Mauro Data Mapper: Documentation"},{"location":"about/development-roadmap/","text":"Development of the core Mauro platform, its interfaces and plugins, are largely driven by the community. A number of features planned for development are listed below, in no particular order. Some of these features are funded and underway; others may be de-prioritised based on user demand. For more information about any of these features, please contact us . Core \u00b6 Improved support for federation between instances of Mauro More advanced, configurable publication workflows Support for publishing DOIs Visual editors for Data Models Customizable dashboards and metrics Refinement of the Dataflow functionality and visualization Security \u00b6 Improved integration with OAuth / OpenID Connect for authorization Refinement of the API key functionality to restrict the scope of client application interactions Import / export \u00b6 Terminology and Reference Data Model Import plugins for SKOS A wider range of configuration options for exporting XML Schema User templating for Word / PDF exports Wizards for advanced import / export of models, including preview and customisation options Other \u00b6 Support for a wider range of email configurations Python API client library Simplified cloud-based deployment Setup wizard for first-time installation. Support for DOI registration and maintenance.","title":"Development Roadmap"},{"location":"about/development-roadmap/#core","text":"Improved support for federation between instances of Mauro More advanced, configurable publication workflows Support for publishing DOIs Visual editors for Data Models Customizable dashboards and metrics Refinement of the Dataflow functionality and visualization","title":"Core"},{"location":"about/development-roadmap/#security","text":"Improved integration with OAuth / OpenID Connect for authorization Refinement of the API key functionality to restrict the scope of client application interactions","title":"Security"},{"location":"about/development-roadmap/#import-export","text":"Terminology and Reference Data Model Import plugins for SKOS A wider range of configuration options for exporting XML Schema User templating for Word / PDF exports Wizards for advanced import / export of models, including preview and customisation options","title":"Import / export"},{"location":"about/development-roadmap/#other","text":"Support for a wider range of email configurations Python API client library Simplified cloud-based deployment Setup wizard for first-time installation. Support for DOI registration and maintenance.","title":"Other"},{"location":"about/history/","text":"The team at Oxford have been engaged in research and development of Metadata Registries for more than a decade. The Mauro Data Mapper represents the third generation of this technology. In the first generation, we used the eXist database as a simple store of XML representations of metadata items as individual attributes with arbitrary XML document types. This version of the catalogue did not make use of or enforce the constraints of a data modelling language, and the approach to semantic linkage was through classification, rather than individual assertions of refinement. It was applied successfully in two large projects, but proved difficult to maintain. The second generation of the technology was developed for the National Institute for Health Research (NIHR) Health Informatics Collaborative: a national programme aimed at facilitating the re-use of routinely-collected hospital data for medical research. This version of the catalogue allowed arbitrary links and included some of the features of the data modelling language presented here, but without a consistent, formal interpretation. The design included our principle that models should be the units of context and versioning. However, links were not contained within attributes, classes, or even models and hence not subject to model finalisation, making it impossible to ensure consistency. Furthermore, it was possible to import arbitrary items across models, without any guarantee that the definition of these items was independent of context. This third generation of the technology was developed from scratch, based on the experience of the previous implementations and, crucially, was developed in parallel with the formal semantics presented above. The result was a catalogue in which it was possible to guarantee consistency of definitions and to achieve a greater degree of scalability through automation and federation. The team have been engaged with the standards community, particularly the development of the ISO/IEC 11179/3 standard for metadata registration and related specifications. While the model defined in this standard is more closely aligned with the original generation of the tool, all subsequent implementations of Mauro maintain compliance with this standard for interoperability.","title":"History"},{"location":"about/introduction/","text":"What is Mauro Data Mapper? \u00b6 Mauro Data Mapper is a toolkit for the design and documentation of databases, data flows, and data standards, as well as related software artefacts such as data schemas and data forms. It was originally developed for the description of data in clinical research, but it is equally applicable in other settings. Data and software artefacts can be described as linked, versioned Data Models . The links let us re-use and relate data definitions, recording and reasoning about semantic interoperability. The versioning lets us keep track of changes in design, in implementation, or in understanding. Why is metadata important? \u00b6 To fully understand the meaning of data, first we need to know some further information about its context, known as metadata. For example, consider a blood pressure reading. Although this has standard units, the method and state of the patient at the time the measurement was taken will affect the recorded value. Therefore, by outlining this additional information, the reading can be understood and interpreted more accurately. In this way, metadata allows data to be more searchable, comparable and standardised enabling further interoperability. How does Mauro Data Mapper work? \u00b6 The Mauro Data Mapper is a web based tool which stores and manages descriptions of data. These can be descriptions of data already collected, such as databases or csv files. Or these can be descriptions of data you wish to collect or transfer between organisations, such as a specification for a webform or an XML schema. Mauro Data Mapper represents both types of descriptions of data as Data Models . These are defined as a structured collection of metadata and effectively model the data that they describe. Each Data Model consists of several Data Classes , which are groups of data that are related in some way. For example, a group of data that appears in the same table of a database or the same section of a form. Data Classes can sometimes also contain Nested Data Classes . Within each Data Class is then a number of Data Elements which are the descriptions of an individual field or variable. For example, a webform where patients enter their details would be a Data Model . This form could consist of two separate sections such as 'Personal details' and 'Contact details' which would each be a Data Class . The individual entries within each of these sections, such as 'First Name' , 'Last Name' , 'Date of Birth' etc, would each be a Data Element . However, there might be a section within another section on the webform, such as 'Correspondence Address' which lies within 'Contact details' . In this case, 'Correspondence Address' would become a Nested Data Class , where the 'Contact details' Data Class would be the parent. By organising metadata in this way, Mauro Data Mapper allows users to easily search data but also automatically import database schemas and export forms; helping to record data in standardised formats. An open-source community \u00b6 The Mauro platform and plugins are distributed under an open source Apache 2.0 license . We are keen to build an active community of users and developers, and encourage contributions to our code and documentation, and facilitate model sharing. Support \u00b6 The development of Mauro Data Mapper has been funded by the NIHR Oxford Biomedical Research Center as part of the NIHR Health Informatics Collaborative (HIC) . The NIHR HIC is a partnership of 28 NHS trusts and health boards, including the 20 hosting NIHR Biomedical Research Centres (BRCs) , working together to facilitate the equitable re-use of NHS data for translational research. The NIHR HIC has established cross-site data collaborations in areas such as cardiovascular medicine, critical care, renal disease, infectious diseases, and cancer. Mauro Data Mapper , and its previous incarnation, the Metadata Catalogue, has been used for collaboratively editing Data Models for research, and for generating software artefacts such as XML Schema.","title":"Introduction"},{"location":"about/introduction/#what-is-mauro-data-mapper","text":"Mauro Data Mapper is a toolkit for the design and documentation of databases, data flows, and data standards, as well as related software artefacts such as data schemas and data forms. It was originally developed for the description of data in clinical research, but it is equally applicable in other settings. Data and software artefacts can be described as linked, versioned Data Models . The links let us re-use and relate data definitions, recording and reasoning about semantic interoperability. The versioning lets us keep track of changes in design, in implementation, or in understanding.","title":"What is Mauro Data Mapper?"},{"location":"about/introduction/#why-is-metadata-important","text":"To fully understand the meaning of data, first we need to know some further information about its context, known as metadata. For example, consider a blood pressure reading. Although this has standard units, the method and state of the patient at the time the measurement was taken will affect the recorded value. Therefore, by outlining this additional information, the reading can be understood and interpreted more accurately. In this way, metadata allows data to be more searchable, comparable and standardised enabling further interoperability.","title":"Why is metadata important?"},{"location":"about/introduction/#how-does-mauro-data-mapper-work","text":"The Mauro Data Mapper is a web based tool which stores and manages descriptions of data. These can be descriptions of data already collected, such as databases or csv files. Or these can be descriptions of data you wish to collect or transfer between organisations, such as a specification for a webform or an XML schema. Mauro Data Mapper represents both types of descriptions of data as Data Models . These are defined as a structured collection of metadata and effectively model the data that they describe. Each Data Model consists of several Data Classes , which are groups of data that are related in some way. For example, a group of data that appears in the same table of a database or the same section of a form. Data Classes can sometimes also contain Nested Data Classes . Within each Data Class is then a number of Data Elements which are the descriptions of an individual field or variable. For example, a webform where patients enter their details would be a Data Model . This form could consist of two separate sections such as 'Personal details' and 'Contact details' which would each be a Data Class . The individual entries within each of these sections, such as 'First Name' , 'Last Name' , 'Date of Birth' etc, would each be a Data Element . However, there might be a section within another section on the webform, such as 'Correspondence Address' which lies within 'Contact details' . In this case, 'Correspondence Address' would become a Nested Data Class , where the 'Contact details' Data Class would be the parent. By organising metadata in this way, Mauro Data Mapper allows users to easily search data but also automatically import database schemas and export forms; helping to record data in standardised formats.","title":"How does Mauro Data Mapper work?"},{"location":"about/introduction/#an-open-source-community","text":"The Mauro platform and plugins are distributed under an open source Apache 2.0 license . We are keen to build an active community of users and developers, and encourage contributions to our code and documentation, and facilitate model sharing.","title":"An open-source community"},{"location":"about/introduction/#support","text":"The development of Mauro Data Mapper has been funded by the NIHR Oxford Biomedical Research Center as part of the NIHR Health Informatics Collaborative (HIC) . The NIHR HIC is a partnership of 28 NHS trusts and health boards, including the 20 hosting NIHR Biomedical Research Centres (BRCs) , working together to facilitate the equitable re-use of NHS data for translational research. The NIHR HIC has established cross-site data collaborations in areas such as cardiovascular medicine, critical care, renal disease, infectious diseases, and cancer. Mauro Data Mapper , and its previous incarnation, the Metadata Catalogue, has been used for collaboratively editing Data Models for research, and for generating software artefacts such as XML Schema.","title":"Support"},{"location":"about/mauro/","text":"Who is Mauro? \u00b6 Mauro Data Mapper is named after Fra Mauro who was a 15 th century Venetian Monk and also a famous cartographer. A diligent scholar, and enthusiast of ancient greek geographers such as Ptolemy , Mauro created the most detailed map of the world at the time, featuring thousands of texts and illustrations. Unlike other explorers who would embark on expeditions to gather their own data about the world, Mauro adopted a different approach and based himself at the port in Venice. He interviewed travellers who would give their detailed interpretations of the countries they had visited. Mauro then merged this information together to generate maps which were rich with contextual data. Unusually for the time, these maps were based on authoritative sources and proofs of existence, rather than religion and superstition. This philosophy lies at the heart of Mauro Data Mapper, which not only aims to be a hub of detailed metadata, but also allows users to make and share their own insights, understanding and experience. The Fra Mauro map of the world, shown above (source: Wikipedia), was produced between 1448 and 1459 as a commission for King Alfonso V of Portugal. Containing thousands of texts and illustrations, the map would have measured over five square metres. For a higher-resolution version, click here . Fra Mauro's Mappa Mundi along with a copy dated 1804 are stored in the British Library. To find out more about these historic items, watch the video below, courtesy of the British Library. Further reading \u00b6 Wikipedia Wikipedia entry on Fra Mauro A Mapmaker's Dream: The Meditations of Fra Mauro, Cartographer to the Court of Venice James Cowan , 1997 Fra Mauro and the Modern Globe Klaus A. Vogel , No. 57/58, Papers, Read at the 11TH Coronelli Symposium, Venice 2007, and other papers (2011 (for 2009/2010)), pp. 81-92, JSTOR.org Fra Mauro's Mappa Mundi Google Arts & Culture","title":"Mauro"},{"location":"about/mauro/#who-is-mauro","text":"Mauro Data Mapper is named after Fra Mauro who was a 15 th century Venetian Monk and also a famous cartographer. A diligent scholar, and enthusiast of ancient greek geographers such as Ptolemy , Mauro created the most detailed map of the world at the time, featuring thousands of texts and illustrations. Unlike other explorers who would embark on expeditions to gather their own data about the world, Mauro adopted a different approach and based himself at the port in Venice. He interviewed travellers who would give their detailed interpretations of the countries they had visited. Mauro then merged this information together to generate maps which were rich with contextual data. Unusually for the time, these maps were based on authoritative sources and proofs of existence, rather than religion and superstition. This philosophy lies at the heart of Mauro Data Mapper, which not only aims to be a hub of detailed metadata, but also allows users to make and share their own insights, understanding and experience. The Fra Mauro map of the world, shown above (source: Wikipedia), was produced between 1448 and 1459 as a commission for King Alfonso V of Portugal. Containing thousands of texts and illustrations, the map would have measured over five square metres. For a higher-resolution version, click here . Fra Mauro's Mappa Mundi along with a copy dated 1804 are stored in the British Library. To find out more about these historic items, watch the video below, courtesy of the British Library.","title":"Who is Mauro?"},{"location":"about/mauro/#further-reading","text":"Wikipedia Wikipedia entry on Fra Mauro A Mapmaker's Dream: The Meditations of Fra Mauro, Cartographer to the Court of Venice James Cowan , 1997 Fra Mauro and the Modern Globe Klaus A. Vogel , No. 57/58, Papers, Read at the 11TH Coronelli Symposium, Venice 2007, and other papers (2011 (for 2009/2010)), pp. 81-92, JSTOR.org Fra Mauro's Mappa Mundi Google Arts & Culture","title":"Further reading"},{"location":"about/release-notes/","text":"This page describes the changes for each component: 'core', 'ui', and the various centrally-maintained plugins and client libraries. For each, a table is presented with version number, notable new features, notable pull requests, and any dependencies. For more information about the structure and architecture of the code, please see our technical architecture pages. We use Semantic Versioning for all repositories, please note that anything labelled with -SNAPSHOT is under development and should be considered potentially unstable. In our code repositories, we use Git Flow , and so the 'main / master ' branch may be considered stable, but 'bleeding edge' features may be available within 'develop' or any feature branch. Please see our Installing Plugins pages for details about build artefacts and dependencies. The current full release is B4.10.0_F6.6.0 . Core API \u00b6 GitHub Version Release Date Major Changes 4.10.0 (GitHub Link) 2nd September 2021 Add API endpoints for editing Reference Data Models Configurable bootstrapping in production mode Many improvements and bug-fixes to Versioned Folders, merging and branching 4.9.0 (GitHub Link) 24th August 2021 Allow profiles to be editable after finalisation Add derived field types for profiles 4.8.0 (GitHub Link) 7th August 2021 Updates to branching and finalising for Versioned Folders Speed increase on finalising models Add rules to merge functionality Handle system configuration in multiple .yml files Various improvements to base import functionality Other bug fixes 4.7.0 (GitHub Link) 1st July 2021 Bug fixes for dynamic profiles Further improvements for versioning, versioned folders, and merging model branches Other bug fixes 4.6.0 (GitHub Link) 18th June 2021 Bug fixes for dynamic profiles More endpoints for versioned folders and versioning Improve performance of terminology importer 4.5.0 (GitHub Link) 18th May 2021 Dynamic Profiles Initial endpoints for Versioned Folders Add Facets to all containers 4.4.1 (GitHub Link) 22nd April 2021 Hidden / internal importer parameters ATOM feed for known models Bug fixes 4.3.0 (GitHub Link) 26th Mar 2021 Multiple bug fixes More control over admin system properties Add change notes to the edit history Custom tag names on model branches 4.2.0 (GitHub Link) 4th Mar 2021 DataModel component import feature Dataflow importer / exporter in XML / JSON Fix for Lucene indexing failure Fix for exception during model import Back-end performance improvements 4.1.0 (GitHub Link) 10th Feb 2021 Fix for multi-model imports 4.0.0 (GitHub Link) 1st Feb 2021 Listing major changes since last release of Metadata Catalogue Upgrade to Grails 4 Complete refactoring of code and underlying database Modular code structure Reference Data Model Removed access controls for individuals, increased the options for groups Updated Security Module API Keys / access tokens Much improved support for branching and merging UI \u00b6 GitHub Version Release Date Major Changes 6.6.1 (GitHub Link) 22nd September 2021 Open ID Connect redirect issue fixed 6.6.0 (GitHub Link) 2nd September 2021 Term search within Terminology fixed Fix for advanced search within a folder Improvements to merge diff tool 6.5.0 (GitHub Link) 24th August 2021 Improvements to profiles and other DOI profile functionality Fix to favourite highlighting DataFlow presentation no-longer using paginated call Open ID Connect conformance issue resolved Various other bug fixes and layout improvements 6.4.0 (GitHub Link) 7th August 2021 Fix for making models publicly readable DOI functionality (requires plugin) Multiple updates to model merging tool Fix to merge graph for Versioned Folders Drag-and-drop ordering for Data Elements \"Select all\" option for adding Terms to a Codeset Tree icons for model items Various other bug fixes 6.3.1 (GitHub Link) 1st July 2021 Basic support for external authentication Improved Export error handling Update merge tool Enable subscription functionality by default 6.2.0 (GitHub Link) 18th June 2021 Context view on model tree Versioned folder features 6.1.0 (GitHub Link) 18th May 2021 Properties, annotations and rules available on folders Bug fix for default datatypes 6.0.0 (GitHub Link) 22nd April 2021 Refresh of most model view screens, maximizing screen usage Support for structured profiles Publish / subscribe functionality Support for user-provided themes Many other bug fixes 5.2.0 (GitHub Link) 26th Mar 2021 New screens for administrator system properties 5.1.0 (GitHub Link) 4th Mar 2021 Global progress indicator Fix for Markdown link editing Other fixes / performance improvements 5.0.0 (GitHub Link) 5th Feb 2021 Listing major changes since last release of Metadata Catalogue Upgrade to Angular 9 Modular code structure Refactored, simplified layout to all screens Reference Data Model API Keys / access tokens WYSIWYG HTML editor Much improved support for branching and merging Plugins, client libraries, others \u00b6 Release notes for plugins, client libraries, and other Mauro repositories are not formally listed here for ease of maintenance. However, you can view the latest versions of each plugin on the Plugins page . More details about tagged releases and issues addressed can be found on the individual GitHub repos (see the 'tags' section to view all releases).","title":"Release Notes"},{"location":"about/release-notes/#core-api","text":"GitHub Version Release Date Major Changes 4.10.0 (GitHub Link) 2nd September 2021 Add API endpoints for editing Reference Data Models Configurable bootstrapping in production mode Many improvements and bug-fixes to Versioned Folders, merging and branching 4.9.0 (GitHub Link) 24th August 2021 Allow profiles to be editable after finalisation Add derived field types for profiles 4.8.0 (GitHub Link) 7th August 2021 Updates to branching and finalising for Versioned Folders Speed increase on finalising models Add rules to merge functionality Handle system configuration in multiple .yml files Various improvements to base import functionality Other bug fixes 4.7.0 (GitHub Link) 1st July 2021 Bug fixes for dynamic profiles Further improvements for versioning, versioned folders, and merging model branches Other bug fixes 4.6.0 (GitHub Link) 18th June 2021 Bug fixes for dynamic profiles More endpoints for versioned folders and versioning Improve performance of terminology importer 4.5.0 (GitHub Link) 18th May 2021 Dynamic Profiles Initial endpoints for Versioned Folders Add Facets to all containers 4.4.1 (GitHub Link) 22nd April 2021 Hidden / internal importer parameters ATOM feed for known models Bug fixes 4.3.0 (GitHub Link) 26th Mar 2021 Multiple bug fixes More control over admin system properties Add change notes to the edit history Custom tag names on model branches 4.2.0 (GitHub Link) 4th Mar 2021 DataModel component import feature Dataflow importer / exporter in XML / JSON Fix for Lucene indexing failure Fix for exception during model import Back-end performance improvements 4.1.0 (GitHub Link) 10th Feb 2021 Fix for multi-model imports 4.0.0 (GitHub Link) 1st Feb 2021 Listing major changes since last release of Metadata Catalogue Upgrade to Grails 4 Complete refactoring of code and underlying database Modular code structure Reference Data Model Removed access controls for individuals, increased the options for groups Updated Security Module API Keys / access tokens Much improved support for branching and merging","title":"Core API"},{"location":"about/release-notes/#ui","text":"GitHub Version Release Date Major Changes 6.6.1 (GitHub Link) 22nd September 2021 Open ID Connect redirect issue fixed 6.6.0 (GitHub Link) 2nd September 2021 Term search within Terminology fixed Fix for advanced search within a folder Improvements to merge diff tool 6.5.0 (GitHub Link) 24th August 2021 Improvements to profiles and other DOI profile functionality Fix to favourite highlighting DataFlow presentation no-longer using paginated call Open ID Connect conformance issue resolved Various other bug fixes and layout improvements 6.4.0 (GitHub Link) 7th August 2021 Fix for making models publicly readable DOI functionality (requires plugin) Multiple updates to model merging tool Fix to merge graph for Versioned Folders Drag-and-drop ordering for Data Elements \"Select all\" option for adding Terms to a Codeset Tree icons for model items Various other bug fixes 6.3.1 (GitHub Link) 1st July 2021 Basic support for external authentication Improved Export error handling Update merge tool Enable subscription functionality by default 6.2.0 (GitHub Link) 18th June 2021 Context view on model tree Versioned folder features 6.1.0 (GitHub Link) 18th May 2021 Properties, annotations and rules available on folders Bug fix for default datatypes 6.0.0 (GitHub Link) 22nd April 2021 Refresh of most model view screens, maximizing screen usage Support for structured profiles Publish / subscribe functionality Support for user-provided themes Many other bug fixes 5.2.0 (GitHub Link) 26th Mar 2021 New screens for administrator system properties 5.1.0 (GitHub Link) 4th Mar 2021 Global progress indicator Fix for Markdown link editing Other fixes / performance improvements 5.0.0 (GitHub Link) 5th Feb 2021 Listing major changes since last release of Metadata Catalogue Upgrade to Angular 9 Modular code structure Refactored, simplified layout to all screens Reference Data Model API Keys / access tokens WYSIWYG HTML editor Much improved support for branching and merging","title":"UI"},{"location":"about/release-notes/#plugins-client-libraries-others","text":"Release notes for plugins, client libraries, and other Mauro repositories are not formally listed here for ease of maintenance. However, you can view the latest versions of each plugin on the Plugins page . More details about tagged releases and issues addressed can be found on the individual GitHub repos (see the 'tags' section to view all releases).","title":"Plugins, client libraries, others"},{"location":"about/research/","text":"The core of Mauro Data Mapper is based on sound theoretical principles developed by the Oxford team over the course of the last decade. Formal, rigorous definitions of semantic interoperability , have been peer-reviewed and presented at academic conferences and in journals. Some of these articles are listed below: A formal, scalable approach to semantic interoperability Jim Davies, James Welch, David Milward, Steve Harris Science of Computer Programming 192, Elsevier. June 2020. Engineering Agile Big\u2212Data Systems Kevin Feeney\u201a Jim Davies\u201a James Welch\u201a Sebastian Hellmann\u201a Christian Dirschl\u201a Andreas Koller\u201a Pieter Francois and Arkadiusz Marciniak River Publishers; Illustrated edition. December 2018. Domain\u2212Specific Modelling for Clinical Research Jim Davies\u201a Jeremy Gibbons\u201a Adam Milward\u201a David Milward\u201a Seyyed Shah\u201a Monika Solanki and James Welch In SPLASH Workshop on Domain\u2212Specific Modelling . October 2015. The CancerGrid Experience: Metadata\u2212Based Model\u2212Driven Engineering for Clinical Trials Jim Davies\u201a Jeremy Gibbons\u201a Steve Harris and Charles Crichton In Science of Computer Programming . Vol. 89B. Pages 126\u2212143. September 2014. Form Follows Function: Model\u2212Driven Engineering for Clinical Trials Jim Davies\u201a Jeremy Gibbons\u201a Radu Calinescu\u201a Charles Crichton\u201a Steve Harris and Andrew Tsui In International Symposium on Foundations of Health Information Engineering and Systems . Vol. 7151 of LNCS. Pages 21\u221238. Springer. August, 2011. Models for Forms Daniel Abler\u201a Charles Crichton\u201a Jim Davies\u201a Steve Harris and James Welch In Proceedings of the 11 th Workshop on Domain-Specific Modeling , 2011. Semantic Interoperability in Practice Jim Davies\u201a Steve Harris and Aadya Shukla In HICSS (Electronic Government Track). 2010. Metadata\u2212Driven Software for Clinical Trials Charles Crichton\u201a Jim Davies\u201a Jeremy Gibbons\u201a Steve Harris\u201a Andrew Tsui and James Brenton In ICSE Workshop on Software Engineering and Health Care . May 2009.","title":"Research"},{"location":"community/build-plugins/","text":"","title":"Build plugins"},{"location":"community/contribute/","text":"Mauro Data Mapper is an open source tool, supported by a small core team of developers who welcome contributions from the user community. There are many ways to contribute, but before doing so we recommend that you join our community through our Zulip organisation so that you can understand the priorities and see which tasks are already in progress. We welcome any contributions, however small, and will ensure any contribution is credited appropriately. Our immediate general priorities are in bug fixes, documentation, and the creation of plugins. In each case, we have tooling or templates to help get started and the core development team are happy to provide additional support for contributing activities. Before contributing to any of our repositories, please have a read of our Contributor License Agreement . Any submissions to our repositories will be under this agreement to ensure that our code stays free of any further restrictions. Documentation \u00b6 New members of the community and those with little experience collaborating with an Open Source community may like to start here. Our documentation is under constant evaluation and improvement: screenshots and instructions need updating; API documentation needs further elaboration and examples while user guides to new features need writing and updating. We're particularly interested in real-world examples of usage - experience reports that can be turned into 'how-to' guides for others wanting to use Mauro in the future. Many of the examples we have are based on health data, so we're also keen to get examples from other domains too. The documentation can be found in the 'docs' repository in the Mauro Github organisation; clone the repository into a suitable location. The whole documentation website can be built using the MkDocs tool . Follow the instructions to install it locally and run mkdocs serve to build and host a version on your local machine. This will automatically update as you make changes to the source files. Please create a new branch with your changes and submit a pull request with your changes when you're ready to submit them. Before accepting, we will check for the following properties: That there are no conflicts with the existing 'develop' branch of the documentation That there are no spelling mistakes or grammatical errors That there are no broken or invalid links in the new text We may also ask some community members to review changes before they are published. Please add comments to your pull request to guide us on how best to accept your changes. Once your pull request has been accepted, the changes will be uploaded to the Github help pages as part of the next documentation 'release' . Bug fixes \u00b6 For developers getting started with our code base, fixing bugs is a great place to start. Lists of open issues are available on our Youtrack and GitHub sites, or you may have found your own issue you'd like to dive in and fix. All our repositories use the GitFlow model for managing code branches. New work should be created in a feature branch. Our core repositories act on a 'pull request' system and each repository will have criteria to meet before committing code. For the Core and UI repositories, more information is given below. With all contributions to the source code, we recommend you engage with the community first, to make sure no work is duplicated, and to understand the current priorities. Core \u00b6 The Core code is built using Gradle and Grails. Instructions for getting the code running locally are provided in the repository's README file. Before accepting any pull requests, the core team will: Checkout the relevant branch and check that any new functionality works correctly Run all integration and unit tests (where this hasn't already been done by our build servers) Run lint tools over the code to ensure new code meets existing quality checks Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch. Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly. UI \u00b6 The UI project is build using npm and ng and instructions for getting the code running locally are provided in the repository's README file. In order to build against a working back-end server, you might like to first check-out the Core code and get that running, or use our Docker installation to bring up a back-end which your copy of the web interface can communicate with. Before approving a pull request in the UI repository, the core team will: Checkout the relevant branch and check that any new functionality works correctly run ng test to ensure that all tests pass correctly run ng lint and npm run eslint to check that any code changes are sensible check through the diff of the current branch to look for any unintended changes Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch. Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly. Plugin development \u00b6 Back-end plugins can be written to provide functionality for specific use cases. They can make use of existing code 'hooks' , such as importers, exporters, and profiles or they may simply provide additional API endpoints for particular purposes. The Mauro Data Mapper Plugins Github organisation provides template plugins for these use cases, which can be cloned and used as a starting point for the development of a new plugin. If your plugin has universal appeal, we'd be happy to host it within our organisation so that other users can find it. Otherwise we can advertise it through these pages, or keep a pinned list within Zulip. Share your models \u00b6 The development team would be pleased to receive details of any models that we might be able to re-use - either for sharing with the community, or example models that can be used for demo purposes. For real-world models that will be useful to other members of the community, we intend to use these pages to advertise either downloadable files, or URLs that can be used to access publicly available models. Please pay particular care to any copyright or licensing information on the model itself before sharing.","title":"Contribute"},{"location":"community/contribute/#documentation","text":"New members of the community and those with little experience collaborating with an Open Source community may like to start here. Our documentation is under constant evaluation and improvement: screenshots and instructions need updating; API documentation needs further elaboration and examples while user guides to new features need writing and updating. We're particularly interested in real-world examples of usage - experience reports that can be turned into 'how-to' guides for others wanting to use Mauro in the future. Many of the examples we have are based on health data, so we're also keen to get examples from other domains too. The documentation can be found in the 'docs' repository in the Mauro Github organisation; clone the repository into a suitable location. The whole documentation website can be built using the MkDocs tool . Follow the instructions to install it locally and run mkdocs serve to build and host a version on your local machine. This will automatically update as you make changes to the source files. Please create a new branch with your changes and submit a pull request with your changes when you're ready to submit them. Before accepting, we will check for the following properties: That there are no conflicts with the existing 'develop' branch of the documentation That there are no spelling mistakes or grammatical errors That there are no broken or invalid links in the new text We may also ask some community members to review changes before they are published. Please add comments to your pull request to guide us on how best to accept your changes. Once your pull request has been accepted, the changes will be uploaded to the Github help pages as part of the next documentation 'release' .","title":"Documentation"},{"location":"community/contribute/#bug-fixes","text":"For developers getting started with our code base, fixing bugs is a great place to start. Lists of open issues are available on our Youtrack and GitHub sites, or you may have found your own issue you'd like to dive in and fix. All our repositories use the GitFlow model for managing code branches. New work should be created in a feature branch. Our core repositories act on a 'pull request' system and each repository will have criteria to meet before committing code. For the Core and UI repositories, more information is given below. With all contributions to the source code, we recommend you engage with the community first, to make sure no work is duplicated, and to understand the current priorities.","title":"Bug fixes"},{"location":"community/contribute/#core","text":"The Core code is built using Gradle and Grails. Instructions for getting the code running locally are provided in the repository's README file. Before accepting any pull requests, the core team will: Checkout the relevant branch and check that any new functionality works correctly Run all integration and unit tests (where this hasn't already been done by our build servers) Run lint tools over the code to ensure new code meets existing quality checks Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch. Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly.","title":"Core"},{"location":"community/contribute/#ui","text":"The UI project is build using npm and ng and instructions for getting the code running locally are provided in the repository's README file. In order to build against a working back-end server, you might like to first check-out the Core code and get that running, or use our Docker installation to bring up a back-end which your copy of the web interface can communicate with. Before approving a pull request in the UI repository, the core team will: Checkout the relevant branch and check that any new functionality works correctly run ng test to ensure that all tests pass correctly run ng lint and npm run eslint to check that any code changes are sensible check through the diff of the current branch to look for any unintended changes Pull requests will only be accepted where there are no conflicts with the existing 'develop' branch. Developers are encouraged to run these checks on any pull request before submission, to ensure that they can be merged quickly.","title":"UI"},{"location":"community/contribute/#plugin-development","text":"Back-end plugins can be written to provide functionality for specific use cases. They can make use of existing code 'hooks' , such as importers, exporters, and profiles or they may simply provide additional API endpoints for particular purposes. The Mauro Data Mapper Plugins Github organisation provides template plugins for these use cases, which can be cloned and used as a starting point for the development of a new plugin. If your plugin has universal appeal, we'd be happy to host it within our organisation so that other users can find it. Otherwise we can advertise it through these pages, or keep a pinned list within Zulip.","title":"Plugin development"},{"location":"community/contribute/#share-your-models","text":"The development team would be pleased to receive details of any models that we might be able to re-use - either for sharing with the community, or example models that can be used for demo purposes. For real-world models that will be useful to other members of the community, we intend to use these pages to advertise either downloadable files, or URLs that can be used to access publicly available models. Please pay particular care to any copyright or licensing information on the model itself before sharing.","title":"Share your models"},{"location":"community/support/","text":"We have an active and ever growing community of Mauro Data Mapper users in a variety of different application domains. As an open source project, we encourage all our users to interact and help solve problems, share experience or offer advice. The main channel for this is our new Zulip organisation which we encourage all potential users to visit. In addition to active users and contributors, the core development team are also active on Zulip and able to help with any technical questions. You can log into our Zulip organisation using your GitHub credentials, or alternatively ask an existing member to send you an invite. Note that although our Slack channels are still open for the core development team, we will be migrating all previous channels / conversations to Zulip where appropriate. Reporting Issues \u00b6 The Mauro team are keen to hear of any bugs, performance problems or other issues that may be affecting your experience of using the tools. The core development team run an instance of JetBrains YouTrack for managing issues. You can view current issues or submit new ones through hosted instances of the Web Interface. Click the 'Report Issue' link in the footer of the page. When an error is encountered through the web interface, there is also an option to submit a new issue directly. This will include any debug details necessary for the development team to investigate. Please first check to see if your issue has been previously reported. If in doubt, submit a new issue, but in many cases it may be more helpful to add additional context or examples to an existing issue rather than creating another. When submitting issues in this way, please record your name in the 'Reporters Name' field. This will allow the development team to follow up with further questions or reports on progress. Please also do include any additional information that will allow the development team to better understand your issue. There are many online guides to help you write good bug reports - one such example is here . As well as YouTrack, you can also submit issues to our GitHub organisations. Please try to report issues against the correct repository. Back-end issues should be submitted against mdm-core while user interface issues should be submitted against mdm-ui and so on. Requesting Features \u00b6 The core team are very pleased to receive suggestions for new features or improvements to the tools - especially those that will benefit a wide audience or encourage adoption of the tools. Our Development Roadmap page shows a few items that are either in progress or planned for development in the near future. The team may also be engaged in other funded projects with other deliverables not reported on that page, so do please discuss with the team if there are features that you would find useful. In particular, early feedback can ensure we cater for as many use-cases as possible when building new functionality and we can let you know directly once support is in progress, ready for testing, or available as part of a tagged code release. Visit our Zulip organisation to discuss new features and get in touch with the development team directly.","title":"Support"},{"location":"community/support/#reporting-issues","text":"The Mauro team are keen to hear of any bugs, performance problems or other issues that may be affecting your experience of using the tools. The core development team run an instance of JetBrains YouTrack for managing issues. You can view current issues or submit new ones through hosted instances of the Web Interface. Click the 'Report Issue' link in the footer of the page. When an error is encountered through the web interface, there is also an option to submit a new issue directly. This will include any debug details necessary for the development team to investigate. Please first check to see if your issue has been previously reported. If in doubt, submit a new issue, but in many cases it may be more helpful to add additional context or examples to an existing issue rather than creating another. When submitting issues in this way, please record your name in the 'Reporters Name' field. This will allow the development team to follow up with further questions or reports on progress. Please also do include any additional information that will allow the development team to better understand your issue. There are many online guides to help you write good bug reports - one such example is here . As well as YouTrack, you can also submit issues to our GitHub organisations. Please try to report issues against the correct repository. Back-end issues should be submitted against mdm-core while user interface issues should be submitted against mdm-ui and so on.","title":"Reporting Issues"},{"location":"community/support/#requesting-features","text":"The core team are very pleased to receive suggestions for new features or improvements to the tools - especially those that will benefit a wide audience or encourage adoption of the tools. Our Development Roadmap page shows a few items that are either in progress or planned for development in the near future. The team may also be engaged in other funded projects with other deliverables not reported on that page, so do please discuss with the team if there are features that you would find useful. In particular, early feedback can ensure we cater for as many use-cases as possible when building new functionality and we can let you know directly once support is in progress, ready for testing, or available as part of a tagged code release. Visit our Zulip organisation to discuss new features and get in touch with the development team directly.","title":"Requesting Features"},{"location":"developer/build_status/","text":"MDM Core \u00b6 Branch Build Status main develop MDM UI \u00b6 Branch Build Status main develop MDM Plugins \u00b6 Grails Plugins \u00b6 mdm-plugin-artdecor \u00b6 Branch Build Status main develop mdm-plugin-authentication-keycloak \u00b6 Branch Build Status main develop mdm-plugin-authentication-openid-connect \u00b6 Branch Build Status main develop mdm-plugin-awsglue \u00b6 Branch Build Status main develop mdm-plugin-csv \u00b6 Branch Build Status main develop mdm-plugin-excel \u00b6 Branch Build Status main develop mdm-plugin-digital-object-identifiers \u00b6 Branch Build Status main develop mdm-plugin-fhir \u00b6 Branch Build Status main develop mdm-plugin-freemarker \u00b6 Branch Build Status main develop mdm-plugin-profile-dcat \u00b6 Branch Build Status main develop mdm-plugin-profile-dementia-platform \u00b6 Branch Build Status main develop mdm-plugin-profile-hdruk \u00b6 Branch Build Status main develop mdm-plugin-profile-schema-org \u00b6 Branch Build Status main develop mdm-plugin-sparql \u00b6 Branch Build Status main develop Gradle Plugins \u00b6 mdm-plugin-database \u00b6 Branch Build Status main develop mdm-plugin-database-mysql \u00b6 Branch Build Status main develop mdm-plugin-database-oracle \u00b6 Oracle Database Plugin for the Mauro Data Mapper Branch Build Status main develop mdm-plugin-database-postgresql \u00b6 Branch Build Status main develop mdm-plugin-database-sqlserver \u00b6 Branch Build Status main develop Other Plugins \u00b6 mdm-plugin-testing-utils \u00b6 Branch Build Status main develop mdm-plugin-template \u00b6 Branch Build Status grails mdm-plugin-template-gradle \u00b6 Branch Build Status gradle Under development \u00b6 mdm-plugin-opensafely \u00b6 Branch Build Status main develop mdm-plugin-xsd \u00b6 Branch Build Status main develop","title":"Build status"},{"location":"developer/build_status/#mdm-core","text":"Branch Build Status main develop","title":"MDM Core"},{"location":"developer/build_status/#mdm-ui","text":"Branch Build Status main develop","title":"MDM UI"},{"location":"developer/build_status/#mdm-plugins","text":"","title":"MDM Plugins"},{"location":"developer/build_status/#grails-plugins","text":"","title":"Grails Plugins"},{"location":"developer/build_status/#mdm-plugin-artdecor","text":"Branch Build Status main develop","title":"mdm-plugin-artdecor"},{"location":"developer/build_status/#mdm-plugin-authentication-keycloak","text":"Branch Build Status main develop","title":"mdm-plugin-authentication-keycloak"},{"location":"developer/build_status/#mdm-plugin-authentication-openid-connect","text":"Branch Build Status main develop","title":"mdm-plugin-authentication-openid-connect"},{"location":"developer/build_status/#mdm-plugin-awsglue","text":"Branch Build Status main develop","title":"mdm-plugin-awsglue"},{"location":"developer/build_status/#mdm-plugin-csv","text":"Branch Build Status main develop","title":"mdm-plugin-csv"},{"location":"developer/build_status/#mdm-plugin-excel","text":"Branch Build Status main develop","title":"mdm-plugin-excel"},{"location":"developer/build_status/#mdm-plugin-digital-object-identifiers","text":"Branch Build Status main develop","title":"mdm-plugin-digital-object-identifiers"},{"location":"developer/build_status/#mdm-plugin-fhir","text":"Branch Build Status main develop","title":"mdm-plugin-fhir"},{"location":"developer/build_status/#mdm-plugin-freemarker","text":"Branch Build Status main develop","title":"mdm-plugin-freemarker"},{"location":"developer/build_status/#mdm-plugin-profile-dcat","text":"Branch Build Status main develop","title":"mdm-plugin-profile-dcat"},{"location":"developer/build_status/#mdm-plugin-profile-dementia-platform","text":"Branch Build Status main develop","title":"mdm-plugin-profile-dementia-platform"},{"location":"developer/build_status/#mdm-plugin-profile-hdruk","text":"Branch Build Status main develop","title":"mdm-plugin-profile-hdruk"},{"location":"developer/build_status/#mdm-plugin-profile-schema-org","text":"Branch Build Status main develop","title":"mdm-plugin-profile-schema-org"},{"location":"developer/build_status/#mdm-plugin-sparql","text":"Branch Build Status main develop","title":"mdm-plugin-sparql"},{"location":"developer/build_status/#gradle-plugins","text":"","title":"Gradle Plugins"},{"location":"developer/build_status/#mdm-plugin-database","text":"Branch Build Status main develop","title":"mdm-plugin-database"},{"location":"developer/build_status/#mdm-plugin-database-mysql","text":"Branch Build Status main develop","title":"mdm-plugin-database-mysql"},{"location":"developer/build_status/#mdm-plugin-database-oracle","text":"Oracle Database Plugin for the Mauro Data Mapper Branch Build Status main develop","title":"mdm-plugin-database-oracle"},{"location":"developer/build_status/#mdm-plugin-database-postgresql","text":"Branch Build Status main develop","title":"mdm-plugin-database-postgresql"},{"location":"developer/build_status/#mdm-plugin-database-sqlserver","text":"Branch Build Status main develop","title":"mdm-plugin-database-sqlserver"},{"location":"developer/build_status/#other-plugins","text":"","title":"Other Plugins"},{"location":"developer/build_status/#mdm-plugin-testing-utils","text":"Branch Build Status main develop","title":"mdm-plugin-testing-utils"},{"location":"developer/build_status/#mdm-plugin-template","text":"Branch Build Status grails","title":"mdm-plugin-template"},{"location":"developer/build_status/#mdm-plugin-template-gradle","text":"Branch Build Status gradle","title":"mdm-plugin-template-gradle"},{"location":"developer/build_status/#under-development","text":"","title":"Under development"},{"location":"developer/build_status/#mdm-plugin-opensafely","text":"Branch Build Status main develop","title":"mdm-plugin-opensafely"},{"location":"developer/build_status/#mdm-plugin-xsd","text":"Branch Build Status main develop","title":"mdm-plugin-xsd"},{"location":"developer/releasing/","text":"Releasing \u00b6 The following process should be followed EXACTLY to release MauroDataMapper in its entirety. Please pay attention to versions and whats changed between each git commit. Warning You will need git flow installed in the command line to be able to perform the release. Installation instructions are here . You will also need to make sure all your repositories are initialised as git-flow repositories. This can be done using git flow init , however you MUST make sure you have checked out the develop and main branches before you initialise. Information Unless patching a release which has failed ALL releases should be the next MINOR release. If there is significant and appropriate change then a MAJOR release should be used. A PATCH release should only be done if the MINOR release failed and this is a release being done to fix that release failure. Information A unified view of the current build status of develop and main branches can be see here You should identify the following from the repositories before starting CORE_VERSION : The appropriate non-snapshot version from mdm-core/gradle.properties on the develop branch UI_VERSION : The appropriate non-snapshot version from mdm-ui/package.json on the develop branch mdm-core \u00b6 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ CORE_VERSION } Update gradle.properties 1 2 git commit -am \"Release ${ CORE_VERSION } \" git flow release finish -m \" ${ CORE_VERSION } \" ${ CORE_VERSION } Update gradle.properties to next minor snapshot 1 2 git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags mdm-resources \u00b6 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ CORE_VERSION } Update package.json version to CORE_VERSION 1 2 3 npm install && npm run build git commit -am \"Release ${ CORE_VERSION } \" git flow release finish -m \" ${ CORE_VERSION } \" ${ CORE_VERSION } Update package.json version to next snapshot CORE_VERSION 1 2 3 npm install && npm run build git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags mdm-ui \u00b6 Warning You must release mdm-resources first, or be able to use an existing release of mdm-resources 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ UI_VERSION } Update package.json version to UI_VERSION \"@maurodatamapper/mdm-resources\": \"git+https://github.com/MauroDataMapper/mdm-resources.git#CORE_VERSION\", 1 rm package-lock.json && npm install Caution MAKE SURE in package.lock that all the lines for mdm-resources that have a commit hash MATCH the hash for the release, if NOT then delete all entries in the lock file for mdm-resources with a commit hash and run npm install again 1 2 git commit -am \"Release ${ UI_VERSION } \" git flow release finish -m \" ${ UI_VERSION } \" ${ UI_VERSION } Update package.json version to next minor snapshot UI_VERSION \"@maurodatamapper/mdm-resources\": \"git+https://github.com/MauroDataMapper/mdm-resources.git#develop\", 1 2 3 rm package-lock.json && npm install git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags mdm-application-build \u00b6 Warning You will need to wait for the main branch of mdm-core to finish before proceeding on the latest mdm-application-build 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ CORE_VERSION } Update gradle.properties 1 2 git commit -am \"Release ${ CORE_VERSION } \" git flow release finish -m \" ${ CORE_VERSION } \" ${ CORE_VERSION } Update gradle.properties to next minor snapshot 1 2 git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags MDM plugins \u00b6 Warning You will need to wait for the main branch of mdm-core to finish before proceeding on the latest plugins Information This repository provides useful scripts and a unified live view of the state of each branch build. Release Order \u00b6 You will need to push some of the repositories in the correct order as they have dependencies on other plugins. The following plugins have a release order, if not listed then there is not required order. mdm-plugin-database mdm-plugin-testing-utils mdm-plugin-database-mysql mdm-plugin-database-oracle mdm-plugin-database-postgresql mdm-plugin-database-sqlserver No changes waiting to be released \u00b6 You don't need to release these every time we release mdm-core However you should make sure all develop branches are updated to mdmCoreVersion=${CORE_VERSION} and then push the update. Any jobs which fail will need to have the code updated. If any of the code changes are in side the code base (not test changes) then you will need to release If only test code changes are needed or no changes are needed then don't do a release as we have proved it's still compatible Changes waiting to be released \u00b6 You should make sure the develop branch is updated to mdmCoreVersion=${CORE_VERSION} and then push the update Any tests which fail will need to have the code updated If any of the code changes are in side the code base (not test changes) then you will need to release with an updated mdmCoreVersion If only test code changes are needed or no changes are needed then release using the last mdmCoreVersion used as its still compatible 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ PLUGIN_VERSION } Update gradle.properties Update README.md \"How to apply\" 1 2 git commit -am \"Release ${ PLUGIN_VERSION } \" git flow release finish -m \" ${ PLUGIN_VERSION } \" ${ PLUGIN_VERSION } Update gradle.properties to next minor snapshot DO NOT change the README.md file 1 2 git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags mdm-docker \u00b6 Warning You will need to wait for the main branch of mdm-application-build and mdm-ui to finish before proceeding 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" Update docker-compose.yml The 2 commit ARGS The image tag to B${CORE_VERSION}_F${UI_VERSION} Dry run and check it comes up as expected 1 2 docker-compose build docker-compose up If it all comes up. 1 2 3 git commit -am \"Release B ${ CORE_VERSION } _F ${ UI_VERSION } \" git flow release finish -m \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" git checkout main && git push && git checkout develop && git push && git push --tags Document & Announce \u00b6 Information To be able to autogenerate the release documentation for plugins you will need to clone https://github.com/MauroDataMapper-Plugins/mdm-plugins . Then place all the plugins into this directory. Warning Remember to remove the following from the autogenerated plugins release output Private Repositories Unreleased plugins Github \u00b6 Each of the repositories requires the tag to be released and links to the issues fixed supplied. Navigate to the supplied tags page Select the latest tag and choose \"Create release\" Copy in the appropriate text from the below list, making sure to update the stated YouTrack version to the tag being released the stated Github milestone mdm-core Tags Page 1 2 See [ Issues fixed ]( https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23B4.8.0 ) and [ Milestones ]( https://github.com/MauroDataMapper/mdm-core/issues?q=is%3Aissue+milestone%3A4.8.0 ) mdm-ui Tags Page 1 2 See [ Issues fixed ]( https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23F6.4.0 ) and [ MileStones ]( https://github.com/MauroDataMapper/mdm-ui/issues?q=is%3Aissue+milestone%3A6.4.0 ) mdm-application Tags Page 1 2 3 See [ Issues fixed ]( https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23B4.8.0 ) and [ Core Milestones ]( https://github.com/MauroDataMapper/mdm-core/issues?q=is%3Aissue+milestone%3A4.8.0 ) and [ Application Milestones ]( https://github.com/MauroDataMapper/mdm-application-build/issues?q=is%3Aissue+milestone%3A4.8.0 ) mdm-docker Tags Page 1 2 3 4 See [ Issues fixed ]( https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23B4.8.0%20%23F6.4.0 ) and [ Core Milestones ]( https://github.com/MauroDataMapper/mdm-core/issues?q=is%3Aissue+milestone%3A4.8.0 ) and [ Application Milestones ]( https://github.com/MauroDataMapper/mdm-application-build/issues?q=is%3Aissue+milestone%3A4.8.0 ) and [ UI Milestones ]( https://github.com/MauroDataMapper/mdm-ui/issues?q=is%3Aissue+milestone%3A6.4.0 ) Documentation \u00b6 Caution Make sure you've done all the github releases first otherwise the release notes won't contain anything useful This should be performed inside the docs repository. 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" Perform the updates as per the 2 sub-sections ( Main Release , Plugins Release ) below 1 2 3 git commit -am \"Release B ${ CORE_VERSION } _F ${ UI_VERSION } \" git flow release finish -m \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" git checkout main && git push && git checkout develop && git push && git push --tags Main Release \u00b6 Update docs/about/release-notes.md file. The td sections, in order, are: Add 1. Release version & link to the release in Github 2. Release date 3. Major Changes: This is an ul so each change should be wrapped in an li element Update the \"Current Full Release\" line: 1 The current full release is **B${CORE_VERSION}_F${UI_VERSION}** . Add the following block with updated versions to the top of the ## Core API section: 1 2 3 4 5 6 7 8 9 < tr > < td >< b > 4.10.0 </ b >< br />< a href = \"https://github.com/MauroDataMapper/mdm-core/releases/tag/4.10.0\" > (GitHub Link) </ a ></ td > < td > 2nd September 2021 </ td > < td > < ul > < li ></ li > </ ul > </ td > </ tr > Add the following block with updated versions to the top of the ## UI section: 1 2 3 4 5 6 7 8 9 < tr > < td >< b > 6.6.0 </ b >< br />< a href = \"https://github.com/MauroDataMapper/mdm-ui/releases/tag/6.6.0\" > (GitHub Link) </ a ></ td > < td > 2nd September 2021 </ td > < td > < ul > < li ></ li > </ ul > </ td > </ tr > Plugins Release \u00b6 Run ./releases.sh in mdm-plugins. Remove the private repositories from the HTML format and copy into the docs/installing/plugins.md file Zulip Announce \u00b6 Caution Make sure you've done all the github releases first otherwise the release notes won't contain anything useful Run ./releases.sh in mdm-plugins. Copy the below markdown block into the announce stream of Zulip Update the versions for the applications Update the tag version for the release notes Copy in the plain text format of the \"releases\" output underneath it 1 2 3 4 5 6 7 # New Release | Application | Version | Release Notes | |----|-----|-----| | Docker | `B4.9.0_F6.5.0` | https://github.com/MauroDataMapper/mdm-docker/releases/tag/B4.9.0_F6.5.0 | | RESTful API | `4.9.0` | https://github.com/MauroDataMapper/mdm-application-build/releases/tag/4.9.0 | | UI | `6.5.0` | https://github.com/MauroDataMapper/mdm-ui/releases/tag/6.5.0 |","title":"Releasing"},{"location":"developer/releasing/#releasing","text":"The following process should be followed EXACTLY to release MauroDataMapper in its entirety. Please pay attention to versions and whats changed between each git commit. Warning You will need git flow installed in the command line to be able to perform the release. Installation instructions are here . You will also need to make sure all your repositories are initialised as git-flow repositories. This can be done using git flow init , however you MUST make sure you have checked out the develop and main branches before you initialise. Information Unless patching a release which has failed ALL releases should be the next MINOR release. If there is significant and appropriate change then a MAJOR release should be used. A PATCH release should only be done if the MINOR release failed and this is a release being done to fix that release failure. Information A unified view of the current build status of develop and main branches can be see here You should identify the following from the repositories before starting CORE_VERSION : The appropriate non-snapshot version from mdm-core/gradle.properties on the develop branch UI_VERSION : The appropriate non-snapshot version from mdm-ui/package.json on the develop branch","title":"Releasing"},{"location":"developer/releasing/#mdm-core","text":"1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ CORE_VERSION } Update gradle.properties 1 2 git commit -am \"Release ${ CORE_VERSION } \" git flow release finish -m \" ${ CORE_VERSION } \" ${ CORE_VERSION } Update gradle.properties to next minor snapshot 1 2 git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags","title":"mdm-core"},{"location":"developer/releasing/#mdm-resources","text":"1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ CORE_VERSION } Update package.json version to CORE_VERSION 1 2 3 npm install && npm run build git commit -am \"Release ${ CORE_VERSION } \" git flow release finish -m \" ${ CORE_VERSION } \" ${ CORE_VERSION } Update package.json version to next snapshot CORE_VERSION 1 2 3 npm install && npm run build git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags","title":"mdm-resources"},{"location":"developer/releasing/#mdm-ui","text":"Warning You must release mdm-resources first, or be able to use an existing release of mdm-resources 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ UI_VERSION } Update package.json version to UI_VERSION \"@maurodatamapper/mdm-resources\": \"git+https://github.com/MauroDataMapper/mdm-resources.git#CORE_VERSION\", 1 rm package-lock.json && npm install Caution MAKE SURE in package.lock that all the lines for mdm-resources that have a commit hash MATCH the hash for the release, if NOT then delete all entries in the lock file for mdm-resources with a commit hash and run npm install again 1 2 git commit -am \"Release ${ UI_VERSION } \" git flow release finish -m \" ${ UI_VERSION } \" ${ UI_VERSION } Update package.json version to next minor snapshot UI_VERSION \"@maurodatamapper/mdm-resources\": \"git+https://github.com/MauroDataMapper/mdm-resources.git#develop\", 1 2 3 rm package-lock.json && npm install git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags","title":"mdm-ui"},{"location":"developer/releasing/#mdm-application-build","text":"Warning You will need to wait for the main branch of mdm-core to finish before proceeding on the latest mdm-application-build 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ CORE_VERSION } Update gradle.properties 1 2 git commit -am \"Release ${ CORE_VERSION } \" git flow release finish -m \" ${ CORE_VERSION } \" ${ CORE_VERSION } Update gradle.properties to next minor snapshot 1 2 git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags","title":"mdm-application-build"},{"location":"developer/releasing/#mdm-plugins","text":"Warning You will need to wait for the main branch of mdm-core to finish before proceeding on the latest plugins Information This repository provides useful scripts and a unified live view of the state of each branch build.","title":"MDM plugins"},{"location":"developer/releasing/#release-order","text":"You will need to push some of the repositories in the correct order as they have dependencies on other plugins. The following plugins have a release order, if not listed then there is not required order. mdm-plugin-database mdm-plugin-testing-utils mdm-plugin-database-mysql mdm-plugin-database-oracle mdm-plugin-database-postgresql mdm-plugin-database-sqlserver","title":"Release Order"},{"location":"developer/releasing/#no-changes-waiting-to-be-released","text":"You don't need to release these every time we release mdm-core However you should make sure all develop branches are updated to mdmCoreVersion=${CORE_VERSION} and then push the update. Any jobs which fail will need to have the code updated. If any of the code changes are in side the code base (not test changes) then you will need to release If only test code changes are needed or no changes are needed then don't do a release as we have proved it's still compatible","title":"No changes waiting to be released"},{"location":"developer/releasing/#changes-waiting-to-be-released","text":"You should make sure the develop branch is updated to mdmCoreVersion=${CORE_VERSION} and then push the update Any tests which fail will need to have the code updated If any of the code changes are in side the code base (not test changes) then you will need to release with an updated mdmCoreVersion If only test code changes are needed or no changes are needed then release using the last mdmCoreVersion used as its still compatible 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start ${ PLUGIN_VERSION } Update gradle.properties Update README.md \"How to apply\" 1 2 git commit -am \"Release ${ PLUGIN_VERSION } \" git flow release finish -m \" ${ PLUGIN_VERSION } \" ${ PLUGIN_VERSION } Update gradle.properties to next minor snapshot DO NOT change the README.md file 1 2 git commit -am 'Next snapshot' git checkout main && git push && git checkout develop && git push && git push --tags","title":"Changes waiting to be released"},{"location":"developer/releasing/#mdm-docker","text":"Warning You will need to wait for the main branch of mdm-application-build and mdm-ui to finish before proceeding 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" Update docker-compose.yml The 2 commit ARGS The image tag to B${CORE_VERSION}_F${UI_VERSION} Dry run and check it comes up as expected 1 2 docker-compose build docker-compose up If it all comes up. 1 2 3 git commit -am \"Release B ${ CORE_VERSION } _F ${ UI_VERSION } \" git flow release finish -m \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" git checkout main && git push && git checkout develop && git push && git push --tags","title":"mdm-docker"},{"location":"developer/releasing/#document-announce","text":"Information To be able to autogenerate the release documentation for plugins you will need to clone https://github.com/MauroDataMapper-Plugins/mdm-plugins . Then place all the plugins into this directory. Warning Remember to remove the following from the autogenerated plugins release output Private Repositories Unreleased plugins","title":"Document &amp; Announce"},{"location":"developer/releasing/#github","text":"Each of the repositories requires the tag to be released and links to the issues fixed supplied. Navigate to the supplied tags page Select the latest tag and choose \"Create release\" Copy in the appropriate text from the below list, making sure to update the stated YouTrack version to the tag being released the stated Github milestone mdm-core Tags Page 1 2 See [ Issues fixed ]( https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23B4.8.0 ) and [ Milestones ]( https://github.com/MauroDataMapper/mdm-core/issues?q=is%3Aissue+milestone%3A4.8.0 ) mdm-ui Tags Page 1 2 See [ Issues fixed ]( https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23F6.4.0 ) and [ MileStones ]( https://github.com/MauroDataMapper/mdm-ui/issues?q=is%3Aissue+milestone%3A6.4.0 ) mdm-application Tags Page 1 2 3 See [ Issues fixed ]( https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23B4.8.0 ) and [ Core Milestones ]( https://github.com/MauroDataMapper/mdm-core/issues?q=is%3Aissue+milestone%3A4.8.0 ) and [ Application Milestones ]( https://github.com/MauroDataMapper/mdm-application-build/issues?q=is%3Aissue+milestone%3A4.8.0 ) mdm-docker Tags Page 1 2 3 4 See [ Issues fixed ]( https://metadatacatalogue.myjetbrains.com/youtrack/issues/MC?q=%23Released%20%23B4.8.0%20%23F6.4.0 ) and [ Core Milestones ]( https://github.com/MauroDataMapper/mdm-core/issues?q=is%3Aissue+milestone%3A4.8.0 ) and [ Application Milestones ]( https://github.com/MauroDataMapper/mdm-application-build/issues?q=is%3Aissue+milestone%3A4.8.0 ) and [ UI Milestones ]( https://github.com/MauroDataMapper/mdm-ui/issues?q=is%3Aissue+milestone%3A6.4.0 )","title":"Github"},{"location":"developer/releasing/#documentation","text":"Caution Make sure you've done all the github releases first otherwise the release notes won't contain anything useful This should be performed inside the docs repository. 1 2 git checkout main && git pull && git checkout develop && git pull git flow release start \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" Perform the updates as per the 2 sub-sections ( Main Release , Plugins Release ) below 1 2 3 git commit -am \"Release B ${ CORE_VERSION } _F ${ UI_VERSION } \" git flow release finish -m \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" \"B ${ CORE_VERSION } _F ${ UI_VERSION } \" git checkout main && git push && git checkout develop && git push && git push --tags","title":"Documentation"},{"location":"developer/releasing/#main-release","text":"Update docs/about/release-notes.md file. The td sections, in order, are: Add 1. Release version & link to the release in Github 2. Release date 3. Major Changes: This is an ul so each change should be wrapped in an li element Update the \"Current Full Release\" line: 1 The current full release is **B${CORE_VERSION}_F${UI_VERSION}** . Add the following block with updated versions to the top of the ## Core API section: 1 2 3 4 5 6 7 8 9 < tr > < td >< b > 4.10.0 </ b >< br />< a href = \"https://github.com/MauroDataMapper/mdm-core/releases/tag/4.10.0\" > (GitHub Link) </ a ></ td > < td > 2nd September 2021 </ td > < td > < ul > < li ></ li > </ ul > </ td > </ tr > Add the following block with updated versions to the top of the ## UI section: 1 2 3 4 5 6 7 8 9 < tr > < td >< b > 6.6.0 </ b >< br />< a href = \"https://github.com/MauroDataMapper/mdm-ui/releases/tag/6.6.0\" > (GitHub Link) </ a ></ td > < td > 2nd September 2021 </ td > < td > < ul > < li ></ li > </ ul > </ td > </ tr >","title":"Main Release"},{"location":"developer/releasing/#plugins-release","text":"Run ./releases.sh in mdm-plugins. Remove the private repositories from the HTML format and copy into the docs/installing/plugins.md file","title":"Plugins Release"},{"location":"developer/releasing/#zulip-announce","text":"Caution Make sure you've done all the github releases first otherwise the release notes won't contain anything useful Run ./releases.sh in mdm-plugins. Copy the below markdown block into the announce stream of Zulip Update the versions for the applications Update the tag version for the release notes Copy in the plain text format of the \"releases\" output underneath it 1 2 3 4 5 6 7 # New Release | Application | Version | Release Notes | |----|-----|-----| | Docker | `B4.9.0_F6.5.0` | https://github.com/MauroDataMapper/mdm-docker/releases/tag/B4.9.0_F6.5.0 | | RESTful API | `4.9.0` | https://github.com/MauroDataMapper/mdm-application-build/releases/tag/4.9.0 | | UI | `6.5.0` | https://github.com/MauroDataMapper/mdm-ui/releases/tag/6.5.0 |","title":"Zulip Announce"},{"location":"glossary/glossary/","text":"A \u00b6 Aliases B \u00b6 Branch C \u00b6 Classification D \u00b6 Data Asset Data Class Data Element Dataflow Data Model Data Standard Data Type E \u00b6 Enumeration Data Type F \u00b6 Finalise Folder Fork L \u00b6 Label M \u00b6 Merging Multi-facet aware Multiplicity P \u00b6 Primitive Data Type Profile R \u00b6 Reference Data Type S \u00b6 Semantic links T \u00b6 Terminology Data Type V \u00b6 Version Versioned Folder","title":"Glossary"},{"location":"glossary/glossary/#a","text":"Aliases","title":"A"},{"location":"glossary/glossary/#b","text":"Branch","title":"B"},{"location":"glossary/glossary/#c","text":"Classification","title":"C"},{"location":"glossary/glossary/#d","text":"Data Asset Data Class Data Element Dataflow Data Model Data Standard Data Type","title":"D"},{"location":"glossary/glossary/#e","text":"Enumeration Data Type","title":"E"},{"location":"glossary/glossary/#f","text":"Finalise Folder Fork","title":"F"},{"location":"glossary/glossary/#l","text":"Label","title":"L"},{"location":"glossary/glossary/#m","text":"Merging Multi-facet aware Multiplicity","title":"M"},{"location":"glossary/glossary/#p","text":"Primitive Data Type Profile","title":"P"},{"location":"glossary/glossary/#r","text":"Reference Data Type","title":"R"},{"location":"glossary/glossary/#s","text":"Semantic links","title":"S"},{"location":"glossary/glossary/#t","text":"Terminology Data Type","title":"T"},{"location":"glossary/glossary/#v","text":"Version Versioned Folder","title":"V"},{"location":"glossary/aliases/aliases/","text":"What is an Alias? \u00b6 An Alias is an alternative name for a catalogue item which helps to locate it when searched for. Data Models , Data Classes and Data Elements must have one primary Label , but can have many Aliases . How are Aliases used? \u00b6 Aliases appear in the first row of the details panel when an item is selected in the Model Tree . An example of some suitable Aliases for the Data Model labelled \u2018Head and Neck Cancer Audit (HANA)\u2019 could be \u2018Neck Cancer\u2019 , \u2018Head Cancer\u2019 , \u2018Head and Neck Cancer\u2019 and \u2018HANA\u2019 . Therefore, if one of these items is searched for, the \u2018Head and Neck Cancer Audit (HANA)\u2019 Data Model will appear in the search results. This helps users access the catalogue item they need, without having to know the exact Label . How do you edit an Alias? \u00b6 To add or remove Aliases click the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. This will allow you to edit the 'Aliases' row at the top of the details panel. Type the name of the new Alias and click the green '+' sign on the right to add it. To delete an Alias , click the red 'x' sign to the right of the Alias you wish to remove. Once you have finished editing, click 'Save changes' at the bottom right of the details panel. A green box should appear at the bottom right of the screen, confirming that the catalogue item has been successfully updated.","title":"Aliases"},{"location":"glossary/aliases/aliases/#what-is-an-alias","text":"An Alias is an alternative name for a catalogue item which helps to locate it when searched for. Data Models , Data Classes and Data Elements must have one primary Label , but can have many Aliases .","title":"What is an Alias?"},{"location":"glossary/aliases/aliases/#how-are-aliases-used","text":"Aliases appear in the first row of the details panel when an item is selected in the Model Tree . An example of some suitable Aliases for the Data Model labelled \u2018Head and Neck Cancer Audit (HANA)\u2019 could be \u2018Neck Cancer\u2019 , \u2018Head Cancer\u2019 , \u2018Head and Neck Cancer\u2019 and \u2018HANA\u2019 . Therefore, if one of these items is searched for, the \u2018Head and Neck Cancer Audit (HANA)\u2019 Data Model will appear in the search results. This helps users access the catalogue item they need, without having to know the exact Label .","title":"How are Aliases used?"},{"location":"glossary/aliases/aliases/#how-do-you-edit-an-alias","text":"To add or remove Aliases click the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. This will allow you to edit the 'Aliases' row at the top of the details panel. Type the name of the new Alias and click the green '+' sign on the right to add it. To delete an Alias , click the red 'x' sign to the right of the Alias you wish to remove. Once you have finished editing, click 'Save changes' at the bottom right of the details panel. A green box should appear at the bottom right of the screen, confirming that the catalogue item has been successfully updated.","title":"How do you edit an Alias?"},{"location":"glossary/branch/branch/","text":"What is a Branch? \u00b6 A Branch is a method of diverging from the main line of a model to continue further changes without interferring with the rest of the model. The name branch can be likened to that of a tree structure; a tree has a main trunk , while branches extend from that trunk . The simplest way to understand Branches in Mauro is to view the Merge Graph of a model: In this example, notice that Model Version Tree DataModel (1.0.0) has been branched into three: main - this is the name given to all default branches. Every new version of a model automatically has a main branch created, acting as the main line , or trunk , of the model's changes. Two separate branches have also been created. These are branched from the original model but may now include different changes compared to main (or each other). Why are Branches useful? \u00b6 Branches allow for more complex model editing and versioning scenarios. If a single editor is working on updates to a data model, then simple versioning may be sufficient to increment each update/version of that data model. If, however, there are multiple editors working on the same model at the same time, this could cause conflicts ; for example, one editor could accidentally lose changes made by another editor. The solution for this scenario is to create one (or more) Branches of a model, allowing multiple editors or teams to make their own changes to the model in question without directly interfering or conflicting with other editors. Once an editor is happy that their Branch is complete, they may then merge it with another branch to ensure that everyones changes are pulled together into one place. An alternative to branching is called Forking , whereby a model is cloned to make new changes. How to create a Branch \u00b6 To create one or more Branches of a model, please refer to the user guide Branching, versioning and forking Data Models .","title":"Branch"},{"location":"glossary/branch/branch/#what-is-a-branch","text":"A Branch is a method of diverging from the main line of a model to continue further changes without interferring with the rest of the model. The name branch can be likened to that of a tree structure; a tree has a main trunk , while branches extend from that trunk . The simplest way to understand Branches in Mauro is to view the Merge Graph of a model: In this example, notice that Model Version Tree DataModel (1.0.0) has been branched into three: main - this is the name given to all default branches. Every new version of a model automatically has a main branch created, acting as the main line , or trunk , of the model's changes. Two separate branches have also been created. These are branched from the original model but may now include different changes compared to main (or each other).","title":"What is a Branch?"},{"location":"glossary/branch/branch/#why-are-branches-useful","text":"Branches allow for more complex model editing and versioning scenarios. If a single editor is working on updates to a data model, then simple versioning may be sufficient to increment each update/version of that data model. If, however, there are multiple editors working on the same model at the same time, this could cause conflicts ; for example, one editor could accidentally lose changes made by another editor. The solution for this scenario is to create one (or more) Branches of a model, allowing multiple editors or teams to make their own changes to the model in question without directly interfering or conflicting with other editors. Once an editor is happy that their Branch is complete, they may then merge it with another branch to ensure that everyones changes are pulled together into one place. An alternative to branching is called Forking , whereby a model is cloned to make new changes.","title":"Why are Branches useful?"},{"location":"glossary/branch/branch/#how-to-create-a-branch","text":"To create one or more Branches of a model, please refer to the user guide Branching, versioning and forking Data Models .","title":"How to create a Branch"},{"location":"glossary/classification/classification/","text":"What is a Classification? \u00b6 A Classification is a container which acts like a tag on a catalogue item, allowing you to categorise and group together related catalogue items, such as models. A Classification is a list-based method of collecting models when compared to Folders , allowing a catalogue item to be linked to more than one Classification if necessary. Why use Classifications? \u00b6 Classifications are a useful method of organising your catalogue items in a \"many-to-many\" relationship - a catalogue item may related to many Classifications , and a Classification may relate to many catalogue items.","title":"Classification"},{"location":"glossary/classification/classification/#what-is-a-classification","text":"A Classification is a container which acts like a tag on a catalogue item, allowing you to categorise and group together related catalogue items, such as models. A Classification is a list-based method of collecting models when compared to Folders , allowing a catalogue item to be linked to more than one Classification if necessary.","title":"What is a Classification?"},{"location":"glossary/classification/classification/#why-use-classifications","text":"Classifications are a useful method of organising your catalogue items in a \"many-to-many\" relationship - a catalogue item may related to many Classifications , and a Classification may relate to many catalogue items.","title":"Why use Classifications?"},{"location":"glossary/data-asset/data-asset/","text":"What is a Data Asset? \u00b6 There are two types of Data Models within Mauro Data Mapper : Data Asset Data Standard A Data Asset contains existing data. This can be in the form of a database, dataset or a number of completed forms. How are Data Assets used? \u00b6 A Data Model which is a Data Asset is represented by a database icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree . Data Assets may also include summary metadata within its properties and its data can also be populated from other Data Assets via a Dataflow . Selecting a Data Model type \u00b6 You will need to assign a Data Model type whenever you are adding or importing a Data Model . When adding a Data Model , select the type from the dropdown menu on the 'Data Model Details form' . For further information on this, visit our 'Create a Data Model user guide' . When importing a Data Model using Excel, you will need to specify the type in the relevant column of the Data Model listing sheet . For further information on this, please see our 'Import a Data Model from Excel user guide' .","title":"Data Asset"},{"location":"glossary/data-asset/data-asset/#what-is-a-data-asset","text":"There are two types of Data Models within Mauro Data Mapper : Data Asset Data Standard A Data Asset contains existing data. This can be in the form of a database, dataset or a number of completed forms.","title":"What is a Data Asset?"},{"location":"glossary/data-asset/data-asset/#how-are-data-assets-used","text":"A Data Model which is a Data Asset is represented by a database icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree . Data Assets may also include summary metadata within its properties and its data can also be populated from other Data Assets via a Dataflow .","title":"How are Data Assets used?"},{"location":"glossary/data-asset/data-asset/#selecting-a-data-model-type","text":"You will need to assign a Data Model type whenever you are adding or importing a Data Model . When adding a Data Model , select the type from the dropdown menu on the 'Data Model Details form' . For further information on this, visit our 'Create a Data Model user guide' . When importing a Data Model using Excel, you will need to specify the type in the relevant column of the Data Model listing sheet . For further information on this, please see our 'Import a Data Model from Excel user guide' .","title":"Selecting a Data Model type"},{"location":"glossary/data-class/data-class/","text":"What is a Data Class? \u00b6 A Data Class is a collection of data, also known as Data Elements , that are related to each other in some way. For example, each Data Element could appear in the same table of a database, or the same section within a form. How are Data Classes used? \u00b6 Data Classes are the building blocks of a Data Model . Within each Data Class lies several Data Elements and these are the descriptions of an individual field, variable, column or property. You can also have a Data Class within a Data Class , known as a Nested Data Class , which can be a useful way of managing complex sets of data. There is no limit on the number of Nested Data Classes you can include. For example, in a webform, there may be a section called 'Contact details' , which would be one Data Class . Within that section however, there may be another Data Class labelled 'Correspondence Address' . This would be a Nested Data Class . Each Data Class has a: Label This is the name of the Data Class which has to be unique within the Data Model or parent Data Class . Aliases Alternative names that can help locate the Data Class when searched for. Description A definition either written in HTML, Markdown, or plain text which explains the types of data items that are grouped together within the Data Class , as well as any contextual details. Parent Hierarchy The parent of a Data Class can either be the Data Model itself, in which case it is described as a \u2018top level data class\u2019 . Or, if it is a Nested Data Class , its parent Data Class . Multiplicity This specifies the minimum and maximum number of times the Data Class appears within its parent. Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally). Classifications These are effectively tags that you can apply to the Data Class . The above are all shown within the details panel, when the Data Class is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Class is selected in the Model Tree . Content This refers to the various Data Elements and Nested Data Classes within the selected Data Class . Properties Arbitrary additional metadata about this Data Class . Comments Any relevant comments or notes. Links Semantic links between relevant Data Classes . Summary Further metadata information on the nature of the Data Elements within the Data Class . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Attachments Files can be added to provide additional information and context.","title":"Data Class"},{"location":"glossary/data-class/data-class/#what-is-a-data-class","text":"A Data Class is a collection of data, also known as Data Elements , that are related to each other in some way. For example, each Data Element could appear in the same table of a database, or the same section within a form.","title":"What is a Data Class?"},{"location":"glossary/data-class/data-class/#how-are-data-classes-used","text":"Data Classes are the building blocks of a Data Model . Within each Data Class lies several Data Elements and these are the descriptions of an individual field, variable, column or property. You can also have a Data Class within a Data Class , known as a Nested Data Class , which can be a useful way of managing complex sets of data. There is no limit on the number of Nested Data Classes you can include. For example, in a webform, there may be a section called 'Contact details' , which would be one Data Class . Within that section however, there may be another Data Class labelled 'Correspondence Address' . This would be a Nested Data Class . Each Data Class has a: Label This is the name of the Data Class which has to be unique within the Data Model or parent Data Class . Aliases Alternative names that can help locate the Data Class when searched for. Description A definition either written in HTML, Markdown, or plain text which explains the types of data items that are grouped together within the Data Class , as well as any contextual details. Parent Hierarchy The parent of a Data Class can either be the Data Model itself, in which case it is described as a \u2018top level data class\u2019 . Or, if it is a Nested Data Class , its parent Data Class . Multiplicity This specifies the minimum and maximum number of times the Data Class appears within its parent. Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally). Classifications These are effectively tags that you can apply to the Data Class . The above are all shown within the details panel, when the Data Class is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Class is selected in the Model Tree . Content This refers to the various Data Elements and Nested Data Classes within the selected Data Class . Properties Arbitrary additional metadata about this Data Class . Comments Any relevant comments or notes. Links Semantic links between relevant Data Classes . Summary Further metadata information on the nature of the Data Elements within the Data Class . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Attachments Files can be added to provide additional information and context.","title":"How are Data Classes used?"},{"location":"glossary/data-element/data-element/","text":"What is a Data Element? \u00b6 A Data Element is a description of an individual field, variable, column or property of a data item. Each Data Element has a name and a Data Type . How are Data Elements used? \u00b6 Data Elements that are related to each other in some way are grouped together in Data Classes . These Data Classes are the building blocks of Data Models . For example, a Data Element could be an individual field such as \u2018Postcode\u2019 within a webform. Each Data Element has a: Label This is the name of the Data Element which has to be unique within its parent Data Class . Aliases Alternative names that can help locate the Data Element when searched for. Description A definition either written in HTML, Markdown, or plain text which explains any contextual details relating to the Data Element . Data Type The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. Parent Hierarchy Details the parent Data Class and Data Model of the Data Element . Multiplicity This specifies the minimum and maximum number of times the Data Element appears within its parent. Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally). Classifications These are effectively tags that you can apply to the Data Element . The above are all shown within the details panel, when the Data Element is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Element is selected in the Model Tree . Properties Additional metadata about this Data Element . This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility. Comments Any relevant comments or notes. Links Semantic links between relevant Data Elements . Summary Further metadata information on the nature of the Data Elements . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Attachments Files can be added to provide additional information and context.","title":"Data Element"},{"location":"glossary/data-element/data-element/#what-is-a-data-element","text":"A Data Element is a description of an individual field, variable, column or property of a data item. Each Data Element has a name and a Data Type .","title":"What is a Data Element?"},{"location":"glossary/data-element/data-element/#how-are-data-elements-used","text":"Data Elements that are related to each other in some way are grouped together in Data Classes . These Data Classes are the building blocks of Data Models . For example, a Data Element could be an individual field such as \u2018Postcode\u2019 within a webform. Each Data Element has a: Label This is the name of the Data Element which has to be unique within its parent Data Class . Aliases Alternative names that can help locate the Data Element when searched for. Description A definition either written in HTML, Markdown, or plain text which explains any contextual details relating to the Data Element . Data Type The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. Parent Hierarchy Details the parent Data Class and Data Model of the Data Element . Multiplicity This specifies the minimum and maximum number of times the Data Element appears within its parent. Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of '*' (which is represented by '-1' internally). Classifications These are effectively tags that you can apply to the Data Element . The above are all shown within the details panel, when the Data Element is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Element is selected in the Model Tree . Properties Additional metadata about this Data Element . This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility. Comments Any relevant comments or notes. Links Semantic links between relevant Data Elements . Summary Further metadata information on the nature of the Data Elements . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Attachments Files can be added to provide additional information and context.","title":"How are Data Elements used?"},{"location":"glossary/data-model/data-model/","text":"What is a Data Model? \u00b6 A Data Model is a description of an existing collection of metadata, or a specification of data that is to be collected. Data Models which contain existing data are known as a Data Asset , while models which contain templates for data collection are known as a Data Standard . Both are called models as they are effectively representations of the data that they describe. How are Data Models used? \u00b6 Data Models make the connection between the names of columns, fields or variables and our understanding of how the corresponding data is acquired, managed and interpreted. Mauro Data Mapper acts as a directory for these Data Models and allows us to create, search and share these data descriptions. Within each Data Model lies several Data Classes which are groups of data that are related in some way. Data Classes contain Data Elements which are the descriptions of an individual field or variable. For example, a webform where patients enter their details would be a Data Model . The 'Personal details' and 'Contact details' sections within the webform would each be a Data Class . While the individual entries such as 'First Name' , 'Last Name' , 'Date of Birth' etc, would each be a Data Element . The 'Correspondence Address' section is within the 'Contact details' section and would therefore be a Nested Data Class . Each Data Model has a: Label This is the unique name of the Data Model . Aliases Alternative names that can help locate the Data Model when searched for. Organisation Details of who is responsible for creating the Data Model . Description A definition either written in html or plain text which explains the types of data items that are grouped together within the Data Model , as well as any contextual details. Type This defines whether the Data Model is a Data Asset , which contains existing data, or a Data Standard , which contains templates for data collection. Classifications These are effectively tags that you can apply to the Data Class . The above are all shown within the details panel, when the Data Model is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Model is selected in the Model Tree . Data Classes This is a list of all the Data Classes within the Data Model . Types The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. Properties Additional metadata about this Data Model . This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility. Summary Further metadata information on the nature of the Data Classes within the Data Model . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Comments Any relevant comments or notes. History A detailed record of user, date, time and description of all the changes made to the Data Model . Diagram A UML diagram which is a graphical way of summarising the Data Classes and Data Elements within the Data Model . Links Semantic links between relevant Data Models . Attachments Files can be added to provide additional information and context. Dataflow A diagram illustrating where the data within the Data Model has come from and how it has moved across different databases and organisations. This gives users valuable information on the history of each data point and how it has been manipulated.","title":"Data Model"},{"location":"glossary/data-model/data-model/#what-is-a-data-model","text":"A Data Model is a description of an existing collection of metadata, or a specification of data that is to be collected. Data Models which contain existing data are known as a Data Asset , while models which contain templates for data collection are known as a Data Standard . Both are called models as they are effectively representations of the data that they describe.","title":"What is a Data Model?"},{"location":"glossary/data-model/data-model/#how-are-data-models-used","text":"Data Models make the connection between the names of columns, fields or variables and our understanding of how the corresponding data is acquired, managed and interpreted. Mauro Data Mapper acts as a directory for these Data Models and allows us to create, search and share these data descriptions. Within each Data Model lies several Data Classes which are groups of data that are related in some way. Data Classes contain Data Elements which are the descriptions of an individual field or variable. For example, a webform where patients enter their details would be a Data Model . The 'Personal details' and 'Contact details' sections within the webform would each be a Data Class . While the individual entries such as 'First Name' , 'Last Name' , 'Date of Birth' etc, would each be a Data Element . The 'Correspondence Address' section is within the 'Contact details' section and would therefore be a Nested Data Class . Each Data Model has a: Label This is the unique name of the Data Model . Aliases Alternative names that can help locate the Data Model when searched for. Organisation Details of who is responsible for creating the Data Model . Description A definition either written in html or plain text which explains the types of data items that are grouped together within the Data Model , as well as any contextual details. Type This defines whether the Data Model is a Data Asset , which contains existing data, or a Data Standard , which contains templates for data collection. Classifications These are effectively tags that you can apply to the Data Class . The above are all shown within the details panel, when the Data Model is selected in the Model Tree . Other characteristics are displayed in the tabs underneath the details panel, when the Data Model is selected in the Model Tree . Data Classes This is a list of all the Data Classes within the Data Model . Types The Data Type describes the range of possible values that the Data Element may take. The Data Types stored within Mauro Data Mapper are: Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. Properties Additional metadata about this Data Model . This can include technical information such as where the data is located, as well as information for users such as the type of data, coverage, geography and accessibility. Summary Further metadata information on the nature of the Data Classes within the Data Model . This can include aggregate data such as the number of entries or distribution information as well as textual information detailing aspects like the geographic representation of the data set or the duration of collection. Comments Any relevant comments or notes. History A detailed record of user, date, time and description of all the changes made to the Data Model . Diagram A UML diagram which is a graphical way of summarising the Data Classes and Data Elements within the Data Model . Links Semantic links between relevant Data Models . Attachments Files can be added to provide additional information and context. Dataflow A diagram illustrating where the data within the Data Model has come from and how it has moved across different databases and organisations. This gives users valuable information on the history of each data point and how it has been manipulated.","title":"How are Data Models used?"},{"location":"glossary/data-standard/data-standard/","text":"What is a Data Standard? \u00b6 There are two types of Data Models within Mauro Data Mapper : Data Standard Data Asset A Data Standard is essentially a template for collecting new data. This can be a form, schema or a specification for distributed data collection. How are Data Standards used? \u00b6 A Data Model which is a Data Standard is represented by a document icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree . As a Data Standard does not contain any collected data, there will be no summary metadata properties or Dataflows associated with this type of model. Selecting a Data Model type \u00b6 You will need to assign a Data Model type whenever you are adding or importing a Data Model . When adding a Data Model , select the type from the dropdown menu on the 'Data Model Details form' . For further information on how to do this, visit our 'Create a Data Model user guide' . When importing a Data Model using Excel, you will need to specify the type in the relevant column on the Data Model listing sheet . For further information on this, please see our 'Import a Data Model from Excel user guide' .","title":"Data Standard"},{"location":"glossary/data-standard/data-standard/#what-is-a-data-standard","text":"There are two types of Data Models within Mauro Data Mapper : Data Standard Data Asset A Data Standard is essentially a template for collecting new data. This can be a form, schema or a specification for distributed data collection.","title":"What is a Data Standard?"},{"location":"glossary/data-standard/data-standard/#how-are-data-standards-used","text":"A Data Model which is a Data Standard is represented by a document icon, as shown below. This helps to quickly identify the type of Data Model in the Model Tree . As a Data Standard does not contain any collected data, there will be no summary metadata properties or Dataflows associated with this type of model.","title":"How are Data Standards used?"},{"location":"glossary/data-standard/data-standard/#selecting-a-data-model-type","text":"You will need to assign a Data Model type whenever you are adding or importing a Data Model . When adding a Data Model , select the type from the dropdown menu on the 'Data Model Details form' . For further information on how to do this, visit our 'Create a Data Model user guide' . When importing a Data Model using Excel, you will need to specify the type in the relevant column on the Data Model listing sheet . For further information on this, please see our 'Import a Data Model from Excel user guide' .","title":"Selecting a Data Model type"},{"location":"glossary/data-type/data-type/","text":"What is a Data Type? \u00b6 A Data Type describes the range of possible values that each Data Element may take. How are Data Types used? \u00b6 There are four different Data Types stored within Data Models of Mauro Data Mapper : Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. When adding a new Data Type to a Data Element , you will need to select the relevant Data Type from the dropdown menu on the 'Data Element Details' form. For more information on how to do this, go to step '5.1 Add Data Elements' on our Document a Health Dataset user guide' Each Data Type has a: Label This is the unique name of the Data Type . Aliases Alternative names that can help locate the Data Type when searched for. Description A definition written in either html or plain text which explains any contextual details relating to the Data Type . DataModel The Data Model that the Data Type belongs to. Type The Data Type ( Enumeration , Primitive , Reference or Terminology ). Classifications These are effectively tags that you can apply to the Data Type . Other characteristics are displayed in the tabs underneath the details panel, when the Data Type is selected in the Model Tree . Properties Arbitrary additional metadata about this Data Type . Data Elements The Data Elements that use the selected Data Type . Comments Any relevant comments or notes. Links Semantic links between relevant Data Types . Attachments Files can be added to provide additional information and context.","title":"Data Type"},{"location":"glossary/data-type/data-type/#what-is-a-data-type","text":"A Data Type describes the range of possible values that each Data Element may take.","title":"What is a Data Type?"},{"location":"glossary/data-type/data-type/#how-are-data-types-used","text":"There are four different Data Types stored within Data Models of Mauro Data Mapper : Enumeration Data Type This is a constrained set of possible Enumeration values , which are typically used to describe lists of data. For example, an ethnicity Enumeration Data Type would include a list of different ethnic categories, each defined by a coded key and a human readable description. Primitive Data Type Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference Data Type Data which refers to another Data Class within the same Data Model . Terminology Data Type A structured collection of Enumerated Values which have relationships between different data terms. When adding a new Data Type to a Data Element , you will need to select the relevant Data Type from the dropdown menu on the 'Data Element Details' form. For more information on how to do this, go to step '5.1 Add Data Elements' on our Document a Health Dataset user guide' Each Data Type has a: Label This is the unique name of the Data Type . Aliases Alternative names that can help locate the Data Type when searched for. Description A definition written in either html or plain text which explains any contextual details relating to the Data Type . DataModel The Data Model that the Data Type belongs to. Type The Data Type ( Enumeration , Primitive , Reference or Terminology ). Classifications These are effectively tags that you can apply to the Data Type . Other characteristics are displayed in the tabs underneath the details panel, when the Data Type is selected in the Model Tree . Properties Arbitrary additional metadata about this Data Type . Data Elements The Data Elements that use the selected Data Type . Comments Any relevant comments or notes. Links Semantic links between relevant Data Types . Attachments Files can be added to provide additional information and context.","title":"How are Data Types used?"},{"location":"glossary/dataflow/dataflow/","text":"What is a Dataflow? \u00b6 A Dataflow is a way of describing the meaning of data. It is a visual representation of how data flows from one Data Model , typically a Data Asset , to another. This helps users to understand where data has been extracted from, how it has been extracted and the series of transformations it has gone through. Each Dataflow has a source Data Model , which is where the data originated from, and a target Data Model , which is where the final data will be located. The transformations in between define how the data from one or more Data Elements within the source Data Model flow into one or more Data Elements within the target Data Model . How are Dataflows used? \u00b6 The Dataflows relating to each Data Asset can be found in the Dataflows tab at the bottom of the details panel, when the relevant Data Model is selected in the Model Tree . The Dataflows tab displays a diagram which consists of a series of annotated grey blocks connected by annotated arrows. This tab allows you to view the Dataflows at a Data Model level, a Data Class level and a Data Element level. Therefore, the grey blocks can represent Data Assets , Data Classes or Data Elements , depending on the selected view. The arrows represent the relevant transformations that have occurred and are annotated with human readable descriptions. Data Model view \u00b6 When you first click the Dataflows tab, the Data Model view will initially be displayed. This gives a high level overview of how data flows from one Data Asset to another. The grey blocks represent Data Assets which can be databases, lab systems or modules of data. Data Class view \u00b6 To find out information on the different components of a particular Dataflow and how tables of data flow from one place to another, you can access the Data Class view. To do this, hover over one of the transformation arrows until the hand icon appears and double click. The Data Class view will then be displayed with each Data Class represented by a grey rectangle. Data Element view \u00b6 To find out more details on the specific fields of a data product and how they have been derived, you can access the Data Element view. Again, hover over a transformation arrow until the hand icon appears and double click. Here, you will find the various fields grouped together and the relationships between them. This is particularly useful for end users as it explains how data has been manipulated. Zoom \u00b6 In all three views, you can zoom in and out by scrolling on your mouse, or by clicking the \u2018+ Zoom in' and \u2018- Zoom out' buttons at the top left of the Dataflows view and in the header bar. Reset \u00b6 To reset the view, select \u2018RESET\u2019 at the top left of the Dataflows view or click the circular arrow \u2018Reset zoom and canvas\u2019 button in the header bar. Fullscreen \u00b6 You can also view the Dataflow tab in fullscreen mode by clicking the 'Popup in full screen' icon on the top right of the header bar. Download image \u00b6 To download an image of the Dataflow select the 'Download as image' icon at the top right of the header bar. This will download a png image of the current view displayed in the Dataflow tab to your downloads folder. Navigating through Dataflows \u00b6 To navigate between the three different views, hover and double click over the transformation arrows in each view to access the next level. For example, to move from the Data Model view to the Data Class view. To navigate up a level of hierarchy, select the upwards arrow \u2018Move up a level of hierarchy\u2019 icon in the top left of the header bar. For example, to move from the Data Class view to the Data Model view. In all three views you can move the blocks and transformation arrows by clicking and holding the left mouse button which will allow you to drag them to the desired position. This can be helpful to visualise the Dataflows in a clearer manner.","title":"Dataflow"},{"location":"glossary/dataflow/dataflow/#what-is-a-dataflow","text":"A Dataflow is a way of describing the meaning of data. It is a visual representation of how data flows from one Data Model , typically a Data Asset , to another. This helps users to understand where data has been extracted from, how it has been extracted and the series of transformations it has gone through. Each Dataflow has a source Data Model , which is where the data originated from, and a target Data Model , which is where the final data will be located. The transformations in between define how the data from one or more Data Elements within the source Data Model flow into one or more Data Elements within the target Data Model .","title":"What is a Dataflow?"},{"location":"glossary/dataflow/dataflow/#how-are-dataflows-used","text":"The Dataflows relating to each Data Asset can be found in the Dataflows tab at the bottom of the details panel, when the relevant Data Model is selected in the Model Tree . The Dataflows tab displays a diagram which consists of a series of annotated grey blocks connected by annotated arrows. This tab allows you to view the Dataflows at a Data Model level, a Data Class level and a Data Element level. Therefore, the grey blocks can represent Data Assets , Data Classes or Data Elements , depending on the selected view. The arrows represent the relevant transformations that have occurred and are annotated with human readable descriptions.","title":"How are Dataflows used?"},{"location":"glossary/dataflow/dataflow/#data-model-view","text":"When you first click the Dataflows tab, the Data Model view will initially be displayed. This gives a high level overview of how data flows from one Data Asset to another. The grey blocks represent Data Assets which can be databases, lab systems or modules of data.","title":"Data Model view"},{"location":"glossary/dataflow/dataflow/#data-class-view","text":"To find out information on the different components of a particular Dataflow and how tables of data flow from one place to another, you can access the Data Class view. To do this, hover over one of the transformation arrows until the hand icon appears and double click. The Data Class view will then be displayed with each Data Class represented by a grey rectangle.","title":"Data Class view"},{"location":"glossary/dataflow/dataflow/#data-element-view","text":"To find out more details on the specific fields of a data product and how they have been derived, you can access the Data Element view. Again, hover over a transformation arrow until the hand icon appears and double click. Here, you will find the various fields grouped together and the relationships between them. This is particularly useful for end users as it explains how data has been manipulated.","title":"Data Element view"},{"location":"glossary/dataflow/dataflow/#zoom","text":"In all three views, you can zoom in and out by scrolling on your mouse, or by clicking the \u2018+ Zoom in' and \u2018- Zoom out' buttons at the top left of the Dataflows view and in the header bar.","title":"Zoom"},{"location":"glossary/dataflow/dataflow/#reset","text":"To reset the view, select \u2018RESET\u2019 at the top left of the Dataflows view or click the circular arrow \u2018Reset zoom and canvas\u2019 button in the header bar.","title":"Reset"},{"location":"glossary/dataflow/dataflow/#fullscreen","text":"You can also view the Dataflow tab in fullscreen mode by clicking the 'Popup in full screen' icon on the top right of the header bar.","title":"Fullscreen"},{"location":"glossary/dataflow/dataflow/#download-image","text":"To download an image of the Dataflow select the 'Download as image' icon at the top right of the header bar. This will download a png image of the current view displayed in the Dataflow tab to your downloads folder.","title":"Download image"},{"location":"glossary/dataflow/dataflow/#navigating-through-dataflows","text":"To navigate between the three different views, hover and double click over the transformation arrows in each view to access the next level. For example, to move from the Data Model view to the Data Class view. To navigate up a level of hierarchy, select the upwards arrow \u2018Move up a level of hierarchy\u2019 icon in the top left of the header bar. For example, to move from the Data Class view to the Data Model view. In all three views you can move the blocks and transformation arrows by clicking and holding the left mouse button which will allow you to drag them to the desired position. This can be helpful to visualise the Dataflows in a clearer manner.","title":"Navigating through Dataflows"},{"location":"glossary/enumeration-data-type/enumeration-data-type/","text":"What is an Enumeration Data Type? \u00b6 An Enumeration Data Type is one of the four possible Data Types within Mauro Data Mapper . Each Enumeration Data Type describes a constrained set of possible Enumeration values. Enumerations are typically used for describing simple lists of data. How are Enumeration Data Types used? \u00b6 Each Enumeration value has an associated label, or a coded key, and a textual description, or human-readable value. These key-value pairs can be used to describe a list of data. For example, one Enumeration Data Type could be 'Ethnic category' , where each Enumeration value describes a specific ethnicity. Further details of the particular Enumeration Data Type can be found in its details panel. The individual Enumeration values, along with their corresponding 'Key' and 'Value' are displayed in an \u2018Enumerations\u2019 table below the details panel. These can be edited by clicking the pencil icon to the right of each row. Enumerations can also be removed by clicking the red bin icon to the right of each row. To add an Enumeration, click \u2018+Add Enumeration\u2019 at the top right of the 'Enumerations' table. This will add a row to the bottom of the table, where you can then populate the \u2018Group\u2019 , \u2018Key\u2019 and \u2018Value\u2019 fields. Once completed, click the green tick to the right of the row, and the new Enumeration will be added. You can also view all the Data Elements that use that Enumeration Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , 'Name' and 'Description' of each Data Element .","title":"Enumeration Data Type"},{"location":"glossary/enumeration-data-type/enumeration-data-type/#what-is-an-enumeration-data-type","text":"An Enumeration Data Type is one of the four possible Data Types within Mauro Data Mapper . Each Enumeration Data Type describes a constrained set of possible Enumeration values. Enumerations are typically used for describing simple lists of data.","title":"What is an Enumeration Data Type?"},{"location":"glossary/enumeration-data-type/enumeration-data-type/#how-are-enumeration-data-types-used","text":"Each Enumeration value has an associated label, or a coded key, and a textual description, or human-readable value. These key-value pairs can be used to describe a list of data. For example, one Enumeration Data Type could be 'Ethnic category' , where each Enumeration value describes a specific ethnicity. Further details of the particular Enumeration Data Type can be found in its details panel. The individual Enumeration values, along with their corresponding 'Key' and 'Value' are displayed in an \u2018Enumerations\u2019 table below the details panel. These can be edited by clicking the pencil icon to the right of each row. Enumerations can also be removed by clicking the red bin icon to the right of each row. To add an Enumeration, click \u2018+Add Enumeration\u2019 at the top right of the 'Enumerations' table. This will add a row to the bottom of the table, where you can then populate the \u2018Group\u2019 , \u2018Key\u2019 and \u2018Value\u2019 fields. Once completed, click the green tick to the right of the row, and the new Enumeration will be added. You can also view all the Data Elements that use that Enumeration Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , 'Name' and 'Description' of each Data Element .","title":"How are Enumeration Data Types used?"},{"location":"glossary/finalise/finalise/","text":"What is a Finalised model? \u00b6 When a model is Finalised , it may not be edited any further, locking it into a Version number and making it read only . The act of Finalising makes it clear that a model is ready leave its Draft state and be considered finished, or ready for use. When working with models, they can be in one of two states: Draft state, meaning an editor may make changes to the model. Finalised state, meaning the model is finished and ready for consumption. Only the main branch may be finalised; if multiple branches exists for a model, each with a set of changes, these must be merged together back into the main branch. Why Finalise a model? \u00b6 Finalisation is tied to Versions and version control - once a model is Finalised , it is locked into a particular Version number . The only way to make further changes to a Finalised model is to start a new version of the model, once again entering it into a draft state. By locking a version of a model, this prevents further unintended changes being made and definitively states that this model contains a set contents. How to Finalise models \u00b6 To Finalise a model, please refer to the user guide How to Finalise a Data Model .","title":"Finalise"},{"location":"glossary/finalise/finalise/#what-is-a-finalised-model","text":"When a model is Finalised , it may not be edited any further, locking it into a Version number and making it read only . The act of Finalising makes it clear that a model is ready leave its Draft state and be considered finished, or ready for use. When working with models, they can be in one of two states: Draft state, meaning an editor may make changes to the model. Finalised state, meaning the model is finished and ready for consumption. Only the main branch may be finalised; if multiple branches exists for a model, each with a set of changes, these must be merged together back into the main branch.","title":"What is a Finalised model?"},{"location":"glossary/finalise/finalise/#why-finalise-a-model","text":"Finalisation is tied to Versions and version control - once a model is Finalised , it is locked into a particular Version number . The only way to make further changes to a Finalised model is to start a new version of the model, once again entering it into a draft state. By locking a version of a model, this prevents further unintended changes being made and definitively states that this model contains a set contents.","title":"Why Finalise a model?"},{"location":"glossary/finalise/finalise/#how-to-finalise-models","text":"To Finalise a model, please refer to the user guide How to Finalise a Data Model .","title":"How to Finalise models"},{"location":"glossary/folder/folder/","text":"What is a Folder? \u00b6 A Folder is a container for organising your data models, functioning very similar to a folder stored on a computer. A Folder may contain data models and other sub-folders to manage and organise your catalogue. Catalogue items may only be stored in one folder at a time, because Folders use a hierarchical-structure. If a catalogue item should be linked to more than one container, consider using Classifications instead. Why use Folders? \u00b6 Folders are a useful method of organising your catalogue, especially as it gets larger in scope. Folders are also subject to the same access rights as any other catalogue item, such as making them: Publicly readable, Readable by authenticated users, or Restricted to certain user groups. Folders are simply containers though; for more complex workflow and organisational scenarios, consider Versioned Folders","title":"Folder"},{"location":"glossary/folder/folder/#what-is-a-folder","text":"A Folder is a container for organising your data models, functioning very similar to a folder stored on a computer. A Folder may contain data models and other sub-folders to manage and organise your catalogue. Catalogue items may only be stored in one folder at a time, because Folders use a hierarchical-structure. If a catalogue item should be linked to more than one container, consider using Classifications instead.","title":"What is a Folder?"},{"location":"glossary/folder/folder/#why-use-folders","text":"Folders are a useful method of organising your catalogue, especially as it gets larger in scope. Folders are also subject to the same access rights as any other catalogue item, such as making them: Publicly readable, Readable by authenticated users, or Restricted to certain user groups. Folders are simply containers though; for more complex workflow and organisational scenarios, consider Versioned Folders","title":"Why use Folders?"},{"location":"glossary/fork/fork/","text":"What is a Fork? \u00b6 A Fork is a copy of an existing Version of a model, with the intention of freely making many alterations to that model without affecting the original. The simplest way to understand Forks in Mauro is to view the Merge Graph of a model: In this example, Model Version Tree DataModel (1.0.0) has progressed to version 2.0.0 . However, a separate Fork has also been created, with its own main branch, which will not follow the same versioning history as the original. This allows the Fork to make many changes and not refer back to the original version history. Why are Forks useful? \u00b6 Forks are useful for two reasons: They allow a simple starting point for creating a new model which may start similar but will deviate greatly once finished. Forks allow an existing model to be directed under a new Authority . Differences with Branches \u00b6 At first glance, a Fork and a Branch may seem to serve the same purpose, but the key difference between them are: A Branch is designed to be a temporary deviation from the main line of a model, with the intention of eventaully merging it back into the main line later. A Fork is designed to be a permanent deviation from the main line of a model, acting as the starting point for a new version history. Forks are not intended to be merged back into their original starting point. How to create a Fork \u00b6 To create a Fork of a model, please refer to the user guide Branching, versioning and forking Data Models .","title":"Fork"},{"location":"glossary/fork/fork/#what-is-a-fork","text":"A Fork is a copy of an existing Version of a model, with the intention of freely making many alterations to that model without affecting the original. The simplest way to understand Forks in Mauro is to view the Merge Graph of a model: In this example, Model Version Tree DataModel (1.0.0) has progressed to version 2.0.0 . However, a separate Fork has also been created, with its own main branch, which will not follow the same versioning history as the original. This allows the Fork to make many changes and not refer back to the original version history.","title":"What is a Fork?"},{"location":"glossary/fork/fork/#why-are-forks-useful","text":"Forks are useful for two reasons: They allow a simple starting point for creating a new model which may start similar but will deviate greatly once finished. Forks allow an existing model to be directed under a new Authority .","title":"Why are Forks useful?"},{"location":"glossary/fork/fork/#differences-with-branches","text":"At first glance, a Fork and a Branch may seem to serve the same purpose, but the key difference between them are: A Branch is designed to be a temporary deviation from the main line of a model, with the intention of eventaully merging it back into the main line later. A Fork is designed to be a permanent deviation from the main line of a model, acting as the starting point for a new version history. Forks are not intended to be merged back into their original starting point.","title":"Differences with Branches"},{"location":"glossary/fork/fork/#how-to-create-a-fork","text":"To create a Fork of a model, please refer to the user guide Branching, versioning and forking Data Models .","title":"How to create a Fork"},{"location":"glossary/label/label/","text":"What is a Label? \u00b6 A Label is a name that describes and uniquely identifies each item within Mauro Data Mapper . This Label will appear in the Model Tree on the left hand side of the catalogue and at the top of the page when the item is selected. The Label is also used to identify the item when searched for. How are Labels used? \u00b6 The Label of each item must be unique within its parent group so that no two items share the same Label . Therefore, each Data Model must have a unique Label . Each Data Class must have a unique Label within its parent Data Model . Each Data Element must have a unique Label within its parent Data Class . For example, there can only be one Data Class called \u2018Personal details\u2019 within a particular Data Model . Therefore, if you need to add a similar Data Class , include version information within the Label such as \u2018Personal details 2.0\u2019 to uniquely identify it. In some cases, two different Data Models could consist of a Data Class with the same Label , such as 'Personal details' . However, because these two Data Classes are each associated with their own unique parent Data Model , then this is acceptable. Only when two items are within the same parent must they each have a unique Label . How do you edit a Label? \u00b6 You can edit the Label of any item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the Label at the top of the details panel.","title":"Label"},{"location":"glossary/label/label/#what-is-a-label","text":"A Label is a name that describes and uniquely identifies each item within Mauro Data Mapper . This Label will appear in the Model Tree on the left hand side of the catalogue and at the top of the page when the item is selected. The Label is also used to identify the item when searched for.","title":"What is a Label?"},{"location":"glossary/label/label/#how-are-labels-used","text":"The Label of each item must be unique within its parent group so that no two items share the same Label . Therefore, each Data Model must have a unique Label . Each Data Class must have a unique Label within its parent Data Model . Each Data Element must have a unique Label within its parent Data Class . For example, there can only be one Data Class called \u2018Personal details\u2019 within a particular Data Model . Therefore, if you need to add a similar Data Class , include version information within the Label such as \u2018Personal details 2.0\u2019 to uniquely identify it. In some cases, two different Data Models could consist of a Data Class with the same Label , such as 'Personal details' . However, because these two Data Classes are each associated with their own unique parent Data Model , then this is acceptable. Only when two items are within the same parent must they each have a unique Label .","title":"How are Labels used?"},{"location":"glossary/label/label/#how-do-you-edit-a-label","text":"You can edit the Label of any item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the Label at the top of the details panel.","title":"How do you edit a Label?"},{"location":"glossary/merging/merging/","text":"What is Merging? \u00b6 To Merge a model means to combine the contents of one Branch into another. Merging branches can be simple when there are few edits, but in complex scenarios with multiple edits made, merging may require manual intervention to determine what conflicted changes must be resolved, and how. Merging is usually the final set of actions to perform when creating Branch to work on changes in isolation. It is usual to merge the changes from one branch back into the main branch. How to Merge \u00b6 To Merge branches, please refer to the user guide How to Merge Branches .","title":"Merging"},{"location":"glossary/merging/merging/#what-is-merging","text":"To Merge a model means to combine the contents of one Branch into another. Merging branches can be simple when there are few edits, but in complex scenarios with multiple edits made, merging may require manual intervention to determine what conflicted changes must be resolved, and how. Merging is usually the final set of actions to perform when creating Branch to work on changes in isolation. It is usual to merge the changes from one branch back into the main branch.","title":"What is Merging?"},{"location":"glossary/merging/merging/#how-to-merge","text":"To Merge branches, please refer to the user guide How to Merge Branches .","title":"How to Merge"},{"location":"glossary/multi-facet-aware/multi-facet-aware/","text":"What does \"multi-facet\" aware mean? \u00b6 The \"multi-facet aware\" trait can be applied to any Mauro catalogue item that has at least these traits: Is metadata aware - that is, may contain additional metadata beyond its core properties. Is annotation aware - that is, may contain annotations or comments. Is semantic link aware - that is, may contain semantic links to other catalogue items. Is reference file aware - that is, may contain files or attachments for further reference material. Is rule aware - that is, may contain rule information. What items are \"multi-facet\" aware? \u00b6 The following Mauro catalogue items are multi-facet aware by the definition listed above: Classifiers Code sets Data classes Data class components Data elements Data element components Data flows Data models Data types Enumeration types Enumeration values Folders Model data types Primitive types Reference data elements Reference data models Reference data types Reference enumeration types Reference enumeration values Reference primitive types Reference types Terms Term relationships Term relationship types Terminologies Versioned folders Usage in REST API \u00b6 The Mauro REST API has many operations that are replicated across multiple catalogue item types, such as: Getting information on a catalogue item Saving data to a catalogue item Removing a catalogue item And so on... Many of these endpoints require a {multiFacetAwareDomainType} parameter in their URLs. One generalised example is fetching all metadata for a catalogue item: /api/ {multiFacetAwareDomainType} / {id} /metadata Replace the {multiFacetAwareDomainType} parameter in these types of URL with a name listed above, remembering to: Remove spaces and whitespace in the domain names. Use camelCase for the domain name. Pluralise the domain name. Some examples would be: Folder becomes folders . Data Model becomes dataModels . Data Class becomes dataClasses . Terminology becomes terminologies . And so on...","title":"Multi facet aware"},{"location":"glossary/multi-facet-aware/multi-facet-aware/#what-does-multi-facet-aware-mean","text":"The \"multi-facet aware\" trait can be applied to any Mauro catalogue item that has at least these traits: Is metadata aware - that is, may contain additional metadata beyond its core properties. Is annotation aware - that is, may contain annotations or comments. Is semantic link aware - that is, may contain semantic links to other catalogue items. Is reference file aware - that is, may contain files or attachments for further reference material. Is rule aware - that is, may contain rule information.","title":"What does \"multi-facet\" aware mean?"},{"location":"glossary/multi-facet-aware/multi-facet-aware/#what-items-are-multi-facet-aware","text":"The following Mauro catalogue items are multi-facet aware by the definition listed above: Classifiers Code sets Data classes Data class components Data elements Data element components Data flows Data models Data types Enumeration types Enumeration values Folders Model data types Primitive types Reference data elements Reference data models Reference data types Reference enumeration types Reference enumeration values Reference primitive types Reference types Terms Term relationships Term relationship types Terminologies Versioned folders","title":"What items are \"multi-facet\" aware?"},{"location":"glossary/multi-facet-aware/multi-facet-aware/#usage-in-rest-api","text":"The Mauro REST API has many operations that are replicated across multiple catalogue item types, such as: Getting information on a catalogue item Saving data to a catalogue item Removing a catalogue item And so on... Many of these endpoints require a {multiFacetAwareDomainType} parameter in their URLs. One generalised example is fetching all metadata for a catalogue item: /api/ {multiFacetAwareDomainType} / {id} /metadata Replace the {multiFacetAwareDomainType} parameter in these types of URL with a name listed above, remembering to: Remove spaces and whitespace in the domain names. Use camelCase for the domain name. Pluralise the domain name. Some examples would be: Folder becomes folders . Data Model becomes dataModels . Data Class becomes dataClasses . Terminology becomes terminologies . And so on...","title":"Usage in REST API"},{"location":"glossary/multiplicity/multiplicity/","text":"What is Multiplicity? \u00b6 An item's Multiplicity refers to the minimum and maximum number of times it can appear within its parent in Mauro Data Mapper . For example, the number of instances a particular Data Element appears within a Data Class . How is Multiplicity used? \u00b6 Every Data Class and Data Element is assigned a Multiplicity . Typically, the Multiplicity is written in the form \u2018minmax \u2019, where min and max are integers representing the minimum and maximum number of times an item will appear within its parent. The symbol * represents an unbounded maximum. Using this notation: Mandatory data has a minimum Multiplicity of 1 and a maximum Multiplicity of a specific integer or if there is no upper bound, then '*' (which is represented by '-1' internally). Optional data has a minimum Multiplicity of 0 and a maximum Multiplicity of 1 or if there is no upper bound, *. For example, each person in a Data Model only has one date of birth and if you want to record this, then the corresponding Multiplicity will be 1..1. However, each person may have many prescription records, which may or may not be relevant. In this case, the Multiplicity would be 0..*. Furthermore, each person will only have one date of death, which again you may or may not want to record, so the Multiplicity for this would be 0..1. How do you edit an item's Multiplicity? \u00b6 The Multiplicity can be found in the details panel of Data Classes and Data Elements . It is a mandatory field when adding or importing a Data Class or Data Elements , as explained in the 'Document a Health Datatset user guide' and 'Import a Data Model from Excel user guide' . You can edit the Multiplicity of an item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the min and max values of that item's Multiplicity .","title":"Multiplicity"},{"location":"glossary/multiplicity/multiplicity/#what-is-multiplicity","text":"An item's Multiplicity refers to the minimum and maximum number of times it can appear within its parent in Mauro Data Mapper . For example, the number of instances a particular Data Element appears within a Data Class .","title":"What is Multiplicity?"},{"location":"glossary/multiplicity/multiplicity/#how-is-multiplicity-used","text":"Every Data Class and Data Element is assigned a Multiplicity . Typically, the Multiplicity is written in the form \u2018minmax \u2019, where min and max are integers representing the minimum and maximum number of times an item will appear within its parent. The symbol * represents an unbounded maximum. Using this notation: Mandatory data has a minimum Multiplicity of 1 and a maximum Multiplicity of a specific integer or if there is no upper bound, then '*' (which is represented by '-1' internally). Optional data has a minimum Multiplicity of 0 and a maximum Multiplicity of 1 or if there is no upper bound, *. For example, each person in a Data Model only has one date of birth and if you want to record this, then the corresponding Multiplicity will be 1..1. However, each person may have many prescription records, which may or may not be relevant. In this case, the Multiplicity would be 0..*. Furthermore, each person will only have one date of death, which again you may or may not want to record, so the Multiplicity for this would be 0..1.","title":"How is Multiplicity used?"},{"location":"glossary/multiplicity/multiplicity/#how-do-you-edit-an-items-multiplicity","text":"The Multiplicity can be found in the details panel of Data Classes and Data Elements . It is a mandatory field when adding or importing a Data Class or Data Elements , as explained in the 'Document a Health Datatset user guide' and 'Import a Data Model from Excel user guide' . You can edit the Multiplicity of an item by selecting it in the Model Tree and clicking the \u2018Edit\u2019 pencil icon at the bottom right of the details panel. You will then be able to amend the min and max values of that item's Multiplicity .","title":"How do you edit an item's Multiplicity?"},{"location":"glossary/primitive-data-type/primitive-data-type/","text":"What is a Primitive Data Type? \u00b6 A Primitive Data Type is one of the four possible Data Types within Mauro Data Mapper . It describes data which has no further details about structure or referencing. For example, if a Data Element is a name, then it\u2019s Primitive Data Type would be \u2018String\u2019 . Whereas, if a Data Element is a number, then it\u2019s Primitive Data Type would be \u2018Integer\u2019 . How are Primitive Data Types used? \u00b6 Primitive Data Types are typically used for data which are strings or integers such as names, dates or times. Further details of the particular Primitive Data Type can be found in its details panel. You can also view all the Data Elements that use that Primitive Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , Name and Description of each Data Element .","title":"Primitive Data Type"},{"location":"glossary/primitive-data-type/primitive-data-type/#what-is-a-primitive-data-type","text":"A Primitive Data Type is one of the four possible Data Types within Mauro Data Mapper . It describes data which has no further details about structure or referencing. For example, if a Data Element is a name, then it\u2019s Primitive Data Type would be \u2018String\u2019 . Whereas, if a Data Element is a number, then it\u2019s Primitive Data Type would be \u2018Integer\u2019 .","title":"What is a Primitive Data Type?"},{"location":"glossary/primitive-data-type/primitive-data-type/#how-are-primitive-data-types-used","text":"Primitive Data Types are typically used for data which are strings or integers such as names, dates or times. Further details of the particular Primitive Data Type can be found in its details panel. You can also view all the Data Elements that use that Primitive Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , Name and Description of each Data Element .","title":"How are Primitive Data Types used?"},{"location":"glossary/profile/profile/","text":"What is a Profile? \u00b6 A profile is a group of related metadata properties, typically sharing the same namespace. A profile allows these metadata properties to be grouped into sections and for each defined key may add a description, a default value and a validation constraint. For more details, see the Properties and Profiles tutorial . Why use Profiles? \u00b6 Grouping related properties together into a profile is useful for cleanly representing data entry, otherwise manually editing metadata becomes more verbose and laborious. The Mauro user interface is able to present profiles in a clean data entry view and uses profile information to allow for: Type-specific data entry controls - such as text fields, data pickers, checkboxes, and so on. Validation of fields. Profiles can be statically defined via Mauro plugins in code, or dynamically defined using the same Mauro Data Models, Classes and Elements as any other model. The Dynamic Profiles user guide explains how this can be achieved.","title":"Profile"},{"location":"glossary/profile/profile/#what-is-a-profile","text":"A profile is a group of related metadata properties, typically sharing the same namespace. A profile allows these metadata properties to be grouped into sections and for each defined key may add a description, a default value and a validation constraint. For more details, see the Properties and Profiles tutorial .","title":"What is a Profile?"},{"location":"glossary/profile/profile/#why-use-profiles","text":"Grouping related properties together into a profile is useful for cleanly representing data entry, otherwise manually editing metadata becomes more verbose and laborious. The Mauro user interface is able to present profiles in a clean data entry view and uses profile information to allow for: Type-specific data entry controls - such as text fields, data pickers, checkboxes, and so on. Validation of fields. Profiles can be statically defined via Mauro plugins in code, or dynamically defined using the same Mauro Data Models, Classes and Elements as any other model. The Dynamic Profiles user guide explains how this can be achieved.","title":"Why use Profiles?"},{"location":"glossary/reference-data-type/reference-data-type/","text":"What is a Reference Data Type? \u00b6 A Reference Data Type is one of the four possible Data Types within Mauro Data Mapper . It is used to describe relationships between different Data Classes within the same Data Model . Typically, Reference Data Types \u2018refer\u2019 and consequently link to another specified Data Class . How are Reference Data Types used? \u00b6 By using Reference Data Types to link similar Data Classes together, users don\u2019t have to repeatedly add in new Data Classes , if a similar version already exists. For example, consider a Data Model where you have a \u2018Patient\u2019 Data Class and a \u2018GP\u2019 Data Class . Within the \u2018Patient\u2019 Data Class , there may be a Data Element called \u2018registeredGP\u2019 . This Data Element refers to the \u2018GP\u2019 Data Class , and therefore it is a Reference Data Type . In the details panel of a Reference Data Type , a link to the Data Class it refers to is displayed in the \u2018Type\u2019 field. Clicking this link will navigate you to the details panel of that particular Data Class , where you can view all its associated Data Elements in the \u2018Content\u2019 tab below the details panel. You can also view all the Data Elements that use that Reference Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , Name and Description of each Data Element .","title":"Reference Data Type"},{"location":"glossary/reference-data-type/reference-data-type/#what-is-a-reference-data-type","text":"A Reference Data Type is one of the four possible Data Types within Mauro Data Mapper . It is used to describe relationships between different Data Classes within the same Data Model . Typically, Reference Data Types \u2018refer\u2019 and consequently link to another specified Data Class .","title":"What is a Reference Data Type?"},{"location":"glossary/reference-data-type/reference-data-type/#how-are-reference-data-types-used","text":"By using Reference Data Types to link similar Data Classes together, users don\u2019t have to repeatedly add in new Data Classes , if a similar version already exists. For example, consider a Data Model where you have a \u2018Patient\u2019 Data Class and a \u2018GP\u2019 Data Class . Within the \u2018Patient\u2019 Data Class , there may be a Data Element called \u2018registeredGP\u2019 . This Data Element refers to the \u2018GP\u2019 Data Class , and therefore it is a Reference Data Type . In the details panel of a Reference Data Type , a link to the Data Class it refers to is displayed in the \u2018Type\u2019 field. Clicking this link will navigate you to the details panel of that particular Data Class , where you can view all its associated Data Elements in the \u2018Content\u2019 tab below the details panel. You can also view all the Data Elements that use that Reference Data Type by selecting the \u2018Data Elements\u2019 tab underneath the details panel. This will display the Multiplicity , Name and Description of each Data Element .","title":"How are Reference Data Types used?"},{"location":"glossary/semantic-links/semantic-links/","text":"What is a Semantic link? \u00b6 The term semantics refers to the meaning of data. It is essential that whenever data is produced in one system and used in another, it\u2019s meaning remains consistent between the two systems. This is known as semantic interoperability. Therefore, a Semantic link indicates that there is some sort of relationship between two descriptions of data. How are Semantic links used? \u00b6 There are two types of Semantic links used within Mauro Data Mapper . Refines \u00b6 The most common type of semantic relationship is a \u2018Refines\u2019 link. Refine means to improve and in this context it signifies improving the quality and amount of information provided. Consequently, when one description refines another, it means that everything that is true about one description is also true about the other description, whilst often adding more information or context. For example, if a description of a column in a database specification Refines an item in a Data Standard , then everything the Data Standard says about that data item applies to the column. The rest of the database specification may contain more information about that column, such as specific conditions or constraints, but the description in the Data Standard still applies. This type of Semantic link can be created between Data Models , Data Classes and Data Elements . Does not refine \u00b6 Similarly, you can also create a \u2018does not refine\u2019 link which is used to indicate that the present definition is not intended as a refinement of another. If a Data Model , Data Class or Data Element contain any Semantic links , these are summarised in a 'Links' table below the details panel when the relevant data is selected in the Model Tree . This table displays a hyperlink to the target data the Semantic link refers to, as well as the type and total number of links. Links can be edited, created or removed in the 'Links' tab below the details panel. To edit the type of link or the target, click the 'Edit' pencil icon to the right. To add a link click '+ Add Link' and you will be able to specify the type of link and select the target. Click the green tick to confirm and save your changes.","title":"Semantic links"},{"location":"glossary/semantic-links/semantic-links/#what-is-a-semantic-link","text":"The term semantics refers to the meaning of data. It is essential that whenever data is produced in one system and used in another, it\u2019s meaning remains consistent between the two systems. This is known as semantic interoperability. Therefore, a Semantic link indicates that there is some sort of relationship between two descriptions of data.","title":"What is a Semantic link?"},{"location":"glossary/semantic-links/semantic-links/#how-are-semantic-links-used","text":"There are two types of Semantic links used within Mauro Data Mapper .","title":"How are Semantic links used?"},{"location":"glossary/semantic-links/semantic-links/#refines","text":"The most common type of semantic relationship is a \u2018Refines\u2019 link. Refine means to improve and in this context it signifies improving the quality and amount of information provided. Consequently, when one description refines another, it means that everything that is true about one description is also true about the other description, whilst often adding more information or context. For example, if a description of a column in a database specification Refines an item in a Data Standard , then everything the Data Standard says about that data item applies to the column. The rest of the database specification may contain more information about that column, such as specific conditions or constraints, but the description in the Data Standard still applies. This type of Semantic link can be created between Data Models , Data Classes and Data Elements .","title":"Refines"},{"location":"glossary/semantic-links/semantic-links/#does-not-refine","text":"Similarly, you can also create a \u2018does not refine\u2019 link which is used to indicate that the present definition is not intended as a refinement of another. If a Data Model , Data Class or Data Element contain any Semantic links , these are summarised in a 'Links' table below the details panel when the relevant data is selected in the Model Tree . This table displays a hyperlink to the target data the Semantic link refers to, as well as the type and total number of links. Links can be edited, created or removed in the 'Links' tab below the details panel. To edit the type of link or the target, click the 'Edit' pencil icon to the right. To add a link click '+ Add Link' and you will be able to specify the type of link and select the target. Click the green tick to confirm and save your changes.","title":"Does not refine"},{"location":"glossary/terminology-data-type/terminology-data-type/","text":"What is a Terminology Data Type? \u00b6 A Terminology Data Type is one of the four possible Data Types within Mauro Data Mapper . It is used to describe a structured collection of Enumerated Values which have relationships between different data terms. How are Terminology Data Types used? \u00b6 Terminology Data Types have now been expanded in Mauro Data Mapper to \u2018Model Reference\u2019 . Model References can now point to a Terminology , a CodeSet or a ReferenceDataModel . Terminology \u00b6 A Terminology is a vocabulary, or a collection of allowable Terms as well as any relationships between them. A Term typically has a coded key and a human-readable value, along with some other information. Any relationships between pairs of Terms can be defined, including the relationship stating that one Term has a broader or narrower meaning than another Term . For example, a hierarchy of Terms denoting patient diagnoses might include a general Term to indicate some form of Diabetes. This may be related to more specific Terms to indicate a particular form of Diabetes such as Type 1 Diabetes. Terminologies are represented by a book icon in Mauro Data Mapper and you can browse through several different databases by selecting \u2018Healthcare Terminologies\u2019 in the Model Tree . Further details of the particular Terminology can be found in its details panel when selected. To view information relating to a specific Term , select the relevant Term in the Model Tree and its corresponding details panel will be displayed. Each Term has a: Label This is the unique name of the Term . Aliases Alternative names that can help locate the Term when searched for. Terminology The Terminology that this Term is associated with. Description A definition either written in html or plain text which explains any contextual details relating to the Term . URL A URL to the original definition of the Term . Classifications These are effectively tags that you can apply to the Term . Below the details panel, is a list of Relationships between the selected Term and other Terms within the specified Terminology . CodeSet \u00b6 A CodeSet is a selection of terms, which may be taken from one or more Terminologies . For example, a CodeSet would be all the Terms that describe the different variants of Diabetes. ReferenceDataModel \u00b6 ReferenceDataModels are similar to a large database containing lots of detailed information and properties that would be too difficult to accurately manage in an Enumeration list. For example, consider the NHS organisation codes. This list includes all the organisation codes that represent each hospital, GP surgery and other practices. Alongside this organisation code could also be a name, an address and the details of the main contact at each hospital or surgery. Therefore, each data item has many different properties associated with it.","title":"Terminology Data Type"},{"location":"glossary/terminology-data-type/terminology-data-type/#what-is-a-terminology-data-type","text":"A Terminology Data Type is one of the four possible Data Types within Mauro Data Mapper . It is used to describe a structured collection of Enumerated Values which have relationships between different data terms.","title":"What is a Terminology Data Type?"},{"location":"glossary/terminology-data-type/terminology-data-type/#how-are-terminology-data-types-used","text":"Terminology Data Types have now been expanded in Mauro Data Mapper to \u2018Model Reference\u2019 . Model References can now point to a Terminology , a CodeSet or a ReferenceDataModel .","title":"How are Terminology Data Types used?"},{"location":"glossary/terminology-data-type/terminology-data-type/#terminology","text":"A Terminology is a vocabulary, or a collection of allowable Terms as well as any relationships between them. A Term typically has a coded key and a human-readable value, along with some other information. Any relationships between pairs of Terms can be defined, including the relationship stating that one Term has a broader or narrower meaning than another Term . For example, a hierarchy of Terms denoting patient diagnoses might include a general Term to indicate some form of Diabetes. This may be related to more specific Terms to indicate a particular form of Diabetes such as Type 1 Diabetes. Terminologies are represented by a book icon in Mauro Data Mapper and you can browse through several different databases by selecting \u2018Healthcare Terminologies\u2019 in the Model Tree . Further details of the particular Terminology can be found in its details panel when selected. To view information relating to a specific Term , select the relevant Term in the Model Tree and its corresponding details panel will be displayed. Each Term has a: Label This is the unique name of the Term . Aliases Alternative names that can help locate the Term when searched for. Terminology The Terminology that this Term is associated with. Description A definition either written in html or plain text which explains any contextual details relating to the Term . URL A URL to the original definition of the Term . Classifications These are effectively tags that you can apply to the Term . Below the details panel, is a list of Relationships between the selected Term and other Terms within the specified Terminology .","title":"Terminology"},{"location":"glossary/terminology-data-type/terminology-data-type/#codeset","text":"A CodeSet is a selection of terms, which may be taken from one or more Terminologies . For example, a CodeSet would be all the Terms that describe the different variants of Diabetes.","title":"CodeSet"},{"location":"glossary/terminology-data-type/terminology-data-type/#referencedatamodel","text":"ReferenceDataModels are similar to a large database containing lots of detailed information and properties that would be too difficult to accurately manage in an Enumeration list. For example, consider the NHS organisation codes. This list includes all the organisation codes that represent each hospital, GP surgery and other practices. Alongside this organisation code could also be a name, an address and the details of the main contact at each hospital or surgery. Therefore, each data item has many different properties associated with it.","title":"ReferenceDataModel"},{"location":"glossary/version/version/","text":"What does Version mean? \u00b6 A Version of a model represents the final state of that model. Changes made to a model throughout its lifetime can be tracked by their Version numbers to provide a history of changes made. All models with a version number are described as Finalised . As an example, given a new Data Model called \"Test Data Model\", the Versions could be created as such: Initial creation of \"Test Data Model\", then finalised to be Version 1.0.0 A new draft is then created to make some alterations to the description of the model. This is then finalised to be Version 2.0.0 Another new draft is then created, this time to alter the description and add Data Classes. This is then finalised to be Version 3.0.0 ...and so on. In this way, it is possible to trace the state of each Version of a model from its creation to now. Version numbers are sequential and typically follow the same pattern as Semantic Versioning using major.minor.patch , such as 1.2.3 : Major update - represents a major change to the model, for example a major restructure. This updates the major number, e.g. 1.X.Y -> 2.0.0 Minor update - represents a minor, less encompasing change to the model, for example significantly updating text content but not altering the structure. This updates the minor number e.g. 1.0.X -> 1.1.0 Patch update - represents a very minor change to the model, for example fixing a gramatical mistake in a description. This updates the patch number e.g. 1.0.0 -> 1.0.1 It is also possible to define your own custom versioning scheme. Why are Versions useful? \u00b6 Versions and Version numbers are useful to definitively state that a model has a set contents to it and will not change; the only way to change a versioned model is to create a new version of it. This allows models to be very easily traced and makes publication of metadata very structured. Versions also allow readers of your models to easily tell what has changed between versions of a model. How are Versions created? \u00b6 To create a new version of a model, please refer to the user guide Branching, versioning and forking Data Models .","title":"Version"},{"location":"glossary/version/version/#what-does-version-mean","text":"A Version of a model represents the final state of that model. Changes made to a model throughout its lifetime can be tracked by their Version numbers to provide a history of changes made. All models with a version number are described as Finalised . As an example, given a new Data Model called \"Test Data Model\", the Versions could be created as such: Initial creation of \"Test Data Model\", then finalised to be Version 1.0.0 A new draft is then created to make some alterations to the description of the model. This is then finalised to be Version 2.0.0 Another new draft is then created, this time to alter the description and add Data Classes. This is then finalised to be Version 3.0.0 ...and so on. In this way, it is possible to trace the state of each Version of a model from its creation to now. Version numbers are sequential and typically follow the same pattern as Semantic Versioning using major.minor.patch , such as 1.2.3 : Major update - represents a major change to the model, for example a major restructure. This updates the major number, e.g. 1.X.Y -> 2.0.0 Minor update - represents a minor, less encompasing change to the model, for example significantly updating text content but not altering the structure. This updates the minor number e.g. 1.0.X -> 1.1.0 Patch update - represents a very minor change to the model, for example fixing a gramatical mistake in a description. This updates the patch number e.g. 1.0.0 -> 1.0.1 It is also possible to define your own custom versioning scheme.","title":"What does Version mean?"},{"location":"glossary/version/version/#why-are-versions-useful","text":"Versions and Version numbers are useful to definitively state that a model has a set contents to it and will not change; the only way to change a versioned model is to create a new version of it. This allows models to be very easily traced and makes publication of metadata very structured. Versions also allow readers of your models to easily tell what has changed between versions of a model.","title":"Why are Versions useful?"},{"location":"glossary/version/version/#how-are-versions-created","text":"To create a new version of a model, please refer to the user guide Branching, versioning and forking Data Models .","title":"How are Versions created?"},{"location":"glossary/versioned-folder/versioned-folder/","text":"What is a Versioned Folder? \u00b6 A Versioned Folder shares the same traits as both a Folder and a catalogue model. It is both: A Folder , in that it is a container for holding other catalogue items, and A catalogue item that can be finalised , versioned , branched and/or forked to make new draft versions. A Versioned Folder is a version controlled folder, allowing the entire contents of the folder to be finalised and set to particular versions in one operation. Finalising a Versioned Folder has the same effect as finalising any other model, locking the folder (and its contents) to a set version . Why use Versioned Folders? \u00b6 A Versioned Folder gives the benefits of both organisation and workflow management in one. There are several scenarios where a Versioned Folder may be useful: Your catalogue may be very large and contain many interconnected models that must be managed together. Larger sections of your catalogue may need to be version controlled as a whole, rather than individually finalising many smaller data models. Synchronisation of version numbers is also a big benefit to this approach. A Versioned Folder may act as the repository for a large model collection which should be published as one collection. Using a Versioned Folder also allows for the more complex workflow strategies of branching and forking but across an entire collection.","title":"Versioned folder"},{"location":"glossary/versioned-folder/versioned-folder/#what-is-a-versioned-folder","text":"A Versioned Folder shares the same traits as both a Folder and a catalogue model. It is both: A Folder , in that it is a container for holding other catalogue items, and A catalogue item that can be finalised , versioned , branched and/or forked to make new draft versions. A Versioned Folder is a version controlled folder, allowing the entire contents of the folder to be finalised and set to particular versions in one operation. Finalising a Versioned Folder has the same effect as finalising any other model, locking the folder (and its contents) to a set version .","title":"What is a Versioned Folder?"},{"location":"glossary/versioned-folder/versioned-folder/#why-use-versioned-folders","text":"A Versioned Folder gives the benefits of both organisation and workflow management in one. There are several scenarios where a Versioned Folder may be useful: Your catalogue may be very large and contain many interconnected models that must be managed together. Larger sections of your catalogue may need to be version controlled as a whole, rather than individually finalising many smaller data models. Synchronisation of version numbers is also a big benefit to this approach. A Versioned Folder may act as the repository for a large model collection which should be published as one collection. Using a Versioned Folder also allows for the more complex workflow strategies of branching and forking but across an entire collection.","title":"Why use Versioned Folders?"},{"location":"installing/administration/","text":"Checking version information \u00b6 Inside the web interface, the Plugins and Modules tab on the administrator dashboard provides information about the loaded components of the Mauro installation. The first list, Plugins , provides a list of names and versions of components which integrate with known plugin hooks. For example Importers , Exporters and Email Providers . The Modules list below provides a more comprehensive account of all Java / Grails components installed, including system libraries. Each Module may include one or more Plugins . To provide automated reporting, the API call behind this dashboard may be called separately. The GET urls below will return lists of plugins, for each named type: /api/admin/providers/importers /api/admin/providers/exporters /api/admin/providers/dataLoaders /api/admin/providers/emailers The endpoint below will return the list of modules: /api/admin/modules Backing up the database \u00b6 In order to back up the data from a running Mauro system, it is usually sufficient to take a simple backup of the underlying Postgres database, which can be achieved through standard Postgres utilities (for example, using pg_dump ). Within the Docker repository, a simple script in postgres/bin/snapshot-data.sh can be used within the docker container to take a copy of the underlying postgres database. This creates a file in a new folder /database on the container which can be copied back out to the host machine. Alternatively, you can run an exec command directly from the host machine. For example the command listed below: 1 docker-compose exec postgres pg_dump -U postgres maurodatamapper | gzip -9 > db-backup- $( date +%d-%m-%y ) .sql.gz This will execute the pg_dump command on the postgres container, connecting to the maurodatamapper database. The result will be zipped using the gzip command, creating a file with today's timestamp on it. Backup requirements vary, but a typical use-case is to combine one of the backup commands listed above with a script to manage regular backups at timed intervals and deleting old backups once a certain period has passed. Example scripts which may be adapted can be found on the official Postgres Wiki here . Re-building the search index \u00b6 The search index improves the performance of searching and this content is stored in-memory (and persisted to files on disk at suitable intervals). In some places in the Core , it may also be used to speed up access to particular model contents. The index is built using Apache Lucene but managed in the code through Hibernate. This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary. For example if searching provides incorrect or inconsistent results. Administrators may rebuild the Lucene index through the user interface. To do this, click the white arrow by your user profile to the right of the menu header. Select 'Configuration' from the dropdown menu. This will take you to configuration page where you can click the 'Lucene' tab and then select 'Rebuild Index' . Please do not leave the page whilst reindexing is in progress. The time required is dependent on the number of models saved in the system, but may take between 5 and 10 minutes for a large system. Alternatively, an API call may be made: see here for details. This POST call may be made with an existing session cookie, by passing username / password parameters as part of the call, or by passing an API Key. Only those with system administrator role may perform this action. The contents of the search index can be hard to inspect for debugging purposes! We use a tool called Marple but be sure to use a compatible version. Docker administration \u00b6 Cleaning up docker \u00b6 Continually building docker images will leave a lot of loose snapshot images floating around, occasionally make use of: Clean up stopped containers - docker rm $(docker ps -a -q) Clean up untagged images - docker rmi $(docker images | grep \"^<none>\" | awk \"{print $3}\") Clean up dangling volumes - docker volume rm $(docker volume ls -qf dangling=true) You can make life easier by adding the following to the appropriate bash profile file: 1 2 3 alias docker-rmi = 'docker rmi $(docker images -q --filter \"dangling=true\")' alias docker-rm = 'docker rm $(docker ps -a -q)' alias docker-rmv = 'docker volume rm $(docker volume ls -qf dangling=true)' Remove all stopped containers first then remove all tagged images. A useful tool is Dockviz , ever since docker did away with docker images --tree you can't see all the layers of images and therefore how many disconneected images you have. Add the following to the appropriate bash profile file: 1 alias dockviz = \"docker run --privileged -it --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz\" Then in a new terminal you can run dockviz images -t to see the tree. The program also does dot notation files for a graphical view as well. Multiple compose files \u00b6 When you supply multiple files, docker-compose combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their successors. 1 2 3 4 5 # Apply the .dev yml file, create and start the containers in the background $ docker-compose -f docker-compose.yml -f docker-compose.dev.yml -d <COMMAND> # Apply the .prod yml file, create and start the containers in the background $ docker-compose -f docker-compose.yml -f docker-compose.prod.yml -d <COMMAND> We recommend adding the following lines to the appropriate bash profile file: 1 2 alias docker-compose-dev = \"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\" alias docker-compose-prod = \"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\" This will allow you to start compose in dev mode without all the extra file definitions. Debugging and advanced configuration \u00b6 Here we present some useful hints for extending or customising the Docker setup for different configurations or use cases: Development file override The docker-compose.dev.yml can be used to override the standard docker-compose.yml file for development. In its initial configuarion, it opens up the ports in the Postgres container for manual connection from the host machine. This .dev compose file rebuilds all of the images, whereas the standard compose file and .prod versions do not build new images. Make use of the wait_scripts While -links and depends_on make sure the services a service requires are brought up first Docker only waits till they are running NOT till they are actually ready. The wait scripts provided test responses on given ports to make sure that a given service is actually available and ready to interact. Use COPY over ADD Docker recommends using COPY instead of ADD unless the source is a URL or a tar file which ADD can retrieve and/or unzip. Use of ENTRYPOINT & CMD If not requiring any dependencies then just set CMD [\"arg1\", ...] and the args will be passed to the ENTRYPOINT If requiring dependencies then set the ENTRYPOINT to the wait script and the CMD to CMD [\"process\", \"arg1\", ...] Try to keep images as small as possible As a general rule, we try to use the base images (e.g. of Tomcat, Postgres) and install additional packages at runtime or start-up. This increases portability and cuts down on disk usage when deploying updates. Exposing ports Exposing ports to other services must be carefully considered, to avoid unnecessary security vulnerabilities If the port only needs to be available to other docker services then use expose . If the port needs to be open to the host machine or beyond, then use ports . If the ports option is used this opens the port from the service to the outside world, it does not affect exposed ports between services, so if a service (e.g. postgres with port 5432) exposes a port then any service which used link to postgres will be able to find the database at postgresql://postgres:5432","title":"Administration"},{"location":"installing/administration/#checking-version-information","text":"Inside the web interface, the Plugins and Modules tab on the administrator dashboard provides information about the loaded components of the Mauro installation. The first list, Plugins , provides a list of names and versions of components which integrate with known plugin hooks. For example Importers , Exporters and Email Providers . The Modules list below provides a more comprehensive account of all Java / Grails components installed, including system libraries. Each Module may include one or more Plugins . To provide automated reporting, the API call behind this dashboard may be called separately. The GET urls below will return lists of plugins, for each named type: /api/admin/providers/importers /api/admin/providers/exporters /api/admin/providers/dataLoaders /api/admin/providers/emailers The endpoint below will return the list of modules: /api/admin/modules","title":"Checking version information"},{"location":"installing/administration/#backing-up-the-database","text":"In order to back up the data from a running Mauro system, it is usually sufficient to take a simple backup of the underlying Postgres database, which can be achieved through standard Postgres utilities (for example, using pg_dump ). Within the Docker repository, a simple script in postgres/bin/snapshot-data.sh can be used within the docker container to take a copy of the underlying postgres database. This creates a file in a new folder /database on the container which can be copied back out to the host machine. Alternatively, you can run an exec command directly from the host machine. For example the command listed below: 1 docker-compose exec postgres pg_dump -U postgres maurodatamapper | gzip -9 > db-backup- $( date +%d-%m-%y ) .sql.gz This will execute the pg_dump command on the postgres container, connecting to the maurodatamapper database. The result will be zipped using the gzip command, creating a file with today's timestamp on it. Backup requirements vary, but a typical use-case is to combine one of the backup commands listed above with a script to manage regular backups at timed intervals and deleting old backups once a certain period has passed. Example scripts which may be adapted can be found on the official Postgres Wiki here .","title":"Backing up the database"},{"location":"installing/administration/#re-building-the-search-index","text":"The search index improves the performance of searching and this content is stored in-memory (and persisted to files on disk at suitable intervals). In some places in the Core , it may also be used to speed up access to particular model contents. The index is built using Apache Lucene but managed in the code through Hibernate. This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary. For example if searching provides incorrect or inconsistent results. Administrators may rebuild the Lucene index through the user interface. To do this, click the white arrow by your user profile to the right of the menu header. Select 'Configuration' from the dropdown menu. This will take you to configuration page where you can click the 'Lucene' tab and then select 'Rebuild Index' . Please do not leave the page whilst reindexing is in progress. The time required is dependent on the number of models saved in the system, but may take between 5 and 10 minutes for a large system. Alternatively, an API call may be made: see here for details. This POST call may be made with an existing session cookie, by passing username / password parameters as part of the call, or by passing an API Key. Only those with system administrator role may perform this action. The contents of the search index can be hard to inspect for debugging purposes! We use a tool called Marple but be sure to use a compatible version.","title":"Re-building the search index"},{"location":"installing/administration/#docker-administration","text":"","title":"Docker administration"},{"location":"installing/administration/#cleaning-up-docker","text":"Continually building docker images will leave a lot of loose snapshot images floating around, occasionally make use of: Clean up stopped containers - docker rm $(docker ps -a -q) Clean up untagged images - docker rmi $(docker images | grep \"^<none>\" | awk \"{print $3}\") Clean up dangling volumes - docker volume rm $(docker volume ls -qf dangling=true) You can make life easier by adding the following to the appropriate bash profile file: 1 2 3 alias docker-rmi = 'docker rmi $(docker images -q --filter \"dangling=true\")' alias docker-rm = 'docker rm $(docker ps -a -q)' alias docker-rmv = 'docker volume rm $(docker volume ls -qf dangling=true)' Remove all stopped containers first then remove all tagged images. A useful tool is Dockviz , ever since docker did away with docker images --tree you can't see all the layers of images and therefore how many disconneected images you have. Add the following to the appropriate bash profile file: 1 alias dockviz = \"docker run --privileged -it --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz\" Then in a new terminal you can run dockviz images -t to see the tree. The program also does dot notation files for a graphical view as well.","title":"Cleaning up docker"},{"location":"installing/administration/#multiple-compose-files","text":"When you supply multiple files, docker-compose combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their successors. 1 2 3 4 5 # Apply the .dev yml file, create and start the containers in the background $ docker-compose -f docker-compose.yml -f docker-compose.dev.yml -d <COMMAND> # Apply the .prod yml file, create and start the containers in the background $ docker-compose -f docker-compose.yml -f docker-compose.prod.yml -d <COMMAND> We recommend adding the following lines to the appropriate bash profile file: 1 2 alias docker-compose-dev = \"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\" alias docker-compose-prod = \"docker-compose -f docker-compose.yml -f docker-compose.dev.yml\" This will allow you to start compose in dev mode without all the extra file definitions.","title":"Multiple compose files"},{"location":"installing/administration/#debugging-and-advanced-configuration","text":"Here we present some useful hints for extending or customising the Docker setup for different configurations or use cases: Development file override The docker-compose.dev.yml can be used to override the standard docker-compose.yml file for development. In its initial configuarion, it opens up the ports in the Postgres container for manual connection from the host machine. This .dev compose file rebuilds all of the images, whereas the standard compose file and .prod versions do not build new images. Make use of the wait_scripts While -links and depends_on make sure the services a service requires are brought up first Docker only waits till they are running NOT till they are actually ready. The wait scripts provided test responses on given ports to make sure that a given service is actually available and ready to interact. Use COPY over ADD Docker recommends using COPY instead of ADD unless the source is a URL or a tar file which ADD can retrieve and/or unzip. Use of ENTRYPOINT & CMD If not requiring any dependencies then just set CMD [\"arg1\", ...] and the args will be passed to the ENTRYPOINT If requiring dependencies then set the ENTRYPOINT to the wait script and the CMD to CMD [\"process\", \"arg1\", ...] Try to keep images as small as possible As a general rule, we try to use the base images (e.g. of Tomcat, Postgres) and install additional packages at runtime or start-up. This increases portability and cuts down on disk usage when deploying updates. Exposing ports Exposing ports to other services must be carefully considered, to avoid unnecessary security vulnerabilities If the port only needs to be available to other docker services then use expose . If the port needs to be open to the host machine or beyond, then use ports . If the ports option is used this opens the port from the service to the outside world, it does not affect exposed ports between services, so if a service (e.g. postgres with port 5432) exposes a port then any service which used link to postgres will be able to find the database at postgresql://postgres:5432","title":"Debugging and advanced configuration"},{"location":"installing/docker-install/","text":"Information See the Docker Setup section before installing Mauro using Docker. Git repository \u00b6 Depending on the operating system of the server you are running on, you may first need to install git to checkout the Mauro application. You can read more about installing git on different operating systems here: Getting Started - Installing Git The Mauro Docker configuration repository can be found here: https://github.com/MauroDataMapper/mdm-docker . Where you clone it is up to you, but on a *nix system we recommend cloning into /opt/ (for optional software packages) Different branches provide different configurations. We recommend checking out the main branch which will provide the latest releases of back-end and front-end. Alternatively, you can check out a specific tag to install a specific front-end / back-end combination. Tagged releases of Docker take the form Ba.b.c_Fx.y.z where a.b.c is the tagged version of the back-end and x.y.z is the tagged version of the front-end. Information If you're running on an internal server with SSH access forbidden by a firewall, you can use the following link to access the repository via HTTPS: SSH over HTTPS document . Overview \u00b6 The Docker Compose configuration defines two interacting containers: Postgres 12 [ postgres ] - Postgres Database Mauro Data Mapper [ maurodatamapper ] - Mauro Data Mapper The first of these is a standard Postgres container with an external volume for persistent storage. The second builds on the standard Apache Tomcat container, which hosts built versions of the Mauro application. The Postgres container must be running whenever the Mauro application starts. The Mauro container persists logs and Lucene indexes to shared folders which can be found in the docker repository folder. Default username / password \u00b6 The docker installation is empty on initialisation - it comes with one pre-configured user: with the username admin@maurodatamapper.com and the password password . Warning We strongly recommend changing this password on first login, and then setting up personal user accounts for individual users. Building \u00b6 Once cloned then running the standard docker-compose build command will build the images necessary to run the services. 1 2 # Build the entire system $ ./docker-compose build Additional Backend Plugins \u00b6 Additional plugins can be found at the Mauro Data Mapper Plugins organisation page. A complete list with versions can also be found in the installation documentation . Please note that while we will do our best to keep this page up-to-date, there may be circumstances where it is behind. Therefore, we recommend using our official GitHub Plugins organisation to find the latest releases and all available plugins. Each of these can be added as runtimeOnly dependencies by adding them to the ADDITIONAL_PLUGINS build argument for the mauro-data-mapper service build. These dependencies should be provided in a semi-colon separated list in the gradle style, they will be split and each will be added as a runtimeOnly dependency. Example: 1 2 3 4 5 mauro-data-mapper : build : context : mauro-data-mapper args : ADDITIONAL_PLUGINS : \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:3.0.0\" This will add the Excel plugin to the dependencies.gradle file: 1 runtimeOnly uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:3.0.0 Dynamic Versions \u00b6 You can use dynamic versioning to add dependencies, however this comes with a risk that it pulls a version which does not comply with your expected version of mdm-application-build/mdm-core ,which may cause conflicts with other plugins. Therefore, we do not advise this approach. Example 1 2 3 4 5 mauro-data-mapper : build : context : mauro-data-mapper args : ADDITIONAL_PLUGINS : \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:3.+\" This will add the latest minor version of the Excel plugin. Theme \u00b6 Mauro comes with a default user interface theme - with the standard blue branding, and default text on the home page. This can be overridden in the docker-compose.yml file, with instructions provided in the Branding guide . The default theme is called default and can be set with: 1 MDM_UI_THEME_NAME : \"default\" Running multiple instances \u00b6 If running multiple docker-compose instances then they will all make use of the same initial images, therefore you only need to run the ./make script once per server. SSH firewalled servers \u00b6 Some servers have the 22 SSH port firewalled for external connections. If this is the case you can change the base_images/sdk_base/ssh/config file: Comment out the Hostname field that's currently active Uncomment both commented out Hostname and Port fields, this will allow git to work using the 443 port which will not be blocked Run environment \u00b6 By adding variables to the <service>.environment section of the docker-compose.yml file, you can pass them into the container as environment variables. These will override any existing configuration variables which are used by default. Any defaults and normally used environment variables can be found in the relevant service's Dockerfile at the ENV command. postgres service \u00b6 POSTGRES_PASSWORD This sets the postgres user password for the service. As per the documentation at Postgres Docker Hub , it must be set for a docker postgres container. We have set a default but you can override if desired. If you do override it, you will also need to change the PGPASSWORD environment variable in the mauro-data-mapper section. DATABASE_USERNAME This is the username which will be created inside the Postgres instance to own the database which the MDM service will use. The username is also used by the MDM service to connect to the postgres instance, therefore if you change this you must** also supply it in the environment args for the MDM service. DATABASE_PASSWORD This is the password set for the DATABASE_USERNAME . It is the password used by the MDM service to connect to this postgres container. mauro-data-mapper service \u00b6 There are a large amount of variables which either need to be set or can be overridden depending on what plugins have been installed and what features you want. Therefore, you can find all the information on configuring MDM here . There are 2 environment variables which are not used directly by MDM and these are both optional to be overridden in the compose file. PGPASSWORD This is the postgres user's password for the postgres server. This is an environment variable set to allow the MDM service to wait till the postgres service has completely finished starting up. It is only used to confirm the Postgres server is running and databases exist. After this it is not used again. Note If you change POSTGRES_PASSWORD you must change this to match. This can only** be overridden in the docker-compose.yml file. CATALINA_OPTS Java Opts to be passed to Tomcat. Note This can only be overridden in the docker-compose.yml file. Environment Notes \u00b6 Database \u00b6 The system is designed to use the postgres service provided in the docker-compose file, therefore there should be no need to alter any of these settings. Only make alterations if running postgres as a separate service outside of docker-compose. Email \u00b6 The standard email properties will allow emails to be sent to a specific SMTP server. Docker Reference \u00b6 Running \u00b6 Before running please read the parameters section first. With docker and docker-compose installed, run the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Build all the images $ docker-compose-dev build # Start all the components up $ docker-compose up -d # To only start 1 service # This will also start up any of the services the named service depends on (defined by `links` or `depends_on`) $ docker-compose up [ SERVICE ] # To push all the output to the background add `-d` $ docker-compose up -d [ SERVICE ] # Stop background running and remove the containers $ docker-compose down # To update an already running service $ docker-compose-dev build [ SERVICE ] $ docker-compose up -d --no-deps [ SERVICE ] # To run in production mode $ docker-compose-prod up -d [ SERVICE ] If you run everything in the background use Kitematic to see the individual container logs. You can do this if running in the foreground and it is easier as it splits each of the containers up. If only starting a service when you stop, the service docker will not stop the dependencies that were started to allow the named service to start. The default compose file will pull the correct version images from Bintray, or a locally defined docker repository. For more information about administration of your running Docker instance, please see the Administration guide","title":"Docker Install"},{"location":"installing/docker-install/#git-repository","text":"Depending on the operating system of the server you are running on, you may first need to install git to checkout the Mauro application. You can read more about installing git on different operating systems here: Getting Started - Installing Git The Mauro Docker configuration repository can be found here: https://github.com/MauroDataMapper/mdm-docker . Where you clone it is up to you, but on a *nix system we recommend cloning into /opt/ (for optional software packages) Different branches provide different configurations. We recommend checking out the main branch which will provide the latest releases of back-end and front-end. Alternatively, you can check out a specific tag to install a specific front-end / back-end combination. Tagged releases of Docker take the form Ba.b.c_Fx.y.z where a.b.c is the tagged version of the back-end and x.y.z is the tagged version of the front-end. Information If you're running on an internal server with SSH access forbidden by a firewall, you can use the following link to access the repository via HTTPS: SSH over HTTPS document .","title":"Git repository"},{"location":"installing/docker-install/#overview","text":"The Docker Compose configuration defines two interacting containers: Postgres 12 [ postgres ] - Postgres Database Mauro Data Mapper [ maurodatamapper ] - Mauro Data Mapper The first of these is a standard Postgres container with an external volume for persistent storage. The second builds on the standard Apache Tomcat container, which hosts built versions of the Mauro application. The Postgres container must be running whenever the Mauro application starts. The Mauro container persists logs and Lucene indexes to shared folders which can be found in the docker repository folder.","title":"Overview"},{"location":"installing/docker-install/#default-username-password","text":"The docker installation is empty on initialisation - it comes with one pre-configured user: with the username admin@maurodatamapper.com and the password password . Warning We strongly recommend changing this password on first login, and then setting up personal user accounts for individual users.","title":"Default username / password"},{"location":"installing/docker-install/#building","text":"Once cloned then running the standard docker-compose build command will build the images necessary to run the services. 1 2 # Build the entire system $ ./docker-compose build","title":"Building"},{"location":"installing/docker-install/#additional-backend-plugins","text":"Additional plugins can be found at the Mauro Data Mapper Plugins organisation page. A complete list with versions can also be found in the installation documentation . Please note that while we will do our best to keep this page up-to-date, there may be circumstances where it is behind. Therefore, we recommend using our official GitHub Plugins organisation to find the latest releases and all available plugins. Each of these can be added as runtimeOnly dependencies by adding them to the ADDITIONAL_PLUGINS build argument for the mauro-data-mapper service build. These dependencies should be provided in a semi-colon separated list in the gradle style, they will be split and each will be added as a runtimeOnly dependency. Example: 1 2 3 4 5 mauro-data-mapper : build : context : mauro-data-mapper args : ADDITIONAL_PLUGINS : \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:3.0.0\" This will add the Excel plugin to the dependencies.gradle file: 1 runtimeOnly uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:3.0.0","title":"Additional Backend Plugins"},{"location":"installing/docker-install/#dynamic-versions","text":"You can use dynamic versioning to add dependencies, however this comes with a risk that it pulls a version which does not comply with your expected version of mdm-application-build/mdm-core ,which may cause conflicts with other plugins. Therefore, we do not advise this approach. Example 1 2 3 4 5 mauro-data-mapper : build : context : mauro-data-mapper args : ADDITIONAL_PLUGINS : \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:3.+\" This will add the latest minor version of the Excel plugin.","title":"Dynamic Versions"},{"location":"installing/docker-install/#theme","text":"Mauro comes with a default user interface theme - with the standard blue branding, and default text on the home page. This can be overridden in the docker-compose.yml file, with instructions provided in the Branding guide . The default theme is called default and can be set with: 1 MDM_UI_THEME_NAME : \"default\"","title":"Theme"},{"location":"installing/docker-install/#running-multiple-instances","text":"If running multiple docker-compose instances then they will all make use of the same initial images, therefore you only need to run the ./make script once per server.","title":"Running multiple instances"},{"location":"installing/docker-install/#ssh-firewalled-servers","text":"Some servers have the 22 SSH port firewalled for external connections. If this is the case you can change the base_images/sdk_base/ssh/config file: Comment out the Hostname field that's currently active Uncomment both commented out Hostname and Port fields, this will allow git to work using the 443 port which will not be blocked","title":"SSH firewalled servers"},{"location":"installing/docker-install/#run-environment","text":"By adding variables to the <service>.environment section of the docker-compose.yml file, you can pass them into the container as environment variables. These will override any existing configuration variables which are used by default. Any defaults and normally used environment variables can be found in the relevant service's Dockerfile at the ENV command.","title":"Run environment"},{"location":"installing/docker-install/#postgres-service","text":"POSTGRES_PASSWORD This sets the postgres user password for the service. As per the documentation at Postgres Docker Hub , it must be set for a docker postgres container. We have set a default but you can override if desired. If you do override it, you will also need to change the PGPASSWORD environment variable in the mauro-data-mapper section. DATABASE_USERNAME This is the username which will be created inside the Postgres instance to own the database which the MDM service will use. The username is also used by the MDM service to connect to the postgres instance, therefore if you change this you must** also supply it in the environment args for the MDM service. DATABASE_PASSWORD This is the password set for the DATABASE_USERNAME . It is the password used by the MDM service to connect to this postgres container.","title":"postgres service"},{"location":"installing/docker-install/#mauro-data-mapper-service","text":"There are a large amount of variables which either need to be set or can be overridden depending on what plugins have been installed and what features you want. Therefore, you can find all the information on configuring MDM here . There are 2 environment variables which are not used directly by MDM and these are both optional to be overridden in the compose file. PGPASSWORD This is the postgres user's password for the postgres server. This is an environment variable set to allow the MDM service to wait till the postgres service has completely finished starting up. It is only used to confirm the Postgres server is running and databases exist. After this it is not used again. Note If you change POSTGRES_PASSWORD you must change this to match. This can only** be overridden in the docker-compose.yml file. CATALINA_OPTS Java Opts to be passed to Tomcat. Note This can only be overridden in the docker-compose.yml file.","title":"mauro-data-mapper service"},{"location":"installing/docker-install/#environment-notes","text":"","title":"Environment Notes"},{"location":"installing/docker-install/#database","text":"The system is designed to use the postgres service provided in the docker-compose file, therefore there should be no need to alter any of these settings. Only make alterations if running postgres as a separate service outside of docker-compose.","title":"Database"},{"location":"installing/docker-install/#email","text":"The standard email properties will allow emails to be sent to a specific SMTP server.","title":"Email"},{"location":"installing/docker-install/#docker-reference","text":"","title":"Docker Reference"},{"location":"installing/docker-install/#running","text":"Before running please read the parameters section first. With docker and docker-compose installed, run the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Build all the images $ docker-compose-dev build # Start all the components up $ docker-compose up -d # To only start 1 service # This will also start up any of the services the named service depends on (defined by `links` or `depends_on`) $ docker-compose up [ SERVICE ] # To push all the output to the background add `-d` $ docker-compose up -d [ SERVICE ] # Stop background running and remove the containers $ docker-compose down # To update an already running service $ docker-compose-dev build [ SERVICE ] $ docker-compose up -d --no-deps [ SERVICE ] # To run in production mode $ docker-compose-prod up -d [ SERVICE ] If you run everything in the background use Kitematic to see the individual container logs. You can do this if running in the foreground and it is easier as it splits each of the containers up. If only starting a service when you stop, the service docker will not stop the dependencies that were started to allow the named service to start. The default compose file will pull the correct version images from Bintray, or a locally defined docker repository. For more information about administration of your running Docker instance, please see the Administration guide","title":"Running"},{"location":"installing/docker-setup/","text":"System requirements \u00b6 The simplest installation method is to run our preconfigured application using Docker . Any operating system, on a server or desktop, running Docker can run Mauro Data Mapper , but please note that some organisations may restrict the use of Docker on virtual machines. We advise a minimum of 2 CPUs and 4GBs RAM just to run this system. This does not allow for the requirements to have an operating system running as well. Therefore we recommend a 4 CPU and 8GB RAM server. The default install of Docker inside Linux configures the docker engine with unlimited access to the server's resources. However, if running in Windows or Mac OS X the Docker Toolbox will need to be configured. Installing Docker and Docker Compose \u00b6 You will need to install Docker and Docker Compose . Docker Compose is included as part of the standard 'Docker Desktop' for Windows and MacOS. To run Mauro Data Mapper , the minimum versions are as follows: Docker Engine: 19.03.8 Docker Compose: 1.25.5 Warning If you are running on Ubuntu, the default version of docker-compose installed with apt-get is currently 1.17.1, and you might get the error message: 1 Building docker compose ERROR: Need service name for --build-arg option In this case, you should uninstall docker-compose and re-install directly from Docker, following the instructions here . Docker Machine configuration \u00b6 The default docker-machine in a Windows or Mac OS X environment is configured to make use of one CPU and 1GB RAM. This is not enough RAM to reliably run the Mauro Data Mapper system and so should be increased. On Linux the docker machine is the host machine so there is no need to build or remove anything. Native Docker \u00b6 If using the Native Docker then edit the preferences of the Docker application and increase the RAM to at least 4GB. You will probably need to restart Docker after doing this. Docker Toolbox \u00b6 If using the Docker Toolbox then you will need to perform the following in a 'docker' terminal: 1 2 3 4 5 6 7 8 # Stop the default docker machine $ docker-machine stop default # Remove the default machine $ docker-machine rm default # Replace with a more powerful machine (4096 is the minimum recommended RAM, if you can give it more then do so) $ docker-machine create --driver virtualbox --virtualbox-cpu-count \"-1\" --virtualbox-memory \"4096\" default Use the default Docker Machine \u00b6 When controlling using Docker Machine via your terminal shell it is useful to set the default docker machine. Type the following at the command line, or add it to the appropriate bash profile file: 1 eval \" $( docker-machine env default ) \" If not you may see the following error: Cannot connect to the Docker daemon. Is the docker daemon running on this host? For more information about administration of your running Docker instance, please see the Administration guide","title":"Docker Setup"},{"location":"installing/docker-setup/#system-requirements","text":"The simplest installation method is to run our preconfigured application using Docker . Any operating system, on a server or desktop, running Docker can run Mauro Data Mapper , but please note that some organisations may restrict the use of Docker on virtual machines. We advise a minimum of 2 CPUs and 4GBs RAM just to run this system. This does not allow for the requirements to have an operating system running as well. Therefore we recommend a 4 CPU and 8GB RAM server. The default install of Docker inside Linux configures the docker engine with unlimited access to the server's resources. However, if running in Windows or Mac OS X the Docker Toolbox will need to be configured.","title":"System requirements"},{"location":"installing/docker-setup/#installing-docker-and-docker-compose","text":"You will need to install Docker and Docker Compose . Docker Compose is included as part of the standard 'Docker Desktop' for Windows and MacOS. To run Mauro Data Mapper , the minimum versions are as follows: Docker Engine: 19.03.8 Docker Compose: 1.25.5 Warning If you are running on Ubuntu, the default version of docker-compose installed with apt-get is currently 1.17.1, and you might get the error message: 1 Building docker compose ERROR: Need service name for --build-arg option In this case, you should uninstall docker-compose and re-install directly from Docker, following the instructions here .","title":"Installing Docker and Docker Compose"},{"location":"installing/docker-setup/#docker-machine-configuration","text":"The default docker-machine in a Windows or Mac OS X environment is configured to make use of one CPU and 1GB RAM. This is not enough RAM to reliably run the Mauro Data Mapper system and so should be increased. On Linux the docker machine is the host machine so there is no need to build or remove anything.","title":"Docker Machine configuration"},{"location":"installing/docker-setup/#native-docker","text":"If using the Native Docker then edit the preferences of the Docker application and increase the RAM to at least 4GB. You will probably need to restart Docker after doing this.","title":"Native Docker"},{"location":"installing/docker-setup/#docker-toolbox","text":"If using the Docker Toolbox then you will need to perform the following in a 'docker' terminal: 1 2 3 4 5 6 7 8 # Stop the default docker machine $ docker-machine stop default # Remove the default machine $ docker-machine rm default # Replace with a more powerful machine (4096 is the minimum recommended RAM, if you can give it more then do so) $ docker-machine create --driver virtualbox --virtualbox-cpu-count \"-1\" --virtualbox-memory \"4096\" default","title":"Docker Toolbox"},{"location":"installing/docker-setup/#use-the-default-docker-machine","text":"When controlling using Docker Machine via your terminal shell it is useful to set the default docker machine. Type the following at the command line, or add it to the appropriate bash profile file: 1 eval \" $( docker-machine env default ) \" If not you may see the following error: Cannot connect to the Docker daemon. Is the docker daemon running on this host? For more information about administration of your running Docker instance, please see the Administration guide","title":"Use the default Docker Machine"},{"location":"installing/migration/","text":"A tool has been built to migrate data from old instances of Metadata Catalogue to Mauro Data Mapper. This is a docker-based tool which applies a number of SQL scripts to move data into a new database schema and transform the data into a new format suitable for Mauro. The GitHub repository for the migration is here: https://github.com/MauroDataMapper/mc-to-mdm-migration . Manual Process \u00b6 Please see the documents in the guide folder for further information. Automated Process \u00b6 You can use one of the two available scripts in the top of the repository to run the migration from start to finish. You have two options as with the manual process documents, depending on if you're running Metadata Catalogue and Mauro Data Mapper inside or outside Docker. The scripts execute the stages described in the guide, so if you're unsure as to what parameters you should feed in then please read the guides to find out more. The scripts all execute using the defaults as if you have built the system using Docker. Docker Based PostgreSQL \u00b6 1 2 3 4 # Help/Usage ./run-complete-migration-docker.sh --help # Default parameters run ./run-complete-migration-docker.sh Remote/Local PostgreSQL \u00b6 1 2 3 4 # Help/Usage ./run-complete-migration-remote.sh --help # Default parameters run ./run-complete-migration-remote.sh Notes \u00b6 Please be aware that DataFlows are currently not migrated using this system, and may need to be manually migrated, or exported / imported.","title":"Migrating Old Data"},{"location":"installing/migration/#manual-process","text":"Please see the documents in the guide folder for further information.","title":"Manual Process"},{"location":"installing/migration/#automated-process","text":"You can use one of the two available scripts in the top of the repository to run the migration from start to finish. You have two options as with the manual process documents, depending on if you're running Metadata Catalogue and Mauro Data Mapper inside or outside Docker. The scripts execute the stages described in the guide, so if you're unsure as to what parameters you should feed in then please read the guides to find out more. The scripts all execute using the defaults as if you have built the system using Docker.","title":"Automated Process"},{"location":"installing/migration/#docker-based-postgresql","text":"1 2 3 4 # Help/Usage ./run-complete-migration-docker.sh --help # Default parameters run ./run-complete-migration-docker.sh","title":"Docker Based PostgreSQL"},{"location":"installing/migration/#remotelocal-postgresql","text":"1 2 3 4 # Help/Usage ./run-complete-migration-remote.sh --help # Default parameters run ./run-complete-migration-remote.sh","title":"Remote/Local PostgreSQL"},{"location":"installing/migration/#notes","text":"Please be aware that DataFlows are currently not migrated using this system, and may need to be manually migrated, or exported / imported.","title":"Notes"},{"location":"installing/plugins/","text":"We host a number of community plugins, the code for which is available in our GitHub 'Plugins' organisation . Commits to master branches kick off a build process on our continuous integration server and successfully-built artefacts are hosted on our instance of Artifactory . Below is a list of all the available plugins, along with their latest version number, release date and any dependencies each has. More details about the changes in each release can be found on the Release Notes page. To install a plugin, use the 'artefact name' as directed in the Installing page. Importers / Exporters \u00b6 Plugin name Version Release date Artefact name Dependencies ART-DECOR 1.0.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-artdecor:1.0.0 Core >= 4.2.0 AWS Glue 1.3.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-awsglue:1.3.0 Core >= 4.2.0 CSV 3.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-csv:3.0.0 Core >= 4.5.0 Excel 4.0.1 2021-07-12 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:4.0.1 Core >= 4.7.0 Digital Object Identifiers 1.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-digital-object-identifiers:1.1.0 Core >= 4.9.0 FHIR 1.1.0 2021-08-08 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-fhir:1.1.0 Core >= 4.8.0 MS SQL 5.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-sqlserver:5.1.0 Core >= 4.9.0 MySQL 3.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-mysql:3.1.0 Core >= 4.9.0 Oracle SQL 4.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-oracle:4.1.0 Core >= 4.9.0 PostgreSQL 4.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-postgresql:4.1.0 Core >= 4.9.0 Security \u00b6 Plugin name Version Release date Artefact name Dependencies Keycloak Integrated Authentication 2.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-keycloak:2.0.0 Core >= 4.5.0 OpenID Connect Authentication 1.0.0 2021-08-08 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-openid-connect:1.0.0 Core >= 4.8.0 Technical / Other \u00b6 Plugin name Version Release date Artefact name Dependencies Freemarker Templating 1.1.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-freemarker:1.1.0 Core >= 4.7.0 SPARQL 1.1.1 2021-03-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-sparql:1.1.1 Core >= 4.2.0","title":"Plugins"},{"location":"installing/plugins/#importers-exporters","text":"Plugin name Version Release date Artefact name Dependencies ART-DECOR 1.0.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-artdecor:1.0.0 Core >= 4.2.0 AWS Glue 1.3.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-awsglue:1.3.0 Core >= 4.2.0 CSV 3.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-csv:3.0.0 Core >= 4.5.0 Excel 4.0.1 2021-07-12 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-excel:4.0.1 Core >= 4.7.0 Digital Object Identifiers 1.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-digital-object-identifiers:1.1.0 Core >= 4.9.0 FHIR 1.1.0 2021-08-08 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-fhir:1.1.0 Core >= 4.8.0 MS SQL 5.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-sqlserver:5.1.0 Core >= 4.9.0 MySQL 3.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-mysql:3.1.0 Core >= 4.9.0 Oracle SQL 4.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-oracle:4.1.0 Core >= 4.9.0 PostgreSQL 4.1.0 2021-08-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-database-postgresql:4.1.0 Core >= 4.9.0","title":"Importers / Exporters"},{"location":"installing/plugins/#security","text":"Plugin name Version Release date Artefact name Dependencies Keycloak Integrated Authentication 2.0.0 2021-05-17 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-keycloak:2.0.0 Core >= 4.5.0 OpenID Connect Authentication 1.0.0 2021-08-08 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-authentication-openid-connect:1.0.0 Core >= 4.8.0","title":"Security"},{"location":"installing/plugins/#technical-other","text":"Plugin name Version Release date Artefact name Dependencies Freemarker Templating 1.1.0 2021-03-04 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-freemarker:1.1.0 Core >= 4.7.0 SPARQL 1.1.1 2021-03-24 uk.ac.ox.softeng.maurodatamapper.plugins:mdm-plugin-sparql:1.1.1 Core >= 4.2.0","title":"Technical / Other"},{"location":"installing/updating/","text":"Updating to the latest version \u00b6 Updating an already running system can be performed in one of two ways. Pull from latest tag \u00b6 The preferred method would be to pull the latest version tag from the mdm-docker repository and then rebuild the mauro-data-mapper service. However this may be hard if multiple changes have been made to the docker-compose.yml and you are not familiar enough with git to handle stashing and merging. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Update an already built system # Fetch the latest commits $ git fetch # Stash any local changes $ git stash # Checkout/pull the version you want to update to # e.g. git checkout B4.4.1_F6.0.0 $ git checkout <TAG> # Unstash local changes, you may need to resolve any merge conflicts $ git stash pop # Build the new image $ docker-compose build mauro-data-mapper # Start the update $ docker-compose up -d mauro-data-mapper Update script \u00b6 The alternative method is to use the update command script and pass in the new versions you want to update to. The downside with this method is if we have made any changes to the Dockerfiles or base versions you will not have them. 1 2 3 # Update an already built system # e.g ./update -b 4.4.1 -f 6.0.0 $ ./update -b <BACKEND_VERSION> -f <FRONTEND VERSION> This will rebuild just the Mauro Data Mapper image with the latest version. Database migrations \u00b6 Occasionally, database migrations are required when updating to a new version. These run automatically when the application restarts, making use of the Flyway versioning system. No manual steps are required from the user.","title":"Updating"},{"location":"installing/updating/#updating-to-the-latest-version","text":"Updating an already running system can be performed in one of two ways.","title":"Updating to the latest version"},{"location":"installing/updating/#pull-from-latest-tag","text":"The preferred method would be to pull the latest version tag from the mdm-docker repository and then rebuild the mauro-data-mapper service. However this may be hard if multiple changes have been made to the docker-compose.yml and you are not familiar enough with git to handle stashing and merging. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Update an already built system # Fetch the latest commits $ git fetch # Stash any local changes $ git stash # Checkout/pull the version you want to update to # e.g. git checkout B4.4.1_F6.0.0 $ git checkout <TAG> # Unstash local changes, you may need to resolve any merge conflicts $ git stash pop # Build the new image $ docker-compose build mauro-data-mapper # Start the update $ docker-compose up -d mauro-data-mapper","title":"Pull from latest tag"},{"location":"installing/updating/#update-script","text":"The alternative method is to use the update command script and pass in the new versions you want to update to. The downside with this method is if we have made any changes to the Dockerfiles or base versions you will not have them. 1 2 3 # Update an already built system # e.g ./update -b 4.4.1 -f 6.0.0 $ ./update -b <BACKEND_VERSION> -f <FRONTEND VERSION> This will rebuild just the Mauro Data Mapper image with the latest version.","title":"Update script"},{"location":"installing/updating/#database-migrations","text":"Occasionally, database migrations are required when updating to a new version. These run automatically when the application restarts, making use of the Flyway versioning system. No manual steps are required from the user.","title":"Database migrations"},{"location":"installing/branding/branding/","text":"Introduction \u00b6 The Mauro Data Mapper Web interface can be customised to match an organisation's brand or styling. Creating a new theme requires developer effort, the steps of which are detailed below. Angular Material \u00b6 The Angular Material framework, which the UI uses for layout and controls, can support multiple themes (colour palettes and typography) as well as dynamic switching of themes. However, the themes must be precompiled into the application as part of the stylesheets for the site. This means that themes must be prepared by a developer. See also: Angular Material theming guide Theming your components Themes \u00b6 Overview \u00b6 Themes in Mauro Data Mapper are organised in the project by name, and uses folder and naming conventions to identify where files and CSS selectors are. The conventions used are: SASS files : src/style/themes/{name}.scss CSS theme class : .{name}-theme Asset files : src/assets/themes/{name}/*.* So, as an example, a custom theme called nhs-digital would be: SASS files : src/style/themes/nhs-digital.scss CSS theme class : .nhs-digital-theme Asset files : src/assets/themes/nhs-digital/*.* Switching Themes \u00b6 To determine the theme to use for the site, update the src/environments/environment.ts : 1 2 3 4 5 export const environment = { // ... themeName : 'nhs-digital' // ... }; Note If the theme name is not provided, this will fall back to default . An alternative for switching the theme for production UI builds is to set an environment variable called MDM_UI_THEME_NAME . This allows the theme setting to be defined as part of a wider build process, such as creating a Docker image. Importing Themes \u00b6 Each theme requires a SASS file to define the colour palette and typography for the Angular Material theming system to use. This SASS file can also override any other CSS selectors that do not affect the Material controls. The src/style/styles.scss file is the place to import all theme files. The key parts are: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @import \"~@angular/material/theming\" ; // Include the common styles for Angular Material. Only required to include this once! @include mat-core (); /* Themes Import more theme SASS files here to make them available to the app */ @import \"style/themes/default.scss\" ; @import \"style/themes/nhs-digital.scss\" ; // ...etc // More SASS files imported here... Note Remember that Angular Material requires all themes to be precompiled, so all SASS files must be imported to include them as options. Only one theme will appear at a time though. Creating Themes \u00b6 To create a new theme it is advised to copy src/style/themes/default.scss and make the necessary adjustments. Then @import the new theme file in src/style/styles.scss . The specifics of the Angular Material theme system should be referenced for better explanation. However, in summary, you will need to define: The colour palettes required to represent: Primary colours Accented colours Warning colours The typography settings to use for text/font The Material mixins should then be included within a top-level CSS selector named after the theme. This CSS selector will be applied to the body of the HTML page and therefore affect all DOM elements below it. The theme file should effectively include the following steps: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // Required for Angular Material @import \"~@angular/material/theming\" ; // Required for custom theming of MDM components @import \"../components/custom\" ; /* Define colour platte maps here ... */ $nhs-digital-theme-primary : mat-palette ( ... ); $nhs-digital-theme-accent : mat-palette ( ... ); $nhs-digital-theme-warn : mat-palette ( ... ); /* Create the colour theme */ $nhs-digital-theme : mat-light-theme ( $nhs-digital-theme-primary , $nhs-digital-theme-accent , $nhs-digital-theme-warn ); /* Define the typography settings to use... */ $nhs-digital-typography : mat-typography-config ( ... ); /* Define the top-level CSS class name. Name this in the format \".{name}-theme\" so the application can automatically use it for the page */ .nhs-digital-theme { // Use the created theme variables to configure Angular Material @include angular-material-theme ( $nhs-digital-theme ); @include angular-material-typography ( $nhs-digital-typography ); // Some MDM components have also been added to the Angular Material theme system. Include this mixin to have these match the same theme @include mdm-custom-components-theme ( $nhs-digital-theme ); /* Add any further CSS class overrides here... */ } Assets \u00b6 Themeable assets, such as logos and images, should be stored under src/assets/themes under the specific sub-folder named after the theme. The file names used should be consistent across all themes e.g. logo.png . To consistently reference the correct asset path to use, the ThemingService can be injected into your component/service and use the getAssetPath() function to get the correct path for an asset according to the current theme. Edit page content \u00b6 Some of the static content may be adjusted by an administrator to allow the end users to make content edits instead of relying on developers. Follow the steps below to edit page content. Sign in and click the white arrow by your user profile on the right of the menu header. Select 'Configuration' from the dropdown menu. Click the 'Properties' tab to view a list of properties, with the cog icon indicating a system property. To edit or delete a property, click the three vetical dots to the right of the relevant row and choose either 'Edit' or 'Delete' from the dropdown menu. To add a new property click the '+Add' button at the top right of the page which will take you to an 'Add Property' form. Firstly, select a property you want to add from the dropdown menu. Then enter a 'Key' , 'Category' and 'Value' . Tick the 'Publicly visible' box if it applies. Once the form is completed, click 'Add property' and a green notifciaiton box should appear at the bottom right of your screen, confirming the changes. Note It may be necessary to manually adjust the HTML source to set correct styling/markup for the content. This can be done by clicking on the `` 'Change Mode' toolbar button in each form edit field. There are configuration properties available to modify the following: Homepage Logo Footer Home Page \u00b6 The homepage of Mauro Data Mapper displays a summary of the tool as well as the most important links to help users easily navigate to the main sections. The homepage can be modified to include custom content to suit your organisation. The homepage is split into two columns and four main sections which can each be modified. Note It is recommended to include correct HTML source and styling in these sections. Introduction - content.home.intro.left This is usually some introduction text on the left column of the starting page Introduction image - content.home.intro.right This is usually an image or illustration on the right column of the starting page that compliments the introduction text Features heading - content.home.detail.heading This is usually a heading for the 'Features' section underneath the introduction Feature boxes - content.home.detail.column1 - 3 These are three feature boxes that usually appear in the 'Features' section underneath the introduction Logo \u00b6 A static asset should also be provided for the logo in the navbar component as a default, however an image URL may also be provided if the logo is hosted in another location e.g. CDN. To set a logo, modify the theme.logo.url property. The theme.logo.width property is also provided to adjust the size of the logo in the space provided. Note that: The value entered for theme.logo.width must be supported by CSS e.g. 20px , 1.2em , etc The maximum width of the logo is 120px . If the image is larger than this then it will be scaled down to fit Footer \u00b6 The copyright notice can be altered in the footer by changing the content.footer.copyright property. Defaults \u00b6 If no property values are provided for the above, then suitable defaults are used instead based on the current theme. To revert back to default values for any property, simply delete the property from the configuration table.","title":"Branding"},{"location":"installing/branding/branding/#introduction","text":"The Mauro Data Mapper Web interface can be customised to match an organisation's brand or styling. Creating a new theme requires developer effort, the steps of which are detailed below.","title":"Introduction"},{"location":"installing/branding/branding/#angular-material","text":"The Angular Material framework, which the UI uses for layout and controls, can support multiple themes (colour palettes and typography) as well as dynamic switching of themes. However, the themes must be precompiled into the application as part of the stylesheets for the site. This means that themes must be prepared by a developer. See also: Angular Material theming guide Theming your components","title":"Angular Material"},{"location":"installing/branding/branding/#themes","text":"","title":"Themes"},{"location":"installing/branding/branding/#overview","text":"Themes in Mauro Data Mapper are organised in the project by name, and uses folder and naming conventions to identify where files and CSS selectors are. The conventions used are: SASS files : src/style/themes/{name}.scss CSS theme class : .{name}-theme Asset files : src/assets/themes/{name}/*.* So, as an example, a custom theme called nhs-digital would be: SASS files : src/style/themes/nhs-digital.scss CSS theme class : .nhs-digital-theme Asset files : src/assets/themes/nhs-digital/*.*","title":"Overview"},{"location":"installing/branding/branding/#switching-themes","text":"To determine the theme to use for the site, update the src/environments/environment.ts : 1 2 3 4 5 export const environment = { // ... themeName : 'nhs-digital' // ... }; Note If the theme name is not provided, this will fall back to default . An alternative for switching the theme for production UI builds is to set an environment variable called MDM_UI_THEME_NAME . This allows the theme setting to be defined as part of a wider build process, such as creating a Docker image.","title":"Switching Themes"},{"location":"installing/branding/branding/#importing-themes","text":"Each theme requires a SASS file to define the colour palette and typography for the Angular Material theming system to use. This SASS file can also override any other CSS selectors that do not affect the Material controls. The src/style/styles.scss file is the place to import all theme files. The key parts are: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @import \"~@angular/material/theming\" ; // Include the common styles for Angular Material. Only required to include this once! @include mat-core (); /* Themes Import more theme SASS files here to make them available to the app */ @import \"style/themes/default.scss\" ; @import \"style/themes/nhs-digital.scss\" ; // ...etc // More SASS files imported here... Note Remember that Angular Material requires all themes to be precompiled, so all SASS files must be imported to include them as options. Only one theme will appear at a time though.","title":"Importing Themes"},{"location":"installing/branding/branding/#creating-themes","text":"To create a new theme it is advised to copy src/style/themes/default.scss and make the necessary adjustments. Then @import the new theme file in src/style/styles.scss . The specifics of the Angular Material theme system should be referenced for better explanation. However, in summary, you will need to define: The colour palettes required to represent: Primary colours Accented colours Warning colours The typography settings to use for text/font The Material mixins should then be included within a top-level CSS selector named after the theme. This CSS selector will be applied to the body of the HTML page and therefore affect all DOM elements below it. The theme file should effectively include the following steps: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // Required for Angular Material @import \"~@angular/material/theming\" ; // Required for custom theming of MDM components @import \"../components/custom\" ; /* Define colour platte maps here ... */ $nhs-digital-theme-primary : mat-palette ( ... ); $nhs-digital-theme-accent : mat-palette ( ... ); $nhs-digital-theme-warn : mat-palette ( ... ); /* Create the colour theme */ $nhs-digital-theme : mat-light-theme ( $nhs-digital-theme-primary , $nhs-digital-theme-accent , $nhs-digital-theme-warn ); /* Define the typography settings to use... */ $nhs-digital-typography : mat-typography-config ( ... ); /* Define the top-level CSS class name. Name this in the format \".{name}-theme\" so the application can automatically use it for the page */ .nhs-digital-theme { // Use the created theme variables to configure Angular Material @include angular-material-theme ( $nhs-digital-theme ); @include angular-material-typography ( $nhs-digital-typography ); // Some MDM components have also been added to the Angular Material theme system. Include this mixin to have these match the same theme @include mdm-custom-components-theme ( $nhs-digital-theme ); /* Add any further CSS class overrides here... */ }","title":"Creating Themes"},{"location":"installing/branding/branding/#assets","text":"Themeable assets, such as logos and images, should be stored under src/assets/themes under the specific sub-folder named after the theme. The file names used should be consistent across all themes e.g. logo.png . To consistently reference the correct asset path to use, the ThemingService can be injected into your component/service and use the getAssetPath() function to get the correct path for an asset according to the current theme.","title":"Assets"},{"location":"installing/branding/branding/#edit-page-content","text":"Some of the static content may be adjusted by an administrator to allow the end users to make content edits instead of relying on developers. Follow the steps below to edit page content. Sign in and click the white arrow by your user profile on the right of the menu header. Select 'Configuration' from the dropdown menu. Click the 'Properties' tab to view a list of properties, with the cog icon indicating a system property. To edit or delete a property, click the three vetical dots to the right of the relevant row and choose either 'Edit' or 'Delete' from the dropdown menu. To add a new property click the '+Add' button at the top right of the page which will take you to an 'Add Property' form. Firstly, select a property you want to add from the dropdown menu. Then enter a 'Key' , 'Category' and 'Value' . Tick the 'Publicly visible' box if it applies. Once the form is completed, click 'Add property' and a green notifciaiton box should appear at the bottom right of your screen, confirming the changes. Note It may be necessary to manually adjust the HTML source to set correct styling/markup for the content. This can be done by clicking on the `` 'Change Mode' toolbar button in each form edit field. There are configuration properties available to modify the following: Homepage Logo Footer","title":"Edit page content"},{"location":"installing/branding/branding/#home-page","text":"The homepage of Mauro Data Mapper displays a summary of the tool as well as the most important links to help users easily navigate to the main sections. The homepage can be modified to include custom content to suit your organisation. The homepage is split into two columns and four main sections which can each be modified. Note It is recommended to include correct HTML source and styling in these sections. Introduction - content.home.intro.left This is usually some introduction text on the left column of the starting page Introduction image - content.home.intro.right This is usually an image or illustration on the right column of the starting page that compliments the introduction text Features heading - content.home.detail.heading This is usually a heading for the 'Features' section underneath the introduction Feature boxes - content.home.detail.column1 - 3 These are three feature boxes that usually appear in the 'Features' section underneath the introduction","title":"Home Page"},{"location":"installing/branding/branding/#logo","text":"A static asset should also be provided for the logo in the navbar component as a default, however an image URL may also be provided if the logo is hosted in another location e.g. CDN. To set a logo, modify the theme.logo.url property. The theme.logo.width property is also provided to adjust the size of the logo in the space provided. Note that: The value entered for theme.logo.width must be supported by CSS e.g. 20px , 1.2em , etc The maximum width of the logo is 120px . If the image is larger than this then it will be scaled down to fit","title":"Logo"},{"location":"installing/branding/branding/#footer","text":"The copyright notice can be altered in the footer by changing the content.footer.copyright property.","title":"Footer"},{"location":"installing/branding/branding/#defaults","text":"If no property values are provided for the above, then suitable defaults are used instead based on the current theme. To revert back to default values for any property, simply delete the property from the configuration table.","title":"Defaults"},{"location":"installing/configuring/application.yml/","text":"Below is the default application.yml file included in the main application. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 database : host : localhost port : 5432 name : maurodatamapper --- #Default for plugins/applications --- maurodatamapper : security : public : false authority : name : 'Mauro Data Mapper' url : http://localhost grails : profile : rest-api codegen : defaultPackage : uk.ac.ox.softeng.maurodatamapper.application gorm : reactor : # Whether to translate GORM events into Reactor events # Disabled by default for performance reasons events : false failOnError : true resources : pattern : /** info : app : name : '@info.app.name@' version : '@info.app.version@' grailsVersion : '@info.app.grailsVersion@' spring : jmx : unique-names : true main : banner-mode : \"off\" groovy : template : check-template-location : false devtools : restart : additional-exclude : - '*.gsp' - '**/*.gsp' - '*.gson' - '**/*.gson' - 'logback.groovy' - '*.properties' # Spring Actuator Endpoints are Disabled by Default management : endpoints : enabled-by-default : false web : exposure : include : - 'health' - 'shutdown' jmx : exposure : include : '*' endpoint : shutdown : enabled : true health : enabled : true --- grails : mime : disable : accept : header : userAgents : - Gecko - WebKit - Presto - Trident types : json : - application/json - text/json hal : - application/hal+json - application/hal+xml xml : - text/xml - application/xml atom : application/atom+xml css : text/css csv : text/csv js : text/javascript rss : application/rss+xml text : text/plain all : '*/*' urlmapping : cache : maxsize : 1000 controllers : defaultScope : singleton converters : encoding : UTF-8 exceptionresolver : params : exclude : - password - tempPassword cors : enabled : true # The following are the defaults allowedOrigins : [ '*' ] allowedMethods : [ 'GET' , 'POST' , 'PUT' , 'DELETE' , 'OPTIONS' , 'HEAD' ] allowedHeaders : [ 'origin' , 'content-type' , 'accept' , 'authorization' , 'pragma' , 'cache-control' ] #exposedHeaders: null #maxAge: 1800 #allowCredentials: true views : markup : autoEscape : true prettyPrint : false autoIndent : false autoNewLine : false --- hibernate : cache : queries : false use_second_level_cache : true use_query_cache : true region : factory_class : org.hibernate.cache.jcache.JCacheRegionFactory javax : cache : provider : org.ehcache.jsr107.EhcacheCachingProvider missing_cache_strategy : create search : default.indexBase : '/tmp/lucene/mdm_application' --- dataSource : pooled : true jmxExport : true formatSql : true driverClassName : org.postgresql.Driver dialect : org.hibernate.dialect.PostgreSQL10Dialect username : maurodatamapper password : MauroDataMapper1234 dbCreate : none url : 'jdbc:postgresql://${database.host}:${database.port}/${database.name}' ---","title":"MDM Application YML"},{"location":"installing/configuring/build.yml/","text":"Below is the default build.yml file built into the MDM image. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 --- # Database connection details --- database : host : postgres port : 5432 name : maurodatamapper dataSource : username : maurodatamapperuser password : \"this is provided but hidden in these docs\" --- # MDM configuration properties --- maurodatamapper : authority : name : 'Mauro Data Mapper' url : http://localhost --- # Standard Email configuration --- simplejavamail : smtp : username : password : host : port : 587 transportstrategy : SMTP_TLS --- # mdm-plugin-email-proxy Configuration --- #emailServiceUrl: #emailServiceUsername: #emailServicePassword: --- # CORS # See http://docs.grails.org/latest/guide/theWebLayer.html#cors --- grails : cors : enabled : true # The following are the defaults allowedOrigins : [ '*' ] allowedMethods : [ 'GET' , 'POST' , 'PUT' , 'DELETE' , 'OPTIONS' , 'HEAD' ] allowedHeaders : [ 'origin' , 'content-type' , 'accept' , 'authorization' , 'pragma' , 'cache-control' ] #exposedHeaders: null #maxAge: 1800 #allowCredentials: true hibernate : search : default : indexBase : '/lucene'","title":"Build YML"},{"location":"installing/configuring/changing-database/","text":"There are 3 variations of changing the database which may be considered: Using the postgres service but using different database name, username and/or password. Using an external database server/service outside of the docker-compose network. Information We cannot recommend using a databasing system other than postgreSQL as we have custom SQL inside the API which uses postgres specific features. Warning The postgres user which MDM uses to connect must be a postgres superuser. This is because we enable and disable foreign keys during major deletes to allow the deletions to happen in a reasonble time. Changing the postgres service variables \u00b6 The following properties can be altered inside the postgres service. Any properties which can be defined at runtime can be set in the docker-compose.yml environment block. There is a fixtures file which is run when the container first starts up, and only when it is first started. Once the container has been started and the postgres volume exists, then the fixtures file will not be run again. If you wish to change or rebuild, then you will need to remove the docker volume for the postgres service. POSTGRES_PASSWORD This is the password for the postgres user. Changing this password requires you to change the PGPASSWORD in the mauro-data-mapper environment. DATABASE_NAME This is the name of the database which will be created for the provided username/password. This can be defined at runtime and will be the database which all the MDM data will be stored into, the database will be created when the postgres container first starts. DATABASE_USERNAME This is the username for the user the MDM service will use to talk to the postgres service. This can be defined at runtime and will be created as a superuser. DATABASE_PASSWORD This is the password set for the given username. This can be defined at runtime. Changing the above 3 database properties will require the following environment block variables to be set for MDM to set the following 3 properties, these will override and set the grails yml property defined removing the need for you to set it in the build.yml or runtime.yml files. postgres property mdm property grails property DATABASE_NAME DATABASE_NAME database.name DATABASE_USERNAME DATASOURCE_USERNAME dataSource.username DATABASE_PASSWORD DATASOURCE_PASSWORD dataSource.password Using an external postgres service \u00b6 Information An example of this might be when using Amazon Web Services (AWS) to provide the database hosting solution. If using this option we recommend commenting out or deleting the postgres service from the docker-compose.yml file to avoid any confusion. You will also need to remove the following block from the MDM service, as this section tells the MDM service to wait till the postgres service is running and adds a network link between the 2 containers. 1 2 depends_on : - postgres You will need to set the following properties in the environments block of the MDM service, this will inform the service where to find postgres and to wait till it can talk to it before proceeding. They will also provide the connection details required, any of the following properties set will override the stated grails yml properties (the property name in brackets), so you should not set them in the build.yml or runtime.yml file as the environment properties will override whatever is there. DATABASE_HOST ( database.host ) This is the full hostname or IP address of the server running the postgres service. DATABASE_POST ( database.port ) This is the port of the server which postgres can be found at. DATABASE_NAME ( database.name ) This is the name of the database which all the MDM data will be stored into. DATASOURCE_USERNAME ( dataSource.username ) This is the username for the user the MDM service will use to talk to the postgres database. DATABASE_PASSWORD ( dataSource.password ) This is the password set for the given username.","title":"Changing Database"},{"location":"installing/configuring/changing-database/#changing-the-postgres-service-variables","text":"The following properties can be altered inside the postgres service. Any properties which can be defined at runtime can be set in the docker-compose.yml environment block. There is a fixtures file which is run when the container first starts up, and only when it is first started. Once the container has been started and the postgres volume exists, then the fixtures file will not be run again. If you wish to change or rebuild, then you will need to remove the docker volume for the postgres service. POSTGRES_PASSWORD This is the password for the postgres user. Changing this password requires you to change the PGPASSWORD in the mauro-data-mapper environment. DATABASE_NAME This is the name of the database which will be created for the provided username/password. This can be defined at runtime and will be the database which all the MDM data will be stored into, the database will be created when the postgres container first starts. DATABASE_USERNAME This is the username for the user the MDM service will use to talk to the postgres service. This can be defined at runtime and will be created as a superuser. DATABASE_PASSWORD This is the password set for the given username. This can be defined at runtime. Changing the above 3 database properties will require the following environment block variables to be set for MDM to set the following 3 properties, these will override and set the grails yml property defined removing the need for you to set it in the build.yml or runtime.yml files. postgres property mdm property grails property DATABASE_NAME DATABASE_NAME database.name DATABASE_USERNAME DATASOURCE_USERNAME dataSource.username DATABASE_PASSWORD DATASOURCE_PASSWORD dataSource.password","title":"Changing the postgres service variables"},{"location":"installing/configuring/changing-database/#using-an-external-postgres-service","text":"Information An example of this might be when using Amazon Web Services (AWS) to provide the database hosting solution. If using this option we recommend commenting out or deleting the postgres service from the docker-compose.yml file to avoid any confusion. You will also need to remove the following block from the MDM service, as this section tells the MDM service to wait till the postgres service is running and adds a network link between the 2 containers. 1 2 depends_on : - postgres You will need to set the following properties in the environments block of the MDM service, this will inform the service where to find postgres and to wait till it can talk to it before proceeding. They will also provide the connection details required, any of the following properties set will override the stated grails yml properties (the property name in brackets), so you should not set them in the build.yml or runtime.yml file as the environment properties will override whatever is there. DATABASE_HOST ( database.host ) This is the full hostname or IP address of the server running the postgres service. DATABASE_POST ( database.port ) This is the port of the server which postgres can be found at. DATABASE_NAME ( database.name ) This is the name of the database which all the MDM data will be stored into. DATASOURCE_USERNAME ( dataSource.username ) This is the username for the user the MDM service will use to talk to the postgres database. DATABASE_PASSWORD ( dataSource.password ) This is the password set for the given username.","title":"Using an external postgres service"},{"location":"installing/configuring/mdm-core-config/","text":"Required to be overridden \u00b6 The following variables need to be overriden or set when building or starting up a new mauro-data-mapper image. grails.cors.allowedOrigins Should be set to a single fully qualified domain name (FQDN) URL which is the host where MDM will be accessed from. If using a proxy to break SSL then the origin would be the hostname where the proxy sits, not the host of the server running the docker containers. The origin must include the protocol, i.e. https or http . maurodatamapper.authority.name A unique name used to distinguish a running MDM instance. maurodatamapper.authority.url The full URL to the location of the catalogue. This is considered a unique identifier to distinguish any instance from another and therefore no 2 instances should use the same URL. maurodatamapper.email.from.address The email address to use when sending emails to let recipients know who sent the email. This should be set to override the email address/username used in simplejavamail.smtp.username . simplejavamail.smtp.username To allow the catalogue to send emails this needs to be a valid username for the simplejavamail.smtp.host . simplejavamail.smtp.password To allow the catalogue to send emails this needs to be a valid password for the simplejavamail.smtp.host and simplejavamail.smtp.username . simplejavamail.smtp.host This is the FQDN of the mail server to use when sending emails. Optional Overrides \u00b6 The below, along with any property found in any config file, can be overridden. We have supplied a brief description of any properties which cannot be found in the grails or spring documentation. database.host The host of the database. If using docker-compose this should be left as postgres or changed to the name of the database service. database.port The port of the database. database.name The name of the database which the catalogue data will be stored in. dataSource.username Username to use to connect to the database. See the Postgres service environment variables for more information. dataSource.password Password to use to connect to the database. See the Postgres service environment variables for more information. simplejavamail.smtp.port The port to use when sending emails. simplejavamail.smtp.transportstrategy The transport strategy to use when sending emails. hibernate.search.default.indexBase The directory to store the lucene index files in.","title":"MDM Core"},{"location":"installing/configuring/mdm-core-config/#required-to-be-overridden","text":"The following variables need to be overriden or set when building or starting up a new mauro-data-mapper image. grails.cors.allowedOrigins Should be set to a single fully qualified domain name (FQDN) URL which is the host where MDM will be accessed from. If using a proxy to break SSL then the origin would be the hostname where the proxy sits, not the host of the server running the docker containers. The origin must include the protocol, i.e. https or http . maurodatamapper.authority.name A unique name used to distinguish a running MDM instance. maurodatamapper.authority.url The full URL to the location of the catalogue. This is considered a unique identifier to distinguish any instance from another and therefore no 2 instances should use the same URL. maurodatamapper.email.from.address The email address to use when sending emails to let recipients know who sent the email. This should be set to override the email address/username used in simplejavamail.smtp.username . simplejavamail.smtp.username To allow the catalogue to send emails this needs to be a valid username for the simplejavamail.smtp.host . simplejavamail.smtp.password To allow the catalogue to send emails this needs to be a valid password for the simplejavamail.smtp.host and simplejavamail.smtp.username . simplejavamail.smtp.host This is the FQDN of the mail server to use when sending emails.","title":"Required to be overridden"},{"location":"installing/configuring/mdm-core-config/#optional-overrides","text":"The below, along with any property found in any config file, can be overridden. We have supplied a brief description of any properties which cannot be found in the grails or spring documentation. database.host The host of the database. If using docker-compose this should be left as postgres or changed to the name of the database service. database.port The port of the database. database.name The name of the database which the catalogue data will be stored in. dataSource.username Username to use to connect to the database. See the Postgres service environment variables for more information. dataSource.password Password to use to connect to the database. See the Postgres service environment variables for more information. simplejavamail.smtp.port The port to use when sending emails. simplejavamail.smtp.transportstrategy The transport strategy to use when sending emails. hibernate.search.default.indexBase The directory to store the lucene index files in.","title":"Optional Overrides"},{"location":"installing/configuring/overview/","text":"There are two preferred methods of overriding the defaults for configuring a MDM instance using docker-compose.yml . Warning This replaces all of the previous releases environment variables setting in docker-compose.yml . Information Each plugin may supply some mandatory fields which need to be overridden. Please see each section to identify which fields these may be. Grails and Spring supply ample documentation for all their standard properties which can all also be overridden via the build or runtime files. Our documentation is intended to supply information for our own plugins or properties which we require to be overridden. The preference order (by MDM) for loaded sources of properties is: Environment Variables . runtime.yml - See the default included here . build.yml - See the default included here . application.yml - See the default included here . plugin.yml - there are multiple versions of these as each plugin we build may supply their own. Environment Variables \u00b6 Any grails configuration property found in any of the yml files can be overridden through environment variables. They simply need to be provided in the \"dot notation\" form or (more appropriately) the \"uppercase underscore separated\" form rather than the \"YML new line\" format. e.g. application.yml 1 2 database : host : localhost would be overridden by docker-compose.yml 1 2 3 4 services : mauro-data-mapper : environment : database.host : another-host or 1 2 3 4 services : mauro-data-mapper : environment : DATABASE_HOST : another-host build.yml File \u00b6 The build.yml file is built into the MDM service when the image is built and is a standard grails configuration file. Therefore, any properties which can be safely set at build time for the image should be set into this file. This includes any properties which may be shared between multiple instances of MDM which all start from the same image. Our recommendation is that if only running 1 instance of MDM from 1 cloned repository then you should load all your properties into the build.yml file. For this reason, we have supplied the build.yml file with all the properties which we either require to be overridden or expect may want to be overridden. runtime.yml File \u00b6 The runtime.yml file will be loaded into the container via the docker-compose.yml file. This is intended as the replacement for environment variable overrides, where each running container might have specifically set properties which differ from a common shared image. Danger Do not change the environment variable runtime.config.path , as this denotes the path inside the container where the config file will be found. If you wish to load a different runtime.yml file then you should alter the volumes mapping in the docker-compose.yml file. Defining a different location for the runtime.yml to load \u00b6 If you wish to store your runtime.yml file in an alternative location to the folder inside the mdm-docker cloned repository, then you will need to alter the volumes mapping in the docker-compose.yml file. The line to change is: - ./mauro-data-mapper/config/runtime.yml:/usr/local/tomcat/conf/runtime.yml The first part of this is the local location (outside docker container) where the yml file can be found, you should not alter the second half (after the : ) as this is where the yml file is mounted into the container and where MDM expects to find it.","title":"Overview"},{"location":"installing/configuring/overview/#environment-variables","text":"Any grails configuration property found in any of the yml files can be overridden through environment variables. They simply need to be provided in the \"dot notation\" form or (more appropriately) the \"uppercase underscore separated\" form rather than the \"YML new line\" format. e.g. application.yml 1 2 database : host : localhost would be overridden by docker-compose.yml 1 2 3 4 services : mauro-data-mapper : environment : database.host : another-host or 1 2 3 4 services : mauro-data-mapper : environment : DATABASE_HOST : another-host","title":"Environment Variables"},{"location":"installing/configuring/overview/#buildyml-file","text":"The build.yml file is built into the MDM service when the image is built and is a standard grails configuration file. Therefore, any properties which can be safely set at build time for the image should be set into this file. This includes any properties which may be shared between multiple instances of MDM which all start from the same image. Our recommendation is that if only running 1 instance of MDM from 1 cloned repository then you should load all your properties into the build.yml file. For this reason, we have supplied the build.yml file with all the properties which we either require to be overridden or expect may want to be overridden.","title":"build.yml File"},{"location":"installing/configuring/overview/#runtimeyml-file","text":"The runtime.yml file will be loaded into the container via the docker-compose.yml file. This is intended as the replacement for environment variable overrides, where each running container might have specifically set properties which differ from a common shared image. Danger Do not change the environment variable runtime.config.path , as this denotes the path inside the container where the config file will be found. If you wish to load a different runtime.yml file then you should alter the volumes mapping in the docker-compose.yml file.","title":"runtime.yml File"},{"location":"installing/configuring/overview/#defining-a-different-location-for-the-runtimeyml-to-load","text":"If you wish to store your runtime.yml file in an alternative location to the folder inside the mdm-docker cloned repository, then you will need to alter the volumes mapping in the docker-compose.yml file. The line to change is: - ./mauro-data-mapper/config/runtime.yml:/usr/local/tomcat/conf/runtime.yml The first part of this is the local location (outside docker container) where the yml file can be found, you should not alter the second half (after the : ) as this is where the yml file is mounted into the container and where MDM expects to find it.","title":"Defining a different location for the runtime.yml to load"},{"location":"installing/configuring/runtime.yml/","text":"Below is the default application.yml file applied to the docker container at runtime. 1 # Nothing overridden by default at runtime","title":"Runtime YML"},{"location":"model/glossary/","text":"The Metadata Catalogue supports a variety of concepts which are not new, but are frequently referred to in different tools or applications by other names, or with different semantics. In this article we give definitions to the terms we use, and describe their intended usage. This page may also serve as a lightweight guide to the underlying data model - the UML diagrams provide guidance to those wishing to understand the underlying data structures and relationships. Data Models \u00b6 A Data Model is a description of an existing collection of data, or the specification of data that is to be collected. We use the same notation for both types, so that they may easily be compared or linked, and we annotate the Data Model to describe them as either a Data Asset or a Data Standard respectively. A Data Model has a label, which is used to uniquely identify it within the catalogue. It may also have a description, a number of alternate names (aliases), an author, and an organisation. A Data Model contains a number of Data Classes : groupings or collections of data points that share some common context: for example appearing in the same table of a database, or the same section in a form. A data class has a name, a description, some aliases, and may contain further (sub-) data classes. It has a maximum and minimum multiplicity, determining how many times data in this class may appear. For example, optional data may have a minimum multiplicity of 0 and a maximum multiplicity of 1; mandatory data may have a minimum multiplicity of 1; data which may occur any number of times is given a multiplicity of '*' (represented as -1 internally). The name of a data class is unique within its context (parent Data Model, or Data Class). A data class contains a number of Data Elements : the description of an individual field, variable, column, or property. A data element has a label, which must be unique within its containing Data Class. It has a description, some alternate names, and a multiplicity. The range of possible values that it may take are described within its data type: a reference to a Data Type stored within this model. A Data Type may be one of three sorts: a Primitive Type such as a String, a Date or an Integer. an Enumerated Type : a constrained set of values such as you might see for a gender or an ethnicity. Each Enumeration Type defines a number of Enumeration Values : a coded key, and a human-readable value. a Reference Type referring to another class within the same model - used to describe relationships between Data Classes within a model. Terminologies \u00b6 A Terminology can represent a complex ontology, a structured collection of enumerated values, or something inbetween. A terminology is stored in the catalogue in a similar manner to a Data Model: it has a unique label, a description, an author, an organisation, and some alternate names. A Terminology contains a number of Terms which themselves have a code, a human-readable definition, a URL which can be used to point to a definitive definition, and a description, which may be more verbose than the stated definition. Pairs of terms inside a terminology may be related: within the Terminology we store a number of Term Relationships - each of which has a source and target term. The Term Relationship is annotated with the Relationship Type - one of a set of types which are defined separately for each terminology. Codesets \u00b6 A CodeSet is a named collection of terms which may be pulled from multiple terminologies. It has similar fields to a Data Model: A label (which must be unique within the system), a description, an author, organisation, and alternate names. Folders \u00b6 A Folder is the principle mechanism for classifying and organising structures within the Metadata Catalogue. Folders may contain sub-folders, and each folder may contain Data Models, Terminologies or Code Sets. A folder has a label, which must be unique within its container (parent folder, or within the top-level of the folder hierarchy). Data Flows \u00b6 Currently undocumented Additional features \u00b6 All items in the Metadata Catalogue conform to a simple notion of Catalogue Item which has additional properties. These are outlined in the following subsections. Metadata / Properties \u00b6 Each Catalogue Item holds an extensible list of Properties (historically called Metadata), which can store arbitrary additional information about the Catalogue Item. Each property is identified by a namespace, and a key, and stored a value. Attachments \u00b6 Each Catalogue Item can also hold Attachments : files relating to the item in question - potentially providing additional documentation or context. Each attachment has a title, a description, and the file itself. Comments \u00b6 Any Catalogue Item may have Comments attached. Each comment has a title and some text, and these comments may themselves have further comments (replies in the sense of a forum or messageboard). Classifiers \u00b6 Classifiers provide a means for further classifying or tagging items within the catalogue. Classifiers contain a label and a description, and may hold further sub-classifiers. These classifiers may be managed separately by users, having their own read/write permissions. Semantic Links \u00b6 Currently undocumented Summary Metadata \u00b6 Currently undocumented","title":"Glossary"},{"location":"model/glossary/#data-models","text":"A Data Model is a description of an existing collection of data, or the specification of data that is to be collected. We use the same notation for both types, so that they may easily be compared or linked, and we annotate the Data Model to describe them as either a Data Asset or a Data Standard respectively. A Data Model has a label, which is used to uniquely identify it within the catalogue. It may also have a description, a number of alternate names (aliases), an author, and an organisation. A Data Model contains a number of Data Classes : groupings or collections of data points that share some common context: for example appearing in the same table of a database, or the same section in a form. A data class has a name, a description, some aliases, and may contain further (sub-) data classes. It has a maximum and minimum multiplicity, determining how many times data in this class may appear. For example, optional data may have a minimum multiplicity of 0 and a maximum multiplicity of 1; mandatory data may have a minimum multiplicity of 1; data which may occur any number of times is given a multiplicity of '*' (represented as -1 internally). The name of a data class is unique within its context (parent Data Model, or Data Class). A data class contains a number of Data Elements : the description of an individual field, variable, column, or property. A data element has a label, which must be unique within its containing Data Class. It has a description, some alternate names, and a multiplicity. The range of possible values that it may take are described within its data type: a reference to a Data Type stored within this model. A Data Type may be one of three sorts: a Primitive Type such as a String, a Date or an Integer. an Enumerated Type : a constrained set of values such as you might see for a gender or an ethnicity. Each Enumeration Type defines a number of Enumeration Values : a coded key, and a human-readable value. a Reference Type referring to another class within the same model - used to describe relationships between Data Classes within a model.","title":"Data Models"},{"location":"model/glossary/#terminologies","text":"A Terminology can represent a complex ontology, a structured collection of enumerated values, or something inbetween. A terminology is stored in the catalogue in a similar manner to a Data Model: it has a unique label, a description, an author, an organisation, and some alternate names. A Terminology contains a number of Terms which themselves have a code, a human-readable definition, a URL which can be used to point to a definitive definition, and a description, which may be more verbose than the stated definition. Pairs of terms inside a terminology may be related: within the Terminology we store a number of Term Relationships - each of which has a source and target term. The Term Relationship is annotated with the Relationship Type - one of a set of types which are defined separately for each terminology.","title":"Terminologies"},{"location":"model/glossary/#codesets","text":"A CodeSet is a named collection of terms which may be pulled from multiple terminologies. It has similar fields to a Data Model: A label (which must be unique within the system), a description, an author, organisation, and alternate names.","title":"Codesets"},{"location":"model/glossary/#folders","text":"A Folder is the principle mechanism for classifying and organising structures within the Metadata Catalogue. Folders may contain sub-folders, and each folder may contain Data Models, Terminologies or Code Sets. A folder has a label, which must be unique within its container (parent folder, or within the top-level of the folder hierarchy).","title":"Folders"},{"location":"model/glossary/#data-flows","text":"Currently undocumented","title":"Data Flows"},{"location":"model/glossary/#additional-features","text":"All items in the Metadata Catalogue conform to a simple notion of Catalogue Item which has additional properties. These are outlined in the following subsections.","title":"Additional features"},{"location":"model/glossary/#metadata-properties","text":"Each Catalogue Item holds an extensible list of Properties (historically called Metadata), which can store arbitrary additional information about the Catalogue Item. Each property is identified by a namespace, and a key, and stored a value.","title":"Metadata / Properties"},{"location":"model/glossary/#attachments","text":"Each Catalogue Item can also hold Attachments : files relating to the item in question - potentially providing additional documentation or context. Each attachment has a title, a description, and the file itself.","title":"Attachments"},{"location":"model/glossary/#comments","text":"Any Catalogue Item may have Comments attached. Each comment has a title and some text, and these comments may themselves have further comments (replies in the sense of a forum or messageboard).","title":"Comments"},{"location":"model/glossary/#classifiers","text":"Classifiers provide a means for further classifying or tagging items within the catalogue. Classifiers contain a label and a description, and may hold further sub-classifiers. These classifiers may be managed separately by users, having their own read/write permissions.","title":"Classifiers"},{"location":"model/glossary/#semantic-links","text":"Currently undocumented","title":"Semantic Links"},{"location":"model/glossary/#summary-metadata","text":"Currently undocumented","title":"Summary Metadata"},{"location":"plugins/configuring/doi/","text":"The following properties can be defined in the config yaml files or set using the RESTful API or the web UI. Information Once MDM has started up the only way to change the properties is via the RESTFful API/Web UI. See the DOI Userguide for more information. Warning The following property must also be set via the RESTful API or the web UI for the DOI plugin to work: site.url . Available Properties \u00b6 maurodatamapper.digitalobjectidentifiers.username The username used to authenticate with when communicating with the DOI registry. maurodatamapper.digitalobjectidentifiers.password The password used to authenticate with when communicating with the DOI registry. maurodatamapper.digitalobjectidentifiers.endpoint The full HTTP web address for the DOI reigstry. e.g. https://api.test.datacite.org . maurodatamapper.digitalobjectidentifiers.prefix The assigned DOI prefix to be used for all registered resources. e.g. 10.80079 .","title":"Digital Object Identifiers"},{"location":"plugins/configuring/doi/#available-properties","text":"maurodatamapper.digitalobjectidentifiers.username The username used to authenticate with when communicating with the DOI registry. maurodatamapper.digitalobjectidentifiers.password The password used to authenticate with when communicating with the DOI registry. maurodatamapper.digitalobjectidentifiers.endpoint The full HTTP web address for the DOI reigstry. e.g. https://api.test.datacite.org . maurodatamapper.digitalobjectidentifiers.prefix The assigned DOI prefix to be used for all registered resources. e.g. 10.80079 .","title":"Available Properties"},{"location":"plugins/configuring/fhir/","text":"The configurable YAML properties for the FHIR plugin are for the handling of the micronaut connections between the FHIR server and MDM. Defaults are provided via the plugin.yml and it is unlikely you need to override them. These are micronaut properties and as such please see the micronaut documentation to understand what they are and how they affect the system. Defaults provided in the plugin.yml \u00b6 1 2 3 4 5 6 7 8 9 micronaut : codec : json : additional-types : - 'application/json+fhir;charset=utf-8' http : client : max-content-length : 33554430 read-timeout : PT30S","title":"FHIR Importer"},{"location":"plugins/configuring/fhir/#defaults-provided-in-the-pluginyml","text":"1 2 3 4 5 6 7 8 9 micronaut : codec : json : additional-types : - 'application/json+fhir;charset=utf-8' http : client : max-content-length : 33554430 read-timeout : PT30S","title":"Defaults provided in the plugin.yml"},{"location":"plugins/configuring/oic/","text":"OpenID Connect has a range of properties which can be set in the yaml files. Some of these take effect each time you start MDM, some are only used once and if they've been enabled before then they will be ignored on subsequent startups. We supply defaults to ensure basic functionality however these can be overridden using the yaml files. We store the basic information to bootstrap certain providers and set them up without any interaction through the admin interface/RESTful API. The providers are currently Google Microsoft (via microsoftonline) KeyCloak Control Properties \u00b6 maurodatamapper.openidConnect.session.timeout This is the length of time which sessions will be kept active before being timed out, the default is 24 hours this overrides the usual session timeout of 30 minutes which is set for all non-OIC authenticated users. We do not recommend setting this lower than 24h as this is the timeout after which users will be required to log back in via the OIC window. Inside this timeout the API will keep the session alive connecting to the OIC provider as necessary to refresh the user's token. Bootstrapped Providers \u00b6 Each of the providers has pre-configured defaults which are used to add them automatically if they are enabled. These defaults (provided in each section for reference only) can be changed once the system has started up by using the admin interface/RESTful API, see the OIC Userguide for more information. Google \u00b6 maurodatamapper.openidConnect.google.enabled Defaults to false. If enabled then the other google properties will need to be provided. maurodatamapper.openidConnect.google.clientId The client id used to identify and authenticate the MDM service. maurodatamapper.openidConnect.google.clientSecret The client id used to identify and authenticate the MDM service. discoveryDocumentUrl : https://accounts.google.com/.well-known/openid-configuration imageUrl : https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg Microsoft \u00b6 Information Post-processing occurs with Microsoft to replace the {tenant} in the issuerUrl provided by the discoveryDocumentUrl with the clientId . If manually configuring a Microsoft provider you may have to manually edit the discovery document via the admin interface to perform the same. maurodatamapper.openidConnect.microsoft.enabled Defaults to false. If enabled then the other google properties will need to be provided. maurodatamapper.openidConnect.microsoft.tenantId The Directory (tenant) id assigned to the Azure AD maurodatamapper.openidConnect.microsoft.accountId The account id or type to use for this app. This is either going to be something like the tenant id, \"organizations\" or \"common\", it will be decided by the Authentication -> Supported Account Types. It will be used to build the discovery document URL. If unsure what to use, click the \"Endpoints\" button in the Azure overview and examine the \"OpenID Connect metadata document\" endpoint, this field should be whatever is in the URL between login.microsoftonline.com/ and /v2.0 . maurodatamapper.openidConnect.microsoft.clientId The client id used to identify and authenticate the MDM service. maurodatamapper.openidConnect.microsoft.clientSecret The client id used to identify and authenticate the MDM service. discoveryDocumentUrl : https://login.microsoftonline.com/${accountId}/v2.0/.well-known/openid-configuration imageUrl : https://upload.wikimedia.org/wikipedia/commons/9/98/Microsoft_logo.jpg KeyCloak \u00b6 Information The baseUrl and realm fields are used to build the discoveryDocumentUrl for the boostrapped provider. If creating a Keycloak provider via the Admin interface/RESTful API these properties are not needed as you can define the full URL to the discovery document maurodatamapper.openidConnect.keycloak.enabled Defaults to false. If enabled then the other google properties will need to be provided. maurodatamapper.openidConnect.keycloak.baseUrl The full URL where the keycloak provider can be found. maurodatamapper.openidConnect.keycloak.realm The realm configured inside the keycloak provider for the MDM service to use. maurodatamapper.openidConnect.keycloak.clientId The client id used to identify and authenticate the MDM service. maurodatamapper.openidConnect.keycloak.clientSecret The client id used to identify and authenticate the MDM service. discoveryDocumentUrl : ${baseUrl}/realms/${realm}/.well-known/openid-configuration imageUrl : https://upload.wikimedia.org/wikipedia/commons/2/29/Keycloak_Logo.png Defaults provided in the plugin.yml \u00b6 1 2 3 4 5 6 7 8 9 10 11 maurodatamapper : openidConnect : session : timeout : 24h google : enabled : false microsoft : enabled : false tenant : common keycloak : enabled : false","title":"OpenID Connect"},{"location":"plugins/configuring/oic/#control-properties","text":"maurodatamapper.openidConnect.session.timeout This is the length of time which sessions will be kept active before being timed out, the default is 24 hours this overrides the usual session timeout of 30 minutes which is set for all non-OIC authenticated users. We do not recommend setting this lower than 24h as this is the timeout after which users will be required to log back in via the OIC window. Inside this timeout the API will keep the session alive connecting to the OIC provider as necessary to refresh the user's token.","title":"Control Properties"},{"location":"plugins/configuring/oic/#bootstrapped-providers","text":"Each of the providers has pre-configured defaults which are used to add them automatically if they are enabled. These defaults (provided in each section for reference only) can be changed once the system has started up by using the admin interface/RESTful API, see the OIC Userguide for more information.","title":"Bootstrapped Providers"},{"location":"plugins/configuring/oic/#google","text":"maurodatamapper.openidConnect.google.enabled Defaults to false. If enabled then the other google properties will need to be provided. maurodatamapper.openidConnect.google.clientId The client id used to identify and authenticate the MDM service. maurodatamapper.openidConnect.google.clientSecret The client id used to identify and authenticate the MDM service. discoveryDocumentUrl : https://accounts.google.com/.well-known/openid-configuration imageUrl : https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg","title":"Google"},{"location":"plugins/configuring/oic/#microsoft","text":"Information Post-processing occurs with Microsoft to replace the {tenant} in the issuerUrl provided by the discoveryDocumentUrl with the clientId . If manually configuring a Microsoft provider you may have to manually edit the discovery document via the admin interface to perform the same. maurodatamapper.openidConnect.microsoft.enabled Defaults to false. If enabled then the other google properties will need to be provided. maurodatamapper.openidConnect.microsoft.tenantId The Directory (tenant) id assigned to the Azure AD maurodatamapper.openidConnect.microsoft.accountId The account id or type to use for this app. This is either going to be something like the tenant id, \"organizations\" or \"common\", it will be decided by the Authentication -> Supported Account Types. It will be used to build the discovery document URL. If unsure what to use, click the \"Endpoints\" button in the Azure overview and examine the \"OpenID Connect metadata document\" endpoint, this field should be whatever is in the URL between login.microsoftonline.com/ and /v2.0 . maurodatamapper.openidConnect.microsoft.clientId The client id used to identify and authenticate the MDM service. maurodatamapper.openidConnect.microsoft.clientSecret The client id used to identify and authenticate the MDM service. discoveryDocumentUrl : https://login.microsoftonline.com/${accountId}/v2.0/.well-known/openid-configuration imageUrl : https://upload.wikimedia.org/wikipedia/commons/9/98/Microsoft_logo.jpg","title":"Microsoft"},{"location":"plugins/configuring/oic/#keycloak","text":"Information The baseUrl and realm fields are used to build the discoveryDocumentUrl for the boostrapped provider. If creating a Keycloak provider via the Admin interface/RESTful API these properties are not needed as you can define the full URL to the discovery document maurodatamapper.openidConnect.keycloak.enabled Defaults to false. If enabled then the other google properties will need to be provided. maurodatamapper.openidConnect.keycloak.baseUrl The full URL where the keycloak provider can be found. maurodatamapper.openidConnect.keycloak.realm The realm configured inside the keycloak provider for the MDM service to use. maurodatamapper.openidConnect.keycloak.clientId The client id used to identify and authenticate the MDM service. maurodatamapper.openidConnect.keycloak.clientSecret The client id used to identify and authenticate the MDM service. discoveryDocumentUrl : ${baseUrl}/realms/${realm}/.well-known/openid-configuration imageUrl : https://upload.wikimedia.org/wikipedia/commons/2/29/Keycloak_Logo.png","title":"KeyCloak"},{"location":"plugins/configuring/oic/#defaults-provided-in-the-pluginyml","text":"1 2 3 4 5 6 7 8 9 10 11 maurodatamapper : openidConnect : session : timeout : 24h google : enabled : false microsoft : enabled : false tenant : common keycloak : enabled : false","title":"Defaults provided in the plugin.yml"},{"location":"plugins/rest-api/doi/","text":"Introduction \u00b6 The Digital Object Identifier (DOI) plugin provides additional API endpoints that allow the integration of any Mauro multi-facet aware catalogue item to be recorded in a DOI system; this then allows unique, permanent identifiers to those catalogue items that can then be distributed across documents or articles over the web to act as links back to the Mauro content. The DOI system always ensures that a DOI name will always refer to a Mauro catalogue item as long as the DOI name is known. The plugin uses DataCite as the repository of Mauro DOI names; more information can be found at the DataCite website. In order to use Digital Object Identifiers, the Mauro instance must: Have the Mauro Digital Object Identifier plugin installed. Set up and configure DataCite to acts as the DOI system. The full details of this set up can be viewed in the user guide Using Digital Object Identifiers . Profile \u00b6 Profile summary \u00b6 The plugin automatically exposes a profile called Digital Object Identifiers DataCite Dataset Schema . Depending on whether the profile is used or unused on a catalogue item, the summary of the profile can be found with one of these endpoints: /api/ {multiFacetAwareDomainType} / {id} /profiles/used /api/ {multiFacetAwareDomainType} / {id} /profiles/unused Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 [ { \"name\" : \"DigitalObjectIdentifiersProfileProviderService\" , \"version\" : \"1.1.0\" , \"displayName\" : \"Digital Object Identifiers DataCite Dataset Schema\" , \"namespace\" : \"uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile\" , \"allowsExtraMetadataKeys\" : false , \"knownMetadataKeys\" : [ \"identifier\" , \"prefix\" , \"suffix\" , \"status\" , \"state\" , \"titles/mainTitle\" , \"descriptions/mainDescription\" , \"version\" , \"creators/creator/creatorName\" , \"creators/creator/creatorNameType\" , \"creators/creator/givenName\" , \"creators/creator/familyName\" , \"creators/creator/nameIdentifier\" , \"creators/creator/affiliation\" , \"publisher\" , \"publicationYear\" , \"resourceType\" , \"titles/title\" , \"titles/titleType\" , \"descriptions/description\" , \"descriptions/descriptionType\" , \"contributors/contributor/contributorName\" , \"contributors/contributor/contributorNameType\" , \"contributors/contributor/contributorType\" , \"contributors/contributor/givenName\" , \"contributors/contributor/familyName\" , \"contributors/contributor/nameIdentifier\" , \"contributors/contributor/affiliation\" , \"language\" ], \"providerType\" : \"Profile\" , \"metadataNamespace\" : \"org.datacite\" , \"domains\" : [ \"VersionedFolder\" , \"Folder\" , \"Classifier\" , \"ReferenceType\" , \"PrimitiveType\" , \"ModelDataType\" , \"EnumerationType\" , \"EnumerationValue\" , \"DataElement\" , \"DataClass\" , \"DataModel\" , \"DataFlow\" , \"DataElementComponent\" , \"DataClassComponent\" , \"Terminology\" , \"TermRelationshipType\" , \"TermRelationship\" , \"Term\" , \"CodeSet\" , \"ReferenceDataModel\" , \"ReferenceDataElement\" , \"ReferencePrimitiveType\" , \"ReferenceEnumerationType\" , \"ReferenceEnumerationValue\" ], \"editableAfterFinalisation\" : true } ] Get the profile \u00b6 To get the profile, use the usual profiles endpoint: /api/ {multiFacetAwareDomainType} / {id} /profiles/ {namespace} / {name} For the case of the DOI profile: The {namespace} will be uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile . The {name} will be DigitalObjectIdentifiersProfileProviderService . Information The GET endpoint will always return the profile data, even if the profile has not yet been assigned to the catalogue item. In the case where it has not been assigned yet, an empty profile structure will be returned containing all the fields to prepare. Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 { \"sections\" : [ { \"name\" : \"Predefined/Supplied Fields\" , \"description\" : \"Fixed fields which cannot be changed.\" , \"fields\" : [ { \"fieldName\" : \"Identifier\" , \"metadataPropertyName\" : \"identifier\" , \"description\" : \"A persistent identifier that identifies a resource. This will be filled in by the API upon submission\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"10.80079/ynk3-sz81\" }, { \"fieldName\" : \"Prefix\" , \"metadataPropertyName\" : \"prefix\" , \"description\" : \"DOI prefix. The first part of the identifier\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"10.80079\" }, { \"fieldName\" : \"Suffix\" , \"metadataPropertyName\" : \"suffix\" , \"description\" : \"DOI suffix. The last part of the identifier\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"ynk3-sz81\" }, { \"fieldName\" : \"Status\" , \"metadataPropertyName\" : \"status\" , \"description\" : \"Status of DOI: draft, final, retired, not submitted.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"retired\" }, { \"fieldName\" : \"State\" , \"metadataPropertyName\" : \"state\" , \"description\" : \"State of DOI inside DataCite: draft, findable, registered (Registered indicates a retired DOI).\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"registered\" }, { \"fieldName\" : \"Main Title\" , \"metadataPropertyName\" : \"titles/mainTitle\" , \"description\" : \"The main title by which the resource is known, derived from the label field of the resource.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"text\" , \"derived\" : true , \"derivedFrom\" : \"label\" , \"uneditable\" : true , \"currentValue\" : \"DOI Test Model 5\" }, { \"fieldName\" : \"Main Description\" , \"metadataPropertyName\" : \"descriptions/mainDescription\" , \"description\" : \"The main description for the resource, derived from the description field of the resource.\" , \"maxMultiplicity\" : -1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"text\" , \"derived\" : true , \"derivedFrom\" : \"description\" , \"uneditable\" : true , \"currentValue\" : \"\" }, { \"fieldName\" : \"Version\" , \"metadataPropertyName\" : \"version\" , \"description\" : \"Version number of the resource. If the primary resource has changed the version number increases. Register a new identifier for a major version change. Individual stewards need to determine which are major vs. minor versions. May be used in conjunction with properties 11 and 12 (AlternateIdentifier and RelatedIdentifier) to indicate various information updates. May be used in conjunction with property 17 (Description) to indicate the nature and file/record range of version.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : true , \"derivedFrom\" : \"modelVersion\" , \"uneditable\" : true , \"currentValue\" : \"1.0.0\" } ] }, { \"name\" : \"Primary Creator\" , \"description\" : \"Resource Creator. This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\" , \"fields\" : [ { \"fieldName\" : \"Name\" , \"metadataPropertyName\" : \"creators/creator/creatorName\" , \"description\" : \"The main researchers involved working on the data, or the authors of the publication in priority order. May be a corporate/institutional or personal name.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"Peter Monks\" }, { \"fieldName\" : \"Name Type\" , \"metadataPropertyName\" : \"creators/creator/creatorNameType\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"Organizational\" , \"Personal\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"Personal\" }, { \"fieldName\" : \"Given Name\" , \"metadataPropertyName\" : \"creators/creator/givenName\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Family Name\" , \"metadataPropertyName\" : \"creators/creator/familyName\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Identifier\" , \"metadataPropertyName\" : \"creators/creator/nameIdentifier\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Affiliation\" , \"metadataPropertyName\" : \"creators/creator/affiliation\" , \"description\" : \"Affiliation of creator, company or institution they represent\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] }, { \"name\" : \"Additional Mandatory Fields\" , \"description\" : null , \"fields\" : [ { \"fieldName\" : \"Publisher\" , \"metadataPropertyName\" : \"publisher\" , \"description\" : \"The name of the entity that holds, archives, publishes prints, distributes, releases, issues, or produces the resource. This property will be used to formulate the citation, so consider the prominence of the role. For software, use Publisher for the code repository. If there is an entity other than a code repository, that 'holds, archives, publishes, prints, distributes, releases, issues, or produces' the code, use the property Contributor/contributorType/hostingInstitution for the code repository.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"OCC\" }, { \"fieldName\" : \"Publication Year\" , \"metadataPropertyName\" : \"publicationYear\" , \"description\" : \"The year when the data was or will be made publicly available. If an embargo period has been in effect, use the date when the embargo period ends.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : \"\\\\d{4}\" , \"dataType\" : \"int\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"2021\" }, { \"fieldName\" : \"Resource Type\" , \"metadataPropertyName\" : \"resourceType\" , \"description\" : \"The type of a resource\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : [ \"Audiovisual\" , \"Book\" , \"BookChapter\" , \"Collection\" , \"ComputationalNotebook\" , \"ConferencePaper\" , \"ConferenceProceeding\" , \"DataPaper\" , \"Dataset\" , \"Dissertation\" , \"Event\" , \"Image\" , \"InteractiveResource\" , \"Journal\" , \"JournalArticle\" , \"Model\" , \"OutputManagementPlan\" , \"PeerReview\" , \"PhysicalObject\" , \"Preprint\" , \"Report\" , \"Service\" , \"Software\" , \"Sound\" , \"Standard\" , \"Text\" , \"Workflow\" , \"Other\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"Collection\" } ] }, { \"name\" : \"Additional Optional Title Section\" , \"description\" : \"This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\" , \"fields\" : [ { \"fieldName\" : \"Title\" , \"metadataPropertyName\" : \"titles/title\" , \"description\" : \"A name or title by which a resource is known.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Title Type\" , \"metadataPropertyName\" : \"titles/titleType\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"AlternativeTitle\" , \"Subtitle\" , \"TranslatedTitle\" , \"Other\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] }, { \"name\" : \"Additional Optional Description Section\" , \"description\" : \"This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\" , \"fields\" : [ { \"fieldName\" : \"Description\" , \"metadataPropertyName\" : \"descriptions/description\" , \"description\" : \"All additional information that does not fit in any of the other categories. May be used for technical information.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"text\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Description Type\" , \"metadataPropertyName\" : \"descriptions/descriptionType\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"Abstract\" , \"Methods\" , \"SeriesInformation\" , \"TableOfContents\" , \"TechnicalInfo\" , \"Other\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] }, { \"name\" : \"Primary Contributor\" , \"description\" : \"Resource Contributor. This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\" , \"fields\" : [ { \"fieldName\" : \"Name\" , \"metadataPropertyName\" : \"contributors/contributor/contributorName\" , \"description\" : \"The institution or person responsible for collecting, creating, or otherwise contributing to the development of the dataset.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Name Type\" , \"metadataPropertyName\" : \"contributors/contributor/contributorNameType\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"Organizational\" , \"Personal\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Contributor Type\" , \"metadataPropertyName\" : \"contributors/contributor/contributorType\" , \"description\" : \"Mandatory if the contributor name is provided.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"ContactPerson\" , \"DataCollector\" , \"DataCurator\" , \"DataManager\" , \"Distributor\" , \"Editor\" , \"HostingInstitution\" , \"Other\" , \"Producer\" , \"ProjectLeader\" , \"ProjectManager\" , \"ProjectMember\" , \"RegistrationAgency\" , \"RegistrationAuthority\" , \"RelatedPerson\" , \"ResearchGroup\" , \"RightsHolder\" , \"Researcher\" , \"Sponsor\" , \"Supervisor\" , \"WorkPackageLeader\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Given Name\" , \"metadataPropertyName\" : \"contributors/contributor/givenName\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Family Name\" , \"metadataPropertyName\" : \"contributors/contributor/familyName\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Identifier\" , \"metadataPropertyName\" : \"contributors/contributor/nameIdentifier\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Affiliation\" , \"metadataPropertyName\" : \"contributors/contributor/affiliation\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] }, { \"name\" : \"Additional Optional Fields\" , \"description\" : \"Optional metadata fields for DOI profiles.\" , \"fields\" : [ { \"fieldName\" : \"Language\" , \"metadataPropertyName\" : \"language\" , \"description\" : \"Primary language of the resource. Allowed values are taken from IETF BCP 47, ISO 639-1 language codes. For English, use 'en'.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] } ], \"id\" : \"36e74acc-a862-43d0-8e2b-e54422c4239f\" , \"label\" : \"DOI Test Model 5\" , \"domainType\" : \"DataModel\" } Save the profile \u00b6 Using the JSON structure from the GET endpoint above and entering values into the sections/fields/currentValue JSON properties, you can save the changes made to the profile, and attach the profile to the catalogue item at the same time, like so. /api/ {multiFacetAwareDomainType} / {id} /profiles/ {namespace} / {name} Where: The {namespace} will be uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile . The {name} will be DigitalObjectIdentifiersProfileProviderService . If successful, this will return the same JSON response as the GET endpoint above. Remove the profile \u00b6 To remove the DOI profile from the catalogue item: /api/ {multiFacetAwareDomainType} / {id} /profiles/ {namespace} / {name} Where: The {namespace} will be uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile . The {name} will be DigitalObjectIdentifiersProfileProviderService . DOI Management \u00b6 The profile endpoints above are standard endpoints used in Mauro. This section covers the custom endpoints added by the DOI plugin. Get DOI status \u00b6 For any catalogue item, the current status of the DOI name on that item can be queried as follows: /api/ {multiFacetAwareDomainType} / {id} /doi If a status exists, a 200 OK response will be returned with this response body: Response body (JSON) 1 2 3 4 { \"identifier\" : \"10.1109/5.771073\" , \"status\" : \"final\" } The fields are as follows: identifier (String): The Digital Object Identifier (DOI) name assigned to this catalogue item as generated by the DOI system. If a DOI has not been generated yet, this will not be provided. state (Constant): The current DOI state of this catalogue item, which can one of: not submitted - no DOI has been requested yet. draft - a draft DOI has been assigned, allowing further changes to the profile. final - a DOI has been assigned and locked, finalising this catalogue item's profile. retired - a previous DOI has been retired and can no longer be used for cross-reference. Submit DOI status \u00b6 Submitting the catalogue item and its DOI profile can only be done for publicly readable and finalised catalogue items. This is to ensure that: Every citation has access to the catalogue item. The catalogue item cannot be modified any further, keeping the citation the same for all future uses. Information It may be possible to not have assigned the Digital Object Identifiers DataCite Dataset Schema to a catalogue item before it was finalised. To solve this issue, the DOI profile has special permissions to allow it to be edited post-finalisation, so long as the DOI is not yet in the Final state. Therefore, you may use the POST /api/{multiFacetAwareDomainType}/{id}/profiles/{namespace}/{name} endpoint as many times as necessary to make alterations to the profile before continuing further. To submit the catalogue item's profile to the DOI system to obtain a DOI name, use this endpoint. /api/ {multiFacetAwareDomainType} / {id} /doi?submissionType={type} This endpoint requires no request body; the profile should already have been saved before this point. The submissionType can be one of the following: draft - submit a draft profile to obtain the DOI name. finalise - submit a final profile to obtain the DOI name. retire - retire an existing DOI name to no longer be in active use. The response returned will be the contents of the catalogue item the DOI was for. The plugin will also automatically populate the DOI profile identifier metadata key for the catalogue item. Remove DOI \u00b6 To remove all DOI metadata: /api/ {multiFacetAwareDomainType} / {id} /doi DOI Resolution \u00b6 The identifier in the DOI profile will hold the unique identifier as registered in the DOI system, for example 10.1109/5.771073 . The DOI system has a guaranteed method of locating every registered DOI name through a non-changing URL; this will actually redirect back to your client URL by providing the DOI name in the URL - in Mauro, this would be similar to https://{domain}/#/doi/10.1109/5.771073 To actually locate the catalogue item assigned to this DOI name, this endpoint will fetch the Mauro catalogue item to which the DOI name is mapped. /api/doi/ {identifier} The JSON response will contain the full catalogue item details for that DOI name. See the REST API for the exact details returned per catalogue item domain type.","title":"Digital Object Identifier (DOI)"},{"location":"plugins/rest-api/doi/#introduction","text":"The Digital Object Identifier (DOI) plugin provides additional API endpoints that allow the integration of any Mauro multi-facet aware catalogue item to be recorded in a DOI system; this then allows unique, permanent identifiers to those catalogue items that can then be distributed across documents or articles over the web to act as links back to the Mauro content. The DOI system always ensures that a DOI name will always refer to a Mauro catalogue item as long as the DOI name is known. The plugin uses DataCite as the repository of Mauro DOI names; more information can be found at the DataCite website. In order to use Digital Object Identifiers, the Mauro instance must: Have the Mauro Digital Object Identifier plugin installed. Set up and configure DataCite to acts as the DOI system. The full details of this set up can be viewed in the user guide Using Digital Object Identifiers .","title":"Introduction"},{"location":"plugins/rest-api/doi/#profile","text":"","title":"Profile"},{"location":"plugins/rest-api/doi/#profile-summary","text":"The plugin automatically exposes a profile called Digital Object Identifiers DataCite Dataset Schema . Depending on whether the profile is used or unused on a catalogue item, the summary of the profile can be found with one of these endpoints: /api/ {multiFacetAwareDomainType} / {id} /profiles/used /api/ {multiFacetAwareDomainType} / {id} /profiles/unused Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 [ { \"name\" : \"DigitalObjectIdentifiersProfileProviderService\" , \"version\" : \"1.1.0\" , \"displayName\" : \"Digital Object Identifiers DataCite Dataset Schema\" , \"namespace\" : \"uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile\" , \"allowsExtraMetadataKeys\" : false , \"knownMetadataKeys\" : [ \"identifier\" , \"prefix\" , \"suffix\" , \"status\" , \"state\" , \"titles/mainTitle\" , \"descriptions/mainDescription\" , \"version\" , \"creators/creator/creatorName\" , \"creators/creator/creatorNameType\" , \"creators/creator/givenName\" , \"creators/creator/familyName\" , \"creators/creator/nameIdentifier\" , \"creators/creator/affiliation\" , \"publisher\" , \"publicationYear\" , \"resourceType\" , \"titles/title\" , \"titles/titleType\" , \"descriptions/description\" , \"descriptions/descriptionType\" , \"contributors/contributor/contributorName\" , \"contributors/contributor/contributorNameType\" , \"contributors/contributor/contributorType\" , \"contributors/contributor/givenName\" , \"contributors/contributor/familyName\" , \"contributors/contributor/nameIdentifier\" , \"contributors/contributor/affiliation\" , \"language\" ], \"providerType\" : \"Profile\" , \"metadataNamespace\" : \"org.datacite\" , \"domains\" : [ \"VersionedFolder\" , \"Folder\" , \"Classifier\" , \"ReferenceType\" , \"PrimitiveType\" , \"ModelDataType\" , \"EnumerationType\" , \"EnumerationValue\" , \"DataElement\" , \"DataClass\" , \"DataModel\" , \"DataFlow\" , \"DataElementComponent\" , \"DataClassComponent\" , \"Terminology\" , \"TermRelationshipType\" , \"TermRelationship\" , \"Term\" , \"CodeSet\" , \"ReferenceDataModel\" , \"ReferenceDataElement\" , \"ReferencePrimitiveType\" , \"ReferenceEnumerationType\" , \"ReferenceEnumerationValue\" ], \"editableAfterFinalisation\" : true } ]","title":"Profile summary"},{"location":"plugins/rest-api/doi/#get-the-profile","text":"To get the profile, use the usual profiles endpoint: /api/ {multiFacetAwareDomainType} / {id} /profiles/ {namespace} / {name} For the case of the DOI profile: The {namespace} will be uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile . The {name} will be DigitalObjectIdentifiersProfileProviderService . Information The GET endpoint will always return the profile data, even if the profile has not yet been assigned to the catalogue item. In the case where it has not been assigned yet, an empty profile structure will be returned containing all the fields to prepare. Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 { \"sections\" : [ { \"name\" : \"Predefined/Supplied Fields\" , \"description\" : \"Fixed fields which cannot be changed.\" , \"fields\" : [ { \"fieldName\" : \"Identifier\" , \"metadataPropertyName\" : \"identifier\" , \"description\" : \"A persistent identifier that identifies a resource. This will be filled in by the API upon submission\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"10.80079/ynk3-sz81\" }, { \"fieldName\" : \"Prefix\" , \"metadataPropertyName\" : \"prefix\" , \"description\" : \"DOI prefix. The first part of the identifier\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"10.80079\" }, { \"fieldName\" : \"Suffix\" , \"metadataPropertyName\" : \"suffix\" , \"description\" : \"DOI suffix. The last part of the identifier\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"ynk3-sz81\" }, { \"fieldName\" : \"Status\" , \"metadataPropertyName\" : \"status\" , \"description\" : \"Status of DOI: draft, final, retired, not submitted.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"retired\" }, { \"fieldName\" : \"State\" , \"metadataPropertyName\" : \"state\" , \"description\" : \"State of DOI inside DataCite: draft, findable, registered (Registered indicates a retired DOI).\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : true , \"currentValue\" : \"registered\" }, { \"fieldName\" : \"Main Title\" , \"metadataPropertyName\" : \"titles/mainTitle\" , \"description\" : \"The main title by which the resource is known, derived from the label field of the resource.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"text\" , \"derived\" : true , \"derivedFrom\" : \"label\" , \"uneditable\" : true , \"currentValue\" : \"DOI Test Model 5\" }, { \"fieldName\" : \"Main Description\" , \"metadataPropertyName\" : \"descriptions/mainDescription\" , \"description\" : \"The main description for the resource, derived from the description field of the resource.\" , \"maxMultiplicity\" : -1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"text\" , \"derived\" : true , \"derivedFrom\" : \"description\" , \"uneditable\" : true , \"currentValue\" : \"\" }, { \"fieldName\" : \"Version\" , \"metadataPropertyName\" : \"version\" , \"description\" : \"Version number of the resource. If the primary resource has changed the version number increases. Register a new identifier for a major version change. Individual stewards need to determine which are major vs. minor versions. May be used in conjunction with properties 11 and 12 (AlternateIdentifier and RelatedIdentifier) to indicate various information updates. May be used in conjunction with property 17 (Description) to indicate the nature and file/record range of version.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : true , \"derivedFrom\" : \"modelVersion\" , \"uneditable\" : true , \"currentValue\" : \"1.0.0\" } ] }, { \"name\" : \"Primary Creator\" , \"description\" : \"Resource Creator. This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\" , \"fields\" : [ { \"fieldName\" : \"Name\" , \"metadataPropertyName\" : \"creators/creator/creatorName\" , \"description\" : \"The main researchers involved working on the data, or the authors of the publication in priority order. May be a corporate/institutional or personal name.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"Peter Monks\" }, { \"fieldName\" : \"Name Type\" , \"metadataPropertyName\" : \"creators/creator/creatorNameType\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"Organizational\" , \"Personal\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"Personal\" }, { \"fieldName\" : \"Given Name\" , \"metadataPropertyName\" : \"creators/creator/givenName\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Family Name\" , \"metadataPropertyName\" : \"creators/creator/familyName\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Identifier\" , \"metadataPropertyName\" : \"creators/creator/nameIdentifier\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Affiliation\" , \"metadataPropertyName\" : \"creators/creator/affiliation\" , \"description\" : \"Affiliation of creator, company or institution they represent\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] }, { \"name\" : \"Additional Mandatory Fields\" , \"description\" : null , \"fields\" : [ { \"fieldName\" : \"Publisher\" , \"metadataPropertyName\" : \"publisher\" , \"description\" : \"The name of the entity that holds, archives, publishes prints, distributes, releases, issues, or produces the resource. This property will be used to formulate the citation, so consider the prominence of the role. For software, use Publisher for the code repository. If there is an entity other than a code repository, that 'holds, archives, publishes, prints, distributes, releases, issues, or produces' the code, use the property Contributor/contributorType/hostingInstitution for the code repository.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"OCC\" }, { \"fieldName\" : \"Publication Year\" , \"metadataPropertyName\" : \"publicationYear\" , \"description\" : \"The year when the data was or will be made publicly available. If an embargo period has been in effect, use the date when the embargo period ends.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : null , \"regularExpression\" : \"\\\\d{4}\" , \"dataType\" : \"int\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"2021\" }, { \"fieldName\" : \"Resource Type\" , \"metadataPropertyName\" : \"resourceType\" , \"description\" : \"The type of a resource\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 1 , \"allowedValues\" : [ \"Audiovisual\" , \"Book\" , \"BookChapter\" , \"Collection\" , \"ComputationalNotebook\" , \"ConferencePaper\" , \"ConferenceProceeding\" , \"DataPaper\" , \"Dataset\" , \"Dissertation\" , \"Event\" , \"Image\" , \"InteractiveResource\" , \"Journal\" , \"JournalArticle\" , \"Model\" , \"OutputManagementPlan\" , \"PeerReview\" , \"PhysicalObject\" , \"Preprint\" , \"Report\" , \"Service\" , \"Software\" , \"Sound\" , \"Standard\" , \"Text\" , \"Workflow\" , \"Other\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"Collection\" } ] }, { \"name\" : \"Additional Optional Title Section\" , \"description\" : \"This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\" , \"fields\" : [ { \"fieldName\" : \"Title\" , \"metadataPropertyName\" : \"titles/title\" , \"description\" : \"A name or title by which a resource is known.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Title Type\" , \"metadataPropertyName\" : \"titles/titleType\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"AlternativeTitle\" , \"Subtitle\" , \"TranslatedTitle\" , \"Other\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] }, { \"name\" : \"Additional Optional Description Section\" , \"description\" : \"This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\" , \"fields\" : [ { \"fieldName\" : \"Description\" , \"metadataPropertyName\" : \"descriptions/description\" , \"description\" : \"All additional information that does not fit in any of the other categories. May be used for technical information.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"text\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Description Type\" , \"metadataPropertyName\" : \"descriptions/descriptionType\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"Abstract\" , \"Methods\" , \"SeriesInformation\" , \"TableOfContents\" , \"TechnicalInfo\" , \"Other\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] }, { \"name\" : \"Primary Contributor\" , \"description\" : \"Resource Contributor. This section will be capable of accepting multiples in the future, however at the moment it only handles a single entry.\" , \"fields\" : [ { \"fieldName\" : \"Name\" , \"metadataPropertyName\" : \"contributors/contributor/contributorName\" , \"description\" : \"The institution or person responsible for collecting, creating, or otherwise contributing to the development of the dataset.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Name Type\" , \"metadataPropertyName\" : \"contributors/contributor/contributorNameType\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"Organizational\" , \"Personal\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Contributor Type\" , \"metadataPropertyName\" : \"contributors/contributor/contributorType\" , \"description\" : \"Mandatory if the contributor name is provided.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : [ \"ContactPerson\" , \"DataCollector\" , \"DataCurator\" , \"DataManager\" , \"Distributor\" , \"Editor\" , \"HostingInstitution\" , \"Other\" , \"Producer\" , \"ProjectLeader\" , \"ProjectManager\" , \"ProjectMember\" , \"RegistrationAgency\" , \"RegistrationAuthority\" , \"RelatedPerson\" , \"ResearchGroup\" , \"RightsHolder\" , \"Researcher\" , \"Sponsor\" , \"Supervisor\" , \"WorkPackageLeader\" ], \"regularExpression\" : null , \"dataType\" : \"enumeration\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Given Name\" , \"metadataPropertyName\" : \"contributors/contributor/givenName\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Family Name\" , \"metadataPropertyName\" : \"contributors/contributor/familyName\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Identifier\" , \"metadataPropertyName\" : \"contributors/contributor/nameIdentifier\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" }, { \"fieldName\" : \"Affiliation\" , \"metadataPropertyName\" : \"contributors/contributor/affiliation\" , \"description\" : null , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] }, { \"name\" : \"Additional Optional Fields\" , \"description\" : \"Optional metadata fields for DOI profiles.\" , \"fields\" : [ { \"fieldName\" : \"Language\" , \"metadataPropertyName\" : \"language\" , \"description\" : \"Primary language of the resource. Allowed values are taken from IETF BCP 47, ISO 639-1 language codes. For English, use 'en'.\" , \"maxMultiplicity\" : 1 , \"minMultiplicity\" : 0 , \"allowedValues\" : null , \"regularExpression\" : null , \"dataType\" : \"string\" , \"derived\" : false , \"derivedFrom\" : null , \"uneditable\" : false , \"currentValue\" : \"\" } ] } ], \"id\" : \"36e74acc-a862-43d0-8e2b-e54422c4239f\" , \"label\" : \"DOI Test Model 5\" , \"domainType\" : \"DataModel\" }","title":"Get the profile"},{"location":"plugins/rest-api/doi/#save-the-profile","text":"Using the JSON structure from the GET endpoint above and entering values into the sections/fields/currentValue JSON properties, you can save the changes made to the profile, and attach the profile to the catalogue item at the same time, like so. /api/ {multiFacetAwareDomainType} / {id} /profiles/ {namespace} / {name} Where: The {namespace} will be uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile . The {name} will be DigitalObjectIdentifiersProfileProviderService . If successful, this will return the same JSON response as the GET endpoint above.","title":"Save the profile"},{"location":"plugins/rest-api/doi/#remove-the-profile","text":"To remove the DOI profile from the catalogue item: /api/ {multiFacetAwareDomainType} / {id} /profiles/ {namespace} / {name} Where: The {namespace} will be uk.ac.ox.softeng.maurodatamapper.plugins.digitalobjectidentifiers.profile . The {name} will be DigitalObjectIdentifiersProfileProviderService .","title":"Remove the profile"},{"location":"plugins/rest-api/doi/#doi-management","text":"The profile endpoints above are standard endpoints used in Mauro. This section covers the custom endpoints added by the DOI plugin.","title":"DOI Management"},{"location":"plugins/rest-api/doi/#get-doi-status","text":"For any catalogue item, the current status of the DOI name on that item can be queried as follows: /api/ {multiFacetAwareDomainType} / {id} /doi If a status exists, a 200 OK response will be returned with this response body: Response body (JSON) 1 2 3 4 { \"identifier\" : \"10.1109/5.771073\" , \"status\" : \"final\" } The fields are as follows: identifier (String): The Digital Object Identifier (DOI) name assigned to this catalogue item as generated by the DOI system. If a DOI has not been generated yet, this will not be provided. state (Constant): The current DOI state of this catalogue item, which can one of: not submitted - no DOI has been requested yet. draft - a draft DOI has been assigned, allowing further changes to the profile. final - a DOI has been assigned and locked, finalising this catalogue item's profile. retired - a previous DOI has been retired and can no longer be used for cross-reference.","title":"Get DOI status"},{"location":"plugins/rest-api/doi/#submit-doi-status","text":"Submitting the catalogue item and its DOI profile can only be done for publicly readable and finalised catalogue items. This is to ensure that: Every citation has access to the catalogue item. The catalogue item cannot be modified any further, keeping the citation the same for all future uses. Information It may be possible to not have assigned the Digital Object Identifiers DataCite Dataset Schema to a catalogue item before it was finalised. To solve this issue, the DOI profile has special permissions to allow it to be edited post-finalisation, so long as the DOI is not yet in the Final state. Therefore, you may use the POST /api/{multiFacetAwareDomainType}/{id}/profiles/{namespace}/{name} endpoint as many times as necessary to make alterations to the profile before continuing further. To submit the catalogue item's profile to the DOI system to obtain a DOI name, use this endpoint. /api/ {multiFacetAwareDomainType} / {id} /doi?submissionType={type} This endpoint requires no request body; the profile should already have been saved before this point. The submissionType can be one of the following: draft - submit a draft profile to obtain the DOI name. finalise - submit a final profile to obtain the DOI name. retire - retire an existing DOI name to no longer be in active use. The response returned will be the contents of the catalogue item the DOI was for. The plugin will also automatically populate the DOI profile identifier metadata key for the catalogue item.","title":"Submit DOI status"},{"location":"plugins/rest-api/doi/#remove-doi","text":"To remove all DOI metadata: /api/ {multiFacetAwareDomainType} / {id} /doi","title":"Remove DOI"},{"location":"plugins/rest-api/doi/#doi-resolution","text":"The identifier in the DOI profile will hold the unique identifier as registered in the DOI system, for example 10.1109/5.771073 . The DOI system has a guaranteed method of locating every registered DOI name through a non-changing URL; this will actually redirect back to your client URL by providing the DOI name in the URL - in Mauro, this would be similar to https://{domain}/#/doi/10.1109/5.771073 To actually locate the catalogue item assigned to this DOI name, this endpoint will fetch the Mauro catalogue item to which the DOI name is mapped. /api/doi/ {identifier} The JSON response will contain the full catalogue item details for that DOI name. See the REST API for the exact details returned per catalogue item domain type.","title":"DOI Resolution"},{"location":"plugins/rest-api/freemarker/","text":"Introduction \u00b6 The Freemarker plugin provides additional API endpoints that allow custom queries to be run and templates to be applied to the result. This can be used as a lightweight way to create custom exporters or provides an easy way to allow interchange between different systems. The functionality makes use of a popular and powerful templating engine: Apache FreeMarker . Warning \u00b6 Warning Allowing users to execute their own code on the server is, in general, bad practice. Badly-written templates could result in the server becoming unresponsive or consuming additional resources. The template framework has also been carefully configured to ensure a minimal amount of data is accessible, but malicious templates may be able to access more data than intended. To mitigate this risk, the Freemarker templating plugin is an optional extra and when installed, the API endpoints are only accessible by registered users with administrator access. It is recommended that template developers test on a local instance of Mauro in the first instance and carefully iterate on templates. This will ensure any complex queries or loops do not accidentally cause a denial of service to the Mauro instance. We have plans to improve the templating plugin in the future to provide greater levels of safety as well as allowing non-administrators to have access to the functionality in a safe manner. Basic usage \u00b6 The main API endpoint provided by the plugin is as follows: /api/ {domainType} / {id} /template In order to call this endpoint, the user must be authenticated as a user with an administrator role, either through a login call and a persistent session cookie, or using an API key. The body of the call must be text corresponding to a valid Apache FreeMarker template. The domainType and id parameters will point to a particular \u2018resource\u2019, that will be queried and have the template applied. Within the template, the resulting resource will be accessible through a variable with the same name as the domain type. See the examples below for more details. Successful return of the API call will have, as body text, the result of applying the given template to the queried resource. Any errors in the template will result in an error being thrown, with a message field indicating the nature of the error and the line number of the template in which the error was found. Template language reference \u00b6 The Apache FreeMarker pages contain a general-purpose introduction to the FreeMarker Template Language (FTL), as well as a detailed reference manual for the syntax. Other online tutorials are provided by Lars Vogel and some useful tricks are given in this article by Baeldung . You may also wish to visit our Zulip community for help and advice. We would also like to publish reusable templates donated by the community on these pages. Example 1: A simple web page \u00b6 In this first example, a basic template will be applied to a Data Model in order to generate a simple HTML page for a given Data Model . The API call: /api/dataModel/ {id} /template Is made with an appropriate API key, and the post body: 1 2 3 4 5 6 7 8 9 10 <html> <body> <h1> ${dataModel.label} </h1> <p> ${(dataModel.description)!} </p> </body> </html> Note in the template, the variable dataModel represents the Data Model object with the id given as the URL parameter. Note also, the use of ! when accessing the description o the Data Model . This is to guard against the case where the description is unset (null). The body of the API call response is as follows: 1 2 3 4 5 6 7 8 9 10 < html > < body > < h1 > Simple Data Model </ h1 > < p > This is the description of a simple data model </ p > </ body > </ html > Note that this example also illustrates some shortcomings of the approach for complex transformations. If the description were to be html formatted already, for example, starting with a <p> tag, the resulting html may be invalid. Some simple transformations are possible within the templating language, particularly with the use of complex macros, but the use of this plugin should be reserved for simple textual templating. More complex algorithms should be implemented within a custom plugin or external script, where programmatic functions and libraries are more easily accessible. Example 2: A CSV file of terms in a terminology \u00b6 This second example uses a slightly more complex template to iterate over the terms of a terminology, printing the code and definition for each. The API call: /api/terminology/ {id} /template Is made with an appropriate API key and with the following text in the body: 1 2 3 4 Code,Definition <#list terminology.terms?sort_by(\"code\") as term> ${term.code},${term.definition} </#list> The <#list> tag indicates a template chunk that is to be repeated for each item in the list - in this case the list of terms given by terminology.term . The ?sort_by annotation indicates that the set of terms should be ordered before iteration. Within these tags, the term in question is given the variable name term . The result is a CSV file of terms, for example: 1 2 3 4 5 Code,Definition CTT00,Complex Test Term 00 CTT1,Complex Test Term 1 CTT10,Complex Test Term 10 CTT100,Complex Test Term 100 Templating Diffs \u00b6 The plugin also provides an endpoint for templating the difference between two models. The API endpoint: /api/ {domainType} / {sourceId} /diff/ {targetId} /template Compares two objects and allows the result to be templated. The method provides the following variables for use within the template: sourceModel The model to be used as the source of the diff (left-hand side), identified by sourceId targetModel The model to be used as the target of the diff (right-hand-side), identified by targetId diff The result of the comparison between the two models, stored as a set of diffs The structure of the diff object is still to be described in more detail, but a simple example is given below. Example 3: Listing differences \u00b6 This third example gives a flavour of the templating required to examine the differences between two Data Models . A more detailed example is needed here, but we illustrate for a very simple template, comparing two very simple (and similar) models. Given the API call: /api/dataModel/ {sourceId} /diff/ {targetId} /template With appropriate authentication and the POST body: 1 2 3 4 5 ${sourceModel.label} ${targetModel.label} <#list diff.diffs as fieldDifference> ${fieldDifference.getFieldName()} :: ${fieldDifference.left} <> ${fieldDifference.right} </#list> This will print first the label of the source model, then the label of the target model. For each basic difference between the two models, it will print the field name, the value of that field in the left-hand model, and the value of that field in the right-hand model. For two very simple models this might give an output such as: 1 2 3 4 Example DataModel 1 Example DataModel 2 label :: Example DataModel 1 <> Example DataModel 2 description :: My first description <> My second description","title":"Freemarker Templating"},{"location":"plugins/rest-api/freemarker/#introduction","text":"The Freemarker plugin provides additional API endpoints that allow custom queries to be run and templates to be applied to the result. This can be used as a lightweight way to create custom exporters or provides an easy way to allow interchange between different systems. The functionality makes use of a popular and powerful templating engine: Apache FreeMarker .","title":"Introduction"},{"location":"plugins/rest-api/freemarker/#warning","text":"Warning Allowing users to execute their own code on the server is, in general, bad practice. Badly-written templates could result in the server becoming unresponsive or consuming additional resources. The template framework has also been carefully configured to ensure a minimal amount of data is accessible, but malicious templates may be able to access more data than intended. To mitigate this risk, the Freemarker templating plugin is an optional extra and when installed, the API endpoints are only accessible by registered users with administrator access. It is recommended that template developers test on a local instance of Mauro in the first instance and carefully iterate on templates. This will ensure any complex queries or loops do not accidentally cause a denial of service to the Mauro instance. We have plans to improve the templating plugin in the future to provide greater levels of safety as well as allowing non-administrators to have access to the functionality in a safe manner.","title":"Warning"},{"location":"plugins/rest-api/freemarker/#basic-usage","text":"The main API endpoint provided by the plugin is as follows: /api/ {domainType} / {id} /template In order to call this endpoint, the user must be authenticated as a user with an administrator role, either through a login call and a persistent session cookie, or using an API key. The body of the call must be text corresponding to a valid Apache FreeMarker template. The domainType and id parameters will point to a particular \u2018resource\u2019, that will be queried and have the template applied. Within the template, the resulting resource will be accessible through a variable with the same name as the domain type. See the examples below for more details. Successful return of the API call will have, as body text, the result of applying the given template to the queried resource. Any errors in the template will result in an error being thrown, with a message field indicating the nature of the error and the line number of the template in which the error was found.","title":"Basic usage"},{"location":"plugins/rest-api/freemarker/#template-language-reference","text":"The Apache FreeMarker pages contain a general-purpose introduction to the FreeMarker Template Language (FTL), as well as a detailed reference manual for the syntax. Other online tutorials are provided by Lars Vogel and some useful tricks are given in this article by Baeldung . You may also wish to visit our Zulip community for help and advice. We would also like to publish reusable templates donated by the community on these pages.","title":"Template language reference"},{"location":"plugins/rest-api/freemarker/#example-1-a-simple-web-page","text":"In this first example, a basic template will be applied to a Data Model in order to generate a simple HTML page for a given Data Model . The API call: /api/dataModel/ {id} /template Is made with an appropriate API key, and the post body: 1 2 3 4 5 6 7 8 9 10 <html> <body> <h1> ${dataModel.label} </h1> <p> ${(dataModel.description)!} </p> </body> </html> Note in the template, the variable dataModel represents the Data Model object with the id given as the URL parameter. Note also, the use of ! when accessing the description o the Data Model . This is to guard against the case where the description is unset (null). The body of the API call response is as follows: 1 2 3 4 5 6 7 8 9 10 < html > < body > < h1 > Simple Data Model </ h1 > < p > This is the description of a simple data model </ p > </ body > </ html > Note that this example also illustrates some shortcomings of the approach for complex transformations. If the description were to be html formatted already, for example, starting with a <p> tag, the resulting html may be invalid. Some simple transformations are possible within the templating language, particularly with the use of complex macros, but the use of this plugin should be reserved for simple textual templating. More complex algorithms should be implemented within a custom plugin or external script, where programmatic functions and libraries are more easily accessible.","title":"Example 1: A simple web page"},{"location":"plugins/rest-api/freemarker/#example-2-a-csv-file-of-terms-in-a-terminology","text":"This second example uses a slightly more complex template to iterate over the terms of a terminology, printing the code and definition for each. The API call: /api/terminology/ {id} /template Is made with an appropriate API key and with the following text in the body: 1 2 3 4 Code,Definition <#list terminology.terms?sort_by(\"code\") as term> ${term.code},${term.definition} </#list> The <#list> tag indicates a template chunk that is to be repeated for each item in the list - in this case the list of terms given by terminology.term . The ?sort_by annotation indicates that the set of terms should be ordered before iteration. Within these tags, the term in question is given the variable name term . The result is a CSV file of terms, for example: 1 2 3 4 5 Code,Definition CTT00,Complex Test Term 00 CTT1,Complex Test Term 1 CTT10,Complex Test Term 10 CTT100,Complex Test Term 100","title":"Example 2: A CSV file of terms in a terminology"},{"location":"plugins/rest-api/freemarker/#templating-diffs","text":"The plugin also provides an endpoint for templating the difference between two models. The API endpoint: /api/ {domainType} / {sourceId} /diff/ {targetId} /template Compares two objects and allows the result to be templated. The method provides the following variables for use within the template: sourceModel The model to be used as the source of the diff (left-hand side), identified by sourceId targetModel The model to be used as the target of the diff (right-hand-side), identified by targetId diff The result of the comparison between the two models, stored as a set of diffs The structure of the diff object is still to be described in more detail, but a simple example is given below.","title":"Templating Diffs"},{"location":"plugins/rest-api/freemarker/#example-3-listing-differences","text":"This third example gives a flavour of the templating required to examine the differences between two Data Models . A more detailed example is needed here, but we illustrate for a very simple template, comparing two very simple (and similar) models. Given the API call: /api/dataModel/ {sourceId} /diff/ {targetId} /template With appropriate authentication and the POST body: 1 2 3 4 5 ${sourceModel.label} ${targetModel.label} <#list diff.diffs as fieldDifference> ${fieldDifference.getFieldName()} :: ${fieldDifference.left} <> ${fieldDifference.right} </#list> This will print first the label of the source model, then the label of the target model. For each basic difference between the two models, it will print the field name, the value of that field in the left-hand model, and the value of that field in the right-hand model. For two very simple models this might give an output such as: 1 2 3 4 Example DataModel 1 Example DataModel 2 label :: Example DataModel 1 <> Example DataModel 2 description :: My first description <> My second description","title":"Example 3: Listing differences"},{"location":"plugins/rest-api/openid-connect/","text":"Introduction \u00b6 The page covering authentication explains how to authenticate with a basic username/password for a user created directly in Mauro. An alternative authentication method is to use an OpenID Connect identity service to authenticate users with an external provider/account system, and then authorize them to use Mauro. In order use OpenID Connect identity providers, the Mauro instance must: Have the Mauro OpenID Connect Authentication plugin installed. Set up and configure one or more identity services that support the OpenID Connect protocol. Add the configuration details of each identity provider to Mauro. The full details of this set up can be viewed in the user guide Using OpenID Connect . Authenticating users \u00b6 Assuming that the Mauro instance has been configured correctly and there is at least one OpenID Connect provider configured in Mauro , endpoints will be exposed to handle authentication using those external services. Using authorization endpoints \u00b6 There is an endpoint to fetch a list of OpenID Connect providers available in Mauro, which will provide the authorization endpoints : /api/openidConnectProviders Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [ { \"id\" : \"0500cd44-6ca9-4bca-aa55-bbb188278d79\" , \"label\" : \"Google\" , \"standardProvider\" : true , \"authorizationEndpoint\" : \"https://accounts.google.com/o/oauth2/v2/auth?scope=openid+email+profile&response_type=code&state=045d33af-2dc7-48cd-9111-a09c94faee49&nonce=%C3%B9%02%C2%BF%C5%93%07%C3%A9%C3%B4%27%EF%BF%BD%2C%22%C2%B3%C3%8D%07%C3%98%E2%80%A2%C3%91%C3%8C%C3%83%EF%BF%BD%26%CB%9C%C3%A4%05%C3%A17%C2%B1%19Q%C3%A13%E2%80%9C&client_id=375980182300-tc8sb8c1jelomnkmvqtkkqpl4g8lkp06.apps.googleusercontent.com\" , \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg\" }, { \"id\" : \"61fa9235-7281-4738-87f1-763bf60e1d79\" , \"label\" : \"Keycloak\" , \"standardProvider\" : true , \"authorizationEndpoint\" : \"https://jenkins.cs.ox.ac.uk/auth/realms/test/protocol/openid-connect/auth?scope=openid+email+profile&response_type=code&state=0f4e7148-28c4-407d-bad8-47052769e721&nonce=%C3%B9%02%C2%BF%C5%93%07%C3%A9%C3%B4%27%EF%BF%BD%2C%22%C2%B3%C3%8D%07%C3%98%E2%80%A2%C3%91%C3%8C%C3%83%EF%BF%BD%26%CB%9C%C3%A4%05%C3%A17%C2%B1%19Q%C3%A13%E2%80%9C&client_id=mdm\" , \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/2/29/Keycloak_Logo.png\" } ] The authorizationEndpoint is the key property value in each case, as this provides the URL to redirect to in order to start authenticating in the external service. The authorizationEndpoint value must include an additional query parameter added by yourself called redirect_uri : this is the URL to the page that the identity provider will redirect back to once it has authenticated a user, also providing the session state needed for Mauro to authorize the user session. Information Before redirecting to the external identity server, track the id value of the OpenID Connect provider somewhere. This will be required later when authorizing the Mauro user session. Mauro authorization \u00b6 When the OpenID Connect identity provider redirects back to the client (via the redirect_uri provided), it will provide three query parameters in the URL: state session_state code All these parameters must be captured and the passed to the standard Mauro authentication endpoint via the request body: /api/authentication/login Request body (JSON) 1 2 3 4 5 6 7 { \"openidConnectProvider\" : \"0500cd44-6ca9-4bca-aa55-bbb188278d79\" , \"state\" : <query param value> , \"sessionState\" : <query param value> , \"code\" : <query param value> , \"redirectUri\" : \"http://my.app.com/authorize\" } Information The redirectUri passed to the login request body must exactly match that passed as the redirect_uri query parameter at the point of redirecting to the OpenID Connect service. If successful, the /api/authentication/login endpoint will return the same response as a standard login using a username and password. The user is now signed into Mauro and has an active session. The user also uses the same permissions and user groups model for accessing catalogue content as any other user. Administration \u00b6 Information The following endpoints can only be accessed by an administrator user. Getting providers \u00b6 To get a list of available OpenID Connect providers registered in Mauro, the following endpoint can be used: /api/admin/openidConnectProviders Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"count\" : 2 , \"items\" : [ { \"id\" : \"0500cd44-6ca9-4bca-aa55-bbb188278d79\" , \"lastUpdated\" : \"2021-06-24T09:01:38.914Z\" , \"label\" : \"Google\" , \"standardProvider\" : true , \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg\" }, { \"id\" : \"61fa9235-7281-4738-87f1-763bf60e1d79\" , \"lastUpdated\" : \"2021-06-24T09:01:39.336Z\" , \"label\" : \"Keycloak\" , \"standardProvider\" : true , \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/2/29/Keycloak_Logo.png\" } ] } To get the full details of a particular OpenID Connect provider: /api/admin/openidConnectProviders/{id} Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"id\" : \"0500cd44-6ca9-4bca-aa55-bbb188278d79\" , \"lastUpdated\" : \"2021-06-24T09:01:38.914Z\" , \"label\" : \"Google\" , \"standardProvider\" : true , \"discoveryDocumentUrl\" : \"https://accounts.google.com/.well-known/openid-configuration\" , \"clientId\" : \"375980182300-tc8sb8c1jelomnkmvqtkkqpl4g8lkp06.apps.googleusercontent.com\" , \"clientSecret\" : \"<secret value>\" , \"authorizationEndpointParameters\" : { \"id\" : \"dae8564f-8272-4b99-ba1d-03c66f3ab34a\" , \"lastUpdated\" : \"2021-06-24T09:01:38.906Z\" , \"scope\" : \"openid email profile\" , \"responseType\" : \"code\" }, \"discoveryDocument\" : { \"id\" : \"52108737-6fda-408e-b167-f6912bd37a18\" , \"lastUpdated\" : \"2021-06-24T09:01:38.91Z\" , \"issuer\" : \"https://accounts.google.com\" , \"authorizationEndpoint\" : \"https://accounts.google.com/o/oauth2/v2/auth\" , \"tokenEndpoint\" : \"https://oauth2.googleapis.com/token\" , \"userinfoEndpoint\" : \"https://openidconnect.googleapis.com/v1/userinfo\" , \"endSessionEndpoint\" : \"https://oauth2.googleapis.com/revoke\" , \"jwksUri\" : \"https://www.googleapis.com/oauth2/v3/certs\" }, \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg\" } Creating a provider \u00b6 To add an OpenID Connect provider to Mauro depends on what discovery information is available. There are two types of provider in Mauro: Standard - a discoveryDocumentUrl is provided, which allows Mauro to discover all other necessary endpoints to complete the setup of the provider (authorization, tokens, etc). Non-Standard - when a discoveryDocumentUrl is not available, the individual endpoints needed to complete the OpenID Connect provider setup can be provided manually. Both use the following endpoint, but sends different request body content. /api/admin/openidConnectProviders To create a Standard provider, include a similar request body: Request body (JSON) 1 2 3 4 5 6 7 8 { \"label\" : \"Provider name\" , \"imageUrl\" : \"http://image.com/logo.png\" , \"standardProvider\" : true , \"clientId\" : \"<Provided by service>\" , \"clientSecret\" : \"<Provided by service>\" , \"discoveryDocumentUrl\" : \"https://url.to.some/discovery-document\" } Information imageUrl is an optional field. To create a Non-Standard provider: Request body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"label\" : \"Provider name\" , \"imageUrl\" : \"http://image.com/logo.png\" , \"standardProvider\" : false , \"clientId\" : \"<Provided by service>\" , \"clientSecret\" : \"<Provided by service>\" , \"discoveryDocument\" : { \"issuer\" : \"<url>\" , \"authorizationEndpoint\" : \"<url>\" , \"tokenEndpoint\" : \"<url>\" , \"userinfoEndpoint\" : \"<url>\" , \"endSessionEndpoint\" : \"<url>\" , \"jwksUri\" : \"<url>\" } } Information userinfoEndpoint and endSessionEndpoint are optional, all other discovery endpoints are required. If successful, both return the same response as getting a provider . Update / Delete \u00b6 To edit the properties of an OpenID Connect provider, use the following endpoints, with a request body similar to the JSON described in Creating a provider . /api/admin/openidConnectProviders/{id} To delete an OpenID Connect provider, use the following endpoint. /api/admin/openidConnectProviders/{id}","title":"OpenID Connect"},{"location":"plugins/rest-api/openid-connect/#introduction","text":"The page covering authentication explains how to authenticate with a basic username/password for a user created directly in Mauro. An alternative authentication method is to use an OpenID Connect identity service to authenticate users with an external provider/account system, and then authorize them to use Mauro. In order use OpenID Connect identity providers, the Mauro instance must: Have the Mauro OpenID Connect Authentication plugin installed. Set up and configure one or more identity services that support the OpenID Connect protocol. Add the configuration details of each identity provider to Mauro. The full details of this set up can be viewed in the user guide Using OpenID Connect .","title":"Introduction"},{"location":"plugins/rest-api/openid-connect/#authenticating-users","text":"Assuming that the Mauro instance has been configured correctly and there is at least one OpenID Connect provider configured in Mauro , endpoints will be exposed to handle authentication using those external services.","title":"Authenticating users"},{"location":"plugins/rest-api/openid-connect/#using-authorization-endpoints","text":"There is an endpoint to fetch a list of OpenID Connect providers available in Mauro, which will provide the authorization endpoints : /api/openidConnectProviders Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [ { \"id\" : \"0500cd44-6ca9-4bca-aa55-bbb188278d79\" , \"label\" : \"Google\" , \"standardProvider\" : true , \"authorizationEndpoint\" : \"https://accounts.google.com/o/oauth2/v2/auth?scope=openid+email+profile&response_type=code&state=045d33af-2dc7-48cd-9111-a09c94faee49&nonce=%C3%B9%02%C2%BF%C5%93%07%C3%A9%C3%B4%27%EF%BF%BD%2C%22%C2%B3%C3%8D%07%C3%98%E2%80%A2%C3%91%C3%8C%C3%83%EF%BF%BD%26%CB%9C%C3%A4%05%C3%A17%C2%B1%19Q%C3%A13%E2%80%9C&client_id=375980182300-tc8sb8c1jelomnkmvqtkkqpl4g8lkp06.apps.googleusercontent.com\" , \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg\" }, { \"id\" : \"61fa9235-7281-4738-87f1-763bf60e1d79\" , \"label\" : \"Keycloak\" , \"standardProvider\" : true , \"authorizationEndpoint\" : \"https://jenkins.cs.ox.ac.uk/auth/realms/test/protocol/openid-connect/auth?scope=openid+email+profile&response_type=code&state=0f4e7148-28c4-407d-bad8-47052769e721&nonce=%C3%B9%02%C2%BF%C5%93%07%C3%A9%C3%B4%27%EF%BF%BD%2C%22%C2%B3%C3%8D%07%C3%98%E2%80%A2%C3%91%C3%8C%C3%83%EF%BF%BD%26%CB%9C%C3%A4%05%C3%A17%C2%B1%19Q%C3%A13%E2%80%9C&client_id=mdm\" , \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/2/29/Keycloak_Logo.png\" } ] The authorizationEndpoint is the key property value in each case, as this provides the URL to redirect to in order to start authenticating in the external service. The authorizationEndpoint value must include an additional query parameter added by yourself called redirect_uri : this is the URL to the page that the identity provider will redirect back to once it has authenticated a user, also providing the session state needed for Mauro to authorize the user session. Information Before redirecting to the external identity server, track the id value of the OpenID Connect provider somewhere. This will be required later when authorizing the Mauro user session.","title":"Using authorization endpoints"},{"location":"plugins/rest-api/openid-connect/#mauro-authorization","text":"When the OpenID Connect identity provider redirects back to the client (via the redirect_uri provided), it will provide three query parameters in the URL: state session_state code All these parameters must be captured and the passed to the standard Mauro authentication endpoint via the request body: /api/authentication/login Request body (JSON) 1 2 3 4 5 6 7 { \"openidConnectProvider\" : \"0500cd44-6ca9-4bca-aa55-bbb188278d79\" , \"state\" : <query param value> , \"sessionState\" : <query param value> , \"code\" : <query param value> , \"redirectUri\" : \"http://my.app.com/authorize\" } Information The redirectUri passed to the login request body must exactly match that passed as the redirect_uri query parameter at the point of redirecting to the OpenID Connect service. If successful, the /api/authentication/login endpoint will return the same response as a standard login using a username and password. The user is now signed into Mauro and has an active session. The user also uses the same permissions and user groups model for accessing catalogue content as any other user.","title":"Mauro authorization"},{"location":"plugins/rest-api/openid-connect/#administration","text":"Information The following endpoints can only be accessed by an administrator user.","title":"Administration"},{"location":"plugins/rest-api/openid-connect/#getting-providers","text":"To get a list of available OpenID Connect providers registered in Mauro, the following endpoint can be used: /api/admin/openidConnectProviders Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"count\" : 2 , \"items\" : [ { \"id\" : \"0500cd44-6ca9-4bca-aa55-bbb188278d79\" , \"lastUpdated\" : \"2021-06-24T09:01:38.914Z\" , \"label\" : \"Google\" , \"standardProvider\" : true , \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg\" }, { \"id\" : \"61fa9235-7281-4738-87f1-763bf60e1d79\" , \"lastUpdated\" : \"2021-06-24T09:01:39.336Z\" , \"label\" : \"Keycloak\" , \"standardProvider\" : true , \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/2/29/Keycloak_Logo.png\" } ] } To get the full details of a particular OpenID Connect provider: /api/admin/openidConnectProviders/{id} Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"id\" : \"0500cd44-6ca9-4bca-aa55-bbb188278d79\" , \"lastUpdated\" : \"2021-06-24T09:01:38.914Z\" , \"label\" : \"Google\" , \"standardProvider\" : true , \"discoveryDocumentUrl\" : \"https://accounts.google.com/.well-known/openid-configuration\" , \"clientId\" : \"375980182300-tc8sb8c1jelomnkmvqtkkqpl4g8lkp06.apps.googleusercontent.com\" , \"clientSecret\" : \"<secret value>\" , \"authorizationEndpointParameters\" : { \"id\" : \"dae8564f-8272-4b99-ba1d-03c66f3ab34a\" , \"lastUpdated\" : \"2021-06-24T09:01:38.906Z\" , \"scope\" : \"openid email profile\" , \"responseType\" : \"code\" }, \"discoveryDocument\" : { \"id\" : \"52108737-6fda-408e-b167-f6912bd37a18\" , \"lastUpdated\" : \"2021-06-24T09:01:38.91Z\" , \"issuer\" : \"https://accounts.google.com\" , \"authorizationEndpoint\" : \"https://accounts.google.com/o/oauth2/v2/auth\" , \"tokenEndpoint\" : \"https://oauth2.googleapis.com/token\" , \"userinfoEndpoint\" : \"https://openidconnect.googleapis.com/v1/userinfo\" , \"endSessionEndpoint\" : \"https://oauth2.googleapis.com/revoke\" , \"jwksUri\" : \"https://www.googleapis.com/oauth2/v3/certs\" }, \"imageUrl\" : \"https://upload.wikimedia.org/wikipedia/commons/5/53/Google_%22G%22_Logo.svg\" }","title":"Getting providers"},{"location":"plugins/rest-api/openid-connect/#creating-a-provider","text":"To add an OpenID Connect provider to Mauro depends on what discovery information is available. There are two types of provider in Mauro: Standard - a discoveryDocumentUrl is provided, which allows Mauro to discover all other necessary endpoints to complete the setup of the provider (authorization, tokens, etc). Non-Standard - when a discoveryDocumentUrl is not available, the individual endpoints needed to complete the OpenID Connect provider setup can be provided manually. Both use the following endpoint, but sends different request body content. /api/admin/openidConnectProviders To create a Standard provider, include a similar request body: Request body (JSON) 1 2 3 4 5 6 7 8 { \"label\" : \"Provider name\" , \"imageUrl\" : \"http://image.com/logo.png\" , \"standardProvider\" : true , \"clientId\" : \"<Provided by service>\" , \"clientSecret\" : \"<Provided by service>\" , \"discoveryDocumentUrl\" : \"https://url.to.some/discovery-document\" } Information imageUrl is an optional field. To create a Non-Standard provider: Request body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"label\" : \"Provider name\" , \"imageUrl\" : \"http://image.com/logo.png\" , \"standardProvider\" : false , \"clientId\" : \"<Provided by service>\" , \"clientSecret\" : \"<Provided by service>\" , \"discoveryDocument\" : { \"issuer\" : \"<url>\" , \"authorizationEndpoint\" : \"<url>\" , \"tokenEndpoint\" : \"<url>\" , \"userinfoEndpoint\" : \"<url>\" , \"endSessionEndpoint\" : \"<url>\" , \"jwksUri\" : \"<url>\" } } Information userinfoEndpoint and endSessionEndpoint are optional, all other discovery endpoints are required. If successful, both return the same response as getting a provider .","title":"Creating a provider"},{"location":"plugins/rest-api/openid-connect/#update-delete","text":"To edit the properties of an OpenID Connect provider, use the following endpoints, with a request body similar to the JSON described in Creating a provider . /api/admin/openidConnectProviders/{id} To delete an OpenID Connect provider, use the following endpoint. /api/admin/openidConnectProviders/{id}","title":"Update / Delete"},{"location":"plugins/rest-api/sparql/","text":"Introduction \u00b6 The SPARQL plugin provides a compliant endpoint for executing SPARQL queries against an RDF rendering of the data. The underlying data store of Mauro Data Mapper is Postgres, and this plugin uses the D2RQ library for extracting the information as RDF triples. The REST API ensures that the mapping is configured with correct access constraints so that users may only see a subset of the triples corresponding to their access privileges. In the most basic cases, a system administrator can query across the whole data corpus in triple form; an unauthenticated user on an instance with no publicly available models will not be able to see any triples. Warning This plugin allows users to execute their own queries against the data store. A malicious user may write arbitrarily complex queries which could cause Mauro to slow down or become unresponsive. If you install this plugin, you may wish to ensure the Mauro instance is behind a firewall, and ensure that users know what they are doing! API Endpoints \u00b6 The plugin provides two (equivalent) endpoints which accept a SPARQL query as part of the request body and return results in a variety of formats. /api/sparql /api/sparql The Accept header determines which format is returned, according to the table below: Accept Header Value Response Format application/sparql-results+xml xml application/xml XML text/csv csv CSV application/sparql-results+json json application/json JSON The default format is for results to be returned in JSON format. In future, it may be possible to extend this plugin to support RDF/XML and other formats. Response format \u00b6 For the request body given in Example 1 below, the response body will have one of the following formats: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \"head\" : { \"vars\" : [ \"s\" , \"p\" , \"o\" ] }, \"results\" : { \"bindings\" : [ { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5\" }, \"p\" : { \"type\" : \"uri\" , \"value\" : \"http://metadata-catalogue.org/label\" }, \"o\" : { \"type\" : \"literal\" , \"value\" : \"Example Data Model\" } }, ... ] } } Response body (XML) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 <?xml version=\"1.0\"?> <sparql xmlns= \"http://www.w3.org/2005/sparql-results#\" > <head> <variable name= \"s\" /> <variable name= \"p\" /> <variable name= \"o\" /> </head> <results> <result> <binding name= \"s\" > <uri> http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5 </uri> </binding> <binding name= \"p\" > <uri> http://metadata-catalogue.org/label </uri> </binding> <binding name= \"o\" > <literal> Example Data Model </literal> </binding> </result> ... </results> </sparql> Response body (CSV) 1 2 3 s,p,o http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5,http://metadata-catalogue.org/label,Example Data Model ... Example Queries \u00b6 This page is not intended to provide a tutorial to writing SPARQL queries - some other tutorials are available online and linked below. These examples serve as a starting point for exploring the triple space. Example 1: Arbitrary triples \u00b6 Select the first 20 triples from the entire graph: 1 2 3 SELECT ?s ?p ?o WHERE { ?s ?p ?o } limit 20 Example 2: Restricting types \u00b6 Select the labels of all Data Models : 1 2 3 4 5 6 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?o WHERE { ?s mdm : label ?o . ?s a mdm : datamodel } Example 3: Relating entities \u00b6 Find the id of the classes which belong to a model called \"Complex Test DataModel\": 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?dcl WHERE { ?dm mdm : label \"Complex Test DataModel\" . ?dm a mdm : datamodel . ?dc mdm : child_class_of ?dm . ?dc mdm : id ?dcl } Example 4: Relations, multiple results \u00b6 Find the Enumeration Values which have a 'Value' of \"Not known\" . Find their keys and the label of the containing data type: 1 2 3 4 5 6 7 8 9 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?dtl ?evk WHERE { ?ev a mdm : enumerationvalue . ?ev mdm : value \"Not known\" . ?ev mdm : key ?evk . ?dt mdm : component_value ?ev . ?dt mdm : label ?dtl } Example 5: Finding metadata \u00b6 Find the schema.org abstract property from a Data Model : 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> PREFIX so : <http://metadata-catalogue.org/schema.org/> SELECT ?a WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dm so : abstract ?a . } Example 6: Transitive searches \u00b6 The first query finds the immediate child classes of a model: 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?l WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dc mdm : child_class_of ?dm . ?dc mdm : label ?l } Compare this with the second, which transitively follows the child_class_of relationship to find child classes of that class: 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?l WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dc mdm : child_class_of + ?dm . ?dc mdm : label ?l } Example 7: Transitive Searches on Terminologies \u00b6 This search recursively finds all terms narrower than another, and provides their code and definition: 1 2 3 4 5 6 7 8 9 10 11 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?code ?definition WHERE { ?ut mdm : code \"N20-N23\" . ?tm mdm : label \"International Classification of Diseases (ICD) Version 10 Edition 5\" . ?t mdm : term_of ?tm . ?ut mdm : term_of ?tm . ?t mdm : narrowerThan + ?ut . ?t mdm : code ?code . ?t mdm : definition ?definition . } ORDER BY ASC ( ?code ) Links to SPARQL Tutorials \u00b6 There are many good tutorials available online - this is not intended to represet a comprehensive list. But here are some some that we've found useful in the past: Apache Jena: SPARQL Tutorial Stardog: Learn SPARQL W3C: SPARQL By Example","title":"SPARQL"},{"location":"plugins/rest-api/sparql/#introduction","text":"The SPARQL plugin provides a compliant endpoint for executing SPARQL queries against an RDF rendering of the data. The underlying data store of Mauro Data Mapper is Postgres, and this plugin uses the D2RQ library for extracting the information as RDF triples. The REST API ensures that the mapping is configured with correct access constraints so that users may only see a subset of the triples corresponding to their access privileges. In the most basic cases, a system administrator can query across the whole data corpus in triple form; an unauthenticated user on an instance with no publicly available models will not be able to see any triples. Warning This plugin allows users to execute their own queries against the data store. A malicious user may write arbitrarily complex queries which could cause Mauro to slow down or become unresponsive. If you install this plugin, you may wish to ensure the Mauro instance is behind a firewall, and ensure that users know what they are doing!","title":"Introduction"},{"location":"plugins/rest-api/sparql/#api-endpoints","text":"The plugin provides two (equivalent) endpoints which accept a SPARQL query as part of the request body and return results in a variety of formats. /api/sparql /api/sparql The Accept header determines which format is returned, according to the table below: Accept Header Value Response Format application/sparql-results+xml xml application/xml XML text/csv csv CSV application/sparql-results+json json application/json JSON The default format is for results to be returned in JSON format. In future, it may be possible to extend this plugin to support RDF/XML and other formats.","title":"API Endpoints"},{"location":"plugins/rest-api/sparql/#response-format","text":"For the request body given in Example 1 below, the response body will have one of the following formats: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \"head\" : { \"vars\" : [ \"s\" , \"p\" , \"o\" ] }, \"results\" : { \"bindings\" : [ { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5\" }, \"p\" : { \"type\" : \"uri\" , \"value\" : \"http://metadata-catalogue.org/label\" }, \"o\" : { \"type\" : \"literal\" , \"value\" : \"Example Data Model\" } }, ... ] } } Response body (XML) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 <?xml version=\"1.0\"?> <sparql xmlns= \"http://www.w3.org/2005/sparql-results#\" > <head> <variable name= \"s\" /> <variable name= \"p\" /> <variable name= \"o\" /> </head> <results> <result> <binding name= \"s\" > <uri> http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5 </uri> </binding> <binding name= \"p\" > <uri> http://metadata-catalogue.org/label </uri> </binding> <binding name= \"o\" > <literal> Example Data Model </literal> </binding> </result> ... </results> </sparql> Response body (CSV) 1 2 3 s,p,o http://metadata-catalogue.org/datamodel/data_element/4447a37d-db1f-4524-92be-05576efe4ce5,http://metadata-catalogue.org/label,Example Data Model ...","title":"Response format"},{"location":"plugins/rest-api/sparql/#example-queries","text":"This page is not intended to provide a tutorial to writing SPARQL queries - some other tutorials are available online and linked below. These examples serve as a starting point for exploring the triple space.","title":"Example Queries"},{"location":"plugins/rest-api/sparql/#example-1-arbitrary-triples","text":"Select the first 20 triples from the entire graph: 1 2 3 SELECT ?s ?p ?o WHERE { ?s ?p ?o } limit 20","title":"Example 1: Arbitrary triples"},{"location":"plugins/rest-api/sparql/#example-2-restricting-types","text":"Select the labels of all Data Models : 1 2 3 4 5 6 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?o WHERE { ?s mdm : label ?o . ?s a mdm : datamodel }","title":"Example 2: Restricting types"},{"location":"plugins/rest-api/sparql/#example-3-relating-entities","text":"Find the id of the classes which belong to a model called \"Complex Test DataModel\": 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?dcl WHERE { ?dm mdm : label \"Complex Test DataModel\" . ?dm a mdm : datamodel . ?dc mdm : child_class_of ?dm . ?dc mdm : id ?dcl }","title":"Example 3: Relating entities"},{"location":"plugins/rest-api/sparql/#example-4-relations-multiple-results","text":"Find the Enumeration Values which have a 'Value' of \"Not known\" . Find their keys and the label of the containing data type: 1 2 3 4 5 6 7 8 9 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?dtl ?evk WHERE { ?ev a mdm : enumerationvalue . ?ev mdm : value \"Not known\" . ?ev mdm : key ?evk . ?dt mdm : component_value ?ev . ?dt mdm : label ?dtl }","title":"Example 4: Relations, multiple results"},{"location":"plugins/rest-api/sparql/#example-5-finding-metadata","text":"Find the schema.org abstract property from a Data Model : 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> PREFIX so : <http://metadata-catalogue.org/schema.org/> SELECT ?a WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dm so : abstract ?a . }","title":"Example 5: Finding metadata"},{"location":"plugins/rest-api/sparql/#example-6-transitive-searches","text":"The first query finds the immediate child classes of a model: 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?l WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dc mdm : child_class_of ?dm . ?dc mdm : label ?l } Compare this with the second, which transitively follows the child_class_of relationship to find child classes of that class: 1 2 3 4 5 6 7 8 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?l WHERE { ?dm a mdm : datamodel . ?dm mdm : label \"Complex Test DataModel\" . ?dc mdm : child_class_of + ?dm . ?dc mdm : label ?l }","title":"Example 6: Transitive searches"},{"location":"plugins/rest-api/sparql/#example-7-transitive-searches-on-terminologies","text":"This search recursively finds all terms narrower than another, and provides their code and definition: 1 2 3 4 5 6 7 8 9 10 11 PREFIX mdm : <http://metadata-catalogue.org/> SELECT ?code ?definition WHERE { ?ut mdm : code \"N20-N23\" . ?tm mdm : label \"International Classification of Diseases (ICD) Version 10 Edition 5\" . ?t mdm : term_of ?tm . ?ut mdm : term_of ?tm . ?t mdm : narrowerThan + ?ut . ?t mdm : code ?code . ?t mdm : definition ?definition . } ORDER BY ASC ( ?code )","title":"Example 7: Transitive Searches on Terminologies"},{"location":"plugins/rest-api/sparql/#links-to-sparql-tutorials","text":"There are many good tutorials available online - this is not intended to represet a comprehensive list. But here are some some that we've found useful in the past: Apache Jena: SPARQL Tutorial Stardog: Learn SPARQL W3C: SPARQL By Example","title":"Links to SPARQL Tutorials"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/","text":"This user guide explains how to set-up the Mauro Digital Object Identifier plugin to allow you to use, edit and remove DOI profiles. You will also find out how to create, submit and retire profile names. 1. Overview \u00b6 Using an optional plugin for Mauro, it is possible to submit Mauro catalogue items from your catalogue to a Digital Object Identifie (DOI) system. This allows persistent, unique identifiers to be recorded in a DOI system to act as network links back to your Mauro catalogue items. The benefits are that the catalogue item may change over time, but the DOI name will remain fixed, always resolving back to the correct catalogue item, making them very useful for citations. The Mauro plugin for managing DOI names connects to the DataCite system to create and manage DOIs with DataCite Fabrica . DOI names can only be applied to public and finalised Mauro catalogue items, to ensure that their contents remain fixed in time. The following catalogue items can have DOI names attached to them: Data Model Data Class Data Element Data Type Code Set Terminology Term Reference Data Model Versioned Folder 2. Administrator setup \u00b6 Note This section applies to administrators of Mauro only. There are a number of steps an administrator must carry out before users can submit DOI names. 2.1 Install the plugin \u00b6 The Mauro Digital Object Identifier plugin must be installed on the Mauro instance. See the instructions for installing plugins , as well as the GitHub repo README file . 2.2 Enable the feature \u00b6 As this is an optional feature, the user interface will not show DOI profiles and features by default. To allow DOI submission to be used, you will first need to click the white arrow next to your user icon and then select 'Configuration' from the dropdown menu. This will bring up the 'Configuration' panel. Click the 'Properties' tab and then click '+Add' . An 'Add Property' form will appear which you will need to complete. Click the 'Select the property to add' box and select 'feature.use_digital_object_identifiers' from the dropdown menu, which will automatically fill in additional fields. Select 'Yes' from the 'Value' dropdown menu and then click the 'Add property' button. A green notification box will appear at the bottom right of your screen confirming that the 'Property was saved successfully' . 2.3 DataCite membership \u00b6 In order to create DOIs you will need to be a member (regular, or consortium-based) of DataCite. Please read the Become a Member DataCite pages to understand how to do this. 2.4 Mauro configuration \u00b6 Once enabled and your DataCite API is configured, there are some additional API properties to configure in Mauro before using the plugin. These can be found in the Configuration > Properties administration table, under the Digital Object Identifier Properties group. username - the username to connect to the DataCite API for authentication. password - the password to connect to the DataCite API for authentication. prefix - the unique prefix identifier representing this organisation. DataCite will provide this. endpoint - the base URL to the DataCite API. Initially, the plugin will automatically add these with the value of \"NOT_SET\"; change these values once you have them available. One final API property to ensure is set is the Site URL under the Site group. The Site URL property is used by the plugin to submit to DataCite, so that all links to the requested DOI get redirected back to the correct Mauro instance. Warning It is important to set the Site URL property, otherwise the Digital Object Identifier (DOI) plugin will be unable to submit profiles to DataCite correctly. 3. Using the profile \u00b6 Assuming all the setup steps were carried out above, catalogue items should now be able to use a new profile called 'Digital Object Identifiers DataCite Dataset Schema '. 3.1 Add the profile \u00b6 Note The DOI profile can only be added by editors of Mauro, and only when the catalogue item is in a draft state (not finalised ). If the profile has not been created before the catalogue item is finalised , there is another opportunity to edit the profile before submitting for a DOI name, as explained below. To add the DOI profile to the catalogue item, first select the catalogue item in the Model Tree that you wish to modify. Once that item's details panel is displayed on the right of your screen, select the 'Description' tab. Expand the 'Default profile' list and click 'Add new profile...' . Select 'Digital Object Identifiers DataCite Dataset Schema ' and click 'Save Changes' . This will display the full DOI profile fields to enter. You can enter values into the fields now, or click 'Save' to store the partially complete profile. 3.2 Editing the profile \u00b6 The DOI profile will present a number of fields which will be recorded as metadata against the catalogue item. These profile field values will also be submitted to the DOI system when a DOI name is requested, so it is important to ensure that the values entered into the profile are as accurate as possible. The profile fields are grouped into a number of sections: Predefined/Supplied Fields Primary Creator Additional Mandatory Fields Additional Optional Title Section Additional Optional Description Section Primary Contributor Additional Optional Fields The Predefined/Supplied Fields are special, read-only fields in the profile. These are used and modified internally by Mauro to track important details about the DOI name, such as the state, identifier, and so on. These are provided in read-only view to the user but are automatically controlled during DOI submission. There are a number of sections and fields in the profile, though only a few a strictly mandatory to submit to the DOI system (marked with a \"*\" symbol next to their field name). Clicking on the help icons next to each field name will provide a description of what the field represents. The profile will appear in read-only form when viewing the catalogue item. If you want to edit the profile, first make sure the 'Digital Object Identifiers DataCite Dataset Schema' profile is selected in the profile selection list and then click the 'Edit' button. This will bring up an 'Edit Profile' form which you will then be able to make changes to. Once you have finished, click 'Save' . Information You are able to save changes to a profile without ensuring that the profile is fully valid. For example, some fields may say 'This field is mandatory' , but changes can be saved temporarily without such fields being filled in. To test that the profile is valid, click on the 'Validate' button, then review any validation alerts that appear in each field. 3.4 Remove the profile \u00b6 If you decide against using the DOI profile, it can be removed from the catalogue item. Once again, make sure that the 'Digital Object Identifiers DataCite Dataset Schema' profile is selected in the profile selection list. Then click the three vertical dot menu to the right of the 'Edit' button and then select 'Remove profile' from the dropdown menu. A notification box will then appear asking if you are sure you want to remove this profile. Click 'Yes, remove' and the profile will then be removed along with all the metadata stored for that profile. A notification box will appear at the bottom right of your screen, confirming that the profile was successfully removed. 4. Submit and create a DOI name \u00b6 The 'Identifier' profile field will store the DOI name once the catalogue item has been submitted to the DOI system, for instance 10.1109/5.771073 . To store this DOI name, the catalogue item must first be submitted to the DOI system. To do this, select the relevant catalogue item from the Model Tree that you want to submit. The item must be finalised and publicly readable to anyone. Once the details panel has appeared on the right of your screen, click the 'Description' tab to view the profiles. Then click the three vertical dot menu to the right of the 'Edit' button and select 'DOI submission' from the dropdown menu. You will then have two possible states to generate the DOI name: Draft Allows for the profile fields to be modified whilst still obtaining a DOI name. Suitable for making edits to the profile whilst it is work-in-progress Final Fixes the DOI name and the metadata associated with it in the DOI system. The profile is finalised and cannot be modified further Information The 'Digital Object Identifiers DataCite Dataset Schema' profile does not need to be selected to carry out this step, nor does it have to be created yet. Mauro will automatically handle the creation of the DOI profile if it does not already exist. Before submitting, an 'Edit Profile' form will appear which will allow you to review the profile fields and make any necessary changes before submitting to the DOI system. Information It is recommended to use the Validate button to ensure that all profile fields are valid before submitting. Once you are happy with the profile, click the 'Submit' button and a green notification box will appear at the bottom right of your screen to confirm the change and the 'Identifier' field will be populated with the newly created DOI name. You may continue making changes to any Draft DOI profiles by repeating the same steps above. However, once changing to the Final state, you will have one last opportunity to review/modify the profile before it is fixed in place forever. 5. Retire a DOI name \u00b6 Once a DOI name is recorded in the profile, it is possible to retire that DOI name. When a DOI name is retired, any network links back to that catalogue item will no longer work. Warning Be careful when retiring DOI names - once retired, this cannot be undone. To retire a DOI name, first select the catalogue item in the Model Tree with the DOI name to retire. Once the details panel has appeared on the right of your screen, click the 'Description' tab to view the profiles. Then click the three vertical dot menu to the right of the 'Edit' button and select 'DOI submission' and then 'Retire' from the dropdown menus. A notification box will appear asking you to confirm that you would like to retire the DOI name. Click 'Yes, retire' and a green notification box shoud appear at the bottom right of your screen confirming that the DOI name was retired successfully. 6. DOI name resolution \u00b6 When Mauro catalogue items are submitted and recorded in the DOI system, all network links for those catalogue items are routed through the DOI system back to Mauro. All hyperlinks back to Mauro will point to: 1 https://{domain}/#/doi/{name} Where {name} is the DOI name in question, for example: 1 https://{domain}/#/doi/10.1109/5.771073 Any URL in this format will automatically resolve and redirect to the correct Mauro catalogue item to view.","title":"Digital Object Identifiers"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#1-overview","text":"Using an optional plugin for Mauro, it is possible to submit Mauro catalogue items from your catalogue to a Digital Object Identifie (DOI) system. This allows persistent, unique identifiers to be recorded in a DOI system to act as network links back to your Mauro catalogue items. The benefits are that the catalogue item may change over time, but the DOI name will remain fixed, always resolving back to the correct catalogue item, making them very useful for citations. The Mauro plugin for managing DOI names connects to the DataCite system to create and manage DOIs with DataCite Fabrica . DOI names can only be applied to public and finalised Mauro catalogue items, to ensure that their contents remain fixed in time. The following catalogue items can have DOI names attached to them: Data Model Data Class Data Element Data Type Code Set Terminology Term Reference Data Model Versioned Folder","title":"1. Overview"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#2-administrator-setup","text":"Note This section applies to administrators of Mauro only. There are a number of steps an administrator must carry out before users can submit DOI names.","title":"2. Administrator setup"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#21-install-the-plugin","text":"The Mauro Digital Object Identifier plugin must be installed on the Mauro instance. See the instructions for installing plugins , as well as the GitHub repo README file .","title":"2.1 Install the plugin"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#22-enable-the-feature","text":"As this is an optional feature, the user interface will not show DOI profiles and features by default. To allow DOI submission to be used, you will first need to click the white arrow next to your user icon and then select 'Configuration' from the dropdown menu. This will bring up the 'Configuration' panel. Click the 'Properties' tab and then click '+Add' . An 'Add Property' form will appear which you will need to complete. Click the 'Select the property to add' box and select 'feature.use_digital_object_identifiers' from the dropdown menu, which will automatically fill in additional fields. Select 'Yes' from the 'Value' dropdown menu and then click the 'Add property' button. A green notification box will appear at the bottom right of your screen confirming that the 'Property was saved successfully' .","title":"2.2 Enable the feature"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#23-datacite-membership","text":"In order to create DOIs you will need to be a member (regular, or consortium-based) of DataCite. Please read the Become a Member DataCite pages to understand how to do this.","title":"2.3 DataCite membership"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#24-mauro-configuration","text":"Once enabled and your DataCite API is configured, there are some additional API properties to configure in Mauro before using the plugin. These can be found in the Configuration > Properties administration table, under the Digital Object Identifier Properties group. username - the username to connect to the DataCite API for authentication. password - the password to connect to the DataCite API for authentication. prefix - the unique prefix identifier representing this organisation. DataCite will provide this. endpoint - the base URL to the DataCite API. Initially, the plugin will automatically add these with the value of \"NOT_SET\"; change these values once you have them available. One final API property to ensure is set is the Site URL under the Site group. The Site URL property is used by the plugin to submit to DataCite, so that all links to the requested DOI get redirected back to the correct Mauro instance. Warning It is important to set the Site URL property, otherwise the Digital Object Identifier (DOI) plugin will be unable to submit profiles to DataCite correctly.","title":"2.4 Mauro configuration"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#3-using-the-profile","text":"Assuming all the setup steps were carried out above, catalogue items should now be able to use a new profile called 'Digital Object Identifiers DataCite Dataset Schema '.","title":"3. Using the profile"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#31-add-the-profile","text":"Note The DOI profile can only be added by editors of Mauro, and only when the catalogue item is in a draft state (not finalised ). If the profile has not been created before the catalogue item is finalised , there is another opportunity to edit the profile before submitting for a DOI name, as explained below. To add the DOI profile to the catalogue item, first select the catalogue item in the Model Tree that you wish to modify. Once that item's details panel is displayed on the right of your screen, select the 'Description' tab. Expand the 'Default profile' list and click 'Add new profile...' . Select 'Digital Object Identifiers DataCite Dataset Schema ' and click 'Save Changes' . This will display the full DOI profile fields to enter. You can enter values into the fields now, or click 'Save' to store the partially complete profile.","title":"3.1 Add the profile"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#32-editing-the-profile","text":"The DOI profile will present a number of fields which will be recorded as metadata against the catalogue item. These profile field values will also be submitted to the DOI system when a DOI name is requested, so it is important to ensure that the values entered into the profile are as accurate as possible. The profile fields are grouped into a number of sections: Predefined/Supplied Fields Primary Creator Additional Mandatory Fields Additional Optional Title Section Additional Optional Description Section Primary Contributor Additional Optional Fields The Predefined/Supplied Fields are special, read-only fields in the profile. These are used and modified internally by Mauro to track important details about the DOI name, such as the state, identifier, and so on. These are provided in read-only view to the user but are automatically controlled during DOI submission. There are a number of sections and fields in the profile, though only a few a strictly mandatory to submit to the DOI system (marked with a \"*\" symbol next to their field name). Clicking on the help icons next to each field name will provide a description of what the field represents. The profile will appear in read-only form when viewing the catalogue item. If you want to edit the profile, first make sure the 'Digital Object Identifiers DataCite Dataset Schema' profile is selected in the profile selection list and then click the 'Edit' button. This will bring up an 'Edit Profile' form which you will then be able to make changes to. Once you have finished, click 'Save' . Information You are able to save changes to a profile without ensuring that the profile is fully valid. For example, some fields may say 'This field is mandatory' , but changes can be saved temporarily without such fields being filled in. To test that the profile is valid, click on the 'Validate' button, then review any validation alerts that appear in each field.","title":"3.2 Editing the profile"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#34-remove-the-profile","text":"If you decide against using the DOI profile, it can be removed from the catalogue item. Once again, make sure that the 'Digital Object Identifiers DataCite Dataset Schema' profile is selected in the profile selection list. Then click the three vertical dot menu to the right of the 'Edit' button and then select 'Remove profile' from the dropdown menu. A notification box will then appear asking if you are sure you want to remove this profile. Click 'Yes, remove' and the profile will then be removed along with all the metadata stored for that profile. A notification box will appear at the bottom right of your screen, confirming that the profile was successfully removed.","title":"3.4 Remove the profile"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#4-submit-and-create-a-doi-name","text":"The 'Identifier' profile field will store the DOI name once the catalogue item has been submitted to the DOI system, for instance 10.1109/5.771073 . To store this DOI name, the catalogue item must first be submitted to the DOI system. To do this, select the relevant catalogue item from the Model Tree that you want to submit. The item must be finalised and publicly readable to anyone. Once the details panel has appeared on the right of your screen, click the 'Description' tab to view the profiles. Then click the three vertical dot menu to the right of the 'Edit' button and select 'DOI submission' from the dropdown menu. You will then have two possible states to generate the DOI name: Draft Allows for the profile fields to be modified whilst still obtaining a DOI name. Suitable for making edits to the profile whilst it is work-in-progress Final Fixes the DOI name and the metadata associated with it in the DOI system. The profile is finalised and cannot be modified further Information The 'Digital Object Identifiers DataCite Dataset Schema' profile does not need to be selected to carry out this step, nor does it have to be created yet. Mauro will automatically handle the creation of the DOI profile if it does not already exist. Before submitting, an 'Edit Profile' form will appear which will allow you to review the profile fields and make any necessary changes before submitting to the DOI system. Information It is recommended to use the Validate button to ensure that all profile fields are valid before submitting. Once you are happy with the profile, click the 'Submit' button and a green notification box will appear at the bottom right of your screen to confirm the change and the 'Identifier' field will be populated with the newly created DOI name. You may continue making changes to any Draft DOI profiles by repeating the same steps above. However, once changing to the Final state, you will have one last opportunity to review/modify the profile before it is fixed in place forever.","title":"4. Submit and create a DOI name"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#5-retire-a-doi-name","text":"Once a DOI name is recorded in the profile, it is possible to retire that DOI name. When a DOI name is retired, any network links back to that catalogue item will no longer work. Warning Be careful when retiring DOI names - once retired, this cannot be undone. To retire a DOI name, first select the catalogue item in the Model Tree with the DOI name to retire. Once the details panel has appeared on the right of your screen, click the 'Description' tab to view the profiles. Then click the three vertical dot menu to the right of the 'Edit' button and select 'DOI submission' and then 'Retire' from the dropdown menus. A notification box will appear asking you to confirm that you would like to retire the DOI name. Click 'Yes, retire' and a green notification box shoud appear at the bottom right of your screen confirming that the DOI name was retired successfully.","title":"5. Retire a DOI name"},{"location":"plugins/user-guides/digital-object-identifiers/digital-object-identifiers/#6-doi-name-resolution","text":"When Mauro catalogue items are submitted and recorded in the DOI system, all network links for those catalogue items are routed through the DOI system back to Mauro. All hyperlinks back to Mauro will point to: 1 https://{domain}/#/doi/{name} Where {name} is the DOI name in question, for example: 1 https://{domain}/#/doi/10.1109/5.771073 Any URL in this format will automatically resolve and redirect to the correct Mauro catalogue item to view.","title":"6. DOI name resolution"},{"location":"plugins/user-guides/openid-connect/openid-connect/","text":"1. Overview \u00b6 Using an optional plugin for Mauro security, it is possible to extend authentication of users in Mauro to also include OpenID Connect authentication. This allows administrators to: Configure external providers that support the OpenID Connect standard, such as Google, Microsoft, etc, to be included as 'sign-in' options for Mauro Manage users of different account types to still be authenticated and authorized to use Mauro, using the existing user groups and permissions features This allows users of Mauro to sign in, or register, using a non-Mauro account, such as a user's existing Google/Microsoft account, and still get access to the Mauro catalogue with suitable Mauro permissions assigned. 2. Administrator setup \u00b6 Note This section applies to administrators of Mauro only. There are a number of steps an administrator must carry out before the OpenID Connect authentication can be used by users. 2.1 Install the plugin \u00b6 The Mauro OpenID Connect Authentication must be installed on the Mauro instance. See the instructions for installing plugins , as well as the GitHub repo README file . 2.2 Enable the feature \u00b6 As this is an optional feature, the user interface will not show OpenID Connect configuration elements by default. To allow OpenID Connect to be used, you will first need to click the white arrow next to your user icon and then select 'Configuration' from the dropdown menu. This will bring up the 'Configuration' panel. Click the 'Properties' tab and then click '+Add' . An 'Add Property' form will appear which you will need to complete. Click the 'Select the property to add' box and select 'feature.use_open_id_connect' from the dropdown menu, which will automatically fill in additional fields. Select 'Yes' from the 'Value' dropdown menu and then click 'Add property' . A green notification box will appear at the bottom right of your screen confirming that the 'Property was saved successfully' and the OpenID Connect option will now be available in the administration navigation menu. 2.3 Create OpenID Connect providers \u00b6 Each OpenID Connect provider service must be created and configured via the site for each provider e.g. Google, Microsoft etc. There are three providers which can be bootstrapped (added automatically when the aplication starts up) using the build configuration files. These three providers are: Keycloak Using ${openidConnectConfig.baseUrl}/realms/${openidConnectConfig.realm}/.well-known/openid-configuration Google Using https://accounts.google.com/.well-known/openid-configuration Microsoft Using https://login.microsoftonline.com/common/.well-known/openid-configuration ) 2.4 Add providers to Mauro \u00b6 Once the OpenID Connect provider service has been configured, the details can be entered into Mauro. Remember that you can have more than one of each type of provider as long as each has a unique Label . Click the white arrow next to your user icon and then select 'OpenID Connect' from the dropdown menu. This will bring up a list of 'Open ID Connect Providers' . Click the '+ Add' button and a 'Add OpenID Connect Provider' form will appear which you will need to complete. Label An identifying label for this provider. This will also be the text used when displaying the login button in the login form, so use a suitable name Image URL (optional) The URL to a public image that can be used as an icon for the login button. A live preview button is available to test this field Client ID The unique client ID that was provided by the OpenID Connect service to be used Client Secret The unique client secret/password that was provided by the OpenID Connect service to be used Next, enter the 'Discovery' details for the OpenID Connect service. There are two 'Discovery' forms: Standard Providing a URL to a standardised dicovery document allows Mauro to automatically connect to the service and ascertain the necessary endpoints required for authorization, obtaining tokens etc Non-Standard If the service does not provide a discovery document, then the alternative is to manually enter the endpoints required Tick the 'Use discovery document for endpoints' checkbox as appropriate and enter the necessary details. Alternatively, you can enter advanced details by expanding the 'Authorization endpoint parameters' section. If not explicitly defined, Mauro will pick suitable defaults for these values. Once finished, click the 'Add provider' button to create the provider in Mauro. 2.5 Edit/delete providers \u00b6 Once created, OpenID Connect providers can be edited or deleted in Mauro through the 'OpenID Connect' administration page. Click the white arrow next to your user icon and then select 'OpenID Connect' from the dropdown menu. This will bring up a list of 'Open ID Connect Providers' . Click the vertical dot menu to the right of the provider you wish to edit or delete and select the relevant option from the dropdown menu. 3. Testing \u00b6 Once the OpenID Connect providers have been added to Mauro, they can be tested by viewing the login form. Click the 'Log in' button at the top right of the navigation bar. A login form will appear and will now show the standard email/password fields, plus a list of all OpenID Connect providers added to Mauro. Click on any of these provider buttons to be automatically redirected to that provider service and authenticate using their account. If successfully authenticated, the user will be automatically redirected back to Mauro and signed in just like any other user.","title":"OpenID Connect Client"},{"location":"plugins/user-guides/openid-connect/openid-connect/#1-overview","text":"Using an optional plugin for Mauro security, it is possible to extend authentication of users in Mauro to also include OpenID Connect authentication. This allows administrators to: Configure external providers that support the OpenID Connect standard, such as Google, Microsoft, etc, to be included as 'sign-in' options for Mauro Manage users of different account types to still be authenticated and authorized to use Mauro, using the existing user groups and permissions features This allows users of Mauro to sign in, or register, using a non-Mauro account, such as a user's existing Google/Microsoft account, and still get access to the Mauro catalogue with suitable Mauro permissions assigned.","title":"1. Overview"},{"location":"plugins/user-guides/openid-connect/openid-connect/#2-administrator-setup","text":"Note This section applies to administrators of Mauro only. There are a number of steps an administrator must carry out before the OpenID Connect authentication can be used by users.","title":"2. Administrator setup"},{"location":"plugins/user-guides/openid-connect/openid-connect/#21-install-the-plugin","text":"The Mauro OpenID Connect Authentication must be installed on the Mauro instance. See the instructions for installing plugins , as well as the GitHub repo README file .","title":"2.1 Install the plugin"},{"location":"plugins/user-guides/openid-connect/openid-connect/#22-enable-the-feature","text":"As this is an optional feature, the user interface will not show OpenID Connect configuration elements by default. To allow OpenID Connect to be used, you will first need to click the white arrow next to your user icon and then select 'Configuration' from the dropdown menu. This will bring up the 'Configuration' panel. Click the 'Properties' tab and then click '+Add' . An 'Add Property' form will appear which you will need to complete. Click the 'Select the property to add' box and select 'feature.use_open_id_connect' from the dropdown menu, which will automatically fill in additional fields. Select 'Yes' from the 'Value' dropdown menu and then click 'Add property' . A green notification box will appear at the bottom right of your screen confirming that the 'Property was saved successfully' and the OpenID Connect option will now be available in the administration navigation menu.","title":"2.2 Enable the feature"},{"location":"plugins/user-guides/openid-connect/openid-connect/#23-create-openid-connect-providers","text":"Each OpenID Connect provider service must be created and configured via the site for each provider e.g. Google, Microsoft etc. There are three providers which can be bootstrapped (added automatically when the aplication starts up) using the build configuration files. These three providers are: Keycloak Using ${openidConnectConfig.baseUrl}/realms/${openidConnectConfig.realm}/.well-known/openid-configuration Google Using https://accounts.google.com/.well-known/openid-configuration Microsoft Using https://login.microsoftonline.com/common/.well-known/openid-configuration )","title":"2.3 Create OpenID Connect providers"},{"location":"plugins/user-guides/openid-connect/openid-connect/#24-add-providers-to-mauro","text":"Once the OpenID Connect provider service has been configured, the details can be entered into Mauro. Remember that you can have more than one of each type of provider as long as each has a unique Label . Click the white arrow next to your user icon and then select 'OpenID Connect' from the dropdown menu. This will bring up a list of 'Open ID Connect Providers' . Click the '+ Add' button and a 'Add OpenID Connect Provider' form will appear which you will need to complete. Label An identifying label for this provider. This will also be the text used when displaying the login button in the login form, so use a suitable name Image URL (optional) The URL to a public image that can be used as an icon for the login button. A live preview button is available to test this field Client ID The unique client ID that was provided by the OpenID Connect service to be used Client Secret The unique client secret/password that was provided by the OpenID Connect service to be used Next, enter the 'Discovery' details for the OpenID Connect service. There are two 'Discovery' forms: Standard Providing a URL to a standardised dicovery document allows Mauro to automatically connect to the service and ascertain the necessary endpoints required for authorization, obtaining tokens etc Non-Standard If the service does not provide a discovery document, then the alternative is to manually enter the endpoints required Tick the 'Use discovery document for endpoints' checkbox as appropriate and enter the necessary details. Alternatively, you can enter advanced details by expanding the 'Authorization endpoint parameters' section. If not explicitly defined, Mauro will pick suitable defaults for these values. Once finished, click the 'Add provider' button to create the provider in Mauro.","title":"2.4 Add providers to Mauro"},{"location":"plugins/user-guides/openid-connect/openid-connect/#25-editdelete-providers","text":"Once created, OpenID Connect providers can be edited or deleted in Mauro through the 'OpenID Connect' administration page. Click the white arrow next to your user icon and then select 'OpenID Connect' from the dropdown menu. This will bring up a list of 'Open ID Connect Providers' . Click the vertical dot menu to the right of the provider you wish to edit or delete and select the relevant option from the dropdown menu.","title":"2.5 Edit/delete providers"},{"location":"plugins/user-guides/openid-connect/openid-connect/#3-testing","text":"Once the OpenID Connect providers have been added to Mauro, they can be tested by viewing the login form. Click the 'Log in' button at the top right of the navigation bar. A login form will appear and will now show the standard email/password fields, plus a list of all OpenID Connect providers added to Mauro. Click on any of these provider buttons to be automatically redirected to that provider service and authenticate using their account. If successfully authenticated, the user will be automatically redirected back to Mauro and signed in just like any other user.","title":"3. Testing"},{"location":"resources/architecture/","text":"Overview \u00b6 Mauro Data Mapper is built using a fairly common layered design. At the heart is a standard, relational database where all data is primarily stored. All interaction with the database is controlled through business logic within the ' Core ' layer. Interfaces around the core allow interaction at different levels of abstraction Relational Database \u00b6 Mauro Data Mapper has been built on top of PostgreSQL and in particular is tried and tested against PostgreSQL version 12. However, we've taken care not to use any clever features or plugins so that the majority of the code should run against any recent version of PostgreSQL . Furthermore, since the interaction with the relational database is based upon the Hibernate ORM it could even be possible to rebuild against other database implementations, but we've yet to try. The only exception is during integration testing, in which an in-memory h2 database is used in order to speed up testing. System administrators with access to the database can access the data directly, and this is the preferred route for taking backups . However, editing or interpreting the data directly through the database is not recommended, as this will bypass the business logic in the core, with potential loss of system integrity. Core \u00b6 The Core component is built using Grails (version 4), which is a Java-based Model-View-Controller framework. Code is typically written in Groovy , which itself compiles down to Java. Much of the Grails framework is built on top of the widely-used Spring components. The Core codebase defines the object-oriented domain model, which specifies the structure and constraints on the underlying model. All program logic is contained within Services and Controllers , with Views defining the structure of any outputs to procedures or requests. REST API \u00b6 The REST API is a logical layer, defined completely within the Core component and is the standard way of interacting with the platform. A standard REST -style interface makes use of standard HTTP commands, for example GET , POST , PUT , and DELETE . Each REST endpoint is defined by a Controller and View within the Grails Core . Some endpoints are aliased for ease of use, or backwards compatibility, and there is genericity built in to make programming against the API easier. Plugins may extend the API with new endpoints. Each endpoint typically receives and responds in JSON; some can use XML but this is less well tested. Custom data formats apply in particular circumstances - for example when dealing with file attachments. Programming APIs \u00b6 The programming APIs wrap REST commands in programming constructs to make it easier for programmers to interact with Mauro Data Mapper without being concerned with the technical details of the REST API . Of the three current APIs, the Java library is most mature and is able to re-use components of the Grails Core for a faithful representation of the underlying object model. The Java API is suitable for programming complex import and export routines and has built in support for a number of batch operations that are not easily achieved through the web user interface. The Java client also supports connections to multiple instances, making it a good tool for implementing more sophisticated federation mechanisms. The Typescript API is a much simpler wrapper around those endpoints used by the web interface. It is hosted as a separate component and can be installed using npm , the standard package manager for javascript applications. User Interface \u00b6 The web-based user interface is defined in Angular 9 using the Angular Material library for look-and-feel. It is a self-contained, single-page web application which makes use of some additional typescript libraries (for rendering diagrams, providing notifications, etc) on top of the standard ones provided by Angular. It only uses the REST API (via the typescript client library) to communicate with the Core , and is built in a modular fashion to allow easy extensibility. Many of the components can be easily re-used in the creation of other web interfaces. Grails Plugins \u00b6 The Grails Core provides an easy mechanism for extension, through standard Grails plugins. A number of pre-defined extension points are available. For example, to implement new importers, exporters, profiles, or authentication mechanisms. However plugins may also arbitrarily extend the REST API with custom functionality, making use of the services and controllers defined within the core. A number of plugins are defined and made available through the central GitHub plugins organisation; developers may feel free to use and adapt those, or write their own, sharing if they wish. Note that some technical plugins such as the SPARQL or Apache Freemarker plugins allow users to perform their own queries against the database and arbitrarily complex queries may affect the performance of the server. The Core component is itself made up of a number of plugins, and can be disassembled for particular use cases. For example, it is possible to disable support for API Keys by compiling a version of the Core with that plugin removed. Search Index \u00b6 The search index improves the performance of searching, this content is stored in-memory (and persisted to files on disk at suitable intervals). In some places in the Core , it may also be used to speed up access to particular model contents. The index is built using Apache Lucene but managed in the code through Hibernate. This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary. The contents of the search index can be hard to inspect for debugging purposes! We use a tool called Marple but be sure to use a compatible version!","title":"Technical Architecture"},{"location":"resources/architecture/#overview","text":"Mauro Data Mapper is built using a fairly common layered design. At the heart is a standard, relational database where all data is primarily stored. All interaction with the database is controlled through business logic within the ' Core ' layer. Interfaces around the core allow interaction at different levels of abstraction","title":"Overview"},{"location":"resources/architecture/#relational-database","text":"Mauro Data Mapper has been built on top of PostgreSQL and in particular is tried and tested against PostgreSQL version 12. However, we've taken care not to use any clever features or plugins so that the majority of the code should run against any recent version of PostgreSQL . Furthermore, since the interaction with the relational database is based upon the Hibernate ORM it could even be possible to rebuild against other database implementations, but we've yet to try. The only exception is during integration testing, in which an in-memory h2 database is used in order to speed up testing. System administrators with access to the database can access the data directly, and this is the preferred route for taking backups . However, editing or interpreting the data directly through the database is not recommended, as this will bypass the business logic in the core, with potential loss of system integrity.","title":"Relational Database"},{"location":"resources/architecture/#core","text":"The Core component is built using Grails (version 4), which is a Java-based Model-View-Controller framework. Code is typically written in Groovy , which itself compiles down to Java. Much of the Grails framework is built on top of the widely-used Spring components. The Core codebase defines the object-oriented domain model, which specifies the structure and constraints on the underlying model. All program logic is contained within Services and Controllers , with Views defining the structure of any outputs to procedures or requests.","title":"Core"},{"location":"resources/architecture/#rest-api","text":"The REST API is a logical layer, defined completely within the Core component and is the standard way of interacting with the platform. A standard REST -style interface makes use of standard HTTP commands, for example GET , POST , PUT , and DELETE . Each REST endpoint is defined by a Controller and View within the Grails Core . Some endpoints are aliased for ease of use, or backwards compatibility, and there is genericity built in to make programming against the API easier. Plugins may extend the API with new endpoints. Each endpoint typically receives and responds in JSON; some can use XML but this is less well tested. Custom data formats apply in particular circumstances - for example when dealing with file attachments.","title":"REST API"},{"location":"resources/architecture/#programming-apis","text":"The programming APIs wrap REST commands in programming constructs to make it easier for programmers to interact with Mauro Data Mapper without being concerned with the technical details of the REST API . Of the three current APIs, the Java library is most mature and is able to re-use components of the Grails Core for a faithful representation of the underlying object model. The Java API is suitable for programming complex import and export routines and has built in support for a number of batch operations that are not easily achieved through the web user interface. The Java client also supports connections to multiple instances, making it a good tool for implementing more sophisticated federation mechanisms. The Typescript API is a much simpler wrapper around those endpoints used by the web interface. It is hosted as a separate component and can be installed using npm , the standard package manager for javascript applications.","title":"Programming APIs"},{"location":"resources/architecture/#user-interface","text":"The web-based user interface is defined in Angular 9 using the Angular Material library for look-and-feel. It is a self-contained, single-page web application which makes use of some additional typescript libraries (for rendering diagrams, providing notifications, etc) on top of the standard ones provided by Angular. It only uses the REST API (via the typescript client library) to communicate with the Core , and is built in a modular fashion to allow easy extensibility. Many of the components can be easily re-used in the creation of other web interfaces.","title":"User Interface"},{"location":"resources/architecture/#grails-plugins","text":"The Grails Core provides an easy mechanism for extension, through standard Grails plugins. A number of pre-defined extension points are available. For example, to implement new importers, exporters, profiles, or authentication mechanisms. However plugins may also arbitrarily extend the REST API with custom functionality, making use of the services and controllers defined within the core. A number of plugins are defined and made available through the central GitHub plugins organisation; developers may feel free to use and adapt those, or write their own, sharing if they wish. Note that some technical plugins such as the SPARQL or Apache Freemarker plugins allow users to perform their own queries against the database and arbitrarily complex queries may affect the performance of the server. The Core component is itself made up of a number of plugins, and can be disassembled for particular use cases. For example, it is possible to disable support for API Keys by compiling a version of the Core with that plugin removed.","title":"Grails Plugins"},{"location":"resources/architecture/#search-index","text":"The search index improves the performance of searching, this content is stored in-memory (and persisted to files on disk at suitable intervals). In some places in the Core , it may also be used to speed up access to particular model contents. The index is built using Apache Lucene but managed in the code through Hibernate. This means that it is always kept in sync with the database contents, but it can be re-indexed if necessary. The contents of the search index can be hard to inspect for debugging purposes! We use a tool called Marple but be sure to use a compatible version!","title":"Search Index"},{"location":"resources/client/java/","text":"Introduction \u00b6 The Java / Groovy client library wraps the REST API in Java methods to make it easy for Java developers to interact with a Mauro instance. The library makes use of the mdm-core grails application, essentially loading a local, in-memory copy of the Mauro Core to take advantage of the services and controllers from Core, as well as re-using the domain model, and in-built validation. The REST API library can connect to a remote Mauro instance, and is typically used to perform bulk operations such as importing and exporting models, such as when scripting a complex import. This is often the easiest way to experiment with importing before building a Grails Plugin . The code can also be used to connect to multiple Mauro instances, for example to copy models from one instance to another. API documentation \u00b6 The API is documented using GroovyDoc and the complete documentation can be found here . Add dependency \u00b6 In order to include the Mauro client library in your Java / Groovy project, use the following dependency in Gradle or Maven: Gradle 1 2 3 4 5 6 7 8 9 10 repositories { ... maven {url \"https://jenkins.cs.ox.ac.uk/artifactory/libs-release\"} ... } dependencies { ... compile \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-api-java-restful:{version}\" } Maven 1 2 3 4 5 6 7 8 9 10 11 12 13 14 <distributionManagement> <snapshotRepository> <id> snapshots </id> <name> jenkins.cs.ox.ac.uk-snapshots </name> <url> http://jenkins.cs.ox.ac.uk/artifactory/libs-snapshot-local </url> </snapshotRepository> ... </distributionManagement> ... <dependency> <groupId> uk.ac.ox.softeng.maurodatamapper.plugins </groupId> <artifactId> mdm-api-java-restful </artifactId> <version> {version} </version> </dependency> Look on our Release Notes page to find the latest version number Getting started \u00b6 Information The examples given here are in Groovy. Conversion to equivalent Java is a fairly simple task. The simplest way to get started is by creating a client manually - using the url of the server that the instance is hosted on, and a username / password. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient class TestConnection { static void main ( String [] args ) { String baseUrl = \"http://localhost:8080\" String username = \"...\" String password = \"...\" new DataMapperClient ( baseUrl , username , password ). withCloseable { client -> // client is now connected with a session to a Mauro instance // Now we can do things with our client client . createFolder ( \"Test Folder\" ) } } } The BindingMauroDataMapperClient creates a connection to a Mauro instance, and maintains a session where necessary. The class implements the Closeable interface which means that any session will be closed at the end of the withCloseable closure. Authentication and passing arguments \u00b6 As in the example above, you can connect to the Mauro instance using a username and password . In this case, any session cookie returned will be stored and used for future calls automatically. You can also connect using an API Key and this will be passed in the parameters for every call. It's usually more convenient to pass arguments such as usernames, passwords or API keys in as parameters, rather than hard-coding them into the application. We take advantage of PicoCli to provide options for passing these parameters to an application. In the most basic case, consider the following application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient class TestConnection2 extends MdmCommandLineTool < MdmConnectionOptions > { static void main ( String [] args ) { TestConnection2 testConnection2 = new TestConnection2 ( args ) TestConnection2 . doStuff () } void doStuff () { options . getMauroDataMapperClient (). withCloseable { client -> // Do something with our client here... } } } The class extends MdmCommandLineTool , which provides an object of type MdmConnectionOptions , representing the options passed in on the command line. This class contains definitions for the following options: -U , --clientBaseUrl , --client.baseUrl The base url of the Mauro instance to connect to - for example http://www.example.com/mauro/ . Any trailing /api will be added automatically. -u , --clientUsername , --client.username The username for logging into this Mauro instance. -p , --clientPassword , --client.password The password for logging into this Mauro instance. -a , --clientApiKey , --client.apiKey The API key for logging into this Mauro instance. -h , --help Displays a help message describing these options -v , --verbose Runs the application in 'verbose' mode, giving additional logging for debug purposes -D , --debug Provide advanced debug information as logs -P , --properties Provides further parameters via a properties file - for example: --properties ./config.properties . See below for more details. The final option allows these properties to be passed in a standard java properties file. Properties should be provided using the format provided in the final options above - for example: 1 2 client.baseUrl = http://localhost:8080/ client.apiKey = 767c6e02-4ad6-4480-8b42-a36160143a24 The MdmConnectionOptions class provides a default mechanism to create a new BindingMauroDataMapperClient from the provided values. The class can be extended to provide additional, application-specific options. To provide specific functionality for these options, you can also extend the MdmCommandLineTool class. Dealing with multiple connections \u00b6 Occasionally, it's useful to deal with multiple connections to a single catalogue, or connections to more than one catalogue instance. The client can store multiple named connections and all methods have an optional final parameter to choose which connection to use. For example, the following code connects to two instances of Mauro, naming the connections as 'source' and 'target': 1 2 3 BindingMauroDataMapperClient bindingMauroDataMapperClient = new BindingMauroDataMapperClient () bindingMauroDataMapperClient . openConnection ( \"source\" , sourceProperties ) bindingMauroDataMapperClient . openConnection ( \"target\" , targetProperties ) Subsequent method calls can pass the name of the connection as a final argument - for example the following code copies a DataModel from the \"source\" instance of Mauro to the \"target\": 1 2 3 4 DataModel dataModel = bindingMauroDataMapperClient . exportAndBindDataModelById ( dataModelId , \"source\" ) bindingMauroDataMapperClient . importDataModel ( dataModel , folderId , dataModelName , finalised , importAsNewDocumentationVersion , \"target\" ) In every method, if no connection is specified by name, the default connection (internally named \"_default\") is used. Binding vs. non-binding clients \u00b6 The library provides two different clients: the first is the simpler MauroDataMapperClient . This provides a number of methods for interacting with the Mauro REST API in a more native form: dealing with responses in Map form. The alternative is the more complex BindingMauroDataMapperClient which extends MauroDataMapperClient with additional methods for binding responses into the appropriate Mauro domain types. For example, compare the following methods: Map exportDataModel(UUID id, String connectionName = defaultConnectionName) and DataModel exportAndBindDataModelById(UUID id, String connectionName = defaultConnectionName) The two methods access the same REST endpoint: the former returns the response JSON in map form; the latter takes that response and binds it to an object of class DataModel . The latter is obviously easier to process, but the former provides a faster response. The former variant also provides an important advantage: when a Map is bound to a Data Model within grails, it is associated with the internal, in-memory database and validated. At this point, any identifiers, or 'last modified' dates associated with any component of that DataModel will be dropped in favour of local variants. So, for example, if you wanted to re-use the identifiers of the DataClasses contained within that DataModel (for example, in order to update their descriptions individually in the remote instance), then you should use the non-binding version of the method. In general, since the BindingMauroDataMapperClient extends the MauroDataMapperClient class, the binding version is all that is required. The binding client has additional methods for manually binding results of calls after intermediate processing.","title":"Groovy / Java"},{"location":"resources/client/java/#introduction","text":"The Java / Groovy client library wraps the REST API in Java methods to make it easy for Java developers to interact with a Mauro instance. The library makes use of the mdm-core grails application, essentially loading a local, in-memory copy of the Mauro Core to take advantage of the services and controllers from Core, as well as re-using the domain model, and in-built validation. The REST API library can connect to a remote Mauro instance, and is typically used to perform bulk operations such as importing and exporting models, such as when scripting a complex import. This is often the easiest way to experiment with importing before building a Grails Plugin . The code can also be used to connect to multiple Mauro instances, for example to copy models from one instance to another.","title":"Introduction"},{"location":"resources/client/java/#api-documentation","text":"The API is documented using GroovyDoc and the complete documentation can be found here .","title":"API documentation"},{"location":"resources/client/java/#add-dependency","text":"In order to include the Mauro client library in your Java / Groovy project, use the following dependency in Gradle or Maven: Gradle 1 2 3 4 5 6 7 8 9 10 repositories { ... maven {url \"https://jenkins.cs.ox.ac.uk/artifactory/libs-release\"} ... } dependencies { ... compile \"uk.ac.ox.softeng.maurodatamapper.plugins:mdm-api-java-restful:{version}\" } Maven 1 2 3 4 5 6 7 8 9 10 11 12 13 14 <distributionManagement> <snapshotRepository> <id> snapshots </id> <name> jenkins.cs.ox.ac.uk-snapshots </name> <url> http://jenkins.cs.ox.ac.uk/artifactory/libs-snapshot-local </url> </snapshotRepository> ... </distributionManagement> ... <dependency> <groupId> uk.ac.ox.softeng.maurodatamapper.plugins </groupId> <artifactId> mdm-api-java-restful </artifactId> <version> {version} </version> </dependency> Look on our Release Notes page to find the latest version number","title":"Add dependency"},{"location":"resources/client/java/#getting-started","text":"Information The examples given here are in Groovy. Conversion to equivalent Java is a fairly simple task. The simplest way to get started is by creating a client manually - using the url of the server that the instance is hosted on, and a username / password. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient class TestConnection { static void main ( String [] args ) { String baseUrl = \"http://localhost:8080\" String username = \"...\" String password = \"...\" new DataMapperClient ( baseUrl , username , password ). withCloseable { client -> // client is now connected with a session to a Mauro instance // Now we can do things with our client client . createFolder ( \"Test Folder\" ) } } } The BindingMauroDataMapperClient creates a connection to a Mauro instance, and maintains a session where necessary. The class implements the Closeable interface which means that any session will be closed at the end of the withCloseable closure.","title":"Getting started"},{"location":"resources/client/java/#authentication-and-passing-arguments","text":"As in the example above, you can connect to the Mauro instance using a username and password . In this case, any session cookie returned will be stored and used for future calls automatically. You can also connect using an API Key and this will be passed in the parameters for every call. It's usually more convenient to pass arguments such as usernames, passwords or API keys in as parameters, rather than hard-coding them into the application. We take advantage of PicoCli to provide options for passing these parameters to an application. In the most basic case, consider the following application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import uk.ac.ox.softeng.maurodatamapper.api.restful.client.MauroDataMapperClient class TestConnection2 extends MdmCommandLineTool < MdmConnectionOptions > { static void main ( String [] args ) { TestConnection2 testConnection2 = new TestConnection2 ( args ) TestConnection2 . doStuff () } void doStuff () { options . getMauroDataMapperClient (). withCloseable { client -> // Do something with our client here... } } } The class extends MdmCommandLineTool , which provides an object of type MdmConnectionOptions , representing the options passed in on the command line. This class contains definitions for the following options: -U , --clientBaseUrl , --client.baseUrl The base url of the Mauro instance to connect to - for example http://www.example.com/mauro/ . Any trailing /api will be added automatically. -u , --clientUsername , --client.username The username for logging into this Mauro instance. -p , --clientPassword , --client.password The password for logging into this Mauro instance. -a , --clientApiKey , --client.apiKey The API key for logging into this Mauro instance. -h , --help Displays a help message describing these options -v , --verbose Runs the application in 'verbose' mode, giving additional logging for debug purposes -D , --debug Provide advanced debug information as logs -P , --properties Provides further parameters via a properties file - for example: --properties ./config.properties . See below for more details. The final option allows these properties to be passed in a standard java properties file. Properties should be provided using the format provided in the final options above - for example: 1 2 client.baseUrl = http://localhost:8080/ client.apiKey = 767c6e02-4ad6-4480-8b42-a36160143a24 The MdmConnectionOptions class provides a default mechanism to create a new BindingMauroDataMapperClient from the provided values. The class can be extended to provide additional, application-specific options. To provide specific functionality for these options, you can also extend the MdmCommandLineTool class.","title":"Authentication and passing arguments"},{"location":"resources/client/java/#dealing-with-multiple-connections","text":"Occasionally, it's useful to deal with multiple connections to a single catalogue, or connections to more than one catalogue instance. The client can store multiple named connections and all methods have an optional final parameter to choose which connection to use. For example, the following code connects to two instances of Mauro, naming the connections as 'source' and 'target': 1 2 3 BindingMauroDataMapperClient bindingMauroDataMapperClient = new BindingMauroDataMapperClient () bindingMauroDataMapperClient . openConnection ( \"source\" , sourceProperties ) bindingMauroDataMapperClient . openConnection ( \"target\" , targetProperties ) Subsequent method calls can pass the name of the connection as a final argument - for example the following code copies a DataModel from the \"source\" instance of Mauro to the \"target\": 1 2 3 4 DataModel dataModel = bindingMauroDataMapperClient . exportAndBindDataModelById ( dataModelId , \"source\" ) bindingMauroDataMapperClient . importDataModel ( dataModel , folderId , dataModelName , finalised , importAsNewDocumentationVersion , \"target\" ) In every method, if no connection is specified by name, the default connection (internally named \"_default\") is used.","title":"Dealing with multiple connections"},{"location":"resources/client/java/#binding-vs-non-binding-clients","text":"The library provides two different clients: the first is the simpler MauroDataMapperClient . This provides a number of methods for interacting with the Mauro REST API in a more native form: dealing with responses in Map form. The alternative is the more complex BindingMauroDataMapperClient which extends MauroDataMapperClient with additional methods for binding responses into the appropriate Mauro domain types. For example, compare the following methods: Map exportDataModel(UUID id, String connectionName = defaultConnectionName) and DataModel exportAndBindDataModelById(UUID id, String connectionName = defaultConnectionName) The two methods access the same REST endpoint: the former returns the response JSON in map form; the latter takes that response and binds it to an object of class DataModel . The latter is obviously easier to process, but the former provides a faster response. The former variant also provides an important advantage: when a Map is bound to a Data Model within grails, it is associated with the internal, in-memory database and validated. At this point, any identifiers, or 'last modified' dates associated with any component of that DataModel will be dropped in favour of local variants. So, for example, if you wanted to re-use the identifiers of the DataClasses contained within that DataModel (for example, in order to update their descriptions individually in the remote instance), then you should use the non-binding version of the method. In general, since the BindingMauroDataMapperClient extends the MauroDataMapperClient class, the binding version is all that is required. The binding client has additional methods for manually binding results of calls after intermediate processing.","title":"Binding vs. non-binding clients"},{"location":"resources/client/net/","text":"The .Net client library wraps the REST API in .net methods to make it easy for .Net developers to interact with a Mauro instance. The library makes use of the .Net Core API application with Controller & Model to call API methods The REST API library can connect to a remote Mauro instance, and is typically used to perform bulk operations such as importing and exporting models. The code can also be used to connect to multiple Mauro instances, for example to copy models from one instance to another. API documentation \u00b6 The API documentation is built using DocFx and is available here : Adding as a dependency \u00b6 In order to include the Mauro client library in your .Net project, you will need to include the following dependencies: CsvHelper Newtonsoft.Json System.Text.Json A .dll file built from the code can be referenced as mdm-api-dotnet-restful API Client \u00b6 MauroDataMapperClient provides constructors to login either by UserId, Password, BaseUrl & Connection Name Properties Object with Properties - UserName, Password & BaseUrl If you connect to an instance using a username and password, any session cookie returned will be stored and used for future calls automatically. It's usually more convenient to pass arguments such as usernames, passwords as command-line parameters, rather than hard-coding them into the application. API Client Methods \u00b6 The methods of the API Client make use of the System.Net.Http package. This library is described in more detail here . Each accepts an object of the class HttpRequestMessage , and each returns an instance of HttpResponseMessage .Net Console Application \u00b6 The mdm-api-dotnet-console application has been built as a Windows-friendly tool for bulk CSV import. In order to include this as part of an application, you need to add a reference to the mdm-api-dotnet-restful .dll file. This application has its own configuration settings: Folder & File location Whether to create each CSV definition as a DataClass or a DataModel The server address & user credentials are read from the app.config file. Once files are read, and their definitions uploaded to Mauro, they are moved to the 'Archive' folder.","title":".NET"},{"location":"resources/client/net/#api-documentation","text":"The API documentation is built using DocFx and is available here :","title":"API documentation"},{"location":"resources/client/net/#adding-as-a-dependency","text":"In order to include the Mauro client library in your .Net project, you will need to include the following dependencies: CsvHelper Newtonsoft.Json System.Text.Json A .dll file built from the code can be referenced as mdm-api-dotnet-restful","title":"Adding as a dependency"},{"location":"resources/client/net/#api-client","text":"MauroDataMapperClient provides constructors to login either by UserId, Password, BaseUrl & Connection Name Properties Object with Properties - UserName, Password & BaseUrl If you connect to an instance using a username and password, any session cookie returned will be stored and used for future calls automatically. It's usually more convenient to pass arguments such as usernames, passwords as command-line parameters, rather than hard-coding them into the application.","title":"API Client"},{"location":"resources/client/net/#api-client-methods","text":"The methods of the API Client make use of the System.Net.Http package. This library is described in more detail here . Each accepts an object of the class HttpRequestMessage , and each returns an instance of HttpResponseMessage","title":"API Client Methods"},{"location":"resources/client/net/#net-console-application","text":"The mdm-api-dotnet-console application has been built as a Windows-friendly tool for bulk CSV import. In order to include this as part of an application, you need to add a reference to the mdm-api-dotnet-restful .dll file. This application has its own configuration settings: Folder & File location Whether to create each CSV definition as a DataClass or a DataModel The server address & user credentials are read from the app.config file. Once files are read, and their definitions uploaded to Mauro, they are moved to the 'Archive' folder.","title":".Net Console Application"},{"location":"resources/client/typescript/","text":"Introduction \u00b6 The TypeScript client library wraps the REST API in TypeScript classes/functions to make it easy for JavaScript and TypeScript developers to interact with a Mauro instance. The TypeScript library that implements communication with the back-end server is available as a standalone repository for incorporation into other applications. For example other web interfaces, or back-end functionality using node.js . This is in fact the client library that the Mauro Data Mapper user interface uses. The GitHub repository is called mdm-resources and is available within the Mauro Data Mapper organisation . API documentation \u00b6 The API is documented using TypeDoc and the complete documentation can be found here . Layout \u00b6 Methods to call API functions are roughly broken down by resource type, with filenames conforming to the pattern: mdm-{resourceType}.resource.ts There are additional utility functions available in mdm-resource.ts . There are also type definitions to assist with requests and responses, which can be found in filenames of the format mdm-{resourceType}.model.ts . An index.ts file lists all files for inclusion. Resources \u00b6 Each mdm-{resourceType}.resource.ts file defines a new class extending the super class MdmResource , and provides methods for each endpoint. These make use of the simpleGet() , simplePost() , etc methods defined in the super class. Every class that extends MdmResource can optionally provide these in the constructor : MdmResourcesConfiguration - object to define configuration options for every HTTP request. MdmRestHandler - object to the REST handler that will process the requests. If not provided, the DefaultMdmRestHandler will be used - see the REST Handlers section for further details. Including in applications \u00b6 If you are using NPM or Yarn , then you need the following line in your .npmrc or .yarnrc file: 1 @maurodatamapper:registry=https://npm.pkg.github.com` You can then add a line such as the following to your package.json file: 1 2 3 4 \"dependencies\" : { ... \"@maurodatamapper/mdm-resources\" : \"github:MauroDataMapper/mdm-resources#{version}\" } Where {version} refers to a git tag or branch name. Within a TypeScript file, you can then add an import statement such as the following: 1 import { MdmResourcesConfiguration } from '@maurodatamapper/mdm-resources' ; Or, as illustrated in the Mauro UI application, create a custom service to pull all the classes into a single location (see mdm-resources.service.ts within the mdm-ui project). REST Handlers \u00b6 mdm-resources provides a default implementation of the MdmRestHandler called DefaultMdmRestHandler . This implementation uses the fetch API to complete HTTP requests and return promises on each response. This default implementation is usually sufficient for most scenarios, but it is also possible to replace this with your own implementation. Reasons why you might want to do this are: To use something other than fetch() . For example, Angular applications tend to use the built-in HTTP Client to return observable streams instead of promises. To intercept any Mauro HTTP requests/responses to perform some custom operations, such as error handling on failed responses. To use a custom REST handler, follow the steps below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Define the class that implements `IMdmRestHandler` export class CustomMdmRestHandler implements IMdmRestHandler { process ( url : string , options : IMdmRestHandlerOptions ) { // (Optional) pre-process step (e.g. logging) const response = /* Send HTTP request */ // (Optional) post-process step (e.g. logging, error handling) return response ; } } // For every MDM resource created, pass in the custom REST handler instance instead const dataModelsResource = new MdmDataModelResource ( null , new CustomMdmRestHandler ()); Handling Responses \u00b6 All endpoints have type definitions that explicitly state their inputs for requests but generalise their response outputs. This is due to the ability to customise the REST handler , which may return different wrapper objects around the core response definitions. Nonetheless, mdm-resources does provide type definitions for responses and will include them in the documentation comments and type reference documentation for the use of the downstream developer. As an example, given this endpoint function type definition: 1 2 3 4 5 export class MdmDataModelResource extends MdmResource { get ( dataModelId : string , query? : QueryParameters , options? : RequestSettings ) : any { //... } } Response types can be explicitly added to return results so that further type checking can be performed, as in these examples: fetch 1 2 3 4 5 6 7 8 9 10 11 const dataModels = new MdmDataModelResource (); dataModels . get ( '679d582c-9f6c-4ce5-99c7-7fad79374637' ) . then (( response : DataModelDetailResponse ) => { // MdmDataModelResponse.get() states it will return `any` but will actually return // Promise<DataModelDetailResponse>. // Adding explicit type information will resolve further code that uses the response. // Do something here with the response... }) Angular 1 2 3 4 5 6 7 8 9 10 11 const dataModels = new MdmDataModelResource (); dataModels . get ( '679d582c-9f6c-4ce5-99c7-7fad79374637' ) . subscribe (( response : DataModelDetailResponse ) => { // MdmDataModelResponse.get() states it will return `any` but will actually return // Observable<DataModelDetailResponse>. // Adding explicit type information will resolve further code that uses the response. // Do something here with the response... })","title":"TypeScript"},{"location":"resources/client/typescript/#introduction","text":"The TypeScript client library wraps the REST API in TypeScript classes/functions to make it easy for JavaScript and TypeScript developers to interact with a Mauro instance. The TypeScript library that implements communication with the back-end server is available as a standalone repository for incorporation into other applications. For example other web interfaces, or back-end functionality using node.js . This is in fact the client library that the Mauro Data Mapper user interface uses. The GitHub repository is called mdm-resources and is available within the Mauro Data Mapper organisation .","title":"Introduction"},{"location":"resources/client/typescript/#api-documentation","text":"The API is documented using TypeDoc and the complete documentation can be found here .","title":"API documentation"},{"location":"resources/client/typescript/#layout","text":"Methods to call API functions are roughly broken down by resource type, with filenames conforming to the pattern: mdm-{resourceType}.resource.ts There are additional utility functions available in mdm-resource.ts . There are also type definitions to assist with requests and responses, which can be found in filenames of the format mdm-{resourceType}.model.ts . An index.ts file lists all files for inclusion.","title":"Layout"},{"location":"resources/client/typescript/#resources","text":"Each mdm-{resourceType}.resource.ts file defines a new class extending the super class MdmResource , and provides methods for each endpoint. These make use of the simpleGet() , simplePost() , etc methods defined in the super class. Every class that extends MdmResource can optionally provide these in the constructor : MdmResourcesConfiguration - object to define configuration options for every HTTP request. MdmRestHandler - object to the REST handler that will process the requests. If not provided, the DefaultMdmRestHandler will be used - see the REST Handlers section for further details.","title":"Resources"},{"location":"resources/client/typescript/#including-in-applications","text":"If you are using NPM or Yarn , then you need the following line in your .npmrc or .yarnrc file: 1 @maurodatamapper:registry=https://npm.pkg.github.com` You can then add a line such as the following to your package.json file: 1 2 3 4 \"dependencies\" : { ... \"@maurodatamapper/mdm-resources\" : \"github:MauroDataMapper/mdm-resources#{version}\" } Where {version} refers to a git tag or branch name. Within a TypeScript file, you can then add an import statement such as the following: 1 import { MdmResourcesConfiguration } from '@maurodatamapper/mdm-resources' ; Or, as illustrated in the Mauro UI application, create a custom service to pull all the classes into a single location (see mdm-resources.service.ts within the mdm-ui project).","title":"Including in applications"},{"location":"resources/client/typescript/#rest-handlers","text":"mdm-resources provides a default implementation of the MdmRestHandler called DefaultMdmRestHandler . This implementation uses the fetch API to complete HTTP requests and return promises on each response. This default implementation is usually sufficient for most scenarios, but it is also possible to replace this with your own implementation. Reasons why you might want to do this are: To use something other than fetch() . For example, Angular applications tend to use the built-in HTTP Client to return observable streams instead of promises. To intercept any Mauro HTTP requests/responses to perform some custom operations, such as error handling on failed responses. To use a custom REST handler, follow the steps below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Define the class that implements `IMdmRestHandler` export class CustomMdmRestHandler implements IMdmRestHandler { process ( url : string , options : IMdmRestHandlerOptions ) { // (Optional) pre-process step (e.g. logging) const response = /* Send HTTP request */ // (Optional) post-process step (e.g. logging, error handling) return response ; } } // For every MDM resource created, pass in the custom REST handler instance instead const dataModelsResource = new MdmDataModelResource ( null , new CustomMdmRestHandler ());","title":"REST Handlers"},{"location":"resources/client/typescript/#handling-responses","text":"All endpoints have type definitions that explicitly state their inputs for requests but generalise their response outputs. This is due to the ability to customise the REST handler , which may return different wrapper objects around the core response definitions. Nonetheless, mdm-resources does provide type definitions for responses and will include them in the documentation comments and type reference documentation for the use of the downstream developer. As an example, given this endpoint function type definition: 1 2 3 4 5 export class MdmDataModelResource extends MdmResource { get ( dataModelId : string , query? : QueryParameters , options? : RequestSettings ) : any { //... } } Response types can be explicitly added to return results so that further type checking can be performed, as in these examples: fetch 1 2 3 4 5 6 7 8 9 10 11 const dataModels = new MdmDataModelResource (); dataModels . get ( '679d582c-9f6c-4ce5-99c7-7fad79374637' ) . then (( response : DataModelDetailResponse ) => { // MdmDataModelResponse.get() states it will return `any` but will actually return // Promise<DataModelDetailResponse>. // Adding explicit type information will resolve further code that uses the response. // Do something here with the response... }) Angular 1 2 3 4 5 6 7 8 9 10 11 const dataModels = new MdmDataModelResource (); dataModels . get ( '679d582c-9f6c-4ce5-99c7-7fad79374637' ) . subscribe (( response : DataModelDetailResponse ) => { // MdmDataModelResponse.get() states it will return `any` but will actually return // Observable<DataModelDetailResponse>. // Adding explicit type information will resolve further code that uses the response. // Do something here with the response... })","title":"Handling Responses"},{"location":"rest-api/admin/","text":"There are a number of endpoints which are specific to administrators: understanding the configuration of the particular instance; discovering the avaiable plugins, etc. Currently logged in users \u00b6 /api/admin/activeSessions This endpoint returns a list of all logged in users. /api/admin/activeSessions If called in post mode, you can pass in user credentials, rather than basing on an existing session. Configuration \u00b6 To find out more about the current instance of the catalogue: what version is running; the version of Java that's runing; the JDBC drivers currently available; call the following endpoint: /api/admin/status Modules \u00b6 To find out which modules are installed, call the following endpoint: /api/admin/modules The post version of the endpoint can be called in order to pass authentication credentials at the same time: /api/admin/modules Plugins \u00b6 To find out which plugins are currently installed, use one of the following endpoints: /api/admin/plugins/exporters /api/admin/plugins/emailers /api/admin/plugins/dataLoaders /api/admin/plugins/importers System actions \u00b6 /api/admin/rebuildLuceneIndexes This endpoint forces the rebuild of the Lucene indexes. This is only necessary when synchronisation between database and indexes is lost; when the search functionality is not returning correct results. Authentication credentials can be passed as part of the request body. Properties \u00b6 There are a number of system-wide properties that can be updated by administrators, such as the text of any emails sent and the email address from which catalogue emails appear to be sent. Properties are composed of keys and values . Keys can be any string with the following restrictions: Must be lowercase alpha characters No spaces are allowed May include periods ('.') and/or underscores ('_') Must be unique Getting properties \u00b6 Properties can be viewed at the following endpoint: /api/admin/properties If successful, the response body will list the available properties: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"count\" : X , \"items\" : [ { \"id\" : \"c7de1358-a4ce-4d72-abca-04013f7f4acc\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : false , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-10T15:17:05.459Z\" }, { \"id\" : \"76becaa3-da04-40d5-a433-51ed203c77b4\" , \"key\" : \"test.property.public\" , \"value\" : \"Public test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : true , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-10T15:17:05.558Z\" } ] } Notice that properties contain a publiclyVisible flag. This is because properties can be created to either be public or restricted to administrators/systems (the default being false ). Only an authenticated session can use the endpoint above, however an anonymous session may use this endpoint to list all publicly available properties: /api/properties To access a single property, this endpoint is provided: /api/admin/properties/ {propertyId} Created, updating and deleting \u00b6 Properties can be created as follows: /api/admin/properties Request body (JSON) 1 2 3 4 5 6 { \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"publiclyVisible\" : false , \"category\" : \"Test\" } If successful, the new property is returned in the response body including the new property id . Response body (JSON) 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : false , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-11T17:46:47.654Z\" } The property can then be updated with the put endpoint: /api/admin/properties Request body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"publiclyVisible\" : false , \"category\" : \"Test\" } And deleted with the delete endpoint: /api/admin/properties/ {propertyId} Data Models \u00b6 The following endpoints provide paginated lists of Data Models (for cleaning / monitoring processes). They list those models which have been deleted, superseded by a new model, and superseded by new documentation, respectively: /api/admin/dataModels/deleted /api/admin/dataModels/modelSuperseded /api/admin/dataModels/documentSuperseded Emails \u00b6 Retrieve the list of emails (recipient, message, date/time) sent by the system: /api/admin/emails","title":"Admin functions"},{"location":"rest-api/admin/#currently-logged-in-users","text":"/api/admin/activeSessions This endpoint returns a list of all logged in users. /api/admin/activeSessions If called in post mode, you can pass in user credentials, rather than basing on an existing session.","title":"Currently logged in users"},{"location":"rest-api/admin/#configuration","text":"To find out more about the current instance of the catalogue: what version is running; the version of Java that's runing; the JDBC drivers currently available; call the following endpoint: /api/admin/status","title":"Configuration"},{"location":"rest-api/admin/#modules","text":"To find out which modules are installed, call the following endpoint: /api/admin/modules The post version of the endpoint can be called in order to pass authentication credentials at the same time: /api/admin/modules","title":"Modules"},{"location":"rest-api/admin/#plugins","text":"To find out which plugins are currently installed, use one of the following endpoints: /api/admin/plugins/exporters /api/admin/plugins/emailers /api/admin/plugins/dataLoaders /api/admin/plugins/importers","title":"Plugins"},{"location":"rest-api/admin/#system-actions","text":"/api/admin/rebuildLuceneIndexes This endpoint forces the rebuild of the Lucene indexes. This is only necessary when synchronisation between database and indexes is lost; when the search functionality is not returning correct results. Authentication credentials can be passed as part of the request body.","title":"System actions"},{"location":"rest-api/admin/#properties","text":"There are a number of system-wide properties that can be updated by administrators, such as the text of any emails sent and the email address from which catalogue emails appear to be sent. Properties are composed of keys and values . Keys can be any string with the following restrictions: Must be lowercase alpha characters No spaces are allowed May include periods ('.') and/or underscores ('_') Must be unique","title":"Properties"},{"location":"rest-api/admin/#getting-properties","text":"Properties can be viewed at the following endpoint: /api/admin/properties If successful, the response body will list the available properties: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"count\" : X , \"items\" : [ { \"id\" : \"c7de1358-a4ce-4d72-abca-04013f7f4acc\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : false , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-10T15:17:05.459Z\" }, { \"id\" : \"76becaa3-da04-40d5-a433-51ed203c77b4\" , \"key\" : \"test.property.public\" , \"value\" : \"Public test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : true , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-10T15:17:05.558Z\" } ] } Notice that properties contain a publiclyVisible flag. This is because properties can be created to either be public or restricted to administrators/systems (the default being false ). Only an authenticated session can use the endpoint above, however an anonymous session may use this endpoint to list all publicly available properties: /api/properties To access a single property, this endpoint is provided: /api/admin/properties/ {propertyId}","title":"Getting properties"},{"location":"rest-api/admin/#created-updating-and-deleting","text":"Properties can be created as follows: /api/admin/properties Request body (JSON) 1 2 3 4 5 6 { \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"publiclyVisible\" : false , \"category\" : \"Test\" } If successful, the new property is returned in the response body including the new property id . Response body (JSON) 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"category\" : \"Test\" , \"publiclyVisible\" : false , \"lastUpdatedBy\" : \"admin@test.com\" , \"createdBy\" : \"admin@test.com\" , \"lastUpdated\" : \"2021-03-11T17:46:47.654Z\" } The property can then be updated with the put endpoint: /api/admin/properties Request body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"fab2b5c4-a8df-4a0a-896e-9c961dbf98aa\" , \"key\" : \"test.property\" , \"value\" : \"Test value\" , \"publiclyVisible\" : false , \"category\" : \"Test\" } And deleted with the delete endpoint: /api/admin/properties/ {propertyId}","title":"Created, updating and deleting"},{"location":"rest-api/admin/#data-models","text":"The following endpoints provide paginated lists of Data Models (for cleaning / monitoring processes). They list those models which have been deleted, superseded by a new model, and superseded by new documentation, respectively: /api/admin/dataModels/deleted /api/admin/dataModels/modelSuperseded /api/admin/dataModels/documentSuperseded","title":"Data Models"},{"location":"rest-api/admin/#emails","text":"Retrieve the list of emails (recipient, message, date/time) sent by the system: /api/admin/emails","title":"Emails"},{"location":"rest-api/apikeys/","text":"API keys offer an alternative way to authenticate to the Mauro Data Mapper REST API instead of logging in with a username and password and saving session cookies. This is the recommended method for authenticating when you: Have long-running processing scripts which could cause sessions to timeout between calls Need to store authentication details in clear text for an external application to use Each user can create multiple API keys, and so when sharing with multiple applications, can disable access individually. API keys are also configured with a default expiry date for additional security. Creating an API Key \u00b6 API keys may be set up through the web interface or via the API. To generate a first API key, the user must be logged in using a username and password - either through the web interface, or through the REST API. On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu. This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys. For each API belonging to the user, the list displays: Name This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys used for different purposes. Key This is the key itself, which is a UUID, unique to this user. Keys can be copied to your clipboard by clicking 'Copy' on the right of the key box. Expiry date This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be 'refreshed' before expiry. Refreshable Specifies whether an API key can be refreshed once it has expired. Status Details whether an API key is 'Active' or 'Disabled' . To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete. Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes. Using an API Key \u00b6 To use an API Key, simply add it into the headers of any REST API call. Info If you use API keys to authenticate, the session cookies are not used to persist identity and so the key should be passed with every call. The header key should be apiKey and the value should be the UUID value of the API key itself. Using Postman \u00b6 If you are using Postman as a client, there are two ways to configure the API key for a request, which both have the same result. Firstly, select the 'Authorization' tab which will display several fields that you need to complete. From the 'TYPE' dropdown menu, select 'API Key' . In the 'Key' box on the right hand side type apiKey . Enter the value of the API key in the 'Value' field and select 'Header' from the 'Add to' dropdown menu. The API key must be passed in the headers, not in the query parameters, which is the second method. This method sets the headers automatically, although you can also set them manually. Select the 'Headers' tab which will display a list of Keys and Values . Again, set the 'Key' field to apiKey and the 'Value' field to the API key value. Refreshing an expired API key \u00b6 When an API key has expired and it has previously been marked as 'Refreshable' , then it may be refreshed with a new expiry date. To do this, navigate to the list of 'API keys' via your user profile. Identify which API keys have 'API Key expired' in the 'Expiry date' column. Click the three vertical dots to the right of the relevant API key and you will now have the option to 'Refresh API Key' in the dropdown menu. Select this option and then enter a new number of days for expiry. Revoking an API key \u00b6 To revoke a particular API key, you can mark it as 'Disabled' . Navigate to the 'API Keys' list and click the three vertical dots to the right of the relevant API key. Select 'Disable' from the dropdown menu. The same option will allow you to re-enable the key if necessary. Info It is good practise to set up different API keys for each application. In this way it is easy to revoke access to a single application without having to recreate all other keys and update other application settings. Managing keys through the REST API \u00b6 Info Note that API Keys can only be managed by the user that they belong to. Once authenticated , the endpoint for listing existing API keys is: /api/catalogueUsers/ {catalogueUserId} /apiKeys This returns a paginated list of API keys as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"count\" : X , \"items\" : [ { \"id\" : \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\" , \"apiKey\" : \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\" , \"name\" : \"Test API Key\" , \"expiryDate\" : \"2022-02-03\" , \"expired\" : false , \"disabled\" : true , \"refreshable\" : false , \"createdDate\" : \"2021-02-03\" }, ... ] } The parameters are as described above. The id field is the global primary key identifer for the key. To create a new API key, post to the following endpoint: /api/catalogueUsers/ {userId} /apiKeys The body of the post method should be structured as follows: Request body (JSON) 1 2 3 4 5 { \"name\" : \"My Name\" , \"expiresInDays\" : 365 , \"refreshable\" : true } Where the parameters are as described above. To enable an existing, disabled API key, you can use it's ID (as described above), with the following endpoint: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /enable Similarly, to disable an existing, enabled API key, use the following: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /disable To refresh an API key, provide the number of days before the next expiry with the following endpoint: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /refresh/ {expiresInDays} Finally, to delete an API key identified by a particular UUID: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {id}","title":"API Keys"},{"location":"rest-api/apikeys/#creating-an-api-key","text":"API keys may be set up through the web interface or via the API. To generate a first API key, the user must be logged in using a username and password - either through the web interface, or through the REST API. On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu. This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys. For each API belonging to the user, the list displays: Name This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys used for different purposes. Key This is the key itself, which is a UUID, unique to this user. Keys can be copied to your clipboard by clicking 'Copy' on the right of the key box. Expiry date This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be 'refreshed' before expiry. Refreshable Specifies whether an API key can be refreshed once it has expired. Status Details whether an API key is 'Active' or 'Disabled' . To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete. Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes.","title":"Creating an API Key"},{"location":"rest-api/apikeys/#using-an-api-key","text":"To use an API Key, simply add it into the headers of any REST API call. Info If you use API keys to authenticate, the session cookies are not used to persist identity and so the key should be passed with every call. The header key should be apiKey and the value should be the UUID value of the API key itself.","title":"Using an API Key"},{"location":"rest-api/apikeys/#using-postman","text":"If you are using Postman as a client, there are two ways to configure the API key for a request, which both have the same result. Firstly, select the 'Authorization' tab which will display several fields that you need to complete. From the 'TYPE' dropdown menu, select 'API Key' . In the 'Key' box on the right hand side type apiKey . Enter the value of the API key in the 'Value' field and select 'Header' from the 'Add to' dropdown menu. The API key must be passed in the headers, not in the query parameters, which is the second method. This method sets the headers automatically, although you can also set them manually. Select the 'Headers' tab which will display a list of Keys and Values . Again, set the 'Key' field to apiKey and the 'Value' field to the API key value.","title":"Using Postman"},{"location":"rest-api/apikeys/#refreshing-an-expired-api-key","text":"When an API key has expired and it has previously been marked as 'Refreshable' , then it may be refreshed with a new expiry date. To do this, navigate to the list of 'API keys' via your user profile. Identify which API keys have 'API Key expired' in the 'Expiry date' column. Click the three vertical dots to the right of the relevant API key and you will now have the option to 'Refresh API Key' in the dropdown menu. Select this option and then enter a new number of days for expiry.","title":"Refreshing an expired API key"},{"location":"rest-api/apikeys/#revoking-an-api-key","text":"To revoke a particular API key, you can mark it as 'Disabled' . Navigate to the 'API Keys' list and click the three vertical dots to the right of the relevant API key. Select 'Disable' from the dropdown menu. The same option will allow you to re-enable the key if necessary. Info It is good practise to set up different API keys for each application. In this way it is easy to revoke access to a single application without having to recreate all other keys and update other application settings.","title":"Revoking an API key"},{"location":"rest-api/apikeys/#managing-keys-through-the-rest-api","text":"Info Note that API Keys can only be managed by the user that they belong to. Once authenticated , the endpoint for listing existing API keys is: /api/catalogueUsers/ {catalogueUserId} /apiKeys This returns a paginated list of API keys as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"count\" : X , \"items\" : [ { \"id\" : \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\" , \"apiKey\" : \"b845e98b-8a42-4332-9323-0c1fc6f5f1db\" , \"name\" : \"Test API Key\" , \"expiryDate\" : \"2022-02-03\" , \"expired\" : false , \"disabled\" : true , \"refreshable\" : false , \"createdDate\" : \"2021-02-03\" }, ... ] } The parameters are as described above. The id field is the global primary key identifer for the key. To create a new API key, post to the following endpoint: /api/catalogueUsers/ {userId} /apiKeys The body of the post method should be structured as follows: Request body (JSON) 1 2 3 4 5 { \"name\" : \"My Name\" , \"expiresInDays\" : 365 , \"refreshable\" : true } Where the parameters are as described above. To enable an existing, disabled API key, you can use it's ID (as described above), with the following endpoint: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /enable Similarly, to disable an existing, enabled API key, use the following: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /disable To refresh an API key, provide the number of days before the next expiry with the following endpoint: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {apiKeyId} /refresh/ {expiresInDays} Finally, to delete an API key identified by a particular UUID: /api/catalogueUsers/ {catalogueUserId} /apiKeys/ {id}","title":"Managing keys through the REST API"},{"location":"rest-api/authentication/","text":"Mauro Data Mapper stores content that may be either publicly accessible, or have access restricted to particular users or groups. Therefore the majority of API requests can be made as an 'anonymous user' (by passing no session information in the request header), or as a 'logged in' user (by passing a valid session key in the request headers). A request to the login will result in a new session token being generated. This is typically 32 hexadecimal characters in length, and uniquely identifies the current session. These tokens should not be shared, and will automatically expire with 30mins of inactivity. Sessions can be manually terminated through a call to the logout resource. At any point the validity of a session may be checked against the server. Login \u00b6 To login to the server, POST to the following API endpoint: /api/authentication/login The request body should contain the username, and the password. The username is not case-sensitive: Request body (JSON) 1 2 3 4 { \"username\" : \"joe.bloggs@test.com\" , \"password\" : \"pa55w0rd\" } Request body (XML) 1 2 3 4 <user> <username> joe.bloggs@test.com </username> <password> pa55w0rd\" </password> </user> Information There is an alternative request body that can be sent if authenticating via OpenID Connect identity providers. Please read the OpenID Connect plugin information page for further details. If successful, the response body will contain the user's id , email address, first and last names, and whether or not that user's account has been disabled (typically false in the case of a successful login). Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"id\" : \"01234567-0123-0123-0123-01234567\" , \"emailAddress\" : \"joe.bloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"pending\" : false , \"disabled\" : false , \"createdBy\" : \"admin@test.com\" } One of the response headers will also contain an identifier for the new session. The header is of the form: 1 Set-Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B; Path=/; HttpOnly The session id (JSESSIONID) can be passed to any subsequent request to ensure that the user's credentials are used. To supply the cookie, it should be placed in the Cookie request header: 1 Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B Further requests without the session cookie will be treated as anonymous requests. Session validation \u00b6 In order to validate whether a session is currently active, or has expired (by logging out, or timed-out due to inactivity): /api/session/isAuthenticated No request body or parameters are required for this request. The response will include a true or false value depending on the validity of the session whose JSESSIONID is passed in as part of the request headers. Response body (JSON) 1 2 3 { \"authenticatedSession\" : true } Administration validation \u00b6 In order to validate whether a session is an administrative role: /api/session/isApplicationAdministration No request body or parameters are required for this request. The response will include a true or false value depending on the validity of the session whose JSESSIONID is passed in as part of the request headers. Response body (JSON) 1 2 3 { \"applicationAdministrationSession\" : true } Logout \u00b6 Every session should ideally be closed manually, rather than leaving it to expire through inactivity. In order to close a user session, you should call the logout endpoint, again including the JSESSIONID cookie as part of the request headers: /api/authentication/logout The response should include the status 204: No Content and the successful response will be empty.","title":"Authentication"},{"location":"rest-api/authentication/#login","text":"To login to the server, POST to the following API endpoint: /api/authentication/login The request body should contain the username, and the password. The username is not case-sensitive: Request body (JSON) 1 2 3 4 { \"username\" : \"joe.bloggs@test.com\" , \"password\" : \"pa55w0rd\" } Request body (XML) 1 2 3 4 <user> <username> joe.bloggs@test.com </username> <password> pa55w0rd\" </password> </user> Information There is an alternative request body that can be sent if authenticating via OpenID Connect identity providers. Please read the OpenID Connect plugin information page for further details. If successful, the response body will contain the user's id , email address, first and last names, and whether or not that user's account has been disabled (typically false in the case of a successful login). Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"id\" : \"01234567-0123-0123-0123-01234567\" , \"emailAddress\" : \"joe.bloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"pending\" : false , \"disabled\" : false , \"createdBy\" : \"admin@test.com\" } One of the response headers will also contain an identifier for the new session. The header is of the form: 1 Set-Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B; Path=/; HttpOnly The session id (JSESSIONID) can be passed to any subsequent request to ensure that the user's credentials are used. To supply the cookie, it should be placed in the Cookie request header: 1 Cookie: JSESSIONID=9257B45A4BCA750570736626C62EE74B Further requests without the session cookie will be treated as anonymous requests.","title":"Login"},{"location":"rest-api/authentication/#session-validation","text":"In order to validate whether a session is currently active, or has expired (by logging out, or timed-out due to inactivity): /api/session/isAuthenticated No request body or parameters are required for this request. The response will include a true or false value depending on the validity of the session whose JSESSIONID is passed in as part of the request headers. Response body (JSON) 1 2 3 { \"authenticatedSession\" : true }","title":"Session validation"},{"location":"rest-api/authentication/#administration-validation","text":"In order to validate whether a session is an administrative role: /api/session/isApplicationAdministration No request body or parameters are required for this request. The response will include a true or false value depending on the validity of the session whose JSESSIONID is passed in as part of the request headers. Response body (JSON) 1 2 3 { \"applicationAdministrationSession\" : true }","title":"Administration validation"},{"location":"rest-api/authentication/#logout","text":"Every session should ideally be closed manually, rather than leaving it to expire through inactivity. In order to close a user session, you should call the logout endpoint, again including the JSESSIONID cookie as part of the request headers: /api/authentication/logout The response should include the status 204: No Content and the successful response will be empty.","title":"Logout"},{"location":"rest-api/errors/","text":"The Mauro Data Mapper API uses standard HTTP response codes to indicate the success or failure of an API request. In addition, some requests also return additional status information relating to the reasons for any error or failure that has occurred. In general, codes of the form 2XX indicate success, codes of the form 4XX indicate an error with the request, and codes of the form 5XX indicate that an error occurred with the server during the processing of a potentially valid request. Hopefully those in the last category are rare! Further details for each of the common error codes are shown in the tables below. Error code tables \u00b6 Code Meaning Description 200 OK The response succeeded as expected 201 Created The POST method was successful and a new resource was created 204 No Content The server successfully processed the request, but no content was returned - for example when deleting a resource Code Meaning Description 400 Bad Request The server cannot process the request - either a required parameter was missing, or the body was badly formatted 401 Unauthorized The requested resource requires authentication , but none was provided as part of the header information 403 Forbidden The server refused to process the request because the authenticated user does not have the correct permissions 404 Not Found The resource requested could not be found. This may be because the URL is malformed, or because the HTTP method was not permitted for this particular URL - for example PUT on a resource which may not be edited 408 Request Timeout The server gave up waiting for a request. This code may occasionally be seen when the upload of a file takes longer than the server is prepared to wait 409 Conflict The server could not process the request because of some conflict in the current state of the resource. Most commonly this occurs when a user tried to log in, despite already being logged in with a valid session Code Meaning Description 500 Internal Server Error This is a catch-all error message, when the request appears valid but the server was unable to process it. This may well be caused by a bug in the software; such error messages may be reported through our issue-tracking software 502 Bad Gateway This is a system error relating to the server. It may be that the Metadata Catalogue is configured incorrectly, or is otherwise not installed correctly 503 Service Unavailable The API server is currently unavailable. It may have been taken down for maintenance, or is otherwise not running 504 Gateway Timeout See 502 - the server may be badly configured or is otherwise unavailable","title":"Errors"},{"location":"rest-api/errors/#error-code-tables","text":"Code Meaning Description 200 OK The response succeeded as expected 201 Created The POST method was successful and a new resource was created 204 No Content The server successfully processed the request, but no content was returned - for example when deleting a resource Code Meaning Description 400 Bad Request The server cannot process the request - either a required parameter was missing, or the body was badly formatted 401 Unauthorized The requested resource requires authentication , but none was provided as part of the header information 403 Forbidden The server refused to process the request because the authenticated user does not have the correct permissions 404 Not Found The resource requested could not be found. This may be because the URL is malformed, or because the HTTP method was not permitted for this particular URL - for example PUT on a resource which may not be edited 408 Request Timeout The server gave up waiting for a request. This code may occasionally be seen when the upload of a file takes longer than the server is prepared to wait 409 Conflict The server could not process the request because of some conflict in the current state of the resource. Most commonly this occurs when a user tried to log in, despite already being logged in with a valid session Code Meaning Description 500 Internal Server Error This is a catch-all error message, when the request appears valid but the server was unable to process it. This may well be caused by a bug in the software; such error messages may be reported through our issue-tracking software 502 Bad Gateway This is a system error relating to the server. It may be that the Metadata Catalogue is configured incorrectly, or is otherwise not installed correctly 503 Service Unavailable The API server is currently unavailable. It may have been taken down for maintenance, or is otherwise not running 504 Gateway Timeout See 502 - the server may be badly configured or is otherwise unavailable","title":"Error code tables"},{"location":"rest-api/importexport/","text":"A number of plugins exist for importing and exporting Data Models, Data Flows, and Terminologies. The endpoint for each import / export contains the details of the plugin to be used, which includes the namespace, the name, and the version number. Data Model \u00b6 Import \u00b6 The endpoint for importing one or more models is as follows: /api/dataModels/import/ {importerNamespace} / {importerName} / {importerVersion} Each importer defines its own set of parameters, relating to the method of import. For example, an XML or JSON import will require a file upload; a SQL import will require a list of connection parameters in order to connect to a relational database. As the import parameters may involve file attachements, standard practise is to provide upload parameters as multipart/form-data format, which allows the attachment of files. The standard importers available with a default installation are as follows: Namespace Name Version ox.softeng.metadatacatalogue.plugins.excel ExcelDataModelImporterService 1.0.0 ox.softeng.metadatacatalogue.core.spi.xml XmlImporterService 2.2 ox.softeng.metadatacatalogue.core.spi.json JsonImporterService 1.1 ox.softeng.metadatacatalogue.plugins.database.postgres PostgresDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.oracle OracleDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.sqlserver SqlServerDatabaseImporterService 2.0.0 These fall into two basic categories - simple file-based importers, or simple database-connection importers. The parameters for each of these types are detailed in the sections below. Simple file-based importers \u00b6 The simple file-based importers include the Excel, XML and JSON importers. These take the following parameters: Parameter Name Description folderId The UUID identifier for the folder that the new model is to be uploaded to. This is mandatory. finalised A mandatory boolean value determining whether the new model is to be marked as finalised. This determines whether the resulting model can be further edited within the interface. importAsNewDocumentationVersion A mandatory boolean value. If this option is selected, then any models with the same name will be superseded. If this option is not set, then the importer will produce an error if there are existing models with the same label. importFile The file containing the data to be imported - for example an XML file, JSON file, or Excel spreadsheet. All fields are mandatory. Simple database-connection importers (example) \u00b6 In order to connect to a database, fields are required to build the connection string, as well as handle the resulting generated model. Each SQL importer is slightly different, but the SQL Server importer serves as an adequate example: Parameter Name Description folderId The UUID identifier for the folder that the new model is to be uploaded to. This is mandatory. finalised A mandatory boolean value determining whether the new model is to be marked as finalised. This determines whether the resulting model can be further edited within the interface. importAsNewDocumentationVersion A mandatory boolean value. If this option is selected, then any models with the same name will be superseded. If this option is not set, then the importer will produce an error if there are existing models with the same label. databaseHost The hostname of the server that is running the database databasePort The port that the database is accessed through. If none is set, then the default port for the specified database type will be used. databaseNames A comma-separated list of database names that are to be analysed and imported. If multiple databases are specified, the same username and password will be used for all. databaseUsername The usesrname used to connect to the database databasePassword The password used to connect to the database domain The User Domain name for SQL Server. This should be used rather than prefixing the username with <DOMAIN>/<username> databasesSSL Whether SSL should be used to connect to the database. The default is false. useNtlmv2 Whether to use NLTMv2 when connecting to the database. The default is false. dataModelName If a single database is imported, this field can be used to override its name schemaNames A comma-separated list of the schema names to import. If not supplied, then all schemas other than 'sys' and 'INFORMATION_SCHEMA' will be imported Other database-connecting import plugins provide a similar list of parameters, to be documented later. Export \u00b6 Data Flow \u00b6 Terminology \u00b6","title":"Import / Export"},{"location":"rest-api/importexport/#data-model","text":"","title":"Data Model"},{"location":"rest-api/importexport/#import","text":"The endpoint for importing one or more models is as follows: /api/dataModels/import/ {importerNamespace} / {importerName} / {importerVersion} Each importer defines its own set of parameters, relating to the method of import. For example, an XML or JSON import will require a file upload; a SQL import will require a list of connection parameters in order to connect to a relational database. As the import parameters may involve file attachements, standard practise is to provide upload parameters as multipart/form-data format, which allows the attachment of files. The standard importers available with a default installation are as follows: Namespace Name Version ox.softeng.metadatacatalogue.plugins.excel ExcelDataModelImporterService 1.0.0 ox.softeng.metadatacatalogue.core.spi.xml XmlImporterService 2.2 ox.softeng.metadatacatalogue.core.spi.json JsonImporterService 1.1 ox.softeng.metadatacatalogue.plugins.database.postgres PostgresDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.oracle OracleDatabaseImporterService 2.0.0 ox.softeng.metadatacatalogue.plugins.database.sqlserver SqlServerDatabaseImporterService 2.0.0 These fall into two basic categories - simple file-based importers, or simple database-connection importers. The parameters for each of these types are detailed in the sections below.","title":"Import"},{"location":"rest-api/importexport/#simple-file-based-importers","text":"The simple file-based importers include the Excel, XML and JSON importers. These take the following parameters: Parameter Name Description folderId The UUID identifier for the folder that the new model is to be uploaded to. This is mandatory. finalised A mandatory boolean value determining whether the new model is to be marked as finalised. This determines whether the resulting model can be further edited within the interface. importAsNewDocumentationVersion A mandatory boolean value. If this option is selected, then any models with the same name will be superseded. If this option is not set, then the importer will produce an error if there are existing models with the same label. importFile The file containing the data to be imported - for example an XML file, JSON file, or Excel spreadsheet. All fields are mandatory.","title":"Simple file-based importers"},{"location":"rest-api/importexport/#simple-database-connection-importers-example","text":"In order to connect to a database, fields are required to build the connection string, as well as handle the resulting generated model. Each SQL importer is slightly different, but the SQL Server importer serves as an adequate example: Parameter Name Description folderId The UUID identifier for the folder that the new model is to be uploaded to. This is mandatory. finalised A mandatory boolean value determining whether the new model is to be marked as finalised. This determines whether the resulting model can be further edited within the interface. importAsNewDocumentationVersion A mandatory boolean value. If this option is selected, then any models with the same name will be superseded. If this option is not set, then the importer will produce an error if there are existing models with the same label. databaseHost The hostname of the server that is running the database databasePort The port that the database is accessed through. If none is set, then the default port for the specified database type will be used. databaseNames A comma-separated list of database names that are to be analysed and imported. If multiple databases are specified, the same username and password will be used for all. databaseUsername The usesrname used to connect to the database databasePassword The password used to connect to the database domain The User Domain name for SQL Server. This should be used rather than prefixing the username with <DOMAIN>/<username> databasesSSL Whether SSL should be used to connect to the database. The default is false. useNtlmv2 Whether to use NLTMv2 when connecting to the database. The default is false. dataModelName If a single database is imported, this field can be used to override its name schemaNames A comma-separated list of the schema names to import. If not supplied, then all schemas other than 'sys' and 'INFORMATION_SCHEMA' will be imported Other database-connecting import plugins provide a similar list of parameters, to be documented later.","title":"Simple database-connection importers (example)"},{"location":"rest-api/importexport/#export","text":"","title":"Export"},{"location":"rest-api/importexport/#data-flow","text":"","title":"Data Flow"},{"location":"rest-api/importexport/#terminology","text":"","title":"Terminology"},{"location":"rest-api/introduction/","text":"The Mauro Data Mapper API conforms to standard REST principles. The API has resource-oriented URLs, accepts XML and JSON body content (or form-encoded parameters where applicable), and can return data in XML or JSON formats. Each call uses standard HTTP response codes, authentication, and verbs. Requests \u00b6 To make a REST API request, you combine: The HTTP method: GET , POST , PUT , PATCH or DELETE The URL to the API service - for example http://modelcatalogue.cs.ox.ac.uk/demo/api The URL to a resource to query, update or delete One or more HTTP request headers , for example the identifier of any session token, or a request to save data back in XML or JSON format Most calls may also require a JSON or XML body representing any new or updated data, or query parameters to filter or restrict the response. HTTP request headers \u00b6 The commonly-used HTTP request headers used are: Accept \u00b6 This header determines the format of the response body for those requests with structured output. The syntax is: 1 Accept: application/<format> Where <format> can be either xml or json . By default, the format of the response body will match that of the request body, where applicable. Content-Type \u00b6 This header specifies the format of the request body, where applicable. The syntax is: 1 Content-Type: application/<format> Where <format> can be either xml or json . By default, the request body is assumed to be JSON unless otherwise specified. Cookie \u00b6 This header stores the session identifier which persists a login between calls. For example, having received a session cookie during login , the token can be used to validate the user. 1 Cookie: JSESSIONID=<sessionid> Typically, a session identifier is 32 characters long and uses hexadecimal characters 0-9 , A-F . Tools \u00b6 We use Postman for testing API calls during development. It has an intuitive interface that lets you set parameters, headers, and message bodies, and preview structured responses. It can also be used as part of an automated testing or debugging requests. If you're looking for a more lightweight solution, curl is a suitable command-line tool which can be easily configured to make complex REST API requests. In this set of documentation, requests are illustrated with the appropriate curl command. Testing \u00b6 There is a test API resource which will show whether the server API is running correctly, and whether the client has been correctly configured. To test this using curl , run the following command: 1 curl -X GET http://localhost:8080/api/test This will return the following JSON: Response body (JSON) 1 2 3 4 5 6 7 { \"message\" : \"Not Found\" , \"error\" : 404 , \"path\" : null , \"object\" : null , \"id\" : null }","title":"Introduction"},{"location":"rest-api/introduction/#requests","text":"To make a REST API request, you combine: The HTTP method: GET , POST , PUT , PATCH or DELETE The URL to the API service - for example http://modelcatalogue.cs.ox.ac.uk/demo/api The URL to a resource to query, update or delete One or more HTTP request headers , for example the identifier of any session token, or a request to save data back in XML or JSON format Most calls may also require a JSON or XML body representing any new or updated data, or query parameters to filter or restrict the response.","title":"Requests"},{"location":"rest-api/introduction/#http-request-headers","text":"The commonly-used HTTP request headers used are:","title":"HTTP request headers"},{"location":"rest-api/introduction/#accept","text":"This header determines the format of the response body for those requests with structured output. The syntax is: 1 Accept: application/<format> Where <format> can be either xml or json . By default, the format of the response body will match that of the request body, where applicable.","title":"Accept"},{"location":"rest-api/introduction/#content-type","text":"This header specifies the format of the request body, where applicable. The syntax is: 1 Content-Type: application/<format> Where <format> can be either xml or json . By default, the request body is assumed to be JSON unless otherwise specified.","title":"Content-Type"},{"location":"rest-api/introduction/#cookie","text":"This header stores the session identifier which persists a login between calls. For example, having received a session cookie during login , the token can be used to validate the user. 1 Cookie: JSESSIONID=<sessionid> Typically, a session identifier is 32 characters long and uses hexadecimal characters 0-9 , A-F .","title":"Cookie"},{"location":"rest-api/introduction/#tools","text":"We use Postman for testing API calls during development. It has an intuitive interface that lets you set parameters, headers, and message bodies, and preview structured responses. It can also be used as part of an automated testing or debugging requests. If you're looking for a more lightweight solution, curl is a suitable command-line tool which can be easily configured to make complex REST API requests. In this set of documentation, requests are illustrated with the appropriate curl command.","title":"Tools"},{"location":"rest-api/introduction/#testing","text":"There is a test API resource which will show whether the server API is running correctly, and whether the client has been correctly configured. To test this using curl , run the following command: 1 curl -X GET http://localhost:8080/api/test This will return the following JSON: Response body (JSON) 1 2 3 4 5 6 7 { \"message\" : \"Not Found\" , \"error\" : 404 , \"path\" : null , \"object\" : null , \"id\" : null }","title":"Testing"},{"location":"rest-api/pagination/","text":"The majority of requests for multiple objects have parameters to manage pagination. By returning results in separate pages, we can minimise network traffic and reduce the load on the server. The size or limit ( max ) and starting position ( offset ) of each page can be passed in as a parameter to the query. The response will always return the total number of objects, along with a list of 'items' corresponding to the specified 'page' of results. Parameter format \u00b6 In these examples we consider the endpoint endpoint for listing all folders: /api/folders To manually specify the offset and max values, these should be passed as form parameters - for example the request: /api/folders?offset=10&max=5 Would return folders 10-14 inclusive in the overall list. To specify that all results should be returned, the boolean parameter all can be passed - for example the request: /api/folders?all=true Will return the complete list of visible folders. The all parameter is an alternative, and should not be specified at the same time as offset and max . Response format \u00b6 Again consider the endpoint endpoint for listing all folders described above. The response body would look something like: Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"count\" : n , \"items\" : [ { ... }, ... ] } Where n is the total number of folders available. The number of items returned will be at most max items. Default settings \u00b6 If no parameters are passed, the default values are: offset : 0 max : 10","title":"Pagination"},{"location":"rest-api/pagination/#parameter-format","text":"In these examples we consider the endpoint endpoint for listing all folders: /api/folders To manually specify the offset and max values, these should be passed as form parameters - for example the request: /api/folders?offset=10&max=5 Would return folders 10-14 inclusive in the overall list. To specify that all results should be returned, the boolean parameter all can be passed - for example the request: /api/folders?all=true Will return the complete list of visible folders. The all parameter is an alternative, and should not be specified at the same time as offset and max .","title":"Parameter format"},{"location":"rest-api/pagination/#response-format","text":"Again consider the endpoint endpoint for listing all folders described above. The response body would look something like: Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"count\" : n , \"items\" : [ { ... }, ... ] } Where n is the total number of folders available. The number of items returned will be at most max items.","title":"Response format"},{"location":"rest-api/pagination/#default-settings","text":"If no parameters are passed, the default values are: offset : 0 max : 10","title":"Default settings"},{"location":"rest-api/postman/","text":"The Postman app is a tool for working with external APIs. Originally a plugin for Google Chrome, it now comes as a desktop app for all operating systems, as well as providing a web version. Downloading \u00b6 The Mauro Postman repository provides definitions for Postman in order to test the Mauro APIs and contains sample environment configurations. To use, simply clone the repository into your local system, or download the files as follows: Navigate to the main branch of the GitHub repository Choose a folder to match the version of mdm-core that you intend to run against Download the two listed JSON files - one for the 'Collection' and another for the 'Environment' Using the collection \u00b6 Within the Postman app, choose 'File' -> 'Import...' and under the 'File' tab, choose 'Upload Files' . Select the collection JSON file, and click 'Import' to import the new collection. If you've previously imported an older version of this collection, you are asked whether you wish to import as a new copy, or overwrite the previous version. In the 'Collections' tab on the left hand side, you can now see a number of folders and subfolders containing configuration for Mauro API endpoints. By clicking on each you can view the endpoint, and optionally execute it against a given server. Parameters to the call, including the server name, are indicated by double braces {{ ... }} - e.g. {{base_url}} in the URL of the endpoint. To instantiate these parameters, you can either replace the text manually, or use an environment to provide consistent replacements across all endpoints. The Mauro Postman Environment provides some default values which can be customised. Using the environment \u00b6 To import the Mauro environment, choose 'File' -> 'Import...' and under the 'File' tab, choose 'Upload Files' . Select the environment JSON file, and click 'Import' to import the new environment. Once imported, the enviroment can be selected from the drop-down at the top-right of the screen. To edit the environment, click 'Manage Envronments' . A number of parameters have been pre-set and can be edited here; new parameters can be added to suit your own usage. Swagger, OpenAPI \u00b6 We are yet to create a description of our APIs compatible with Swagger , or OpenAPI . However, there are tools that should automate the conversion of Postman collections to these formats - albeit in a manner specific to a particular environment or use case. Some example tools include: APIMatic REST United APITransform The Mauro team have limited experience with these tools and so would welcome any feedback! Submitting changes \u00b6 The Postman library is not yet complete. There are endpoints undocumented, and plenty of improvements that could be made to the environment, or particular usage scenarios we've not yet catered for. If you've made changes to the Postman Library and think they would be of more general use, please do consider submitting a pull request, so we can make them more widely available.","title":"Postman Library"},{"location":"rest-api/postman/#downloading","text":"The Mauro Postman repository provides definitions for Postman in order to test the Mauro APIs and contains sample environment configurations. To use, simply clone the repository into your local system, or download the files as follows: Navigate to the main branch of the GitHub repository Choose a folder to match the version of mdm-core that you intend to run against Download the two listed JSON files - one for the 'Collection' and another for the 'Environment'","title":"Downloading"},{"location":"rest-api/postman/#using-the-collection","text":"Within the Postman app, choose 'File' -> 'Import...' and under the 'File' tab, choose 'Upload Files' . Select the collection JSON file, and click 'Import' to import the new collection. If you've previously imported an older version of this collection, you are asked whether you wish to import as a new copy, or overwrite the previous version. In the 'Collections' tab on the left hand side, you can now see a number of folders and subfolders containing configuration for Mauro API endpoints. By clicking on each you can view the endpoint, and optionally execute it against a given server. Parameters to the call, including the server name, are indicated by double braces {{ ... }} - e.g. {{base_url}} in the URL of the endpoint. To instantiate these parameters, you can either replace the text manually, or use an environment to provide consistent replacements across all endpoints. The Mauro Postman Environment provides some default values which can be customised.","title":"Using the collection"},{"location":"rest-api/postman/#using-the-environment","text":"To import the Mauro environment, choose 'File' -> 'Import...' and under the 'File' tab, choose 'Upload Files' . Select the environment JSON file, and click 'Import' to import the new environment. Once imported, the enviroment can be selected from the drop-down at the top-right of the screen. To edit the environment, click 'Manage Envronments' . A number of parameters have been pre-set and can be edited here; new parameters can be added to suit your own usage.","title":"Using the environment"},{"location":"rest-api/postman/#swagger-openapi","text":"We are yet to create a description of our APIs compatible with Swagger , or OpenAPI . However, there are tools that should automate the conversion of Postman collections to these formats - albeit in a manner specific to a particular environment or use case. Some example tools include: APIMatic REST United APITransform The Mauro team have limited experience with these tools and so would welcome any feedback!","title":"Swagger, OpenAPI"},{"location":"rest-api/postman/#submitting-changes","text":"The Postman library is not yet complete. There are endpoints undocumented, and plenty of improvements that could be made to the environment, or particular usage scenarios we've not yet catered for. If you've made changes to the Postman Library and think they would be of more general use, please do consider submitting a pull request, so we can make them more widely available.","title":"Submitting changes"},{"location":"rest-api/trees/","text":"","title":"Trees"},{"location":"rest-api/resources/catalogue-item/","text":"A Catalogue Item in the catalogue is an abstract class containing properties that are common to most objects in the catalogue - for example DataModels, DataClasses, DataElements, DataTypes, EnumerationValues, Terminologies, etc. These properties include metadata (properties) , summary metadata , permissions , annotations (comments) and so on. In some cases the url for each endpoint uses the word 'facet'; in others the data type (DataModel, DataClass, etc) are used. This page lists all the endpoints and describes the structure of each property. Metadata \u00b6 The metadata , or properties , of a Catalogue Item are extensible key/value pairs to store any further information about an object - including technical properies, or field conforming to an external model. A single item of metadata is structured as follows: Response body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"c9a36d30-2c6a-4dd0-a792-a337a2eca9c8\" , \"namespace\" : \"ox.softeng.metadatacatalogue.dataloaders.hdf\" , \"key\" : \"Volumes\" , \"value\" : \"Varies annually: in 2013/14, 18.2m finished consultant episodes (FCEs) and 15.5m Finished Admission Episodes (FAEs)\" , \"lastUpdated\" : \"2019-10-03T09:15:12.082Z\" } The fields are as follows: id (UUID): The unique identifier of this property namespace (String): a namespace used to group particular properties - and can be used to filter properties for particular uses key (String): the title or label of this property. The combination of namespace and key should be unique for this object. value (String): the value that this property holds. This field may take HTML or MarkDown syntax, and may include links to other objects in the catalogue. lastUpdated (DateTime): The date/time when this Metadata property was last modified The endpoints for using metadata properties are listed below. To retrieve all the properties for a particular object, use the following endpoint. The metadata properties are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata Where {catalogueItemDomainType} can be one of: folders , dataModels , dataClasses , dataTypes , terminologies , terms , or referenceDataModels To get a specific property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} To add a new property to an object you have write-access to, post a structure similar to the one displayed above (ignoring id and lastUpdated fields, which will be automatically set to the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata To edit an existing property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} To delete an existing property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} The following endpoint returns all known namespaces for a particular object (given by the id field). To find all namespaces across the whole catalogue, the final component of the URL can be left off. /api/metadata/namespaces/ {id} ? Permissions \u00b6 Logged in users may query to discover who is able to read or write a particular object (that they themselves have read-access to). The structure of a response is as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"readableByEveryone\" : false , \"readableByAuthenticated\" : true , \"readableByGroups\" : [], \"writeableByGroups\" : [ { \"id\" : \"cb1b7f4e-6955-41ba-8f91-2ca92b97c189\" , \"label\" : \"Test Group\" , \"createdBy\" : { \"id\" : \"dc7a7c25-5622-4cb0-869f-6d0e688b490f\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } } ], \"readableByUsers\" : [ { \"id\" : \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } ], \"writeableByUsers\" : [ { \"id\" : \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } ] } The fields are as follows: readableByEveryone (Boolean): whether the object in question is publicly available - i.e. can be read by any un-authenticated user of the system readableByAuthenticated (Boolean): whether the object in question can be read by any authenticated (logged-in) user of the system readableByGroups (Set(Group)): the set of groups who have permission to read a particular object. The group has a label, an identifier, and the details of the user responsible for creating that group writeableByGroups (Set(Group)): the set of groups who have permission to edit a particular object. Note that this set of groups is always a subset of the groups listed in readableByGroups readableByUsers (Set(User)): the set of users who have permission to read a particular object. The user has an identifier, first name, last name, email address and flag indicating whether their access is currently valid or disabled writeableByGroups (Set(Group)): the set of users who have permission to edit a particular object. Note that this set of users is always a subset of the users listed in readableByUsers Note Note that read/write permissions are propagated through folders and sub-folders, and the list of permissions given is the inferred list. So changes to that list may not always have an affect if they are contradicted by another assertion further up the tree. The endpoint for getting the permissions each of DataModel, ReferenceDataModel, Folder, CodeSet and Classifier are listed below. The details for updating permissions are listed on their respective pages. /api/dataModels/ {dataModelId} /permissions /api/referenceDataModels/ {referenceDataModelId} /permissions /api/folders/ {folderId} /permissions /api/codeSets/ {codeSetId} /permissions /api/classifiers/ {classifierId} /permissions Annotations \u00b6 Annotations, or comments, can be attached to any item in the catalogue. The structure is as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \"count\" : 2 , \"items\" : [ { \"id\" : \"da3d6229-b152-4cbb-8667-eede523c7eb1\" , \"description\" : \"DataModel finalised by Joe Bloggs on 2018-09-28T20:21:35.995Z\" , \"createdBy\" : { \"id\" : \"5b96991a-d350-4470-958a-29bfac557ed0\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false }, \"lastUpdated\" : \"2018-09-28T20:21:37.655Z\" , \"label\" : \"Finalised Model\" }, { \"id\" : \"670e7c31-00fd-425f-903f-6d024845e63e\" , \"createdBy\" : { \"id\" : \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false }, \"lastUpdated\" : \"2018-07-17T15:51:45.643Z\" , \"label\" : \"Is this model is ready for finalisation?\" } ] } Listing annotations \u00b6 To get all the annotations for a particular object, use the following endpoint. The results are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations Where {catalogueItemDomainType} can be one of: folders , dataModels , dataClasses , dataTypes , terminologies , terms , or referenceDataModels To get the details of a particular annotation / comment, whose id is known, use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {id} To create a new top-level annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and lastUpdated fields) /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations To delete an annotation / comment whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {id} Child annotations (responses) \u00b6 Comments can have child comments (or replies). To get all the child comments for a particular comment, use the following endpoint. The results are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations To get the details of a particular child annotation / comment, whose id is known, use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations/ {id} To create a new child annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and lastUpdated fields) /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations To delete a child annotation / comment whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations/ {id} Searching \u00b6 An advanced search is powered by Lucene. The parameters for an advanced search can be provided as query parameters of a get request, as follows: Parameter Description searchTerm (String): the search string - this can take a number of standard search operators - for example \"smoking + pregnancy\" limit (Integer): the number of results returned in a paginated list offset (Integer): the index of the first result returned in a paginated list domainTypes (Set(String)): the catalogue object types that should be searched. These can include: DataModel , DataClass , DataElement , DataType , EnumerationType . labelOnly (Boolean): whether the search should only query the label of all objects dataModelTypes (Set(String)): the types of data model that should be searched - for example Data Asset , Data Standard classifiers (Set(String)): a set of classifier labels, such that all results must be classified by one of those tags lastUpdatedAfter (DateTime): Only include objects in the search results if they have been modified more recently than the given date lastUpdatedBefore (DateTime): Only include objects in the search results if they have been modified earlier than the given date createdAfter (DateTime): Only include objects in the search results if they were created more recently than the given date createdBefore (DateTime): Only include objects in the search results if they were created earlier than the given date The response will be a paginated list of items, where each item has the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\" : \"127bdf61-cbfe-47dc-9854-fdce276f13bf\" , \"domainType\" : \"DataElement\" , \"label\" : \"AGE AT ONSET OF SYMPTOMS (CHILDREN TEENAGERS AND YOUNG ADULTS CANCER)\" , \"description\" : \"AGE BAND AT SMOKING QUIT DATE is derived as the number of completed years between the PERSON BIRTH DATE of the PERSON and the SMOKING QUIT DATE of the Person Stop Smoking Episode. Permitted National Codes: 01 Under 18 years of age 02 18 to 34 years of age 03 35 - 44 years of age 04 45 - 59 years of age 05 60 and over years of age\" , \"breadcrumbs\" : [ { \"id\" : \"078955c7-6c0f-4fc2-a30e-55629a85b9da\" , \"label\" : \"NHS Data Dictionary\" , \"domainType\" : \"DataModel\" , \"finalised\" : true }, { \"id\" : \"012e8dd5-b4b1-4d26-82aa-17430baf2e2b\" , \"label\" : \"All Data Elements\" , \"domainType\" : \"DataClass\" } ] } where the fields are defined as follows: id (UUID): The identifier of the returned object domainType (String): The type of the returned object - for example \"DataModel\", \"DataClass\", \"DataElement\" label (String): The name of the returned object description (String): The description of the returned object. This may include formatting specified in HTML, or MarkDown. breadcrumbs (List(Breadcrumb)): An ordered list of, e.g. DataModels and DataClasses to show the location of an object in the hierarchy of a model. This will include, for each component of the breadcrumb, an id , a label and a domainType . To search across the whole catalogue, use the following endpoint, optionally passing the above query parameters: /api/tree/folders/search Similarly, to search within a particular data model, use the following: /api/dataModels/ {dataModelId} /search Finally, to search within a specific Data Class, use the following: /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /search Item history \u00b6 The edit history for various catalogue items can be retrieved using the endpoints listed below. The format of a response is a paginated list of edits, with the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 { \"dateCreated\" : \"2018-07-17T15:53:17.276Z\" , \"createdBy\" : { \"id\" : \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\" , \"emailAddress\" : \"ollie.freeman@gmail.com\" , \"firstName\" : \"Oliver\" , \"lastName\" : \"Freeman\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } \"description\" : \"[Data Standard:HIC: Hepatitis v2.0.0] changed properties [folder]\" } The fields have the following definition: dateCreated (DateTime): the date and time when this modification was made createdBy (User): the user responsible for making the edit. This will include their id , emailAddress , firstName , lastName **, **userRole and whether the user account is currently disabled . It may also include the profile image of the user in question description (String): The human-readable description of the edit made. These descriptions are automatically generated by the catalogue The endpoints for getting the edit history for each of DataModel, Terminology, Folder, CodeSet, Classifier and UserGroup are listed below. /api/dataModels/ {dataModelId} /edits /api/terminologies/ {terminologyId} /edits /api/folders/ {folderId} /edits /api/codeSets/ {codeSetId} /edits /api/classifiers/ {classifierId} /edits /api/userGroups/ {userGroupId} /edits Reference files \u00b6 Reference files (or attachments) can be stored alongside various catalogue items to supplement information about the catalogue item. Reference files have the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 { \"id\" : \"eea67c19-1833-4125-9934-b06f45844c20\" , \"domainType\" : \"ReferenceFile\" , \"fileName\" : \"uploadedFile.png\" , \"fileSize\" : 80899 , \"fileType\" : \"image/png\" , \"lastUpdated\" : \"2021-05-13T12:50:37.523Z\" } The fields have the following definition: id (UUID): The identifier of the returned object domainType (String): The type of the returned object - in this case, \"ReferenceFile\" fileName (String): The name of the uploaded file fileSize (Number): The size of the uploaded reference file, in bytes fileType (String): The MIME type of the uploaded reference file lastUpdated (DateTime): the date and time when this modification was made To upload and attach a new reference file to a catalogue item, use the following endpoint and request payload: /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles Request body (JSON) 1 2 3 4 5 6 7 8 { \"fileName\" : \"uploadedFile.png\" , \"fileSize\" : 80899 , \"fileType\" : \"image/png\" , \"fileContents\" : [ // Array o f by tes ] } Where {catalogueItemDomainType} can be one of: dataModels , terminologies , codeSets , or referenceDataModels To get either a paginated list of reference files for a catalogue item, or an individual reference file known by {id} : /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles/ {id} To delete a reference file from a catalogue item whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles/ {id} Summary Metadata \u00b6 /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} Summary Metadata Reports \u00b6 /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id}","title":"Catalogue item"},{"location":"rest-api/resources/catalogue-item/#metadata","text":"The metadata , or properties , of a Catalogue Item are extensible key/value pairs to store any further information about an object - including technical properies, or field conforming to an external model. A single item of metadata is structured as follows: Response body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"c9a36d30-2c6a-4dd0-a792-a337a2eca9c8\" , \"namespace\" : \"ox.softeng.metadatacatalogue.dataloaders.hdf\" , \"key\" : \"Volumes\" , \"value\" : \"Varies annually: in 2013/14, 18.2m finished consultant episodes (FCEs) and 15.5m Finished Admission Episodes (FAEs)\" , \"lastUpdated\" : \"2019-10-03T09:15:12.082Z\" } The fields are as follows: id (UUID): The unique identifier of this property namespace (String): a namespace used to group particular properties - and can be used to filter properties for particular uses key (String): the title or label of this property. The combination of namespace and key should be unique for this object. value (String): the value that this property holds. This field may take HTML or MarkDown syntax, and may include links to other objects in the catalogue. lastUpdated (DateTime): The date/time when this Metadata property was last modified The endpoints for using metadata properties are listed below. To retrieve all the properties for a particular object, use the following endpoint. The metadata properties are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata Where {catalogueItemDomainType} can be one of: folders , dataModels , dataClasses , dataTypes , terminologies , terms , or referenceDataModels To get a specific property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} To add a new property to an object you have write-access to, post a structure similar to the one displayed above (ignoring id and lastUpdated fields, which will be automatically set to the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata To edit an existing property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} To delete an existing property (whose id field is known), use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /metadata/ {id} The following endpoint returns all known namespaces for a particular object (given by the id field). To find all namespaces across the whole catalogue, the final component of the URL can be left off. /api/metadata/namespaces/ {id} ?","title":"Metadata"},{"location":"rest-api/resources/catalogue-item/#permissions","text":"Logged in users may query to discover who is able to read or write a particular object (that they themselves have read-access to). The structure of a response is as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"readableByEveryone\" : false , \"readableByAuthenticated\" : true , \"readableByGroups\" : [], \"writeableByGroups\" : [ { \"id\" : \"cb1b7f4e-6955-41ba-8f91-2ca92b97c189\" , \"label\" : \"Test Group\" , \"createdBy\" : { \"id\" : \"dc7a7c25-5622-4cb0-869f-6d0e688b490f\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } } ], \"readableByUsers\" : [ { \"id\" : \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } ], \"writeableByUsers\" : [ { \"id\" : \"5e70dbc8-4a1f-4c97-82dd-b05438ba7fae\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } ] } The fields are as follows: readableByEveryone (Boolean): whether the object in question is publicly available - i.e. can be read by any un-authenticated user of the system readableByAuthenticated (Boolean): whether the object in question can be read by any authenticated (logged-in) user of the system readableByGroups (Set(Group)): the set of groups who have permission to read a particular object. The group has a label, an identifier, and the details of the user responsible for creating that group writeableByGroups (Set(Group)): the set of groups who have permission to edit a particular object. Note that this set of groups is always a subset of the groups listed in readableByGroups readableByUsers (Set(User)): the set of users who have permission to read a particular object. The user has an identifier, first name, last name, email address and flag indicating whether their access is currently valid or disabled writeableByGroups (Set(Group)): the set of users who have permission to edit a particular object. Note that this set of users is always a subset of the users listed in readableByUsers Note Note that read/write permissions are propagated through folders and sub-folders, and the list of permissions given is the inferred list. So changes to that list may not always have an affect if they are contradicted by another assertion further up the tree. The endpoint for getting the permissions each of DataModel, ReferenceDataModel, Folder, CodeSet and Classifier are listed below. The details for updating permissions are listed on their respective pages. /api/dataModels/ {dataModelId} /permissions /api/referenceDataModels/ {referenceDataModelId} /permissions /api/folders/ {folderId} /permissions /api/codeSets/ {codeSetId} /permissions /api/classifiers/ {classifierId} /permissions","title":"Permissions"},{"location":"rest-api/resources/catalogue-item/#annotations","text":"Annotations, or comments, can be attached to any item in the catalogue. The structure is as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \"count\" : 2 , \"items\" : [ { \"id\" : \"da3d6229-b152-4cbb-8667-eede523c7eb1\" , \"description\" : \"DataModel finalised by Joe Bloggs on 2018-09-28T20:21:35.995Z\" , \"createdBy\" : { \"id\" : \"5b96991a-d350-4470-958a-29bfac557ed0\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false }, \"lastUpdated\" : \"2018-09-28T20:21:37.655Z\" , \"label\" : \"Finalised Model\" }, { \"id\" : \"670e7c31-00fd-425f-903f-6d024845e63e\" , \"createdBy\" : { \"id\" : \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\" , \"emailAddress\" : \"joebloggs@test.com\" , \"firstName\" : \"Joe\" , \"lastName\" : \"Bloggs\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false }, \"lastUpdated\" : \"2018-07-17T15:51:45.643Z\" , \"label\" : \"Is this model is ready for finalisation?\" } ] }","title":"Annotations"},{"location":"rest-api/resources/catalogue-item/#listing-annotations","text":"To get all the annotations for a particular object, use the following endpoint. The results are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations Where {catalogueItemDomainType} can be one of: folders , dataModels , dataClasses , dataTypes , terminologies , terms , or referenceDataModels To get the details of a particular annotation / comment, whose id is known, use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {id} To create a new top-level annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and lastUpdated fields) /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations To delete an annotation / comment whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {id}","title":"Listing annotations"},{"location":"rest-api/resources/catalogue-item/#child-annotations-responses","text":"Comments can have child comments (or replies). To get all the child comments for a particular comment, use the following endpoint. The results are returned in a paginated list /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations To get the details of a particular child annotation / comment, whose id is known, use the following endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations/ {id} To create a new child annotation, post to the following endpoint, with a body similar to the JSON above (but without the id and lastUpdated fields) /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations To delete a child annotation / comment whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /annotations/ {annotationId} /annotations/ {id}","title":"Child annotations (responses)"},{"location":"rest-api/resources/catalogue-item/#searching","text":"An advanced search is powered by Lucene. The parameters for an advanced search can be provided as query parameters of a get request, as follows: Parameter Description searchTerm (String): the search string - this can take a number of standard search operators - for example \"smoking + pregnancy\" limit (Integer): the number of results returned in a paginated list offset (Integer): the index of the first result returned in a paginated list domainTypes (Set(String)): the catalogue object types that should be searched. These can include: DataModel , DataClass , DataElement , DataType , EnumerationType . labelOnly (Boolean): whether the search should only query the label of all objects dataModelTypes (Set(String)): the types of data model that should be searched - for example Data Asset , Data Standard classifiers (Set(String)): a set of classifier labels, such that all results must be classified by one of those tags lastUpdatedAfter (DateTime): Only include objects in the search results if they have been modified more recently than the given date lastUpdatedBefore (DateTime): Only include objects in the search results if they have been modified earlier than the given date createdAfter (DateTime): Only include objects in the search results if they were created more recently than the given date createdBefore (DateTime): Only include objects in the search results if they were created earlier than the given date The response will be a paginated list of items, where each item has the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"id\" : \"127bdf61-cbfe-47dc-9854-fdce276f13bf\" , \"domainType\" : \"DataElement\" , \"label\" : \"AGE AT ONSET OF SYMPTOMS (CHILDREN TEENAGERS AND YOUNG ADULTS CANCER)\" , \"description\" : \"AGE BAND AT SMOKING QUIT DATE is derived as the number of completed years between the PERSON BIRTH DATE of the PERSON and the SMOKING QUIT DATE of the Person Stop Smoking Episode. Permitted National Codes: 01 Under 18 years of age 02 18 to 34 years of age 03 35 - 44 years of age 04 45 - 59 years of age 05 60 and over years of age\" , \"breadcrumbs\" : [ { \"id\" : \"078955c7-6c0f-4fc2-a30e-55629a85b9da\" , \"label\" : \"NHS Data Dictionary\" , \"domainType\" : \"DataModel\" , \"finalised\" : true }, { \"id\" : \"012e8dd5-b4b1-4d26-82aa-17430baf2e2b\" , \"label\" : \"All Data Elements\" , \"domainType\" : \"DataClass\" } ] } where the fields are defined as follows: id (UUID): The identifier of the returned object domainType (String): The type of the returned object - for example \"DataModel\", \"DataClass\", \"DataElement\" label (String): The name of the returned object description (String): The description of the returned object. This may include formatting specified in HTML, or MarkDown. breadcrumbs (List(Breadcrumb)): An ordered list of, e.g. DataModels and DataClasses to show the location of an object in the hierarchy of a model. This will include, for each component of the breadcrumb, an id , a label and a domainType . To search across the whole catalogue, use the following endpoint, optionally passing the above query parameters: /api/tree/folders/search Similarly, to search within a particular data model, use the following: /api/dataModels/ {dataModelId} /search Finally, to search within a specific Data Class, use the following: /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /search","title":"Searching"},{"location":"rest-api/resources/catalogue-item/#item-history","text":"The edit history for various catalogue items can be retrieved using the endpoints listed below. The format of a response is a paginated list of edits, with the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 { \"dateCreated\" : \"2018-07-17T15:53:17.276Z\" , \"createdBy\" : { \"id\" : \"6c02358a-d3e3-4bee-93d5-839ead6a0acd\" , \"emailAddress\" : \"ollie.freeman@gmail.com\" , \"firstName\" : \"Oliver\" , \"lastName\" : \"Freeman\" , \"userRole\" : \"EDITOR\" , \"disabled\" : false } \"description\" : \"[Data Standard:HIC: Hepatitis v2.0.0] changed properties [folder]\" } The fields have the following definition: dateCreated (DateTime): the date and time when this modification was made createdBy (User): the user responsible for making the edit. This will include their id , emailAddress , firstName , lastName **, **userRole and whether the user account is currently disabled . It may also include the profile image of the user in question description (String): The human-readable description of the edit made. These descriptions are automatically generated by the catalogue The endpoints for getting the edit history for each of DataModel, Terminology, Folder, CodeSet, Classifier and UserGroup are listed below. /api/dataModels/ {dataModelId} /edits /api/terminologies/ {terminologyId} /edits /api/folders/ {folderId} /edits /api/codeSets/ {codeSetId} /edits /api/classifiers/ {classifierId} /edits /api/userGroups/ {userGroupId} /edits","title":"Item history"},{"location":"rest-api/resources/catalogue-item/#reference-files","text":"Reference files (or attachments) can be stored alongside various catalogue items to supplement information about the catalogue item. Reference files have the following structure: Response body (JSON) 1 2 3 4 5 6 7 8 { \"id\" : \"eea67c19-1833-4125-9934-b06f45844c20\" , \"domainType\" : \"ReferenceFile\" , \"fileName\" : \"uploadedFile.png\" , \"fileSize\" : 80899 , \"fileType\" : \"image/png\" , \"lastUpdated\" : \"2021-05-13T12:50:37.523Z\" } The fields have the following definition: id (UUID): The identifier of the returned object domainType (String): The type of the returned object - in this case, \"ReferenceFile\" fileName (String): The name of the uploaded file fileSize (Number): The size of the uploaded reference file, in bytes fileType (String): The MIME type of the uploaded reference file lastUpdated (DateTime): the date and time when this modification was made To upload and attach a new reference file to a catalogue item, use the following endpoint and request payload: /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles Request body (JSON) 1 2 3 4 5 6 7 8 { \"fileName\" : \"uploadedFile.png\" , \"fileSize\" : 80899 , \"fileType\" : \"image/png\" , \"fileContents\" : [ // Array o f by tes ] } Where {catalogueItemDomainType} can be one of: dataModels , terminologies , codeSets , or referenceDataModels To get either a paginated list of reference files for a catalogue item, or an individual reference file known by {id} : /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles/ {id} To delete a reference file from a catalogue item whose identifier is known, use the following delete endpoint: /api/ {catalogueItemDomainType} / {catalogueItemId} /referenceFiles/ {id}","title":"Reference files"},{"location":"rest-api/resources/catalogue-item/#summary-metadata","text":"/api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {id}","title":"Summary Metadata"},{"location":"rest-api/resources/catalogue-item/#summary-metadata-reports","text":"/api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id} /api/ {catalogueItemDomainType} / {catalogueItemId} /summaryMetadata/ {summaryMetadataId} /summaryMetadataReports/ {id}","title":"Summary Metadata Reports"},{"location":"rest-api/resources/classifier/","text":"A Classifier is a container type and can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"Classification\" , \"label\" : \"classifier\" , \"description\" : \"Represents a classifier.\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" } The fields are as follows: id (UUID): The unique identifier of this classifier domainType (Type): The domain type of this catalogue object. Will always be Classification in this case. label (String): The human-readable identifier of this classifier. description (String): A long description of the classifier, and any important characteristics of the data. This field may include HTML, or MarkDown. lastUpdated (DateTime): The date/time when this Classifier was last modified As well as the endpoints listed below, a Classifier is also a CatalogueItem, and so a Classifier identifier can also be used as the parameter to any of those endpoints Child Classifiers \u00b6 A classifier may contain child classifiers. Endpoints are provided to differentiate between parent and child classifiers. Getting information \u00b6 The following endpoints returns a paginated list of all the Classifiers. The first requests all root classifiers in Mauro, the second requests the classifiers for a parent classifier. /api/classifiers /api/classifiers/ {classifierId} /classifiers These endpoints provide the detailed information about a particular Classifier; the first requests a root classifier in Mauro, the second requests a classifier from a parent classifier. /api/classifiers/ {id} /api/classifiers/ {classifierId} /classifiers/ {id} Finally, these endpoints request a list of catalogue items mapped to a classifier, and the inverse to list all classifiers mapped to a catalogue item, respectively. /api/classifiers/ {classifierId} /catalogueItems /api/ {catalogueItemDomainType} / {catalogueItemId} /classifiers Create / Update / Delete \u00b6 To create a new Classifier from scratch, use the following post endpoints, depending on whether to create one with or without a parent. /api/classifiers /api/classifiers/ {classifierId} /classifiers To edit the properties of a Classifier, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent. /api/classifiers/ {id} /api/classifiers/ {classifierId} /classifiers/ {id} To delete a Classifier, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/classifiers/ {id} ?permanent= {true/false} /api/classifiers/ {classifierId} /classifiers/ {id} ?permanent= {true/false} Security \u00b6 /api/classifiers/ {classifierId} /readByAuthenticated /api/classifiers/ {classifierId} /readByAuthenticated /api/classifiers/ {classifierId} /readByEveryone /api/classifiers/ {classifierId} /readByEveryone","title":"Classifier"},{"location":"rest-api/resources/classifier/#child-classifiers","text":"A classifier may contain child classifiers. Endpoints are provided to differentiate between parent and child classifiers.","title":"Child Classifiers"},{"location":"rest-api/resources/classifier/#getting-information","text":"The following endpoints returns a paginated list of all the Classifiers. The first requests all root classifiers in Mauro, the second requests the classifiers for a parent classifier. /api/classifiers /api/classifiers/ {classifierId} /classifiers These endpoints provide the detailed information about a particular Classifier; the first requests a root classifier in Mauro, the second requests a classifier from a parent classifier. /api/classifiers/ {id} /api/classifiers/ {classifierId} /classifiers/ {id} Finally, these endpoints request a list of catalogue items mapped to a classifier, and the inverse to list all classifiers mapped to a catalogue item, respectively. /api/classifiers/ {classifierId} /catalogueItems /api/ {catalogueItemDomainType} / {catalogueItemId} /classifiers","title":"Getting information"},{"location":"rest-api/resources/classifier/#create-update-delete","text":"To create a new Classifier from scratch, use the following post endpoints, depending on whether to create one with or without a parent. /api/classifiers /api/classifiers/ {classifierId} /classifiers To edit the properties of a Classifier, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent. /api/classifiers/ {id} /api/classifiers/ {classifierId} /classifiers/ {id} To delete a Classifier, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/classifiers/ {id} ?permanent= {true/false} /api/classifiers/ {classifierId} /classifiers/ {id} ?permanent= {true/false}","title":"Create / Update / Delete"},{"location":"rest-api/resources/classifier/#security","text":"/api/classifiers/ {classifierId} /readByAuthenticated /api/classifiers/ {classifierId} /readByAuthenticated /api/classifiers/ {classifierId} /readByEveryone /api/classifiers/ {classifierId} /readByEveryone","title":"Security"},{"location":"rest-api/resources/codeset/","text":"In its simplest form, a Code Set can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"CodeSet\" , \"label\" : \"Sample Codeset\" , \"aliases\" : [ \"sample\" ], \"description\" : \"Example of a Codeset\" , \"author\" : \"NHS Digital\" , \"organisation\" : \"NHS Digital\" , \"documentationVersion\" : \"2.0.0\" , \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"type\" : \"CodeSet\" , \"finalised\" : false , } The fields are as follows: id (UUID): The unique identifier of this code set domainType (Type): The domain type of this catalogue object - always CodeSet in this case label (String): The human-readable identifier of this code set. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this code set is known description (String): A long description of the code set, and any important characteristics of the data. This field may include HTML, or MarkDown. author (String): The names of those creating and maintaining this code set (not any underlying dataset itself) organisation (String): The name of the organisation holding the dataset documentationVersion (Version): The version of the description of an underlying dataset lastUpdated (DateTime): The date/time when this code set was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this code set (see classifiers ) type (CodeSet Type): Will always be defined as CodeSet . finalised (Boolean): Whether this code set has been 'finalised', or is in draft mode Endpoints which return multiple code sets typically include sufficient fields for generating links on the interface - a separate call to return the details of the Code Set is usually required. As well as the endpoints listed below, a Code Set is also a CatalogueItem, and so a Code Set identifier can also be used as the parameter to any of those endpoints List all code sets \u00b6 The following endpoint returns a paginated list of all code set objects readable by the current user: /api/codeSets This endpoint returns all the code sets within a particular folder; again, this result is paginated . /api/folders/ {folderId} /codeSets Get information about a particular code set \u00b6 This endpoint provides the default information about a code set, as per the JSON at the top of the page. /api/codeSets/ {id} Create code set \u00b6 To create a new code set from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/codeSets There are two ways of versioning code set in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/codeSets/ {codeSetId} /newBranchModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/codeSets/ {codeSetId} /newDocumentationVersion By default, this will supersede the original data model. It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set: /api/codeSets/ {codeSetId} /newBranchModelVersion Request body (JSON) 1 2 3 { \"branchName\" : \"newBranch\" } To create a fork of the original data model: /api/codeSets/ {codeSetId} /newForkModel Request body (JSON) 1 2 3 { \"label\" : \"newForkLabel\" } Update code set \u00b6 To edit the primitive properties of a code set, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/codeSets/ {id} To move a code set from one folder to another, call the following, using the id fields for the code set, and the new folder: /api/folders/ {folderId} /codeSets/ {codeSetId} Alternatively, you can call this equivalent endpoint: /api/codeSets/ {codeSetId} /folder/ {folderId} To move a code set from a draft state to 'finalised', use the following endpoint: /api/codeSets/ {codeSetId} /finalise Sharing \u00b6 To allow a code set to be read by any authenticated user of the system, use the following endpoint: /api/codeSets/ {codeSetId} /readByAuthenticated ... and to remove this flag, use the following: /api/codeSets/ {codeSetId} /readByAuthenticated Similarly, to allow the code set to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/codeSets/ {codeSetId} /readByEveryone ... and the following to remove this flag: /api/codeSets/ {codeSetId} /readByEveryone Delete code set \u00b6 To delete a code set, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/codeSets/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/codeSets/ {id} /undoSoftDelete Import / export a code set \u00b6 To export a code set using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple code sets: /api/codeSets/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single code set, you can use the following endpoint with the id of the code sets specified: /api/codeSets/ {codeSetId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more code sets, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/codeSets/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Codeset"},{"location":"rest-api/resources/codeset/#list-all-code-sets","text":"The following endpoint returns a paginated list of all code set objects readable by the current user: /api/codeSets This endpoint returns all the code sets within a particular folder; again, this result is paginated . /api/folders/ {folderId} /codeSets","title":"List all code sets"},{"location":"rest-api/resources/codeset/#get-information-about-a-particular-code-set","text":"This endpoint provides the default information about a code set, as per the JSON at the top of the page. /api/codeSets/ {id}","title":"Get information about a particular code set"},{"location":"rest-api/resources/codeset/#create-code-set","text":"To create a new code set from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/codeSets There are two ways of versioning code set in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/codeSets/ {codeSetId} /newBranchModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/codeSets/ {codeSetId} /newDocumentationVersion By default, this will supersede the original data model. It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set: /api/codeSets/ {codeSetId} /newBranchModelVersion Request body (JSON) 1 2 3 { \"branchName\" : \"newBranch\" } To create a fork of the original data model: /api/codeSets/ {codeSetId} /newForkModel Request body (JSON) 1 2 3 { \"label\" : \"newForkLabel\" }","title":"Create code set"},{"location":"rest-api/resources/codeset/#update-code-set","text":"To edit the primitive properties of a code set, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/codeSets/ {id} To move a code set from one folder to another, call the following, using the id fields for the code set, and the new folder: /api/folders/ {folderId} /codeSets/ {codeSetId} Alternatively, you can call this equivalent endpoint: /api/codeSets/ {codeSetId} /folder/ {folderId} To move a code set from a draft state to 'finalised', use the following endpoint: /api/codeSets/ {codeSetId} /finalise","title":"Update code set"},{"location":"rest-api/resources/codeset/#sharing","text":"To allow a code set to be read by any authenticated user of the system, use the following endpoint: /api/codeSets/ {codeSetId} /readByAuthenticated ... and to remove this flag, use the following: /api/codeSets/ {codeSetId} /readByAuthenticated Similarly, to allow the code set to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/codeSets/ {codeSetId} /readByEveryone ... and the following to remove this flag: /api/codeSets/ {codeSetId} /readByEveryone","title":"Sharing"},{"location":"rest-api/resources/codeset/#delete-code-set","text":"To delete a code set, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/codeSets/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/codeSets/ {id} /undoSoftDelete","title":"Delete code set"},{"location":"rest-api/resources/codeset/#import-export-a-code-set","text":"To export a code set using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple code sets: /api/codeSets/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single code set, you can use the following endpoint with the id of the code sets specified: /api/codeSets/ {codeSetId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more code sets, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/codeSets/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Import / export a code set"},{"location":"rest-api/resources/data-class/","text":"A DataClass can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"DataClass\" , \"label\" : \"parent\" , \"description\" : \"Represents a parent data class.\" , \"aliases\" : [ \"root\" ], \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" \"model\" : \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\" , \"parentDataClass\" : \"363c202b-e6d9-4098-a5bf-78194d57b70d\" , \"minMultipicity\" : 0 , \"maxMultiplicity\" : -1 , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], } The fields are as follows: id (UUID): The unique identifier of this data class domainType (Type): The domain type of this catalogue object. Will always be DataClass in this case. label (String): The human-readable identifier of this class. description (String): A long description of the data class, and any important characteristics of the data. This field may include HTML, or MarkDown. aliases (Set(String)): Any other names by which this data class is known lastUpdated (DateTime): The date/time when this DataClass was last modified model (UUID): The unique identifier of the owning data model parentDataClass (UUID): The unique identifier of the data class of which this is a child of. If the data class does not have a parent, this field is undefined/not provided. minMultiplicity (Number): Defines the minimum uses of this data class may be applied to a data model. See the multipicity . maxMultiplicity (Number): Defines the maximum uses of this data class may be applied to a data model. See the multipicity . classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this data class (see classifiers ) As well as the endpoints listed below, a DataClass is also a CatalogueItem, and so a DataClass identifier can also be used as the parameter to any of those endpoints Child Classes \u00b6 A DataClass may be composed of child classes to define more complex definitions for a DataModel. Endpoints are provided to differentiate between parent and child data classes. Multiplicity \u00b6 Each DataClass defines its multiplicity to state how many occurances of this class should be expected in a model. Multipicities are defined in the notation x..y , where: x represents the minimum multipicity. y represents the maximum multipicity. A minimum multipicity must be provided and be greater than or equal to 0 . A maximum multiplicity may be provided that is greater than or equal to 1 . To represent unbounded multipicity, the * symbol is used - numerically, for endpoints, this is represented as the integer -1 . Some examples of multipicities and what they represent: 0..* - an optional, unbounded data class. This may be present or not, and has not limit on how many are present. 1..* - a required, unbounded data class. Similar to above but with the added constraint that at least one must be present. 0..1 - an optional, singular data class. Either the class is present in the model or not. 1..1 - a required, singular data class. This represents that the class must be present in the model. Getting information \u00b6 The following endpoints returns a paginated list of all the DataTypes within a particular DataModel. The first requests the data classes for a data model, the second requests the data classes for a parent data class. /api/dataModels/ {dataModelId} /dataClasses /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses These endpoints provide the detailed information about a particular DataClass; the first requests a DataType under a particular DataModel, the second requests a DataClass from a parent DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id} Create / Update / Delete \u00b6 To create a new DataClass from scratch, use the following post endpoints, depending on whether to create one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses To edit the properties of a DataClass, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {id} To delete a DataClass, use the following endpoint, depending on whether to delete one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id} Copying \u00b6 Instead of creating a new DataClass from scratch, it is also possible to copy an existing DataClass from another DataModel. Use the following endpoints to accomplish this, depending on whether to copy one directly under a DataModel or under a parent DataClass respectively. The dataModelId and dataClassId refers to the target DataModel or parent DataClass to copy to; otherDataModelId and otherDataCLassId refers to the source DataModel/Class to copy from. /api/dataModels/ {dataModelId} /dataClasses/ {otherDataModelId} / {otherDataClassId} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {otherDataModelId} / {otherDataClassId}","title":"Data class"},{"location":"rest-api/resources/data-class/#child-classes","text":"A DataClass may be composed of child classes to define more complex definitions for a DataModel. Endpoints are provided to differentiate between parent and child data classes.","title":"Child Classes"},{"location":"rest-api/resources/data-class/#multiplicity","text":"Each DataClass defines its multiplicity to state how many occurances of this class should be expected in a model. Multipicities are defined in the notation x..y , where: x represents the minimum multipicity. y represents the maximum multipicity. A minimum multipicity must be provided and be greater than or equal to 0 . A maximum multiplicity may be provided that is greater than or equal to 1 . To represent unbounded multipicity, the * symbol is used - numerically, for endpoints, this is represented as the integer -1 . Some examples of multipicities and what they represent: 0..* - an optional, unbounded data class. This may be present or not, and has not limit on how many are present. 1..* - a required, unbounded data class. Similar to above but with the added constraint that at least one must be present. 0..1 - an optional, singular data class. Either the class is present in the model or not. 1..1 - a required, singular data class. This represents that the class must be present in the model.","title":"Multiplicity"},{"location":"rest-api/resources/data-class/#getting-information","text":"The following endpoints returns a paginated list of all the DataTypes within a particular DataModel. The first requests the data classes for a data model, the second requests the data classes for a parent data class. /api/dataModels/ {dataModelId} /dataClasses /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses These endpoints provide the detailed information about a particular DataClass; the first requests a DataType under a particular DataModel, the second requests a DataClass from a parent DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id}","title":"Getting information"},{"location":"rest-api/resources/data-class/#create-update-delete","text":"To create a new DataClass from scratch, use the following post endpoints, depending on whether to create one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses To edit the properties of a DataClass, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {id} To delete a DataClass, use the following endpoint, depending on whether to delete one directly under a DataModel or under a parent DataClass respectively. /api/dataModels/ {dataModelId} /dataClasses/ {id} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {id}","title":"Create / Update / Delete"},{"location":"rest-api/resources/data-class/#copying","text":"Instead of creating a new DataClass from scratch, it is also possible to copy an existing DataClass from another DataModel. Use the following endpoints to accomplish this, depending on whether to copy one directly under a DataModel or under a parent DataClass respectively. The dataModelId and dataClassId refers to the target DataModel or parent DataClass to copy to; otherDataModelId and otherDataCLassId refers to the source DataModel/Class to copy from. /api/dataModels/ {dataModelId} /dataClasses/ {otherDataModelId} / {otherDataClassId} /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataClasses/ {otherDataModelId} / {otherDataClassId}","title":"Copying"},{"location":"rest-api/resources/data-element/","text":"A DataElement can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"DataElement\" , \"label\" : \"element\" , \"description\" : \"Description of the Data Element.\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" \"model\" : \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\" , \"dataClass\" : \"afb3dcda-fd7d-40b9-857c-23fb5af8cbbf\" , \"dataType\" : { \"id\" : \"c85d78d3-cac8-449b-a22b-52da144b9a8f\" , \"domainType\" : \"PrimitiveType\" , \"label\" : \"integer\" } } The fields are as follows: id (UUID): The unique identifier of this data element domainType (Type): The domain type of this catalogue object. This is always DataElement in this case. label (String): The human-readable identifier of this element. description (String): A long description of the data element, and any important characteristics of the data. This field may include HTML, or MarkDown. lastUpdated (DateTime): The date/time when this DataElement was last modified model (UUID): The unique identifier of the parent data model dataClass (UUID): The unique identifier of the parent data class dataType (Object): The type definition of this data element. The object returned matches the JSON defined in Data type As well as the endpoints listed below, a DataElement is also a CatalogueItem, and so a DataElement identifier can also be used as the parameter to any of those endpoints Getting information \u00b6 The following endpoint returns a paginated list of all the DataElements within a particular DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements This endpoint provides the detailed information about a particular DataElement under a DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id} Create / Update / Delete \u00b6 To create a new DataElement from scratch, use the following post endpoint. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements To edit the properties of a DataElement, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id} To delete a DataElement, use the following endpoint. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id} Copying \u00b6 Instead of creating a new DataElement from scratch, it is also possible to copy an existing DataElement from another DataClass. Use the following endpoint to accomplish this. The dataModelId and dataClassId refers to the target DataClass to copy to; otherDataModelId , otherDataClassId and dataElementId refer to the source DataModel/Class/Element to copy from. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {otherDataModelId} / {otherDataClassId} / {dataElementId}","title":"Data element"},{"location":"rest-api/resources/data-element/#getting-information","text":"The following endpoint returns a paginated list of all the DataElements within a particular DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements This endpoint provides the detailed information about a particular DataElement under a DataClass. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id}","title":"Getting information"},{"location":"rest-api/resources/data-element/#create-update-delete","text":"To create a new DataElement from scratch, use the following post endpoint. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements To edit the properties of a DataElement, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id} To delete a DataElement, use the following endpoint. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {id}","title":"Create / Update / Delete"},{"location":"rest-api/resources/data-element/#copying","text":"Instead of creating a new DataElement from scratch, it is also possible to copy an existing DataElement from another DataClass. Use the following endpoint to accomplish this. The dataModelId and dataClassId refers to the target DataClass to copy to; otherDataModelId , otherDataClassId and dataElementId refer to the source DataModel/Class/Element to copy from. /api/dataModels/ {dataModelId} /dataClasses/ {dataClassId} /dataElements/ {otherDataModelId} / {otherDataClassId} / {dataElementId}","title":"Copying"},{"location":"rest-api/resources/data-flow-component/","text":"/api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataClassFlows/ {dataClassId} /dataFlowComponents /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {dataFlowComponentId} / {type} / {dataElementId} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataFlowComponents/ {dataFlowComponentId} / {type} / {dataElementId}","title":"Data flow component"},{"location":"rest-api/resources/data-flow/","text":"/api/dataModels/ {dataModelId} /dataFlows/import/ {importerNamespace} / {importerName} / {importerVersion} /api/dataModels/ {dataModelId} /dataFlows/export/ {exporterNamespace} / {exporterName} / {exporterVersion} /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /dataClassFlows /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /diagramLayout /api/dataModels/ {dataModelId} /dataFlows/ {dataFlowId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} /api/dataModels/ {dataModelId} /dataFlows /api/dataModels/ {dataModelId} /dataFlows /api/dataModels/ {dataModelId} /dataFlows/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {id} /api/dataModels/ {dataModelId} /dataFlows/ {id}","title":"Data flow"},{"location":"rest-api/resources/data-model/","text":"DataModel object description \u00b6 In its simplest form, a DataModel can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"DataModel\" , \"label\" : \"Diagnostic Imaging Dataset\" , \"aliases\" : [ \"DID\" ], \"description\" : \"Central collection of detailed information about diagnostic imaging tests carried out on NHS patients (such as x-rays and MRI scans). Any organisation providing diagnostic imaging tests to NHS patients in England, i.e.:\\n* NHS (Foundation) trusts / hospitals\\n* NHS-funded activity with independent sector providers\\nNOT included are breast screening services or any other diagnostic imaging tests not typically recorded on the local provider's Radiology Information Systems.\\nDiagnostic Imaging Dataset (DID) does not store the images themselves, or the outcomes/diagnoses related to these images.\" , \"author\" : \"NHS Digital\" , \"organisation\" : \"NHS Digital\" , \"editable\" : true , \"documentationVersion\" : \"2.0.0\" , \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"type\" : \"Data Asset\" , \"finalised\" : false , } The fields are as follows: id (UUID): The unique identifier of this data model domainType (Type): The domain type of this catalogue object - always \"DataModel\" in this case label (String): The human-readable identifier of this model. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this datamodel is known description (String): A long description of the data model, and any important characteristics of the data. This field may include HTML, or MarkDown. author (String): The names of those creating and maintaining this Datamodel (not any underlying dataset itself) organisation (String): The name of the organisation holding the dataset editable (Boolean): Whether the current user (see authentication ) is allowed to edit this DataModel documentationVersion (Version): The version of the description of an underlying dataset lastUpdated (DateTime): The date/time when this DataModel was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this data model (see classifiers ) type (DataModel Type): Whether this DataModel is a \"Data Asset\", or a \"Data Standard\" finalised (Boolean): Whether this DataModel has been 'finalised', or is in draft mode Endpoints which return multiple models typically include sufficient fields for generating links on the interface - a separate call to return the details of the DataModel is usually required. As well as the endpoints listed below, a DataModel is also a CatalogueItem, and so a DataModel identifier can also be used as the parameter to any of those endpoints List all data models \u00b6 The following endpoint returns a paginated list of all DataModel objects readable by the current user: /api/dataModels This endpoint returns all the DataModels within a particular folder; again, this result is paginated . /api/folders/ {folderId} /dataModels Get information about a particular data model \u00b6 This endpoint provides the default information about a DataModel, as per the JSON at the top of the page. /api/dataModels/ {id} The 'hierarchy' endpoint provides a structured representation of the entire datamodel - child DataClasses, sub-DataClasses, and their child DataElements. Warning This call can take a long time for large data models /api/dataModels/ {dataModelId} /hierarchy The 'types' endpoint lists all the datatypes for a given datamodel. This returns primitive, reference, enumeration and terminology types owned by the data model, whether used or not. /api/dataModels/types Create data model \u00b6 To create a new data model from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/dataModels There are two ways of versioning Data Models in the catalogue. To create an entirely new version of an existing model, please use the following endpoint with no request body: /api/dataModels/ {dataModelId} /newBranchModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/dataModels/ {dataModelId} /newDocumentationVersion By default, this will supersede the original data model. It is also possible to branch and fork Data Models to create drafts before finalising them. To create a new branch from an existing Data Model: /api/dataModels/ {dataModelId} /newBranchModelVersion Request body (JSON) 1 2 3 { \"branchName\" : \"newBranch\" } To create a fork of the original data model: /api/dataModels/ {dataModelId} /newForkModel Request body (JSON) 1 2 3 { \"label\" : \"newForkLabel\" } Update data model \u00b6 To edit the primitive properties of a data model, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {id} To move a data model from one folder to another, call the following, using the id fields for the data model, and the new folder: /api/folders/ {folderId} /dataModels/ {dataModelId} Alternatively, you can call this equivalent endpoint: /api/dataModels/ {dataModelId} /folder/ {folderId} To move a data model from a draft state to 'finalised', use the following endpoint: /api/dataModels/ {dataModelId} /finalise Sharing \u00b6 To allow a model to be read by any authenticated user of the system, use the following endpoint: /api/dataModels/ {dataModelId} /readByAuthenticated ... and to remove this flag, use the following: /api/dataModels/ {dataModelId} /readByAuthenticated Similarly, to allow the model to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/dataModels/ {dataModelId} /readByEveryone ... and the following to remove this flag: /api/dataModels/ {dataModelId} /readByEveryone Delete data model \u00b6 To delete a data model, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/dataModels/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/dataModels/ {id} /undoSoftDelete Import / export a data model \u00b6 To export a data model using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple data models: /api/dataModels/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single model, you can use the following endpoint with the id of the data model specified: /api/dataModels/ {dataModelId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more data models, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/dataModels/import/ {importerNamespace} / {importerName} / {importerVersion} Finalise a data model \u00b6 To finalise a data model means to lock it to a particular version and make it read-only; only new versions can be created to make further modifications after that point. Use this endpoint with a similar payloads described below to finalise a data model. /api/dataModels/ {id} /finalise To automatically let Mauro choose the next version number, set the versionChange property to either 'Major' , 'Minor' or 'Patch' . Request body (JSON) 1 2 3 { \"versionChange\" : \"Major\" | \"Minor\" | \"Patch\" } Mauro uses Semantic Versioning rules to determine the next appropriate version number based on the versionChange value provided. To optionally choose your own version number, provide this payload. If versionChange is 'Custom' , then version must also be provided. Request body (JSON) 1 2 3 4 { \"versionChange\" : \"Custom\" , \"version\" : \"1.2.3.4\" } In all cases you may also supply an optional tag name to assign with the finalised version to help provide more context, as follows: Request body (JSON) 1 2 3 4 { \"versionChange\" : \"Major\" | \"Minor\" | \"Patch\" , \"versionTag\" : \"My first version\" } Merging data models \u00b6 If creating branches of data models, it is possible to merge the data values from one data model to another. The first step is to calculate the differences between two data models, as follows: /api/dataModels/ {sourceId} /mergeDiff/ {targetId} ?isLegacy=false Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 { \"sourceId\" : \"f9a4e390-6259-4616-b725-d45524851a82\" , \"targetId\" : \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch\" , \"label\" : \"Model Version Tree DataModel\" , \"count\" : 8 , \"diffs\" : [ { \"fieldName\" : \"author\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch@author\" , \"sourceValue\" : \"Mauro User\" , \"targetValue\" : \"Dante\" , \"commonAncestorValue\" : \"Dante\" , \"isMergeConflict\" : false , \"type\" : \"modification\" }, { \"fieldName\" : \"organisation\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch@organisation\" , \"sourceValue\" : \"Mauro\" , \"targetValue\" : \"Baal\" , \"commonAncestorValue\" : \"Baal\" , \"isMergeConflict\" : false , \"type\" : \"modification\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|ann:Test Comment\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|dc:Test Data Class\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|md:v1Versioning.com.mdk1\" , \"isMergeConflict\" : false , \"isSourceDeletionAndTargetModification\" : false , \"type\" : \"deletion\" }, { \"fieldName\" : \"value\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|md:org.datacite.creator@value\" , \"sourceValue\" : \"Peter Monks\" , \"targetValue\" : \"Mauro Administrator\" , \"commonAncestorValue\" : null , \"isMergeConflict\" : true , \"type\" : \"modification\" }, { \"fieldName\" : \"value\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|md:test.com.testProperty@value\" , \"sourceValue\" : \"Oliver Freeman\" , \"targetValue\" : \"Peter Monks\" , \"commonAncestorValue\" : null , \"isMergeConflict\" : true , \"type\" : \"modification\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" } ] } The diffs collection will hold each change found between the two data models and how they relate. All changes need to be manually organised into patches so that they can be applied to the target data model. Then the following endpoint is used to commit: /api/dataModels/ {sourceId} /mergeInto/ {targetId} ?isLegacy=false Request body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"changeNotice\" : \"Change comment\" , \"deleteBranch\" : false , \"patch\" : { \"sourceId\" : \"f9a4e390-6259-4616-b725-d45524851a82\" , \"targetId\" : \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\" , \"label\" : \"Model Version Tree DataModel\" , \"count\" : 3 , \"patches\" : [ { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"fieldName\" : \"author\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch@author\" , \"sourceValue\" : \"Mauro User\" , \"targetValue\" : \"Mauro User\" , \"commonAncestorValue\" : \"Dante\" , \"isMergeConflict\" : false , \"type\" : \"modification\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|md:v1Versioning.com.mdk1\" , \"isMergeConflict\" : false , \"isSourceDeletionAndTargetModification\" : false , \"type\" : \"deletion\" }, ] } } The key point is to set the targetValue of every patch item to change - this value is what will be written to the target data model when committing the merge.","title":"Data model"},{"location":"rest-api/resources/data-model/#datamodel-object-description","text":"In its simplest form, a DataModel can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"DataModel\" , \"label\" : \"Diagnostic Imaging Dataset\" , \"aliases\" : [ \"DID\" ], \"description\" : \"Central collection of detailed information about diagnostic imaging tests carried out on NHS patients (such as x-rays and MRI scans). Any organisation providing diagnostic imaging tests to NHS patients in England, i.e.:\\n* NHS (Foundation) trusts / hospitals\\n* NHS-funded activity with independent sector providers\\nNOT included are breast screening services or any other diagnostic imaging tests not typically recorded on the local provider's Radiology Information Systems.\\nDiagnostic Imaging Dataset (DID) does not store the images themselves, or the outcomes/diagnoses related to these images.\" , \"author\" : \"NHS Digital\" , \"organisation\" : \"NHS Digital\" , \"editable\" : true , \"documentationVersion\" : \"2.0.0\" , \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"type\" : \"Data Asset\" , \"finalised\" : false , } The fields are as follows: id (UUID): The unique identifier of this data model domainType (Type): The domain type of this catalogue object - always \"DataModel\" in this case label (String): The human-readable identifier of this model. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this datamodel is known description (String): A long description of the data model, and any important characteristics of the data. This field may include HTML, or MarkDown. author (String): The names of those creating and maintaining this Datamodel (not any underlying dataset itself) organisation (String): The name of the organisation holding the dataset editable (Boolean): Whether the current user (see authentication ) is allowed to edit this DataModel documentationVersion (Version): The version of the description of an underlying dataset lastUpdated (DateTime): The date/time when this DataModel was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this data model (see classifiers ) type (DataModel Type): Whether this DataModel is a \"Data Asset\", or a \"Data Standard\" finalised (Boolean): Whether this DataModel has been 'finalised', or is in draft mode Endpoints which return multiple models typically include sufficient fields for generating links on the interface - a separate call to return the details of the DataModel is usually required. As well as the endpoints listed below, a DataModel is also a CatalogueItem, and so a DataModel identifier can also be used as the parameter to any of those endpoints","title":"DataModel object description"},{"location":"rest-api/resources/data-model/#list-all-data-models","text":"The following endpoint returns a paginated list of all DataModel objects readable by the current user: /api/dataModels This endpoint returns all the DataModels within a particular folder; again, this result is paginated . /api/folders/ {folderId} /dataModels","title":"List all data models"},{"location":"rest-api/resources/data-model/#get-information-about-a-particular-data-model","text":"This endpoint provides the default information about a DataModel, as per the JSON at the top of the page. /api/dataModels/ {id} The 'hierarchy' endpoint provides a structured representation of the entire datamodel - child DataClasses, sub-DataClasses, and their child DataElements. Warning This call can take a long time for large data models /api/dataModels/ {dataModelId} /hierarchy The 'types' endpoint lists all the datatypes for a given datamodel. This returns primitive, reference, enumeration and terminology types owned by the data model, whether used or not. /api/dataModels/types","title":"Get information about a particular data model"},{"location":"rest-api/resources/data-model/#create-data-model","text":"To create a new data model from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/dataModels There are two ways of versioning Data Models in the catalogue. To create an entirely new version of an existing model, please use the following endpoint with no request body: /api/dataModels/ {dataModelId} /newBranchModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/dataModels/ {dataModelId} /newDocumentationVersion By default, this will supersede the original data model. It is also possible to branch and fork Data Models to create drafts before finalising them. To create a new branch from an existing Data Model: /api/dataModels/ {dataModelId} /newBranchModelVersion Request body (JSON) 1 2 3 { \"branchName\" : \"newBranch\" } To create a fork of the original data model: /api/dataModels/ {dataModelId} /newForkModel Request body (JSON) 1 2 3 { \"label\" : \"newForkLabel\" }","title":"Create data model"},{"location":"rest-api/resources/data-model/#update-data-model","text":"To edit the primitive properties of a data model, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {id} To move a data model from one folder to another, call the following, using the id fields for the data model, and the new folder: /api/folders/ {folderId} /dataModels/ {dataModelId} Alternatively, you can call this equivalent endpoint: /api/dataModels/ {dataModelId} /folder/ {folderId} To move a data model from a draft state to 'finalised', use the following endpoint: /api/dataModels/ {dataModelId} /finalise","title":"Update data model"},{"location":"rest-api/resources/data-model/#sharing","text":"To allow a model to be read by any authenticated user of the system, use the following endpoint: /api/dataModels/ {dataModelId} /readByAuthenticated ... and to remove this flag, use the following: /api/dataModels/ {dataModelId} /readByAuthenticated Similarly, to allow the model to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/dataModels/ {dataModelId} /readByEveryone ... and the following to remove this flag: /api/dataModels/ {dataModelId} /readByEveryone","title":"Sharing"},{"location":"rest-api/resources/data-model/#delete-data-model","text":"To delete a data model, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/dataModels/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/dataModels/ {id} /undoSoftDelete","title":"Delete data model"},{"location":"rest-api/resources/data-model/#import-export-a-data-model","text":"To export a data model using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple data models: /api/dataModels/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single model, you can use the following endpoint with the id of the data model specified: /api/dataModels/ {dataModelId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more data models, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/dataModels/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Import / export a data model"},{"location":"rest-api/resources/data-model/#finalise-a-data-model","text":"To finalise a data model means to lock it to a particular version and make it read-only; only new versions can be created to make further modifications after that point. Use this endpoint with a similar payloads described below to finalise a data model. /api/dataModels/ {id} /finalise To automatically let Mauro choose the next version number, set the versionChange property to either 'Major' , 'Minor' or 'Patch' . Request body (JSON) 1 2 3 { \"versionChange\" : \"Major\" | \"Minor\" | \"Patch\" } Mauro uses Semantic Versioning rules to determine the next appropriate version number based on the versionChange value provided. To optionally choose your own version number, provide this payload. If versionChange is 'Custom' , then version must also be provided. Request body (JSON) 1 2 3 4 { \"versionChange\" : \"Custom\" , \"version\" : \"1.2.3.4\" } In all cases you may also supply an optional tag name to assign with the finalised version to help provide more context, as follows: Request body (JSON) 1 2 3 4 { \"versionChange\" : \"Major\" | \"Minor\" | \"Patch\" , \"versionTag\" : \"My first version\" }","title":"Finalise a data model"},{"location":"rest-api/resources/data-model/#merging-data-models","text":"If creating branches of data models, it is possible to merge the data values from one data model to another. The first step is to calculate the differences between two data models, as follows: /api/dataModels/ {sourceId} /mergeDiff/ {targetId} ?isLegacy=false Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 { \"sourceId\" : \"f9a4e390-6259-4616-b725-d45524851a82\" , \"targetId\" : \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch\" , \"label\" : \"Model Version Tree DataModel\" , \"count\" : 8 , \"diffs\" : [ { \"fieldName\" : \"author\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch@author\" , \"sourceValue\" : \"Mauro User\" , \"targetValue\" : \"Dante\" , \"commonAncestorValue\" : \"Dante\" , \"isMergeConflict\" : false , \"type\" : \"modification\" }, { \"fieldName\" : \"organisation\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch@organisation\" , \"sourceValue\" : \"Mauro\" , \"targetValue\" : \"Baal\" , \"commonAncestorValue\" : \"Baal\" , \"isMergeConflict\" : false , \"type\" : \"modification\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|ann:Test Comment\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|dc:Test Data Class\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|md:v1Versioning.com.mdk1\" , \"isMergeConflict\" : false , \"isSourceDeletionAndTargetModification\" : false , \"type\" : \"deletion\" }, { \"fieldName\" : \"value\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|md:org.datacite.creator@value\" , \"sourceValue\" : \"Peter Monks\" , \"targetValue\" : \"Mauro Administrator\" , \"commonAncestorValue\" : null , \"isMergeConflict\" : true , \"type\" : \"modification\" }, { \"fieldName\" : \"value\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|md:test.com.testProperty@value\" , \"sourceValue\" : \"Oliver Freeman\" , \"targetValue\" : \"Peter Monks\" , \"commonAncestorValue\" : null , \"isMergeConflict\" : true , \"type\" : \"modification\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" } ] } The diffs collection will hold each change found between the two data models and how they relate. All changes need to be manually organised into patches so that they can be applied to the target data model. Then the following endpoint is used to commit: /api/dataModels/ {sourceId} /mergeInto/ {targetId} ?isLegacy=false Request body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"changeNotice\" : \"Change comment\" , \"deleteBranch\" : false , \"patch\" : { \"sourceId\" : \"f9a4e390-6259-4616-b725-d45524851a82\" , \"targetId\" : \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\" , \"label\" : \"Model Version Tree DataModel\" , \"count\" : 3 , \"patches\" : [ { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"fieldName\" : \"author\" , \"path\" : \"dm:Model Version Tree DataModel$interestingBranch@author\" , \"sourceValue\" : \"Mauro User\" , \"targetValue\" : \"Mauro User\" , \"commonAncestorValue\" : \"Dante\" , \"isMergeConflict\" : false , \"type\" : \"modification\" }, { \"path\" : \"dm:Model Version Tree DataModel$interestingBranch|md:v1Versioning.com.mdk1\" , \"isMergeConflict\" : false , \"isSourceDeletionAndTargetModification\" : false , \"type\" : \"deletion\" }, ] } } The key point is to set the targetValue of every patch item to change - this value is what will be written to the target data model when committing the merge.","title":"Merging data models"},{"location":"rest-api/resources/data-type/","text":"A DataType can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"PrimitiveType\" , \"label\" : \"integer\" , \"description\" : \"Represents a number.\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" \"model\" : \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\" , } The fields are as follows: id (UUID): The unique identifier of this data type domainType (Type): The domain type of this catalogue object. Could be PrimitiveType or EnumerationType . label (String): The human-readable identifier of this type. description (String): A long description of the data type, and any important characteristics of the data. This field may include HTML, or MarkDown. lastUpdated (DateTime): The date/time when this DataType was last modified model (UUID): The unique identifier of the parent data model As well as the endpoints listed below, a DataType is also a CatalogueItem, and so a DataType identifier can also be used as the parameter to any of those endpoints Default Data Types \u00b6 When creating DataModels , a default data type provider can be used to automatically define data types for a data model. To list all available data type providers: /api/dataModels/defaultDataTypeProviders Getting information \u00b6 The following endpoint returns a paginated list of all the DataTypes within a particular DataModel. /api/dataModels/ {dataModelId} /dataTypes This endpoint provides the detailed information about a particular DataType under a DataModel. /api/dataModels/ {dataModelId} /dataTypes/ {id} Create / Update / Delete \u00b6 To create a new DataType from scratch, use the following post endpoint. /api/dataModels/ {dataModelId} /dataTypes To edit the properties of a DataType, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {dataModelId} /dataTypes/ {id} To delete a DataType, use the following endpoint. /api/dataModels/ {dataModelId} /dataTypes/ {id} Copying \u00b6 Instead of creating a new DataType from scratch, it is also possible to copy an existing DataType from another DataModel. Use the following endpoint to accomplish this. The dataModelId refers to the target DataModel to copy to; otherDataModelId and dataTypeId refer to the source DataModel/Type to copy from. /api/dataModels/ {dataModelId} /dataTypes/ {otherDataModelId} / {dataTypeId}","title":"Data type"},{"location":"rest-api/resources/data-type/#default-data-types","text":"When creating DataModels , a default data type provider can be used to automatically define data types for a data model. To list all available data type providers: /api/dataModels/defaultDataTypeProviders","title":"Default Data Types"},{"location":"rest-api/resources/data-type/#getting-information","text":"The following endpoint returns a paginated list of all the DataTypes within a particular DataModel. /api/dataModels/ {dataModelId} /dataTypes This endpoint provides the detailed information about a particular DataType under a DataModel. /api/dataModels/ {dataModelId} /dataTypes/ {id}","title":"Getting information"},{"location":"rest-api/resources/data-type/#create-update-delete","text":"To create a new DataType from scratch, use the following post endpoint. /api/dataModels/ {dataModelId} /dataTypes To edit the properties of a DataType, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/dataModels/ {dataModelId} /dataTypes/ {id} To delete a DataType, use the following endpoint. /api/dataModels/ {dataModelId} /dataTypes/ {id}","title":"Create / Update / Delete"},{"location":"rest-api/resources/data-type/#copying","text":"Instead of creating a new DataType from scratch, it is also possible to copy an existing DataType from another DataModel. Use the following endpoint to accomplish this. The dataModelId refers to the target DataModel to copy to; otherDataModelId and dataTypeId refer to the source DataModel/Type to copy from. /api/dataModels/ {dataModelId} /dataTypes/ {otherDataModelId} / {dataTypeId}","title":"Copying"},{"location":"rest-api/resources/enumeration-value/","text":"/api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues /api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues /api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues/ {id} /api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues/ {id} /api/dataModels/ {dataModelId} /enumerationTypes/ {enumerationTypeId} /enumerationValues/ {id}","title":"Enumeration value"},{"location":"rest-api/resources/folder/","text":"A Folder is a container type and can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"Folder\" , \"label\" : \"folder\" , \"description\" : \"Represents a folder.\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" , \"hasChildFolders\" : true } The fields are as follows: id (UUID): The unique identifier of this folder domainType (Type): The domain type of this catalogue object. Will always be Folder in this case. label (String): The human-readable identifier of this folder. description (String): A long description of the folder, and any important characteristics of the data. This field may include HTML, or MarkDown. lastUpdated (DateTime): The date/time when this folder was last modified hasChildFolders (Boolean): Determines if this folder contains child folders. As well as the endpoints listed below, a Folder is also a CatalogueItem, and so a Folder identifier can also be used as the parameter to any of those endpoints Child Folders \u00b6 A folder may contain child folders. Endpoints are provided to differentiate between parent and child folders. Getting information \u00b6 The following endpoints returns a paginated list of all the folders. The first requests all root folders in Mauro, the second requests the folders for a parent folder. /api/folders /api/folders/ {folderId} /folders These endpoints provide the detailed information about a particular folder; the first requests a root folder in Mauro, the second requests a folder from a parent folder. /api/folders/ {id} /api/folders/ {folderId} /folders/ {id} Create / Update / Delete \u00b6 To create a new folder from scratch, use the following post endpoints, depending on whether to create one with or without a parent. /api/folders /api/folders/ {folderId} /folders To edit the properties of a folder, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent. /api/folders/ {id} /api/folders/ {folderId} /folders/ {id} To delete a folder, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/folders/ {id} ?permanent= {true/false} /api/folders/ {folderId} /folders/ {id} ?permanent= {true/false} Security \u00b6 /api/folders/ {folderId} /readByAuthenticated /api/folders/ {folderId} /readByAuthenticated /api/folders/ {folderId} /readByEveryone /api/folders/ {folderId} /readByEveryone","title":"Folder"},{"location":"rest-api/resources/folder/#child-folders","text":"A folder may contain child folders. Endpoints are provided to differentiate between parent and child folders.","title":"Child Folders"},{"location":"rest-api/resources/folder/#getting-information","text":"The following endpoints returns a paginated list of all the folders. The first requests all root folders in Mauro, the second requests the folders for a parent folder. /api/folders /api/folders/ {folderId} /folders These endpoints provide the detailed information about a particular folder; the first requests a root folder in Mauro, the second requests a folder from a parent folder. /api/folders/ {id} /api/folders/ {folderId} /folders/ {id}","title":"Getting information"},{"location":"rest-api/resources/folder/#create-update-delete","text":"To create a new folder from scratch, use the following post endpoints, depending on whether to create one with or without a parent. /api/folders /api/folders/ {folderId} /folders To edit the properties of a folder, use the following endpoints, with a body similar to the JSON described at the top of this page. Use the appropriate endpoint depending on whether to edit one with or without a parent. /api/folders/ {id} /api/folders/ {folderId} /folders/ {id} To delete a folder, use the following endpoint, depending on whether to delete one with or without a parent. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/folders/ {id} ?permanent= {true/false} /api/folders/ {folderId} /folders/ {id} ?permanent= {true/false}","title":"Create / Update / Delete"},{"location":"rest-api/resources/folder/#security","text":"/api/folders/ {folderId} /readByAuthenticated /api/folders/ {folderId} /readByAuthenticated /api/folders/ {folderId} /readByEveryone /api/folders/ {folderId} /readByEveryone","title":"Security"},{"location":"rest-api/resources/plugin/","text":"/api/public/plugins/dataFlowImporters /api/public/plugins/dataFlowExporters /api/public/plugins/dataModelImporters /api/public/plugins/dataModelExporters Importer \u00b6 /api/importer/parameters/ {ns} ?/ {name} ?/ {version} ?","title":"Plugin"},{"location":"rest-api/resources/plugin/#importer","text":"/api/importer/parameters/ {ns} ?/ {name} ?/ {version} ?","title":"Importer"},{"location":"rest-api/resources/semantic-link/","text":"/api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks /api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks /api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /semanticLinks/ {id} /api/catalogueItems/ {catalogueItemId} /semanticLinks /api/catalogueItems/ {catalogueItemId} /semanticLinks /api/catalogueItems/ {catalogueItemId} /semanticLinks/ {id} /api/catalogueItems/ {catalogueItemId} /semanticLinks/ {id} /api/catalogueItems/ {catalogueItemId} /semanticLinks/ {id}","title":"Semantic link"},{"location":"rest-api/resources/term-relationship/","text":"/api/terminologies/ {terminologyId} /termRelationshipTypes/ {termRelationshipTypeId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /termRelationshipTypes/ {termRelationshipTypeId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} Term relationship types \u00b6 /api/terminologies/ {terminologyId} /termRelationshipTypes /api/terminologies/ {terminologyId} /termRelationshipTypes/ {id}","title":"Term relationship"},{"location":"rest-api/resources/term-relationship/#term-relationship-types","text":"/api/terminologies/ {terminologyId} /termRelationshipTypes /api/terminologies/ {terminologyId} /termRelationshipTypes/ {id}","title":"Term relationship types"},{"location":"rest-api/resources/term/","text":"A Term is part a Terminology and can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"Term\" , \"model\" : \"20b1fd65-a7bf-4a39-a14b-c80b0174b03e\" , \"code\" : \"CT1\" , \"definition\" : \"Custom Term\" , \"label\" : \"CT1: Custom Term\" , \"lastUpdated\" : \"2021-04-28T10:10:13.945Z\" } The fields are as follows: id (UUID): The unique identifier of this term domainType (Type): The domain type of this catalogue object. Will always be Term in this case. model (UUID): The unique identifier of the owning data model code (String): A unique code identifier for the term. definition (String): The definition/name of this term. label (String): The human-readable identifier of this term. This is the combination of code and definition . lastUpdated (DateTime): The date/time when this term was last modified As well as the endpoints listed below, a term is also a CatalogueItem, and so a term identifier can also be used as the parameter to any of those endpoints Getting information \u00b6 The following endpoint returns a paginated list of all the terms within a particular Terminology. /api/terminologies/ {terminologyId} /terms This endpoint provides the detailed information about a particular term. /api/terminologies/ {terminologyId} /terms/ {id} Create / Update / Delete \u00b6 To create a new term from scratch, use the following post endpoint with a JSON request body similar to above. /api/terminologies/ {terminologyId} /terms To edit the properties of a term, use the following endpoint, with a body similar to the JSON described at the top of this page. /api/terminologies/ {terminologyId} /terms/ {id} To delete a term, use the following endpoint. /api/terminologies/ {terminologyId} /terms/ {id} Relationships \u00b6 /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id}","title":"Term"},{"location":"rest-api/resources/term/#getting-information","text":"The following endpoint returns a paginated list of all the terms within a particular Terminology. /api/terminologies/ {terminologyId} /terms This endpoint provides the detailed information about a particular term. /api/terminologies/ {terminologyId} /terms/ {id}","title":"Getting information"},{"location":"rest-api/resources/term/#create-update-delete","text":"To create a new term from scratch, use the following post endpoint with a JSON request body similar to above. /api/terminologies/ {terminologyId} /terms To edit the properties of a term, use the following endpoint, with a body similar to the JSON described at the top of this page. /api/terminologies/ {terminologyId} /terms/ {id} To delete a term, use the following endpoint. /api/terminologies/ {terminologyId} /terms/ {id}","title":"Create / Update / Delete"},{"location":"rest-api/resources/term/#relationships","text":"/api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id} /api/terminologies/ {terminologyId} /terms/ {termId} /termRelationships/ {id}","title":"Relationships"},{"location":"rest-api/resources/terminology/","text":"In its simplest form, a Terminology can be represented as follows: Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"Terminology\" , \"label\" : \"Sample Terminology\" , \"aliases\" : [ \"sample\" ], \"description\" : \"Example of a Terminology\" , \"author\" : \"NHS Digital\" , \"organisation\" : \"NHS Digital\" , \"documentationVersion\" : \"2.0.0\" , \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"NIHR Health Data Finder\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"type\" : \"Terminology\" , \"finalised\" : false , } The fields are as follows: id (UUID): The unique identifier of this terminology domainType (Type): The domain type of this catalogue object - always Terminology in this case label (String): The human-readable identifier of this terminology. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this terminology is known description (String): A long description of the description, and any important characteristics of the data. This field may include HTML, or MarkDown. author (String): The names of those creating and maintaining this terminology (not any underlying dataset itself) organisation (String): The name of the organisation holding the terminology documentationVersion (Version): The version of the description of an underlying dataset lastUpdated (DateTime): The date/time when this terminology was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this terminology (see classifiers ) type (Terminology Type): Will always be defined as Terminology . finalised (Boolean): Whether this terminology has been 'finalised', or is in draft mode Endpoints which return multiple terminologies typically include sufficient fields for generating links on the interface - a separate call to return the details of the terminology is usually required. As well as the endpoints listed below, a terminology is also a CatalogueItem, and so a terminology identifier can also be used as the parameter to any of those endpoints List all terminologies \u00b6 The following endpoint returns a paginated list of all terminologies readable by the current user: /api/terminologies This endpoint returns all the terminologies within a particular folder; again, this result is paginated . /api/folders/ {folderId} /terminologies Get information about a particular terminology \u00b6 This endpoint provides the default information about a terminology, as per the JSON at the top of the page. /api/terminologies/ {id} Create terminologies \u00b6 There are two ways of versioning terminologies in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/terminology/ {terminologyId} /newBranchModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/terminology/ {terminologyId} /newDocumentationVersion By default, this will supersede the original terminology. It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set: /api/terminology/ {terminologyId} /newBranchModelVersion Request body (JSON) 1 2 3 { \"branchName\" : \"newBranch\" } To create a fork of the original terminology: /api/terminology/ {terminologyId} /newForkModel Request body (JSON) 1 2 3 { \"label\" : \"newForkLabel\" } Update terminology \u00b6 To edit the primitive properties of a terminology, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/terminology/ {id} To move a terminology from one folder to another, call the following, using the id fields for the terminology, and the new folder: /api/folders/ {folderId} /terminologies/ {terminologyId} Alternatively, you can call this equivalent endpoint: /api/terminologies/ {terminologyId} /folder/ {folderId} To move a terminology from a draft state to 'finalised', use the following endpoint: /api/terminologies/ {terminologyId} /finalise Sharing \u00b6 To allow a terminology to be read by any authenticated user of the system, use the following endpoint: /api/terminologies/ {terminologyId} /readByAuthenticated ... and to remove this flag, use the following: /api/terminologies/ {terminologyId} /readByAuthenticated Similarly, to allow the terminology to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/terminologies/ {terminologyId} /readByEveryone ... and the following to remove this flag: /api/terminologies/ {terminologyId} /readByEveryone Delete terminology \u00b6 To delete a terminology, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/terminologies/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted terminology using the following endpoint: /api/admin/terminologies/ {id} /undoSoftDelete Import / export a terminology \u00b6 To export a terminology using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple code sets: /api/terminologies/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single terminology, you can use the following endpoint with the id of the terminologies specified: /api/terminologies/ {terminologyId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more terminologies, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/terminologies/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Terminology"},{"location":"rest-api/resources/terminology/#list-all-terminologies","text":"The following endpoint returns a paginated list of all terminologies readable by the current user: /api/terminologies This endpoint returns all the terminologies within a particular folder; again, this result is paginated . /api/folders/ {folderId} /terminologies","title":"List all terminologies"},{"location":"rest-api/resources/terminology/#get-information-about-a-particular-terminology","text":"This endpoint provides the default information about a terminology, as per the JSON at the top of the page. /api/terminologies/ {id}","title":"Get information about a particular terminology"},{"location":"rest-api/resources/terminology/#create-terminologies","text":"There are two ways of versioning terminologies in the catalogue. To create an entirely new version of a model, please use the following endpoint: /api/terminology/ {terminologyId} /newBranchModelVersion The name must be different to the original model. To create a new 'documentation version', use the following endpoint: /api/terminology/ {terminologyId} /newDocumentationVersion By default, this will supersede the original terminology. It is also possible to branch and fork code sets to create drafts before finalising them. To create a new branch from an existing code set: /api/terminology/ {terminologyId} /newBranchModelVersion Request body (JSON) 1 2 3 { \"branchName\" : \"newBranch\" } To create a fork of the original terminology: /api/terminology/ {terminologyId} /newForkModel Request body (JSON) 1 2 3 { \"label\" : \"newForkLabel\" }","title":"Create terminologies"},{"location":"rest-api/resources/terminology/#update-terminology","text":"To edit the primitive properties of a terminology, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/terminology/ {id} To move a terminology from one folder to another, call the following, using the id fields for the terminology, and the new folder: /api/folders/ {folderId} /terminologies/ {terminologyId} Alternatively, you can call this equivalent endpoint: /api/terminologies/ {terminologyId} /folder/ {folderId} To move a terminology from a draft state to 'finalised', use the following endpoint: /api/terminologies/ {terminologyId} /finalise","title":"Update terminology"},{"location":"rest-api/resources/terminology/#sharing","text":"To allow a terminology to be read by any authenticated user of the system, use the following endpoint: /api/terminologies/ {terminologyId} /readByAuthenticated ... and to remove this flag, use the following: /api/terminologies/ {terminologyId} /readByAuthenticated Similarly, to allow the terminology to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/terminologies/ {terminologyId} /readByEveryone ... and the following to remove this flag: /api/terminologies/ {terminologyId} /readByEveryone","title":"Sharing"},{"location":"rest-api/resources/terminology/#delete-terminology","text":"To delete a terminology, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/terminologies/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted terminology using the following endpoint: /api/admin/terminologies/ {id} /undoSoftDelete","title":"Delete terminology"},{"location":"rest-api/resources/terminology/#import-export-a-terminology","text":"To export a terminology using a particular export plugin, you will need to know the namespace, the name, and the version of the plugin. The following endpoint can be used to export multiple code sets: /api/terminologies/export/ {exporterNamespace} / {exporterName} / {exporterVersion} To export a single terminology, you can use the following endpoint with the id of the terminologies specified: /api/terminologies/ {terminologyId} /export/ {exporterNamespace} / {exporterName} / {exporterVersion} Similarly, to import one or more terminologies, the namespace, name and version of the import plugin must be known. The body of this method should be the parameters for the import, including any files that are required. /api/terminologies/import/ {importerNamespace} / {importerName} / {importerVersion}","title":"Import / export a terminology"},{"location":"rest-api/resources/user-group/","text":"/api/userGroups/ {userGroupId} /catalogueUsers /api/userGroups/ {userGroupId} /catalogueUsers/ {catalogueUserId} /api/userGroups/ {userGroupId} /catalogueUsers/ {catalogueUserId} /api/userGroups /api/userGroups /api/userGroups/ {id} /api/userGroups/ {id} /api/userGroups/ {id}","title":"User group"},{"location":"rest-api/resources/user/","text":"/api/catalogueUsers /api/catalogueUsers /api/catalogueUsers/ {id} /api/catalogueUsers/ {id} /api/catalogueUsers/ {id} Admin functionality \u00b6 /api/catalogueUsers/adminRegister /api/catalogueUsers/pending /api/catalogueUsers/userExists/ {emailAddress} /api/catalogueUsers/ {catalogueUserId} /adminPasswordReset /api/catalogueUsers/ {catalogueUserId} /resetPasswordLink /api/catalogueUsers/ {catalogueUserId} /rejectRegistration /api/catalogueUsers/ {catalogueUserId} /approveRegistration /api/catalogueUsers/ {catalogueUserId} /changePassword /api/catalogueUsers/ {catalogueUserId} /userPreferences /api/catalogueUsers/ {catalogueUserId} /userPreferences Search \u00b6 /api/catalogueUsers/search/ {searchTerm?} /api/catalogueUsers/search Profile images \u00b6 /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image","title":"User"},{"location":"rest-api/resources/user/#admin-functionality","text":"/api/catalogueUsers/adminRegister /api/catalogueUsers/pending /api/catalogueUsers/userExists/ {emailAddress} /api/catalogueUsers/ {catalogueUserId} /adminPasswordReset /api/catalogueUsers/ {catalogueUserId} /resetPasswordLink /api/catalogueUsers/ {catalogueUserId} /rejectRegistration /api/catalogueUsers/ {catalogueUserId} /approveRegistration /api/catalogueUsers/ {catalogueUserId} /changePassword /api/catalogueUsers/ {catalogueUserId} /userPreferences /api/catalogueUsers/ {catalogueUserId} /userPreferences","title":"Admin functionality"},{"location":"rest-api/resources/user/#search","text":"/api/catalogueUsers/search/ {searchTerm?} /api/catalogueUsers/search","title":"Search"},{"location":"rest-api/resources/user/#profile-images","text":"/api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image /api/catalogueUsers/ {catalogueUserId} /image","title":"Profile images"},{"location":"rest-api/resources/versioned-folder/","text":"Description \u00b6 A Versioned Folder is a container type and can be represented as a mixture of a Folder and a Data Model , since it shares the functional characteristics of both - a container for holding other catalogue items, and a model that can be version controlled. Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"VersionedFolder\" , \"label\" : \"Sample Versioned Folder\" , \"aliases\" : [ \"SVF\" ], \"description\" : \"First sample version controlled folder.\" , \"branchName\" : \"main\" , \"documentationVersion\" : \"2.0.0\" , \"modelVersion\" : \"2.0.0\" \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"Samples\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"finalised\" : true , \"hasChildFolders\" : true } The fields are as follows: id (UUID): The unique identifier of this versioned folder. domainType (Type): The domain type of this catalogue object - always \"VersionedFolder\" in this case label (String): The human-readable identifier of this folder. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this versioned folder is known description (String): A long description of the versioned folder, and any important characteristics of the folder contents. This field may include HTML, or MarkDown. documentationVersion (Version): The version of the description of an underlying versioned folder. modelVersion (Version): The version of the folder of an underlying versioned folder. lastUpdated (DateTime): The date/time when this Versioned Folder was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this versioned folder (see classifiers ) finalised (Boolean): Whether this Versioned Folder has been 'finalised', or is in draft mode hasChildFolders (Boolean): Determines if this folder contains child folders. Endpoints which return multiple versioned folders typically include sufficient fields for generating links on the interface - a separate call to return the details of the Versioned Folder is usually required. As well as the endpoints listed below, a Versioned Folder is also a CatalogueItem, and so a Versioned Folder identifier can also be used as the parameter to any of those endpoints Getting information \u00b6 The following endpoint returns a paginated list of all versioned folders readable by the current user: /api/versionedFolders This endpoint returns all the Versioned Folders within a particular folder; again, this result is paginated . /api/folders/ {folderId} /versionedFolders This endpoint provides the default information about a Versioned Folder, as per the JSON at the top of the page. /api/versionedFolders/ {id} Create versioned folder \u00b6 To create a new versioned folder from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/versionedFolder There are two ways of versioning Versioned Folders in the catalogue. To create an entirely new version of an existing folder, please use the following endpoint with no request body: /api/versionedFolders/ {id} /newBranchModelVersion The name must be different to the original folder. To create a new 'documentation version', use the following endpoint: /api/versionedFolders/ {id} /newDocumentationVersion By default, this will supersede the original versioned folder. It is also possible to branch and fork Versioned Folders to create drafts before finalising them. To create a new branch from an existing Versioned Folder: /api/versionedFolders/ {id} /newBranchModelVersion Request body (JSON) 1 2 3 { \"branchName\" : \"newBranch\" } To create a fork of the original versioned folder: /api/versionedFolders/ {id} /newForkModel Request body (JSON) 1 2 3 { \"label\" : \"newForkLabel\" } Update versioned folder \u00b6 To edit the primitive properties of a versioned folder, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/versionedFolders/ {id} To move a versioned folder from one folder to another, call the following, using the id fields for the versioned folder, and the new folder: /api/folders/ {folderId} /versionedFolders/ {id} Alternatively, you can call this equivalent endpoint: /api/versionedFolders/ {id} /folder/ {folderId} Sharing \u00b6 To allow a versioned folder to be read by any authenticated user of the system, use the following endpoint: /api/versionedFolders/ {id} /readByAuthenticated ... and to remove this flag, use the following: /api/versionedFolders/ {id} /readByAuthenticated Similarly, to allow the versioned folder to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/versionedFolders/ {id} /readByEveryone ... and the following to remove this flag: /api/versionedFolders/ {id} /readByEveryone Delete versioned folder \u00b6 To delete a versioned folder, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/versionedFolders/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/versionedFolders/ {id} /undoSoftDelete Finalise a versioned folder \u00b6 To finalise a versioned folder means to lock it to a particular version and make it read-only; only new versions can be created to make further modifications after that point. This also applies to the child contents of this versioned folder too, all child items will share the same finalised state and version number assigned. Use this endpoint with a similar payloads described below to finalise a versioned folder. /api/versionedFolders/ {id} /finalise To automatically let Mauro choose the next version number, set the versionChange property to either 'Major' , 'Minor' or 'Patch' . Request body (JSON) 1 2 3 { \"versionChange\" : \"Major\" | \"Minor\" | \"Patch\" } Mauro uses Semantic Versioning rules to determine the next appropriate version number based on the versionChange value provided. To optionally choose your own version number, provide this payload. If versionChange is 'Custom' , then version must also be provided. Request body (JSON) 1 2 3 4 { \"versionChange\" : \"Custom\" , \"version\" : \"1.2.3.4\" } In all cases you may also supply an optional tag name to assign with the finalised version to help provide more context, as follows: Request body (JSON) 1 2 3 4 { \"versionChange\" : \"Major\" | \"Minor\" | \"Patch\" , \"versionTag\" : \"My first version\" } Merging versioned folders \u00b6 If creating branches of versioned folders, it is possible to merge the data values from one versioned folder to another. The first step is to calculate the differences between two versioned folders, as follows: /api/versionedFolders/ {sourceId} /mergeDiff/ {targetId} ?isLegacy=false Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 { \"sourceId\" : \"f9a4e390-6259-4616-b725-d45524851a82\" , \"targetId\" : \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\" , \"path\" : \"vf:Model Version Tree Folder$interestingBranch\" , \"label\" : \"Model Version Tree Folder\" , \"count\" : 6 , \"diffs\" : [ { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|ann:Test Comment\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|dm:Test Data Model\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|md:v1Versioning.com.mdk1\" , \"isMergeConflict\" : false , \"isSourceDeletionAndTargetModification\" : false , \"type\" : \"deletion\" }, { \"fieldName\" : \"value\" , \"path\" : \"vf:Model Version Tree Folder$interestingBranch|md:org.datacite.creator@value\" , \"sourceValue\" : \"Peter Monks\" , \"targetValue\" : \"Mauro Administrator\" , \"commonAncestorValue\" : null , \"isMergeConflict\" : true , \"type\" : \"modification\" }, { \"fieldName\" : \"value\" , \"path\" : \"vf:Model Version Tree Folder$interestingBranch|md:test.com.testProperty@value\" , \"sourceValue\" : \"Oliver Freeman\" , \"targetValue\" : \"Peter Monks\" , \"commonAncestorValue\" : null , \"isMergeConflict\" : true , \"type\" : \"modification\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" } ] } The diffs collection will hold each change found between the two versioned folders and how they relate. All changes need to be manually organised into patches so that they can be applied to the target versioned folder. Then the following endpoint is used to commit: /api/versionedFolders/ {sourceId} /mergeInto/ {targetId} ?isLegacy=false Request body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"changeNotice\" : \"Change comment\" , \"deleteBranch\" : false , \"patch\" : { \"sourceId\" : \"f9a4e390-6259-4616-b725-d45524851a82\" , \"targetId\" : \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\" , \"label\" : \"Model Version Tree Folder\" , \"count\" : 4 , \"patches\" : [ { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"fieldName\" : \"description\" , \"path\" : \"vf:Model Version Tree Folder$interestingBranch@description\" , \"sourceValue\" : \"\" , \"targetValue\" : \"Test description\" , \"commonAncestorValue\" : \"\" , \"isMergeConflict\" : false , \"type\" : \"modification\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|md:v1Versioning.com.mdk1\" , \"isMergeConflict\" : false , \"isSourceDeletionAndTargetModification\" : false , \"type\" : \"deletion\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|dm:Test Data Model\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, ] } } The key point is to set the targetValue of every patch item to change - this value is what will be written to the target versioned folder when committing the merge.","title":"Versioned folder"},{"location":"rest-api/resources/versioned-folder/#description","text":"A Versioned Folder is a container type and can be represented as a mixture of a Folder and a Data Model , since it shares the functional characteristics of both - a container for holding other catalogue items, and a model that can be version controlled. Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"id\" : \"81d00110-40e3-4ec0-b279-6004fa1b9b52\" , \"domainType\" : \"VersionedFolder\" , \"label\" : \"Sample Versioned Folder\" , \"aliases\" : [ \"SVF\" ], \"description\" : \"First sample version controlled folder.\" , \"branchName\" : \"main\" , \"documentationVersion\" : \"2.0.0\" , \"modelVersion\" : \"2.0.0\" \"lastUpdated\" : \"2019-10-03T12:00:05.95Z\" , \"classifiers\" : [ { \"id\" : \"2fd9e8c7-4545-42f4-99a3-f93f14d35786\" , \"label\" : \"Samples\" , \"lastUpdated\" : \"2019-10-03T09:15:37.323Z\" } ], \"finalised\" : true , \"hasChildFolders\" : true } The fields are as follows: id (UUID): The unique identifier of this versioned folder. domainType (Type): The domain type of this catalogue object - always \"VersionedFolder\" in this case label (String): The human-readable identifier of this folder. The combination of label and documentationVersion are unique across the catalogue aliases (Set(String)): Any other names by which this versioned folder is known description (String): A long description of the versioned folder, and any important characteristics of the folder contents. This field may include HTML, or MarkDown. documentationVersion (Version): The version of the description of an underlying versioned folder. modelVersion (Version): The version of the folder of an underlying versioned folder. lastUpdated (DateTime): The date/time when this Versioned Folder was last modified classifiers (Set(Classifier)): The id , label and lastUpdated date of any classifiers used to tag or categorise this versioned folder (see classifiers ) finalised (Boolean): Whether this Versioned Folder has been 'finalised', or is in draft mode hasChildFolders (Boolean): Determines if this folder contains child folders. Endpoints which return multiple versioned folders typically include sufficient fields for generating links on the interface - a separate call to return the details of the Versioned Folder is usually required. As well as the endpoints listed below, a Versioned Folder is also a CatalogueItem, and so a Versioned Folder identifier can also be used as the parameter to any of those endpoints","title":"Description"},{"location":"rest-api/resources/versioned-folder/#getting-information","text":"The following endpoint returns a paginated list of all versioned folders readable by the current user: /api/versionedFolders This endpoint returns all the Versioned Folders within a particular folder; again, this result is paginated . /api/folders/ {folderId} /versionedFolders This endpoint provides the default information about a Versioned Folder, as per the JSON at the top of the page. /api/versionedFolders/ {id}","title":"Getting information"},{"location":"rest-api/resources/versioned-folder/#create-versioned-folder","text":"To create a new versioned folder from scratch, use the following post endpoint. Within the body of this call, you should include a folder identifier. /api/versionedFolder There are two ways of versioning Versioned Folders in the catalogue. To create an entirely new version of an existing folder, please use the following endpoint with no request body: /api/versionedFolders/ {id} /newBranchModelVersion The name must be different to the original folder. To create a new 'documentation version', use the following endpoint: /api/versionedFolders/ {id} /newDocumentationVersion By default, this will supersede the original versioned folder. It is also possible to branch and fork Versioned Folders to create drafts before finalising them. To create a new branch from an existing Versioned Folder: /api/versionedFolders/ {id} /newBranchModelVersion Request body (JSON) 1 2 3 { \"branchName\" : \"newBranch\" } To create a fork of the original versioned folder: /api/versionedFolders/ {id} /newForkModel Request body (JSON) 1 2 3 { \"label\" : \"newForkLabel\" }","title":"Create versioned folder"},{"location":"rest-api/resources/versioned-folder/#update-versioned-folder","text":"To edit the primitive properties of a versioned folder, use the following endpoint, with a body similar to the JSON described at the top of this page: /api/versionedFolders/ {id} To move a versioned folder from one folder to another, call the following, using the id fields for the versioned folder, and the new folder: /api/folders/ {folderId} /versionedFolders/ {id} Alternatively, you can call this equivalent endpoint: /api/versionedFolders/ {id} /folder/ {folderId}","title":"Update versioned folder"},{"location":"rest-api/resources/versioned-folder/#sharing","text":"To allow a versioned folder to be read by any authenticated user of the system, use the following endpoint: /api/versionedFolders/ {id} /readByAuthenticated ... and to remove this flag, use the following: /api/versionedFolders/ {id} /readByAuthenticated Similarly, to allow the versioned folder to be publicly readable - ie. readable by any unauthenticated user of the system, use the following endpoint: /api/versionedFolders/ {id} /readByEveryone ... and the following to remove this flag: /api/versionedFolders/ {id} /readByEveryone","title":"Sharing"},{"location":"rest-api/resources/versioned-folder/#delete-versioned-folder","text":"To delete a versioned folder, use the following endpoint. The permanent parameter is a boolean value that controls whether a 'hard' or 'soft' delete is used if the user is an administrator. /api/versionedFolders/ {id} ?permanent= {true/false} An administrator is able to restore a 'soft' deleted code set using the following endpoint: /api/admin/versionedFolders/ {id} /undoSoftDelete","title":"Delete versioned folder"},{"location":"rest-api/resources/versioned-folder/#finalise-a-versioned-folder","text":"To finalise a versioned folder means to lock it to a particular version and make it read-only; only new versions can be created to make further modifications after that point. This also applies to the child contents of this versioned folder too, all child items will share the same finalised state and version number assigned. Use this endpoint with a similar payloads described below to finalise a versioned folder. /api/versionedFolders/ {id} /finalise To automatically let Mauro choose the next version number, set the versionChange property to either 'Major' , 'Minor' or 'Patch' . Request body (JSON) 1 2 3 { \"versionChange\" : \"Major\" | \"Minor\" | \"Patch\" } Mauro uses Semantic Versioning rules to determine the next appropriate version number based on the versionChange value provided. To optionally choose your own version number, provide this payload. If versionChange is 'Custom' , then version must also be provided. Request body (JSON) 1 2 3 4 { \"versionChange\" : \"Custom\" , \"version\" : \"1.2.3.4\" } In all cases you may also supply an optional tag name to assign with the finalised version to help provide more context, as follows: Request body (JSON) 1 2 3 4 { \"versionChange\" : \"Major\" | \"Minor\" | \"Patch\" , \"versionTag\" : \"My first version\" }","title":"Finalise a versioned folder"},{"location":"rest-api/resources/versioned-folder/#merging-versioned-folders","text":"If creating branches of versioned folders, it is possible to merge the data values from one versioned folder to another. The first step is to calculate the differences between two versioned folders, as follows: /api/versionedFolders/ {sourceId} /mergeDiff/ {targetId} ?isLegacy=false Response body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 { \"sourceId\" : \"f9a4e390-6259-4616-b725-d45524851a82\" , \"targetId\" : \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\" , \"path\" : \"vf:Model Version Tree Folder$interestingBranch\" , \"label\" : \"Model Version Tree Folder\" , \"count\" : 6 , \"diffs\" : [ { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|ann:Test Comment\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|dm:Test Data Model\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|md:v1Versioning.com.mdk1\" , \"isMergeConflict\" : false , \"isSourceDeletionAndTargetModification\" : false , \"type\" : \"deletion\" }, { \"fieldName\" : \"value\" , \"path\" : \"vf:Model Version Tree Folder$interestingBranch|md:org.datacite.creator@value\" , \"sourceValue\" : \"Peter Monks\" , \"targetValue\" : \"Mauro Administrator\" , \"commonAncestorValue\" : null , \"isMergeConflict\" : true , \"type\" : \"modification\" }, { \"fieldName\" : \"value\" , \"path\" : \"vf:Model Version Tree Folder$interestingBranch|md:test.com.testProperty@value\" , \"sourceValue\" : \"Oliver Freeman\" , \"targetValue\" : \"Peter Monks\" , \"commonAncestorValue\" : null , \"isMergeConflict\" : true , \"type\" : \"modification\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" } ] } The diffs collection will hold each change found between the two versioned folders and how they relate. All changes need to be manually organised into patches so that they can be applied to the target versioned folder. Then the following endpoint is used to commit: /api/versionedFolders/ {sourceId} /mergeInto/ {targetId} ?isLegacy=false Request body (JSON) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"changeNotice\" : \"Change comment\" , \"deleteBranch\" : false , \"patch\" : { \"sourceId\" : \"f9a4e390-6259-4616-b725-d45524851a82\" , \"targetId\" : \"f5841f3f-7a63-4aa2-9c72-a64305d44dcf\" , \"label\" : \"Model Version Tree Folder\" , \"count\" : 4 , \"patches\" : [ { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|ru:Bootstrapped versioning V2Model Rule|rr:sql\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, { \"fieldName\" : \"description\" , \"path\" : \"vf:Model Version Tree Folder$interestingBranch@description\" , \"sourceValue\" : \"\" , \"targetValue\" : \"Test description\" , \"commonAncestorValue\" : \"\" , \"isMergeConflict\" : false , \"type\" : \"modification\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|md:v1Versioning.com.mdk1\" , \"isMergeConflict\" : false , \"isSourceDeletionAndTargetModification\" : false , \"type\" : \"deletion\" }, { \"path\" : \"vf:Model Version Tree Folder$interestingBranch|dm:Test Data Model\" , \"isMergeConflict\" : false , \"isSourceModificationAndTargetDeletion\" : false , \"type\" : \"creation\" }, ] } } The key point is to set the targetValue of every patch item to change - this value is what will be written to the target versioned folder when committing the merge.","title":"Merging versioned folders"},{"location":"tutorials/introduction/","text":"These tutorials are intended to provide more information about particular aspects of functionality. Unlike the User Guides , which explain how to achieve things in Mauro Data Mapper , these Tutorials will explain why such functionality is available and how it is intended to be used. Our current Tutorials: Semantic Links In this tutorial we explain why Semantic links are important, and how they can be used to assist re-use of existing data. Properties and Profiles This tutorial explains how to add custom information to models and model components using properties, and how to group and constrain properties using profiles.","title":"Introduction"},{"location":"tutorials/properties-profiles/","text":"Overview \u00b6 Mauro provides a minimal set of fields that can be stored against any item in the catalogue. These are typically a Label , some other names ( Aliases ) and a description. It is expected that for any particular use of Mauro, there may be additional fields that users would want to store against a Data Model , or component of a model. To facilitate this, Mauro allows every catalogue item to be extensible , by allowing any arbitrary fields to be added as the properties of that object. Example 1 \u00b6 As a first example, consider the use of Mauro as a repository for research datasets and their discovery. For each dataset, the Data Model corresponding to that dataset should have the following information stored against it: Responsible organisation name Dataset collection date Size of complete dataset in Megabytes Link to data request form Furthermore, each Data Element within that Data Model should record the following fields: A boolean indicating whether the field contains identifiable information A score denoting the completeness of the field - the number of non-null values Example 2 \u00b6 Consider the use of Mauro as a data asset register. Each model within Mauro represents a data asset held by the organisation. For each model (each data asset held within the organisation), the following fields should be recorded: Department name Contact email address Server IP address Backup status Properties \u00b6 Each Mauro item can hold a number of properties , stored as a pair of key and value fields. For example, a Data Model property may have the key : \u201cResponsible Organisation Name\u201d and the value : \u201cUniversity of Oxford\u201d. Another Data Model may use the same key and a different value such as \u201cUniversity of Manchester\u201d. Each property also has a namespace which is a field that can disambiguate the use of the same key for different uses. For instance, the examples above may choose to use the key location for their intended use case without clashing. Namespaces typically take the form of a url (usually owned or controlled by, the use case). More information can be found here: Namespace on Wikipedia . Mauro enforces a basic constraint upon properties where for any given catalogue item, there must be no two attached properties with the same namespace and key. There are no constraints on values: each is stored internally as a string and can be used to store JSON or XML for more complex values. Profiles \u00b6 A profile is a group of related properties, typically sharing the same namespace. A profile allows these properties to be grouped into sections and for each defined key may add a description, a default value and a validation constraint. The user interface is able to take known profiles and present a cleaner interface for data entry against them. This involves hiding the namespace, grouping keys, providing the description for each field and providing type-specific data entry controls such as date pickers. Validation can be performed for all the fields on submission. As part of their specification, profiles can determine which types of catalogue item can use them. For example, you may have a profile which should only be stored against Data Models , or one which may be used for Data Classes and Data Elements only. Static vs Dynamic \u00b6 Profiles in Mauro can be defined in one of two ways: static or dynamic . A static profile is defined as part of a plugin, typically either stand-alone, bundled as part of an importer or exporter, or with additional functionality / REST endpoints for processing the values. These profiles are usually defined through a JSON file, and plenty of examples can be found in the shared plugin repositories , such as in the SQL importers and the defined profile plugins. Alternatively, profiles can be defined dynamically as Data Models within Mauro. Any model which uses the Profile Specification Profile is recognised as a profile. Each Data Class defined at the top-level of the profile specification model represents a profile section and each Data Element within those Data Classes defines the property keys themselves. As with any normal model, the Profile Specification Model can be finalised and versioned, and it is recommended that such models are finalised before use. Find out how to create Profile Specification models in our 'Dynamic Profiles' user guide . Static profiles are useful when additional functionality is to be tied to the particular property values, so it is important that keys are not changed independently. Dynamic profiles are much easier for an end-user to create, without the need to program a new Mauro Plugin. They are also easier to re-use and extend. However, their definition must be carefully controlled to ensure the conceptual integrity of the data stored against them.","title":"Properties and profiles"},{"location":"tutorials/properties-profiles/#overview","text":"Mauro provides a minimal set of fields that can be stored against any item in the catalogue. These are typically a Label , some other names ( Aliases ) and a description. It is expected that for any particular use of Mauro, there may be additional fields that users would want to store against a Data Model , or component of a model. To facilitate this, Mauro allows every catalogue item to be extensible , by allowing any arbitrary fields to be added as the properties of that object.","title":"Overview"},{"location":"tutorials/properties-profiles/#example-1","text":"As a first example, consider the use of Mauro as a repository for research datasets and their discovery. For each dataset, the Data Model corresponding to that dataset should have the following information stored against it: Responsible organisation name Dataset collection date Size of complete dataset in Megabytes Link to data request form Furthermore, each Data Element within that Data Model should record the following fields: A boolean indicating whether the field contains identifiable information A score denoting the completeness of the field - the number of non-null values","title":"Example 1"},{"location":"tutorials/properties-profiles/#example-2","text":"Consider the use of Mauro as a data asset register. Each model within Mauro represents a data asset held by the organisation. For each model (each data asset held within the organisation), the following fields should be recorded: Department name Contact email address Server IP address Backup status","title":"Example 2"},{"location":"tutorials/properties-profiles/#properties","text":"Each Mauro item can hold a number of properties , stored as a pair of key and value fields. For example, a Data Model property may have the key : \u201cResponsible Organisation Name\u201d and the value : \u201cUniversity of Oxford\u201d. Another Data Model may use the same key and a different value such as \u201cUniversity of Manchester\u201d. Each property also has a namespace which is a field that can disambiguate the use of the same key for different uses. For instance, the examples above may choose to use the key location for their intended use case without clashing. Namespaces typically take the form of a url (usually owned or controlled by, the use case). More information can be found here: Namespace on Wikipedia . Mauro enforces a basic constraint upon properties where for any given catalogue item, there must be no two attached properties with the same namespace and key. There are no constraints on values: each is stored internally as a string and can be used to store JSON or XML for more complex values.","title":"Properties"},{"location":"tutorials/properties-profiles/#profiles","text":"A profile is a group of related properties, typically sharing the same namespace. A profile allows these properties to be grouped into sections and for each defined key may add a description, a default value and a validation constraint. The user interface is able to take known profiles and present a cleaner interface for data entry against them. This involves hiding the namespace, grouping keys, providing the description for each field and providing type-specific data entry controls such as date pickers. Validation can be performed for all the fields on submission. As part of their specification, profiles can determine which types of catalogue item can use them. For example, you may have a profile which should only be stored against Data Models , or one which may be used for Data Classes and Data Elements only.","title":"Profiles"},{"location":"tutorials/properties-profiles/#static-vs-dynamic","text":"Profiles in Mauro can be defined in one of two ways: static or dynamic . A static profile is defined as part of a plugin, typically either stand-alone, bundled as part of an importer or exporter, or with additional functionality / REST endpoints for processing the values. These profiles are usually defined through a JSON file, and plenty of examples can be found in the shared plugin repositories , such as in the SQL importers and the defined profile plugins. Alternatively, profiles can be defined dynamically as Data Models within Mauro. Any model which uses the Profile Specification Profile is recognised as a profile. Each Data Class defined at the top-level of the profile specification model represents a profile section and each Data Element within those Data Classes defines the property keys themselves. As with any normal model, the Profile Specification Model can be finalised and versioned, and it is recommended that such models are finalised before use. Find out how to create Profile Specification models in our 'Dynamic Profiles' user guide . Static profiles are useful when additional functionality is to be tied to the particular property values, so it is important that keys are not changed independently. Dynamic profiles are much easier for an end-user to create, without the need to program a new Mauro Plugin. They are also easier to re-use and extend. However, their definition must be carefully controlled to ensure the conceptual integrity of the data stored against them.","title":"Static vs Dynamic"},{"location":"tutorials/semantic-links/","text":"Introduction \u00b6 When defining the meaning of data, it can be helpful to point to definitions elsewhere that are already known or understood. For example, we might like to consider that the 'Date of Birth' field in one dataset might mean exactly the same as 'Birth Date' in another. In practice, however, we rarely find that two Data Elements share exactly the same context. Measurements might be taken using different apparatus; answers to a question on a form may differ depending on how the question is phrased or presented; the timing or ordering of data collection may alter the possible values. To provide a more practical approach, in Mauro Data Mapper we can link two definitions to indicate that one refines the other: it says everything that the other definition says, and possibly more. In the general case, this allows us to define abstract definitions of data with minimal context (for example as a dictionary or a data specification) and relate more concrete definitions (for example the design of a data collection form or the description of a data asset). We may go further and for any definitions A and B, indicate that both A refines B and B refines A. The two links together imply that the two fields really are identical and share the exact same context. In practice, we've found this to be an overly strong statement and use it very rarely. Example \u00b6 As an example, consider the three definitions given in the diagram below: Here, we assume that the name of the Data Element which could be 'Word Count' and the Data Type , which could be 'Positive Integer' is the same in each case, and we are focusing simply on the explanatory text. The refinement arrows represent assertions that the definitions to the left and right are both refinements of the definition at the centre. In this example, the description 'number of words in document' defines a more abstract notion; the descriptions 'number of words in document according to Microsoft' and 'number of words in document according to Apple' provide extra context about the means of calculation. Such assertions cannot be derived automatically from the explanatory text. These explanations may be subjective and will only give a partial account of the context. It may be that several people with expertise in how the data is captured, recorded and analysed may all need to be consulted to provide an accurate assertion about the relationship between two descriptions. Note that there may not be any link defined between two more concrete definitions. In this example, there is no direct relationship between the Microsoft and Apple interpretations of the value - only that they both share some common abstract interpretation. Of course there may be some mapping of values, or conversion algorithm, which allows data collected according to one definition to be converted into a form matching another definition. In this case, perhaps opening the document on another computer and re-running the word count. Simple mappings, for example when converting units of measurement, can be uncontroversial, but in general they will also be subjective or suitable only for a particular purpose. Interpretation \u00b6 There are two immediate practical applications of linking information. Considering the example above, first suppose that some data assets exist for each of the two concrete definitions above - some documents whose word count has been calculated by either Microsoft of Apple software. If the analysis only requires a value for the 'number of words in a document' , then both types of data will be equally applicable and can be included in the analysis. If, however, the requirement is for counts according to Microsoft, then those documents whose word count has been calculated by Apple may not be suitable. Another application might be in the provision of data according to some specification. If a data specification requires values according to our abstract description: 'the number of words in document' and if we already have values for those documents as calculated by Microsoft software, then those values are suitable to provide. If, however, the specification is more concrete, and requires those word counts to have been determined by Microsoft software, then any word counts calculated by other software, or, more importantly, any document word count whose provenance is not known , will not be suitable to provide. Linking in Mauro \u00b6 In Mauro Data Mapper , Semantic links can be recorded against any model components, but are typically used between two Data Elements in different Data Models . Or between an enumeration value in a model and a term in a Terminology . Other mappings, for example to indicate refinement between two Data Classes , may be much harder to interpret and may only be of use in particular circumstances. As described above, the assertion of Semantic links may require domain knowledge and cannot be automatically inferred from the text of a description. In Mauro Data Mapper , links are not created automatically during ingest of models, but may be manually asserted between items individually. However, Mauro includes a tool which will make suggestions based on the description text of the fields and also the name of the field, the name of the Data Type and containing Data Class , and any enumeration values. The user interface allows users to select the best match, or choose from other alternatives ranked by \u2018closest match\u2019 . Although this doesn't remove the need for a human to make the final decision, it can save time searching for the correct item within a target Data Model . Does not refine \u00b6 As our models may represent incomplete information about the artefacts they represent and the semantic relationships between them, we cannot infer that no refinement exists simply because there is no refines link present. To record the assertion that no refinement exists, we can use the ** does not refine **link. As a further example, consider the three attribute definitions given in the diagram below. Here, we have a more specific definition at the centre of the diagram: \u2018number of words in a document ignoring hyphens\u2019 . We have also an assertion that this refines the definition with the explanatory text \u2018number of words in a document\u2019 . If we accept this assertion, then any data collected against the new definition can be used in any situation in which the original definition was accepted. We have also an assertion that the definition \u2018number of words in a document according to Microsoft\u2019 is not a refinement of this new, more specific definition. The \u2018word count\u2019 feature in Microsoft's Word application treats hyphenated phrases as single words: it does not ignore hyphens. In contrast, the same feature in Apple's Pages application does ignore hyphens, treating them as if they were spaces. For example, Word will count the phrase \u2018strongly-connected\u2019 as one word, whereas Pages will count it as two words. In general, specifying 'does not refine' as a universal statement - suggesting that there are no circumstances where item A may be used according to definition B - is a strong statement whose use will be limited. However, there can be value in disambiguating or asserting a distinction between two similarly-defined items whose descriptions may otherwise cause confusion. To find out how to add a Semantic link between two descriptions of data, see our Semantic links user guide","title":"Semantic Links"},{"location":"tutorials/semantic-links/#introduction","text":"When defining the meaning of data, it can be helpful to point to definitions elsewhere that are already known or understood. For example, we might like to consider that the 'Date of Birth' field in one dataset might mean exactly the same as 'Birth Date' in another. In practice, however, we rarely find that two Data Elements share exactly the same context. Measurements might be taken using different apparatus; answers to a question on a form may differ depending on how the question is phrased or presented; the timing or ordering of data collection may alter the possible values. To provide a more practical approach, in Mauro Data Mapper we can link two definitions to indicate that one refines the other: it says everything that the other definition says, and possibly more. In the general case, this allows us to define abstract definitions of data with minimal context (for example as a dictionary or a data specification) and relate more concrete definitions (for example the design of a data collection form or the description of a data asset). We may go further and for any definitions A and B, indicate that both A refines B and B refines A. The two links together imply that the two fields really are identical and share the exact same context. In practice, we've found this to be an overly strong statement and use it very rarely.","title":"Introduction"},{"location":"tutorials/semantic-links/#example","text":"As an example, consider the three definitions given in the diagram below: Here, we assume that the name of the Data Element which could be 'Word Count' and the Data Type , which could be 'Positive Integer' is the same in each case, and we are focusing simply on the explanatory text. The refinement arrows represent assertions that the definitions to the left and right are both refinements of the definition at the centre. In this example, the description 'number of words in document' defines a more abstract notion; the descriptions 'number of words in document according to Microsoft' and 'number of words in document according to Apple' provide extra context about the means of calculation. Such assertions cannot be derived automatically from the explanatory text. These explanations may be subjective and will only give a partial account of the context. It may be that several people with expertise in how the data is captured, recorded and analysed may all need to be consulted to provide an accurate assertion about the relationship between two descriptions. Note that there may not be any link defined between two more concrete definitions. In this example, there is no direct relationship between the Microsoft and Apple interpretations of the value - only that they both share some common abstract interpretation. Of course there may be some mapping of values, or conversion algorithm, which allows data collected according to one definition to be converted into a form matching another definition. In this case, perhaps opening the document on another computer and re-running the word count. Simple mappings, for example when converting units of measurement, can be uncontroversial, but in general they will also be subjective or suitable only for a particular purpose.","title":"Example"},{"location":"tutorials/semantic-links/#interpretation","text":"There are two immediate practical applications of linking information. Considering the example above, first suppose that some data assets exist for each of the two concrete definitions above - some documents whose word count has been calculated by either Microsoft of Apple software. If the analysis only requires a value for the 'number of words in a document' , then both types of data will be equally applicable and can be included in the analysis. If, however, the requirement is for counts according to Microsoft, then those documents whose word count has been calculated by Apple may not be suitable. Another application might be in the provision of data according to some specification. If a data specification requires values according to our abstract description: 'the number of words in document' and if we already have values for those documents as calculated by Microsoft software, then those values are suitable to provide. If, however, the specification is more concrete, and requires those word counts to have been determined by Microsoft software, then any word counts calculated by other software, or, more importantly, any document word count whose provenance is not known , will not be suitable to provide.","title":"Interpretation"},{"location":"tutorials/semantic-links/#linking-in-mauro","text":"In Mauro Data Mapper , Semantic links can be recorded against any model components, but are typically used between two Data Elements in different Data Models . Or between an enumeration value in a model and a term in a Terminology . Other mappings, for example to indicate refinement between two Data Classes , may be much harder to interpret and may only be of use in particular circumstances. As described above, the assertion of Semantic links may require domain knowledge and cannot be automatically inferred from the text of a description. In Mauro Data Mapper , links are not created automatically during ingest of models, but may be manually asserted between items individually. However, Mauro includes a tool which will make suggestions based on the description text of the fields and also the name of the field, the name of the Data Type and containing Data Class , and any enumeration values. The user interface allows users to select the best match, or choose from other alternatives ranked by \u2018closest match\u2019 . Although this doesn't remove the need for a human to make the final decision, it can save time searching for the correct item within a target Data Model .","title":"Linking in Mauro"},{"location":"tutorials/semantic-links/#does-not-refine","text":"As our models may represent incomplete information about the artefacts they represent and the semantic relationships between them, we cannot infer that no refinement exists simply because there is no refines link present. To record the assertion that no refinement exists, we can use the ** does not refine **link. As a further example, consider the three attribute definitions given in the diagram below. Here, we have a more specific definition at the centre of the diagram: \u2018number of words in a document ignoring hyphens\u2019 . We have also an assertion that this refines the definition with the explanatory text \u2018number of words in a document\u2019 . If we accept this assertion, then any data collected against the new definition can be used in any situation in which the original definition was accepted. We have also an assertion that the definition \u2018number of words in a document according to Microsoft\u2019 is not a refinement of this new, more specific definition. The \u2018word count\u2019 feature in Microsoft's Word application treats hyphenated phrases as single words: it does not ignore hyphens. In contrast, the same feature in Apple's Pages application does ignore hyphens, treating them as if they were spaces. For example, Word will count the phrase \u2018strongly-connected\u2019 as one word, whereas Pages will count it as two words. In general, specifying 'does not refine' as a universal statement - suggesting that there are no circumstances where item A may be used according to definition B - is a strong statement whose use will be limited. However, there can be value in disambiguating or asserting a distinction between two similarly-defined items whose descriptions may otherwise cause confusion. To find out how to add a Semantic link between two descriptions of data, see our Semantic links user guide","title":"Does not refine"},{"location":"tutorials/document-assets/","text":"This document explains how to create a standard description for a health dataset using the metadata catalogue. Such a description will come in two parts: a description of the dataset as a whole descriptions of the individual data items within the dataset, and of the structural relationships between them There is no single, applicable standard for the first part. For the moment, pending the development of the gateway interface, we require only a minimal set of properties, sufficient to uniquely identify the dataset in question. A slightly longer list will be required once the gateway specification has been agreed. For the second part, we are able to adapt and extend an existing international standard for metadata registration: ISO/IEC 11179. We specify a list of properties that should be recorded for each item, and each relationship; this list will remain unchanged, although the information provided may be updated over time. If the dataset is held in a relational datastore, then we may be able to determine the name and type of each data item, and the structural relationships between them, automatically. We will not, however, be able to determine an adequate, human-readable explanation of each item ; this will need to be entered by hand and/or carefully extracted from existing, electronic documentation. Describing the dataset \u00b6 Each of the \u2018top level\u2019 descriptions in the catalogue is called a model . Models are stored in a familiar folder (or directory) structure . Folders may have sub-folders. The folder tree is shown to the left of the screen. To create a new model of a health dataset, first choose the folder in which you wish to store it. To create a new folder at the top-level, click the \u2018plus\u2019 button at the top of the tree view. To create a sub-folder of an existing folder, right-click the folder and choose \u2018Add folder\u2019. When creating a new folder, you must give it a name , and should enter a short description describing the purpose of the folder. To create a new model, right-click upon the folder in which you want it to appear, and choose \u2018Add Data Model\u2019: You will be presented with a short form to enter the details of your new model: Choose a label for your model. This should be enough to uniquely identify the dataset that you are describing. You will be able to add other names or labels later on. The author field should be used to record the names of those creating and maintaining this data model (not the dataset itself). The organisation field should be used to record the name of the organisation holding the dataset. In the description field, enter a short (e.g. 2 paragraphs), human-readable description of the data stored within that dataset, and any important characteristics of the data. For type , choose \u2018data asset\u2019. Once you have entered all mandatory fields, labelled with \u2018 * \u2019, you can click \u2018 Next \u2019 to choose a default set of data types to be imported into your model. For example, you may choose to import the default datatypes of a MS SQL Server database. If you are unsure at this stage, you can leave the field blank - you can always import these later on. Click \u2018 Submit \u2019 to finish creating the new data model. This will take you to the \u2018overview page\u2019 of your new model. You are welcome to record further characteristics of the dataset - this could help the gateway providers when the come to design their interface . If you wish to do this, Click the \u2018 Properties \u2019 tab on the panel below, and choose the \u2018 + \u2019 button to add each new property. namespace : This will be used to select the correct profile / property list. key : This is the property name - e.g. \u2018contact email\u2019. You should add a property for each of the names listed below, but may add further properties if you wish. value : This is the value of the given property - e.g. \u2018 enquiries-mydataset@hub.org \u2019. Describing the data items \u00b6 Data items are created and managed within data classes. If a dataset is managed as a collection of tables, then you may wish to create a class for each table. This is the default approach. Alternatively, you may wish to create a set of classes to provide a more abstract account of the data set - grouping and presenting the data items in a way that is quite different from the way in which they are stored and managed. To create a new class, select a data model from the model tree, choose the \u2018DataClasses\u2019 tab, and click the \u2018+\u2019 button: In documenting an existing data set, you will be creating rather than copying classes, so choose the first option and click \u2018 Next \u2019. You should then choose a name or label for your class. Again, you will be able to add further names later, as aliases, if you wish. The description of a class may explain what kind of data items are grouped together here; alternatively, it may explain some common context for the items it contains, to avoid the need to include that information in the description of each individual item. The multiplicity values specify the number of instances of that class that may appear in an instance of the model. For example, if a class were to correspond to a table in a relational database, the multiplicity values would be the minimum, and the maximum, number of rows allowed in the table. In a model of a dataset, there is usually no need to specify the multiplicity of a class. Once all mandatory fields have been completed, you may click \u2018 Submit \u2019 to create the new data class. This will take you to the page for the newly -created class. You can click the link back to the parent data model to continue adding further classes of data as necessary. You may also choose to add further \u2018child\u2019 classes to this class: choose the 'Content\u2019 tab on the DataClass page, and click the \u2018+\u2019 symbol to add a new data class. Data items are represented as \u2018data elements\u2019 within the model. To start adding data elements to a class, visit its DataClass page, and click the \u2018+\u2019 button on the \u2018Content\u2019 tab. This will give you two options - to create a new contained \u2018child\u2019 class, as above, or to create a new data element - choose the second option. As with creating a new class, this will give you the option to copy an existing data element from elsewhere in the model (or from another model which you have read access to). In creating a model of an existing dataset, you will almost certainly want to create a new data element. The next form that appears will ask for the details of the new data element: For label , enter the preferred name of the data item. This could be the name of a column in a relational database, for example. You can add further names as aliases later. In description , you should describe the data item and the values it may take. This may include information about provenance - the context of collection, and any subsequent processing, but should also say something about intended interpretation or possible use. This description should explain the data point in terms that might be useful to prospective data users. The multiplicity of a data element specifies the number of values that it may take at the same time. For example, the number of \u2018date of death \u2019 items contained within a \u2018patient\u2019 record might be at least 0, and at most 1. The min field should be a positive whole number; the max field should be a positive whole number no smaller than min, or may be \u2018 * \u2019 to indicate that no maximum number is set. If you are unsure, this field may be left blank. The data type field describes the values that this data item may take. A data type may be either: primitive : for example a String, an Integer, or a Date. enumerated : chosen from a given list of values, which may be described using codes or free-text. For example M = Male, F = Female, U = Unknown reference-valued : a reference to another class of data. For example, the \u2018Registered GP\u2019 data element of a \u2018Patient\u2019 may refer to a separate class of \u2018GP\u2019 (containing name, surgery, address, etc) When creating a new data element, you can choose to use an existing data type that belongs to the data model in question, or you can create a new data type (by providing its name, the list of values, or a class to reference, respectively). From the data model page it is also possible to import data types from another model, or some pre-defined sets of data types (such as those found in MS SQLServer databases, for example). Click \u2018 Submit \u2019 to create the new Data Element. Repeat this process to add the other data elements for each class. Entering this information offline \u00b6 While creating a model of an existing dataset, you may find it more convenient to enter and share the information above using an Excel spreadsheet - and then upload the contents of that spreadsheet into the catalogue. You can create more than one model using the same spreadsheet. A blank spreadsheet should be included with this document, together with an example of a completed spreadsheet, describing the Diagnostic Imaging Dataset. The first sheet o the spreadsheet (which must be called DataModels) should introduce one or more data models. Each subsequent sheet should describe the contents of one of these data models. In the DataModels sheet, there should be one row for each data model described, and the following columns can be completed: SHEET_KEY : the name of the sheet (in this spreadsheet) describing the contents of the data model Name : the name or label of the data model (as explained above) Description : a brief description of the dataset Author : the author of this data model of the dataset Organisation : the organisation holding the dataset Type : this should be \u2018Data Asset\u2019 In the blank spreadsheet supplied, there is a KEY_1 sheet with the column headings required for each of the subsequent sheets. You can rename or copy this sheet. Whatever name is chosen should be included in the list of data models presented in the opening \u2018DataModels\u2019 sheet (or the contents will not be uploaded). The following columns should be completed: DataClass Path (essential): this should be the path from the top level of the model to the data class described in the current row, or the class containing the item described in the current row; for a top-level class, it will be simply the class name; for child classes, it will be a list of class names, using \u201c|\u201d as a delimiter. DataElement Name (essential for data elements): if the row is describing a data element, rather than a data class, then the name of the element should be inserted here Description : an explanation of the intended interpretation (and perhaps also the context of collection or provenance) of the DataClass or DataElement Minimum Multiplicity (may be left blank): the minimum number of instances of the class or element, usually 0 or 1 Maximum Multiplicity (may be left blank): the maximum number of instances, with -1 (rather than *) used to indicate that there is no upper bound DataType Name (essential for data elements): the name of the data type of the data element being described DataType Description (needed only for the first time that the data type in question is mentioned): the description of the data type Reference to DataClass Path : if the data type is another class (if the data element is a reference to an instance of another class) then insert the path to that class here Enumeration Key and Enumeration Value : if the data type is an enumeration, then you may add several pairs of entries in these two columns, one for each key-value pair in the enumeration; in each case, the key is the text or string that may appear in a column of the dataset, and the value is its expansion or explanatory text. Once you have done this, you will need to select the corresponding cells in the DataClass Path column - the first column - and merge them. You may wish to merge the corresponding cells in the other columns as well, for a clearer presentation of the information: for example, A completed spreadsheet can be imported into the catalogue using the \u2018import\u2019 button, located on the toolbar at the top of the screen: Select the Excel importer. Having chosen to import from a spreadsheet, you will have the opportunity to provide some more information: Folder : this is the folder in which the newly created model(s) will reside. The drop-down menu lets you choose a folder which you have access to Finalised : we recommend that you keep models as \u2018draft\u2019 until the gateway presentation of model descriptions has been decided Import : as new documentation version check this option if you intend to replace the current (catalogue) version of an existing model description File : Choose the spreadsheet file for upload. You may drag/drop a file from your file browser into the box here. Press \u2018 Submit \u2019 to import the model. Excel files can be safely used to \"round-trip\" data model descriptions. You can export a model from the catalogue in spreadsheet form, edit the spreadsheet, and import the new version of the spreadsheet to produce an updated version of the model - in this case, you need to select \u201cNew Documentation Version\u201d. Extracting metadata from a relational database \u00b6 If the dataset to be described is held in a relational database, and you have direct access to that database, you can also use the catalogue to extract basic metadata - table names, column names, types, and structural relationships - from the database itself. The effort of manual data entry can then be focussed upon producing adequate accounts of the account of the intended interpretation of each data element. Clicking on the \u2018import\u2019 button as above, you can select an importer for most types of relational database. Having chosen to import from a relational database, the next set of options allow you to configure the import. Fields marked with a \u2018*\u2019 are mandatory. The fields should be completed as follows: Folder : this is the folder in which the newly created model(s) will reside. The drop-down menu lets you choose a folder which you have access to Data Model Name : this will be the name of the new data model. If no name is specified, the name of the database will be used. Finalised : if the newly-imported model is immediately \u2018finalised\u2019, you will no-longer be able to make changes to it. We recommend that you keep models as \u2018draft\u2019; otherwise if you wish to edit the descriptions you will need to create a new version. Import as new documentation version : this determines how the newly imported model supersedes any existing model with the same name. We recommend you check this option if you intend to overwrite an older version of the model. Database Name(s) : please enter the database name. If you\u2019d like to import multiple databases in one go, please enter the list of names , separated by a comma. Database Host : this is the IP address, or the server name, of the machine that the database is installed on. Username : this is the username which is used to connect to the database. Ideally, a user with read-only access will be used. This is not stored within the Metadata Catalogue application. Password : the password used by the user to connect to the database. This is not stored within the Metadata Catalogue application. Database port : the port which the database is communicating on - eg. 1433 for MS SQL Server. If no port is specified, the default port for the database will be used (MS SQL Server: 1433, Postgres: 5432, OracleDB: 1521) SSL : whether the database requires an encrypted connection (usually false). Other features \u00b6 The catalogue toolkit has a range of related functions for creating and updating models of datasets and data standards. As the features of the gateway interface are determined, we will update this documentation to address any additional metadata requirements, and to describe any additional functions that have become relevant. We will update it also to describe any improvements made to the functions described above.","title":"Index"},{"location":"tutorials/document-assets/#describing-the-dataset","text":"Each of the \u2018top level\u2019 descriptions in the catalogue is called a model . Models are stored in a familiar folder (or directory) structure . Folders may have sub-folders. The folder tree is shown to the left of the screen. To create a new model of a health dataset, first choose the folder in which you wish to store it. To create a new folder at the top-level, click the \u2018plus\u2019 button at the top of the tree view. To create a sub-folder of an existing folder, right-click the folder and choose \u2018Add folder\u2019. When creating a new folder, you must give it a name , and should enter a short description describing the purpose of the folder. To create a new model, right-click upon the folder in which you want it to appear, and choose \u2018Add Data Model\u2019: You will be presented with a short form to enter the details of your new model: Choose a label for your model. This should be enough to uniquely identify the dataset that you are describing. You will be able to add other names or labels later on. The author field should be used to record the names of those creating and maintaining this data model (not the dataset itself). The organisation field should be used to record the name of the organisation holding the dataset. In the description field, enter a short (e.g. 2 paragraphs), human-readable description of the data stored within that dataset, and any important characteristics of the data. For type , choose \u2018data asset\u2019. Once you have entered all mandatory fields, labelled with \u2018 * \u2019, you can click \u2018 Next \u2019 to choose a default set of data types to be imported into your model. For example, you may choose to import the default datatypes of a MS SQL Server database. If you are unsure at this stage, you can leave the field blank - you can always import these later on. Click \u2018 Submit \u2019 to finish creating the new data model. This will take you to the \u2018overview page\u2019 of your new model. You are welcome to record further characteristics of the dataset - this could help the gateway providers when the come to design their interface . If you wish to do this, Click the \u2018 Properties \u2019 tab on the panel below, and choose the \u2018 + \u2019 button to add each new property. namespace : This will be used to select the correct profile / property list. key : This is the property name - e.g. \u2018contact email\u2019. You should add a property for each of the names listed below, but may add further properties if you wish. value : This is the value of the given property - e.g. \u2018 enquiries-mydataset@hub.org \u2019.","title":"Describing the dataset"},{"location":"tutorials/document-assets/#describing-the-data-items","text":"Data items are created and managed within data classes. If a dataset is managed as a collection of tables, then you may wish to create a class for each table. This is the default approach. Alternatively, you may wish to create a set of classes to provide a more abstract account of the data set - grouping and presenting the data items in a way that is quite different from the way in which they are stored and managed. To create a new class, select a data model from the model tree, choose the \u2018DataClasses\u2019 tab, and click the \u2018+\u2019 button: In documenting an existing data set, you will be creating rather than copying classes, so choose the first option and click \u2018 Next \u2019. You should then choose a name or label for your class. Again, you will be able to add further names later, as aliases, if you wish. The description of a class may explain what kind of data items are grouped together here; alternatively, it may explain some common context for the items it contains, to avoid the need to include that information in the description of each individual item. The multiplicity values specify the number of instances of that class that may appear in an instance of the model. For example, if a class were to correspond to a table in a relational database, the multiplicity values would be the minimum, and the maximum, number of rows allowed in the table. In a model of a dataset, there is usually no need to specify the multiplicity of a class. Once all mandatory fields have been completed, you may click \u2018 Submit \u2019 to create the new data class. This will take you to the page for the newly -created class. You can click the link back to the parent data model to continue adding further classes of data as necessary. You may also choose to add further \u2018child\u2019 classes to this class: choose the 'Content\u2019 tab on the DataClass page, and click the \u2018+\u2019 symbol to add a new data class. Data items are represented as \u2018data elements\u2019 within the model. To start adding data elements to a class, visit its DataClass page, and click the \u2018+\u2019 button on the \u2018Content\u2019 tab. This will give you two options - to create a new contained \u2018child\u2019 class, as above, or to create a new data element - choose the second option. As with creating a new class, this will give you the option to copy an existing data element from elsewhere in the model (or from another model which you have read access to). In creating a model of an existing dataset, you will almost certainly want to create a new data element. The next form that appears will ask for the details of the new data element: For label , enter the preferred name of the data item. This could be the name of a column in a relational database, for example. You can add further names as aliases later. In description , you should describe the data item and the values it may take. This may include information about provenance - the context of collection, and any subsequent processing, but should also say something about intended interpretation or possible use. This description should explain the data point in terms that might be useful to prospective data users. The multiplicity of a data element specifies the number of values that it may take at the same time. For example, the number of \u2018date of death \u2019 items contained within a \u2018patient\u2019 record might be at least 0, and at most 1. The min field should be a positive whole number; the max field should be a positive whole number no smaller than min, or may be \u2018 * \u2019 to indicate that no maximum number is set. If you are unsure, this field may be left blank. The data type field describes the values that this data item may take. A data type may be either: primitive : for example a String, an Integer, or a Date. enumerated : chosen from a given list of values, which may be described using codes or free-text. For example M = Male, F = Female, U = Unknown reference-valued : a reference to another class of data. For example, the \u2018Registered GP\u2019 data element of a \u2018Patient\u2019 may refer to a separate class of \u2018GP\u2019 (containing name, surgery, address, etc) When creating a new data element, you can choose to use an existing data type that belongs to the data model in question, or you can create a new data type (by providing its name, the list of values, or a class to reference, respectively). From the data model page it is also possible to import data types from another model, or some pre-defined sets of data types (such as those found in MS SQLServer databases, for example). Click \u2018 Submit \u2019 to create the new Data Element. Repeat this process to add the other data elements for each class.","title":"Describing the data items"},{"location":"tutorials/document-assets/#entering-this-information-offline","text":"While creating a model of an existing dataset, you may find it more convenient to enter and share the information above using an Excel spreadsheet - and then upload the contents of that spreadsheet into the catalogue. You can create more than one model using the same spreadsheet. A blank spreadsheet should be included with this document, together with an example of a completed spreadsheet, describing the Diagnostic Imaging Dataset. The first sheet o the spreadsheet (which must be called DataModels) should introduce one or more data models. Each subsequent sheet should describe the contents of one of these data models. In the DataModels sheet, there should be one row for each data model described, and the following columns can be completed: SHEET_KEY : the name of the sheet (in this spreadsheet) describing the contents of the data model Name : the name or label of the data model (as explained above) Description : a brief description of the dataset Author : the author of this data model of the dataset Organisation : the organisation holding the dataset Type : this should be \u2018Data Asset\u2019 In the blank spreadsheet supplied, there is a KEY_1 sheet with the column headings required for each of the subsequent sheets. You can rename or copy this sheet. Whatever name is chosen should be included in the list of data models presented in the opening \u2018DataModels\u2019 sheet (or the contents will not be uploaded). The following columns should be completed: DataClass Path (essential): this should be the path from the top level of the model to the data class described in the current row, or the class containing the item described in the current row; for a top-level class, it will be simply the class name; for child classes, it will be a list of class names, using \u201c|\u201d as a delimiter. DataElement Name (essential for data elements): if the row is describing a data element, rather than a data class, then the name of the element should be inserted here Description : an explanation of the intended interpretation (and perhaps also the context of collection or provenance) of the DataClass or DataElement Minimum Multiplicity (may be left blank): the minimum number of instances of the class or element, usually 0 or 1 Maximum Multiplicity (may be left blank): the maximum number of instances, with -1 (rather than *) used to indicate that there is no upper bound DataType Name (essential for data elements): the name of the data type of the data element being described DataType Description (needed only for the first time that the data type in question is mentioned): the description of the data type Reference to DataClass Path : if the data type is another class (if the data element is a reference to an instance of another class) then insert the path to that class here Enumeration Key and Enumeration Value : if the data type is an enumeration, then you may add several pairs of entries in these two columns, one for each key-value pair in the enumeration; in each case, the key is the text or string that may appear in a column of the dataset, and the value is its expansion or explanatory text. Once you have done this, you will need to select the corresponding cells in the DataClass Path column - the first column - and merge them. You may wish to merge the corresponding cells in the other columns as well, for a clearer presentation of the information: for example, A completed spreadsheet can be imported into the catalogue using the \u2018import\u2019 button, located on the toolbar at the top of the screen: Select the Excel importer. Having chosen to import from a spreadsheet, you will have the opportunity to provide some more information: Folder : this is the folder in which the newly created model(s) will reside. The drop-down menu lets you choose a folder which you have access to Finalised : we recommend that you keep models as \u2018draft\u2019 until the gateway presentation of model descriptions has been decided Import : as new documentation version check this option if you intend to replace the current (catalogue) version of an existing model description File : Choose the spreadsheet file for upload. You may drag/drop a file from your file browser into the box here. Press \u2018 Submit \u2019 to import the model. Excel files can be safely used to \"round-trip\" data model descriptions. You can export a model from the catalogue in spreadsheet form, edit the spreadsheet, and import the new version of the spreadsheet to produce an updated version of the model - in this case, you need to select \u201cNew Documentation Version\u201d.","title":"Entering this information offline"},{"location":"tutorials/document-assets/#extracting-metadata-from-a-relational-database","text":"If the dataset to be described is held in a relational database, and you have direct access to that database, you can also use the catalogue to extract basic metadata - table names, column names, types, and structural relationships - from the database itself. The effort of manual data entry can then be focussed upon producing adequate accounts of the account of the intended interpretation of each data element. Clicking on the \u2018import\u2019 button as above, you can select an importer for most types of relational database. Having chosen to import from a relational database, the next set of options allow you to configure the import. Fields marked with a \u2018*\u2019 are mandatory. The fields should be completed as follows: Folder : this is the folder in which the newly created model(s) will reside. The drop-down menu lets you choose a folder which you have access to Data Model Name : this will be the name of the new data model. If no name is specified, the name of the database will be used. Finalised : if the newly-imported model is immediately \u2018finalised\u2019, you will no-longer be able to make changes to it. We recommend that you keep models as \u2018draft\u2019; otherwise if you wish to edit the descriptions you will need to create a new version. Import as new documentation version : this determines how the newly imported model supersedes any existing model with the same name. We recommend you check this option if you intend to overwrite an older version of the model. Database Name(s) : please enter the database name. If you\u2019d like to import multiple databases in one go, please enter the list of names , separated by a comma. Database Host : this is the IP address, or the server name, of the machine that the database is installed on. Username : this is the username which is used to connect to the database. Ideally, a user with read-only access will be used. This is not stored within the Metadata Catalogue application. Password : the password used by the user to connect to the database. This is not stored within the Metadata Catalogue application. Database port : the port which the database is communicating on - eg. 1433 for MS SQL Server. If no port is specified, the default port for the database will be used (MS SQL Server: 1433, Postgres: 5432, OracleDB: 1521) SSL : whether the database requires an encrypted connection (usually false).","title":"Extracting metadata from a relational database"},{"location":"tutorials/document-assets/#other-features","text":"The catalogue toolkit has a range of related functions for creating and updating models of datasets and data standards. As the features of the gateway interface are determined, we will update this documentation to address any additional metadata requirements, and to describe any additional functions that have become relevant. We will update it also to describe any improvements made to the functions described above.","title":"Other features"},{"location":"user/models/","text":"","title":"Models"},{"location":"user-guides/introduction/","text":"In this section you will find a variety of user guides which explain how to use Mauro Data Mapper . You don't need to follow them all in sequence - just dive into a guide that interests you! Our current User Guides: Create a Data Model In this user guide we explain how to use the web interface to create a new folder and a new Data Model from scratch, including selecting some default Data Types . Organising Data Models This user guide summarises how to organise Data Models including rearranging folders, adding classifiers and selecting favourites. Finalising Data Models This user guide explains how to finalise catalogue items such as Versioned Folders and Data Models . Branching, versioning and forking Data Models This user guide explains how to create a new draft of a Finalised Data Model by either creating a new Version , a new Branch or a new Fork . It also explains how to view a merge graph of a model. Merging Data Models This user guide explains how to merge Data Models including comparing and committing changes as well as how to successfully resolve conflicts. Document a Health Dataset This longer guide explains how to add structure to your Data Model such as incorporating Data Classes and Data Elements . Exporting Data Models This user guide will explain how to export a Data Model using a variety of exporters including XML, JSON, XML Schema, Grails and Excel. Import a Data Model from Excel This guide presents the steps for uploading a Data Model from an Excel Spreadsheet, and explains how you can use the import / export capabilities to 'round-trip' documentation with external users. How to search In this guide we walk you through the searching capabilities of Mauro Data Mapper , including the different search types, the syntaxes used as well as tips on how to conduct specific searches. Semantic links This is a short guide which explains the steps required to add, edit and delete Semantic links . Publish/Subscribe This guide explains how to connect to another Mauro Data Mapper instance to consume its Atom feed of Federated Data Models . Each Federated Data Model can then be subscribed to for use in your own Mauro Data Mapper instance. User Profile This short guide explains how to update your User profile within the Mauro Data Mapper web interface and how to change your login password. Admin functionality This user guide will walk you through all the options and settings that are available to administrators on Mauro Data Mapper . User permissions This user guide explains the multiple levels of access to catalogue items and adminstration supported in Mauro Data Mapper . Feature switches This user guide will walk you through feature switches that allow administrators to enable or disable certain Mauro features. Dynamic Profiles This user guide walks through the steps for creating and managing dynamic profiles . These are profiles which are defined by a model elsewhere in the system. Digital Object Identifiers This user guide explains how to set-up the Mauro Digital Object Identifier plugin to allow you to use, edit and remove DOI profiles. You will also find out how to create, submit and retire profile names.","title":"Introduction"},{"location":"user-guides/add-a-semantic-link/semantic-links/","text":"This user guide will explain the steps you need to follow to add a Semantic link between two descriptions of data. 1. Add a Semantic link \u00b6 Firstly, navigate to the source Data Model , Data Class or Data Element that you want to create the link from. Select the relevant data item in the Model Tree to display the details panel on the right. Any existing Semantic links are summarised in the 'Links' table below the details panel. To add, edit or remove a Semantic link , select the 'Links' tab, which will also display a list of existing Semantic links . Click the '+ Add Link' button at the top right of the 'Links' table which will add a new row. Complete the fields as described below: Source This is the Data Model , Data Class or Data Element which you are linking from. Link From the dropdown menu, select the type of Semantic link between 'Refines' or 'Does Not Refine' . A 'Refines' link is used when the description of the source 'Refines' that of the target. In other words, everything that is true about the target description is also true about the source description, with the source description often adding more information or context. A 'Does Not Refine' link is used when the description of the source is not intended to refine that of the target. Target Select the target description that you want to link to. To do this, click 'Add target' which will open up a seperate box. Choose whether the target is a Data Model or a Data Class and then select the relevant item from the Model Tree . This will automatically populate the 'Target' field and once completed, click the green tick to save and a green notification box should appear at the bottom right of your screen confirming that the 'Link saved successfuly' . 2. Delete a Semantic link \u00b6 To delete an existing Semantic link , navigate to the relevant link in the 'Links' tab underneath the details panel. Click the 'Delete' bin icon where you will then be asked to confirm the change. Click the green tick and a green notification box should appear at the bottom right of your screen confirming that the 'Link deleted successfuly' . 3. Edit a Semantic link \u00b6 To edit an existing Semantic link , navigate to the relevant link in the 'Links' tab underneath the details panel. Click the 'Edit' pencil icon which will allow you to change the 'Link' and 'Target' columns. Click the green tick to the right to confirm your changes and a green notification box should appear at the bottom right of your screen confirming that the 'Link updated successfuly' .","title":"Semantic links"},{"location":"user-guides/add-a-semantic-link/semantic-links/#1-add-a-semantic-link","text":"Firstly, navigate to the source Data Model , Data Class or Data Element that you want to create the link from. Select the relevant data item in the Model Tree to display the details panel on the right. Any existing Semantic links are summarised in the 'Links' table below the details panel. To add, edit or remove a Semantic link , select the 'Links' tab, which will also display a list of existing Semantic links . Click the '+ Add Link' button at the top right of the 'Links' table which will add a new row. Complete the fields as described below: Source This is the Data Model , Data Class or Data Element which you are linking from. Link From the dropdown menu, select the type of Semantic link between 'Refines' or 'Does Not Refine' . A 'Refines' link is used when the description of the source 'Refines' that of the target. In other words, everything that is true about the target description is also true about the source description, with the source description often adding more information or context. A 'Does Not Refine' link is used when the description of the source is not intended to refine that of the target. Target Select the target description that you want to link to. To do this, click 'Add target' which will open up a seperate box. Choose whether the target is a Data Model or a Data Class and then select the relevant item from the Model Tree . This will automatically populate the 'Target' field and once completed, click the green tick to save and a green notification box should appear at the bottom right of your screen confirming that the 'Link saved successfuly' .","title":"1. Add a Semantic link"},{"location":"user-guides/add-a-semantic-link/semantic-links/#2-delete-a-semantic-link","text":"To delete an existing Semantic link , navigate to the relevant link in the 'Links' tab underneath the details panel. Click the 'Delete' bin icon where you will then be asked to confirm the change. Click the green tick and a green notification box should appear at the bottom right of your screen confirming that the 'Link deleted successfuly' .","title":"2. Delete a Semantic link"},{"location":"user-guides/add-a-semantic-link/semantic-links/#3-edit-a-semantic-link","text":"To edit an existing Semantic link , navigate to the relevant link in the 'Links' tab underneath the details panel. Click the 'Edit' pencil icon which will allow you to change the 'Link' and 'Target' columns. Click the green tick to the right to confirm your changes and a green notification box should appear at the bottom right of your screen confirming that the 'Link updated successfuly' .","title":"3. Edit a Semantic link"},{"location":"user-guides/admin-functionality/admin-functionality/","text":"This user guide will walk you through all the options and settings that are available to adminsitrators on Mauro Data Mapper . 1. Dashboard \u00b6 To access your admin dashboard, log in to Mauro Data Mapper and click the white arrow by your user profile to the right of the menu header. From the dropdown menu under 'Admin settings' select 'Dashboard' . Your dashboard displays two tabs. One shows your 'Active sessions' and the other shows the 'Plugins and Modules' in the repository. 2. Model management \u00b6 As an administrator, you can delete several elements such as Data Models and Terminologies . To do this, select 'Model management' from the user profile dropdown menu. Select the relevant options to filter elements by type and status and the Model Tree displayed at the bottom of the page will automatically filter. Once you've found the element you wish to delete, click the checkbox until a green tick appears. You can do this for multiple elements and a summary list will be displayed on the right. Here, you will then have the option to either 'Delete Permanently' or 'Mark as Deleted' . 3. Emails \u00b6 To access your emails, select 'Emails' from the user profile dropdown menu. This will take you to your inbox where you can compose, edit and delete messages. 4. Manage users \u00b6 As an administrator you can add, activate and deactive users. Firstly, select 'Manage users' from the user profile dropdown menu. This will navigate you to a full list of all the users within the repository. Each user's full name, email, organisation, role, any groups they are associated with as well as the status of their account will be displayed. To order the list, you can click the small arrow to the right of each column heading. This will display the list in chronological order according to that category. To order from A-Z click the arrow until it points up. To order from Z-A click the arrow until it points down. You can also filter the user list, by clicking the filter symbol, to the right of 'repository' . This will display three boxes at the top of the list where you can then enter either a 'Name' , 'Email' , or 'Organisation' to filter the list by. 4.1 Add a new user \u00b6 To add a new user click the '+Add user' button at the top right of the 'Manage users' page. This will bring up an 'Add User' form which you will need to complete. Enter the email address, first name, last name, organisation and role for the new user. You then need to add the user to the correct group to define what permissions they should have. To do this, click the 'Choose a Group' box and select the relevant group from the dorpdown menu. Once all the fields have been completed, click 'Add user' and a green notification box should appear at the bottom right of your screen confirming that the 'User saved successfully' . 4.2 Reset password \u00b6 To reset a user's password, click the 'Actions' button to the right of the relevant row and select 'Reset password' from the dropdown menu. A green notification box will appear at the bottom right of your screen, confirming that a 'Reset password email sent successfully' . The user can then follow the instructions in the email to reset their password. 4.3 Activate or Deactivate a user \u00b6 The details of whether a user's account is 'Active' or 'Disabled' is displayed in the 'Status' column. To activate a user's account, click the 'Actions' button to the right of the relevant user and select 'Activate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully' . The status of the user will now change to 'Active' . To deactivate a user's account, click the 'Actions' button to the right of the relevant user and select 'Deactivate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully' . The status of the user will now change to 'Disabled' . 5. Pending users \u00b6 To approve or reject users, select 'Pending users' from the user profile dropdown menu. This will navigate you to a list of all the users that are waiting for approval. To approve or reject a user, click the 'Actions' button to the right of the relevant user and select either 'Approve user' or 'Reject user' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming the change. 6. Manage groups \u00b6 As an administrator you can also add, edit and delete groups. To do this, select 'Manage groups' from the user profile dropdown menu. This will navigate you to a list of all the groups currently stored in the repository. To order the list, click the small arrow to the right of 'Name' . This will display the list in chronological order from A-Z when the arrow points up and from Z-A when the arrow points down. You can also filter the user list, by clicking the filter symbol to the right of 'repository' . This will display a 'Name' box at the top of the list where you can then enter a group name and the list will automatically filter. 6.1 Add a group \u00b6 To add a new group, click the '+Add' button at the top right of the 'Manage groups' page. Enter the 'Name' and 'Description' of the new group and click 'Save group' . A green notifiction box will appear at the bottom right of your screen confirming that the 'Group saved successfully' . 6.2 Edit a group \u00b6 To edit a group, click the 'Actions' button to the right of the relevant group and select 'Edit group' from the dropdown menu. This will take you to a summary page of the group, where you can amend the name and description. Make sure to click 'Save group' to save any changes. Also displayed is a list of members, which can be found under the 'Members' tab. Here, you can see the status, details and role of each member. You can aslo delete members by clicking the red bin icon to the right of the relevant row. Any past amendments to the group can be viewed under the 'History' tab. 6.2 Delete a group \u00b6 To delete a group, click the 'Actions' button to the right of the relevant row and select 'Delete group' . A green notification box will appear at the bottom right of your screen to confirm that the 'Group deleted successfully' . 7. Configuration \u00b6 As an administrator you can edit the various email templates associated with Mauro Data Mapper including: Admin confirm user registration email Admin registered user email User invited to edit email User invited to view email User self registered email Forgotten password email Reset password email To access these templates, select 'Configuration' from the user profile dropdown menu. If you make any changes remember to press 'Save email configuration' at the bottom of the page. You can also rebuild the Lucene Search Index by clicking the 'Lucene' tab and then the 'Rebuild index' button.","title":"Admin functionality"},{"location":"user-guides/admin-functionality/admin-functionality/#1-dashboard","text":"To access your admin dashboard, log in to Mauro Data Mapper and click the white arrow by your user profile to the right of the menu header. From the dropdown menu under 'Admin settings' select 'Dashboard' . Your dashboard displays two tabs. One shows your 'Active sessions' and the other shows the 'Plugins and Modules' in the repository.","title":"1. Dashboard"},{"location":"user-guides/admin-functionality/admin-functionality/#2-model-management","text":"As an administrator, you can delete several elements such as Data Models and Terminologies . To do this, select 'Model management' from the user profile dropdown menu. Select the relevant options to filter elements by type and status and the Model Tree displayed at the bottom of the page will automatically filter. Once you've found the element you wish to delete, click the checkbox until a green tick appears. You can do this for multiple elements and a summary list will be displayed on the right. Here, you will then have the option to either 'Delete Permanently' or 'Mark as Deleted' .","title":"2. Model management"},{"location":"user-guides/admin-functionality/admin-functionality/#3-emails","text":"To access your emails, select 'Emails' from the user profile dropdown menu. This will take you to your inbox where you can compose, edit and delete messages.","title":"3. Emails"},{"location":"user-guides/admin-functionality/admin-functionality/#4-manage-users","text":"As an administrator you can add, activate and deactive users. Firstly, select 'Manage users' from the user profile dropdown menu. This will navigate you to a full list of all the users within the repository. Each user's full name, email, organisation, role, any groups they are associated with as well as the status of their account will be displayed. To order the list, you can click the small arrow to the right of each column heading. This will display the list in chronological order according to that category. To order from A-Z click the arrow until it points up. To order from Z-A click the arrow until it points down. You can also filter the user list, by clicking the filter symbol, to the right of 'repository' . This will display three boxes at the top of the list where you can then enter either a 'Name' , 'Email' , or 'Organisation' to filter the list by.","title":"4. Manage users"},{"location":"user-guides/admin-functionality/admin-functionality/#41-add-a-new-user","text":"To add a new user click the '+Add user' button at the top right of the 'Manage users' page. This will bring up an 'Add User' form which you will need to complete. Enter the email address, first name, last name, organisation and role for the new user. You then need to add the user to the correct group to define what permissions they should have. To do this, click the 'Choose a Group' box and select the relevant group from the dorpdown menu. Once all the fields have been completed, click 'Add user' and a green notification box should appear at the bottom right of your screen confirming that the 'User saved successfully' .","title":"4.1 Add a new user"},{"location":"user-guides/admin-functionality/admin-functionality/#42-reset-password","text":"To reset a user's password, click the 'Actions' button to the right of the relevant row and select 'Reset password' from the dropdown menu. A green notification box will appear at the bottom right of your screen, confirming that a 'Reset password email sent successfully' . The user can then follow the instructions in the email to reset their password.","title":"4.2 Reset password"},{"location":"user-guides/admin-functionality/admin-functionality/#43-activate-or-deactivate-a-user","text":"The details of whether a user's account is 'Active' or 'Disabled' is displayed in the 'Status' column. To activate a user's account, click the 'Actions' button to the right of the relevant user and select 'Activate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully' . The status of the user will now change to 'Active' . To deactivate a user's account, click the 'Actions' button to the right of the relevant user and select 'Deactivate' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming that the 'User details updated successfully' . The status of the user will now change to 'Disabled' .","title":"4.3 Activate or Deactivate a user"},{"location":"user-guides/admin-functionality/admin-functionality/#5-pending-users","text":"To approve or reject users, select 'Pending users' from the user profile dropdown menu. This will navigate you to a list of all the users that are waiting for approval. To approve or reject a user, click the 'Actions' button to the right of the relevant user and select either 'Approve user' or 'Reject user' from the dropdown menu. A green notification box will appear at the bottom right of your screen confirming the change.","title":"5. Pending users"},{"location":"user-guides/admin-functionality/admin-functionality/#6-manage-groups","text":"As an administrator you can also add, edit and delete groups. To do this, select 'Manage groups' from the user profile dropdown menu. This will navigate you to a list of all the groups currently stored in the repository. To order the list, click the small arrow to the right of 'Name' . This will display the list in chronological order from A-Z when the arrow points up and from Z-A when the arrow points down. You can also filter the user list, by clicking the filter symbol to the right of 'repository' . This will display a 'Name' box at the top of the list where you can then enter a group name and the list will automatically filter.","title":"6. Manage groups"},{"location":"user-guides/admin-functionality/admin-functionality/#61-add-a-group","text":"To add a new group, click the '+Add' button at the top right of the 'Manage groups' page. Enter the 'Name' and 'Description' of the new group and click 'Save group' . A green notifiction box will appear at the bottom right of your screen confirming that the 'Group saved successfully' .","title":"6.1 Add a group"},{"location":"user-guides/admin-functionality/admin-functionality/#62-edit-a-group","text":"To edit a group, click the 'Actions' button to the right of the relevant group and select 'Edit group' from the dropdown menu. This will take you to a summary page of the group, where you can amend the name and description. Make sure to click 'Save group' to save any changes. Also displayed is a list of members, which can be found under the 'Members' tab. Here, you can see the status, details and role of each member. You can aslo delete members by clicking the red bin icon to the right of the relevant row. Any past amendments to the group can be viewed under the 'History' tab.","title":"6.2 Edit a group"},{"location":"user-guides/admin-functionality/admin-functionality/#62-delete-a-group","text":"To delete a group, click the 'Actions' button to the right of the relevant row and select 'Delete group' . A green notification box will appear at the bottom right of your screen to confirm that the 'Group deleted successfully' .","title":"6.2 Delete a group"},{"location":"user-guides/admin-functionality/admin-functionality/#7-configuration","text":"As an administrator you can edit the various email templates associated with Mauro Data Mapper including: Admin confirm user registration email Admin registered user email User invited to edit email User invited to view email User self registered email Forgotten password email Reset password email To access these templates, select 'Configuration' from the user profile dropdown menu. If you make any changes remember to press 'Save email configuration' at the bottom of the page. You can also rebuild the Lucene Search Index by clicking the 'Lucene' tab and then the 'Rebuild index' button.","title":"7. Configuration"},{"location":"user-guides/branch-version-fork/branch-version-fork/","text":"This user guide explains how to create a new draft of a Finalised Data Model by either creating a new Version , a new Branch or a new Fork . It also explains how to view a merge graph of a model. 1. Overview \u00b6 When a Data Model is finalised it cannot be modified any further in that state. It represents the final state/contents of that model for a particular version. To find out how to Finalise catalogue items, visit our 'Finalising Data Models' user guide . If you want to edit a Finalised model, you will need to create a new draft which you can then work on. There are three ways to create a new draft: Create a new Version Create a new Branch Create a new Fork Selecting the right method depends on what you want to achieve: Create a new Version if: You are a single-person team making edits You want to make changes to a model in a linear fashion You want a simpler workflow i.e. not requiring merging of branches Create a new Branch if: You work in a multi-person team who are making edits at the same time You eventually plan on merging all branches back together into the main branch to finalise You want to review everyone's changes before they get merged into the main branch Create a new Fork if: You plan to take the model in a new direction. For example, under a new authority or if you want to use an existing model as the starting point for a new model You do not plan on merging this continuation back into the original workflow 2. Criteria \u00b6 The following catalogue items can be put into a draft state: Data Models Terminologies Code Sets Reference Data Models Versioned Folders Any of the catalogue items above can be used to create a new draft , so long as they are: Already Finalised Have a version number 2.1 Creating a draft of a Versioned Folder \u00b6 When creating a new draft of a Versioned Folder , every model within this folder will also be put into a draft state as well. This is shown by 'Draft' highlighted in yellow to the right of the model's name in the details panel. However, a Data Model within a Versioned Folder cannot be individually put into a draft state as it is version controlled by it's parent folder. Therefore, only when the Versioned Folder is put into a draft state will the model within it become a draft too. 3. Creating a new draft \u00b6 To create a new draft, first select the relevant catalogue item in the Model Tree . As mentioned above, make sure that this item has already been Finalised . Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel and select 'Create a New Version' from the dropdown menu. A 'New version' form will appear which you will need to complete. When selecting the type of new version that you want to create, there are three 'Actions' to choose from in the dropdown menu. These are: 'New Version' , 'New Branch' or a 'New Fork' . If you want to create a 'New Version' , select this option from the dropdown menu and then click 'Create' . A green notification box should appear at the bottom right of your screen confirming that the 'New version created successfully' . If you want to create a 'New Branch' , select this option from the dropdown menu. Enter a new 'Branch name' which helps describe it and then click 'Create' . A green notification box should appear at the bottom right of your screen confirming that the 'New branch created successfully' . When creating a new Branch , if a main branch doesn't already exist, then this will automatically be created as well. The main branch acts as the main line , or trunk of the changes made to a model. If you want to create a 'New Fork' , select this option from the dropdown menu. Then enter a 'New label' for the model and click 'Create' . A green notification box should appear at the bottom right of your screen confirming that the 'New fork created successfully' . Once completed, the new item will then be displayed in the draft state. 4. Merge graph \u00b6 To help you visualise and track Versions , Branches and Forks , Mauro has the ability to illustrate the relationships between each version of a model via a merge graph . To access this merge graph , select a catalogue item that is in a draft state in the Model Tree . Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Merge' from the dropdown menu and then 'Show merge graph' from the secondary dropdown menu. A 'Versioning graph' window will then appear. This illustrates the different versions of the model throughout it's history and how all the versions link together. You can zoom in or out by using the '+' and '-' buttons at the top left of the window, or within the graph itself. You can refresh the graph by selecting the circular arrows or the 'RESET' button at the top left. You can also download a copy of the graph or view it in full screen using the buttons at the top right of the window. The various colours of the version boxes represent different states of the model: Dark orange The current version/branch of the model being viewed Yellow Finalised version of a model Light yellow Draft branch of a model Dark blue Fork of a model","title":"Branching, versioning and forking Data Models"},{"location":"user-guides/branch-version-fork/branch-version-fork/#1-overview","text":"When a Data Model is finalised it cannot be modified any further in that state. It represents the final state/contents of that model for a particular version. To find out how to Finalise catalogue items, visit our 'Finalising Data Models' user guide . If you want to edit a Finalised model, you will need to create a new draft which you can then work on. There are three ways to create a new draft: Create a new Version Create a new Branch Create a new Fork Selecting the right method depends on what you want to achieve: Create a new Version if: You are a single-person team making edits You want to make changes to a model in a linear fashion You want a simpler workflow i.e. not requiring merging of branches Create a new Branch if: You work in a multi-person team who are making edits at the same time You eventually plan on merging all branches back together into the main branch to finalise You want to review everyone's changes before they get merged into the main branch Create a new Fork if: You plan to take the model in a new direction. For example, under a new authority or if you want to use an existing model as the starting point for a new model You do not plan on merging this continuation back into the original workflow","title":"1. Overview"},{"location":"user-guides/branch-version-fork/branch-version-fork/#2-criteria","text":"The following catalogue items can be put into a draft state: Data Models Terminologies Code Sets Reference Data Models Versioned Folders Any of the catalogue items above can be used to create a new draft , so long as they are: Already Finalised Have a version number","title":"2. Criteria"},{"location":"user-guides/branch-version-fork/branch-version-fork/#21-creating-a-draft-of-a-versioned-folder","text":"When creating a new draft of a Versioned Folder , every model within this folder will also be put into a draft state as well. This is shown by 'Draft' highlighted in yellow to the right of the model's name in the details panel. However, a Data Model within a Versioned Folder cannot be individually put into a draft state as it is version controlled by it's parent folder. Therefore, only when the Versioned Folder is put into a draft state will the model within it become a draft too.","title":"2.1 Creating a draft of a Versioned Folder"},{"location":"user-guides/branch-version-fork/branch-version-fork/#3-creating-a-new-draft","text":"To create a new draft, first select the relevant catalogue item in the Model Tree . As mentioned above, make sure that this item has already been Finalised . Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel and select 'Create a New Version' from the dropdown menu. A 'New version' form will appear which you will need to complete. When selecting the type of new version that you want to create, there are three 'Actions' to choose from in the dropdown menu. These are: 'New Version' , 'New Branch' or a 'New Fork' . If you want to create a 'New Version' , select this option from the dropdown menu and then click 'Create' . A green notification box should appear at the bottom right of your screen confirming that the 'New version created successfully' . If you want to create a 'New Branch' , select this option from the dropdown menu. Enter a new 'Branch name' which helps describe it and then click 'Create' . A green notification box should appear at the bottom right of your screen confirming that the 'New branch created successfully' . When creating a new Branch , if a main branch doesn't already exist, then this will automatically be created as well. The main branch acts as the main line , or trunk of the changes made to a model. If you want to create a 'New Fork' , select this option from the dropdown menu. Then enter a 'New label' for the model and click 'Create' . A green notification box should appear at the bottom right of your screen confirming that the 'New fork created successfully' . Once completed, the new item will then be displayed in the draft state.","title":"3. Creating a new draft"},{"location":"user-guides/branch-version-fork/branch-version-fork/#4-merge-graph","text":"To help you visualise and track Versions , Branches and Forks , Mauro has the ability to illustrate the relationships between each version of a model via a merge graph . To access this merge graph , select a catalogue item that is in a draft state in the Model Tree . Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Merge' from the dropdown menu and then 'Show merge graph' from the secondary dropdown menu. A 'Versioning graph' window will then appear. This illustrates the different versions of the model throughout it's history and how all the versions link together. You can zoom in or out by using the '+' and '-' buttons at the top left of the window, or within the graph itself. You can refresh the graph by selecting the circular arrows or the 'RESET' button at the top left. You can also download a copy of the graph or view it in full screen using the buttons at the top right of the window. The various colours of the version boxes represent different states of the model: Dark orange The current version/branch of the model being viewed Yellow Finalised version of a model Light yellow Draft branch of a model Dark blue Fork of a model","title":"4. Merge graph"},{"location":"user-guides/create-a-data-model/create-a-data-model/","text":"This user guide will explain the steps you need to follow to create a new Data Model . 1. Create a new folder \u00b6 Data Models are stored in their own folders and subfolders which are displayed in the Model Tree . Therefore, to create a new Data Model , first you need to either create a new folder, or add a subfolder. Ensure the 'Models' tab is selected above the Model Tree . To create a new top-level folder click the 'Create a new Folder' icon at the top right of the Model Tree . To create a new subfolder, right click on an existing folder and select '+ Create' and then 'Folder' from the dropdown menu. A 'Create a new Folder' dialogue box will appear. Enter a name for the folder and tick 'Version control this folder' if you wish to create a Versioned Folder . Once complete, click 'Add folder' to save your changes. The 'Version control this folder' option will only appear if adding to the top level of the Model Tree or under another folder. Versioned folders cannot be created under other Versioned folders . 2. Add Data Model \u00b6 To add a Data Model , right click the relevant folder and select 'Add Data Model' . A 'New Data Model' form will appear on the right. 3. Complete New Data Model form \u00b6 3.1 Complete Data Model Details \u00b6 Please complete both the mandatory and optional fields of the 'New Data Model' form. The defintions of each field are detailed below: Label Enter a unique name for the Data Model and include any version information, as two Data Models cannot share the same Label . Author Use this field to record the name(s) of the authors who are creating and maintaining this Data Model . Organisation Type the name of the organisation responsible for the Data Model , or the underlying data. Description Enter a detailed description in either plain text or html. Include any important contextual details relating to the Data Model . Select a Data Model Type Select whether the Data Model is a Data Asset or a Data Standard from the dropdown menu. A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema. Classifications Select any relevant Classifications (also known as tags) from the dropdown menu. You can select as many Classifications as you like. Once all the fields have been populated click 'Next Step' to complete the 'Default Data Types' section of the form. 3.2 Select Default Data Types \u00b6 Select the relevant set of 'Default Data Types' from the dropdown menu. These will be imported into your Data Model . You should select the category that includes all the Data Types that you will likely have within your Data Model , however if you are unsure at this stage, then leave this field blank and import them later on. 4. Submit Data Model \u00b6 Once completed, click 'Submit Data Model' and your new Data Model will be added. When selected in the Model Tree the details of the Data Model will now be displayed on the right.","title":"Create a Data Model"},{"location":"user-guides/create-a-data-model/create-a-data-model/#1-create-a-new-folder","text":"Data Models are stored in their own folders and subfolders which are displayed in the Model Tree . Therefore, to create a new Data Model , first you need to either create a new folder, or add a subfolder. Ensure the 'Models' tab is selected above the Model Tree . To create a new top-level folder click the 'Create a new Folder' icon at the top right of the Model Tree . To create a new subfolder, right click on an existing folder and select '+ Create' and then 'Folder' from the dropdown menu. A 'Create a new Folder' dialogue box will appear. Enter a name for the folder and tick 'Version control this folder' if you wish to create a Versioned Folder . Once complete, click 'Add folder' to save your changes. The 'Version control this folder' option will only appear if adding to the top level of the Model Tree or under another folder. Versioned folders cannot be created under other Versioned folders .","title":"1. Create a new folder"},{"location":"user-guides/create-a-data-model/create-a-data-model/#2-add-data-model","text":"To add a Data Model , right click the relevant folder and select 'Add Data Model' . A 'New Data Model' form will appear on the right.","title":" 2. Add Data Model"},{"location":"user-guides/create-a-data-model/create-a-data-model/#3-complete-new-data-model-form","text":"","title":" 3. Complete New Data Model form"},{"location":"user-guides/create-a-data-model/create-a-data-model/#31-complete-data-model-details","text":"Please complete both the mandatory and optional fields of the 'New Data Model' form. The defintions of each field are detailed below: Label Enter a unique name for the Data Model and include any version information, as two Data Models cannot share the same Label . Author Use this field to record the name(s) of the authors who are creating and maintaining this Data Model . Organisation Type the name of the organisation responsible for the Data Model , or the underlying data. Description Enter a detailed description in either plain text or html. Include any important contextual details relating to the Data Model . Select a Data Model Type Select whether the Data Model is a Data Asset or a Data Standard from the dropdown menu. A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema. Classifications Select any relevant Classifications (also known as tags) from the dropdown menu. You can select as many Classifications as you like. Once all the fields have been populated click 'Next Step' to complete the 'Default Data Types' section of the form.","title":"3.1 Complete Data Model Details"},{"location":"user-guides/create-a-data-model/create-a-data-model/#32-select-default-data-types","text":"Select the relevant set of 'Default Data Types' from the dropdown menu. These will be imported into your Data Model . You should select the category that includes all the Data Types that you will likely have within your Data Model , however if you are unsure at this stage, then leave this field blank and import them later on.","title":"3.2 Select Default Data Types"},{"location":"user-guides/create-a-data-model/create-a-data-model/#4-submit-data-model","text":"Once completed, click 'Submit Data Model' and your new Data Model will be added. When selected in the Model Tree the details of the Data Model will now be displayed on the right.","title":"4. Submit Data Model"},{"location":"user-guides/document-a-dataset/document-a-dataset/","text":"This user guide will explain the steps you need to follow to manually add a health dataset to Mauro Data Mapper . 1. Create a Data Model \u00b6 Datasets are stored in their own Data Models within Mauro Data Mapper . Therefore, you first need to create a new Data Model . To do this, follow the steps in the 'Create a Data Model user guide' . Once you have reached step 3 , 'Complete New Data Model form' you will need to select the 'Data Model Type' as Data Asset from the dropdown menu. Fill in the rest of the 'New Data Model' form and submit the Data Model as explained in steps 3.2 and 4 . 2. Add a property \u00b6 Once you've created your Data Model , it's important to record further characteristics of the corresponding dataset, particularly to help gateway providers when designing interfaces. To do this, select the Data Model in the Model Tree and then click the 'Properties' tab underneath the Data Model details panel. Click the '+ Add Property' button on the right to add a new row to the property table. Complete the details of the new property as follows: Namespace This will be used to select the correct profile / selection of properties. Key Where an existing namespace has been chosen, select a relevant property name such as 'contact email' . Otherwise, enter a new property name. Value This is the value of the given property, for example \u2018 enquiries-mydataset@hub.org \u2019 . You can also add a relevant element to the value of a property. Click '+ Add Element' in the 'Value' column and select the element type from the menu. Search for the element you require and once selected, it will automatically import into the 'Value' column of the properties table. Once you have filled in the details of the property, click the green 'Save' tick and the new property will be added to the table. 3. Create a Data Class \u00b6 Each Data Model is made up of several Data Classes which is where data items are both created and managed. If your dataset is a collection of tables, the conventional approach is to create a new class for each table. Alternatively, you can create a set of classes to provide a more abstract account of the data set, which group and present the data differently to how it is stored and managed. To create a Data Class , select the relevant Data Model in the Model Tree and click the 'Data Classes' tab on the panel below the Data Model details. Click the '+ Add' button on the right and a 'New Data Class' form will appear. There are two ways to import a Data Class into a Data Model . You can either create a new Data Class or copy a Data Class from an existing Data Model . 3.1 Create a New Data Class \u00b6 To create a new Data Class , select this option in the first section of the 'New Data Class' form and then click the 'Next step' button. Now you need to complete the 'Data Class Details' section of the form as follows: Label Enter a name for the new Data Class which has to be unique within the Data Model . Description Complete a description in either html or plain text which explains the types of data items grouped together within this Data Class . Also include contextual details which are common to the data items, to avoid having to add descriptions to each individual data item. Multiplicity The Multiplicity specifies the minimum and maximum number of times that the Data Class will appear within the Data Model . Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of * which represents -1 internally. Once you have completed the 'Data Class Details' form, click 'Submit Data Class' and the new Data Class will now be permanently displayed under the 'Data Classes' tab of the Data Model . You can add as many Data Classes as necessary. 3.2 Copy a Data Class \u00b6 To import Data Classes from an existing Data Model , select the 'Copy Data Classes(s) from...' option in the first section of the 'New Data Class' form. Select the relevant Data Model by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can then select the relevant Data Model . Once selected, click 'Next step' . The 'Data Class Details' section of the form will then appear, with a list of all the Data Classes within the selected Data Model . Select the Data Classes you wish to import and then click 'Submit Data Class' . The selected Data Classes will then be imported into your original Data Model , with the progress illustrated by a green loading bar at the bottom of the form. 4. Add a Nested Data Class \u00b6 A useful way of managing complex data sets is to use Nested Data Classes which are essentially a Data Class within a Data Class . For example, in a webform, there may be a section called 'Contact details' , which would be one Data Class . Within that section however, there may be another labelled 'Correspondence Address' , which would be a Nested Data Class . To add a Nested Data Class , click the relevant Data Class from the Model Tree and click the 'Content' tab on the panel below the model overview. Then click '+ Add' and select 'Add Data Class' from the dropdown menu. Complete the 'New Data Class' form as explained above in step '3. Create a Data Class' . 5. Add Data Elements \u00b6 Within each Data Class lies several Data Elements which are the descriptions of an individual field, variable, column or property of a data item. To create a Data Element you can use the same approach as creating a Data Class . Select the relevant Data Class in the Model Tree and click the 'Content' tab on the panel below the Data Class details. Click the '+ Add' button on the right and you will be given the choice to either add a Data Class or a Data Element . Select 'Add Data Element' and a 'New Data Element' form will appear. Similar to adding a Data Class , there are two ways to import a Data Element . You can either create a new Data Element or copy Data Elements from an existing Data Class . 5.1 Create a New Data Element \u00b6 Follow the steps in '3.1 Create a new Data Class' until you have completed the 'Label' , 'Description' and Multiplicity fields for the Data Element . Each Data Element then needs to be assigned a relevant Data Type . This can either be selected from an existing list, or you can add a new Data Type . 5.1.1 Select an existing Data Type \u00b6 Click the 'Search' box and a dropdown list of existing Data Types will appear. Select the relevant Data Type . You can then assign several Classifications to the Data Type by selecting them from the dropdown menu. Once all fields are complete, click 'Submit Data Element' to add the Data Element to the Data Class . Repeat this process to add other Data Elements . 5.1.2 Add a new Data Type \u00b6 To add a new Data Type , click 'Add a new Data Type' on the 'Data Element Details' form. Fill in the Label and Description fields and select the relevant Data Type . A Data Type can either be: Enumeration : A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value. If 'Enumeration' has been selected, an additional table will appear where you can add several Enumerations and specify a 'Group' , 'Key' and 'Value' . Primitive : Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference : Data which refers to another Data Class within the same Data Model . If Reference has been selected, the Reference Data Class can be selected from a dropdown menu. Terminology : A structured collection of Enumeration Values which have relationships between different data terms. Similarly, if Terminology has been selected, the relevant category can be chosen from a dropdown menu. You can then assign several 'Classifications' by selecting them from the dropdown menu. Once all fields are complete, click 'Submit Data Element' to add the new Data Element to the Data Class . Go back to step '5.1 Create a new Data Element' and repeat the process to add other Data Elements . 5.2 Copy a Data Element \u00b6 To import a Data Element from an existing Data Class , select the 'Copy Data Element(s) from...' option in the first section of the 'New Data Element' form. Select the relevant Data Class by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can select the relevant Data Model and Data Class . Once selected, click 'Next step' . The 'Data Element Details' section of the form will then appear, with a list of all the Data Elements within the selected Data Class . Select the Data Elements you wish to import by ticking the relevant boxes and these will then appear in a 'Summary of Selected Data Elements' table at the bottom of the form. Once you have checked this table is correct, click 'Submit Data Element' . The selected Data Elements will then be imported into your original Data Class , with the progress illustrated by a green loading bar at the bottom of the form.","title":"Document a Dataset"},{"location":"user-guides/document-a-dataset/document-a-dataset/#1-create-a-data-model","text":"Datasets are stored in their own Data Models within Mauro Data Mapper . Therefore, you first need to create a new Data Model . To do this, follow the steps in the 'Create a Data Model user guide' . Once you have reached step 3 , 'Complete New Data Model form' you will need to select the 'Data Model Type' as Data Asset from the dropdown menu. Fill in the rest of the 'New Data Model' form and submit the Data Model as explained in steps 3.2 and 4 .","title":"1. Create a Data Model"},{"location":"user-guides/document-a-dataset/document-a-dataset/#2-add-a-property","text":"Once you've created your Data Model , it's important to record further characteristics of the corresponding dataset, particularly to help gateway providers when designing interfaces. To do this, select the Data Model in the Model Tree and then click the 'Properties' tab underneath the Data Model details panel. Click the '+ Add Property' button on the right to add a new row to the property table. Complete the details of the new property as follows: Namespace This will be used to select the correct profile / selection of properties. Key Where an existing namespace has been chosen, select a relevant property name such as 'contact email' . Otherwise, enter a new property name. Value This is the value of the given property, for example \u2018 enquiries-mydataset@hub.org \u2019 . You can also add a relevant element to the value of a property. Click '+ Add Element' in the 'Value' column and select the element type from the menu. Search for the element you require and once selected, it will automatically import into the 'Value' column of the properties table. Once you have filled in the details of the property, click the green 'Save' tick and the new property will be added to the table.","title":"2. Add a property"},{"location":"user-guides/document-a-dataset/document-a-dataset/#3-create-a-data-class","text":"Each Data Model is made up of several Data Classes which is where data items are both created and managed. If your dataset is a collection of tables, the conventional approach is to create a new class for each table. Alternatively, you can create a set of classes to provide a more abstract account of the data set, which group and present the data differently to how it is stored and managed. To create a Data Class , select the relevant Data Model in the Model Tree and click the 'Data Classes' tab on the panel below the Data Model details. Click the '+ Add' button on the right and a 'New Data Class' form will appear. There are two ways to import a Data Class into a Data Model . You can either create a new Data Class or copy a Data Class from an existing Data Model .","title":" 3. Create a Data Class"},{"location":"user-guides/document-a-dataset/document-a-dataset/#31-create-a-new-data-class","text":"To create a new Data Class , select this option in the first section of the 'New Data Class' form and then click the 'Next step' button. Now you need to complete the 'Data Class Details' section of the form as follows: Label Enter a name for the new Data Class which has to be unique within the Data Model . Description Complete a description in either html or plain text which explains the types of data items grouped together within this Data Class . Also include contextual details which are common to the data items, to avoid having to add descriptions to each individual data item. Multiplicity The Multiplicity specifies the minimum and maximum number of times that the Data Class will appear within the Data Model . Optional data may have a minimum Multiplicity of 0 and a maximum of 1, whereas mandatory data has a minimum Multiplicity of 1. Data which occurs any number of times is given by a Multiplicity of * which represents -1 internally. Once you have completed the 'Data Class Details' form, click 'Submit Data Class' and the new Data Class will now be permanently displayed under the 'Data Classes' tab of the Data Model . You can add as many Data Classes as necessary.","title":" 3.1 Create a New Data Class"},{"location":"user-guides/document-a-dataset/document-a-dataset/#32-copy-a-data-class","text":"To import Data Classes from an existing Data Model , select the 'Copy Data Classes(s) from...' option in the first section of the 'New Data Class' form. Select the relevant Data Model by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can then select the relevant Data Model . Once selected, click 'Next step' . The 'Data Class Details' section of the form will then appear, with a list of all the Data Classes within the selected Data Model . Select the Data Classes you wish to import and then click 'Submit Data Class' . The selected Data Classes will then be imported into your original Data Model , with the progress illustrated by a green loading bar at the bottom of the form.","title":"3.2 Copy a Data Class"},{"location":"user-guides/document-a-dataset/document-a-dataset/#4-add-a-nested-data-class","text":"A useful way of managing complex data sets is to use Nested Data Classes which are essentially a Data Class within a Data Class . For example, in a webform, there may be a section called 'Contact details' , which would be one Data Class . Within that section however, there may be another labelled 'Correspondence Address' , which would be a Nested Data Class . To add a Nested Data Class , click the relevant Data Class from the Model Tree and click the 'Content' tab on the panel below the model overview. Then click '+ Add' and select 'Add Data Class' from the dropdown menu. Complete the 'New Data Class' form as explained above in step '3. Create a Data Class' .","title":"4. Add a Nested Data Class"},{"location":"user-guides/document-a-dataset/document-a-dataset/#5-add-data-elements","text":"Within each Data Class lies several Data Elements which are the descriptions of an individual field, variable, column or property of a data item. To create a Data Element you can use the same approach as creating a Data Class . Select the relevant Data Class in the Model Tree and click the 'Content' tab on the panel below the Data Class details. Click the '+ Add' button on the right and you will be given the choice to either add a Data Class or a Data Element . Select 'Add Data Element' and a 'New Data Element' form will appear. Similar to adding a Data Class , there are two ways to import a Data Element . You can either create a new Data Element or copy Data Elements from an existing Data Class .","title":" 5. Add Data Elements"},{"location":"user-guides/document-a-dataset/document-a-dataset/#51-create-a-new-data-element","text":"Follow the steps in '3.1 Create a new Data Class' until you have completed the 'Label' , 'Description' and Multiplicity fields for the Data Element . Each Data Element then needs to be assigned a relevant Data Type . This can either be selected from an existing list, or you can add a new Data Type .","title":" 5.1 Create a New Data Element"},{"location":"user-guides/document-a-dataset/document-a-dataset/#511-select-an-existing-data-type","text":"Click the 'Search' box and a dropdown list of existing Data Types will appear. Select the relevant Data Type . You can then assign several Classifications to the Data Type by selecting them from the dropdown menu. Once all fields are complete, click 'Submit Data Element' to add the Data Element to the Data Class . Repeat this process to add other Data Elements .","title":"5.1.1 Select an existing Data Type"},{"location":"user-guides/document-a-dataset/document-a-dataset/#512-add-a-new-data-type","text":"To add a new Data Type , click 'Add a new Data Type' on the 'Data Element Details' form. Fill in the Label and Description fields and select the relevant Data Type . A Data Type can either be: Enumeration : A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value. If 'Enumeration' has been selected, an additional table will appear where you can add several Enumerations and specify a 'Group' , 'Key' and 'Value' . Primitive : Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference : Data which refers to another Data Class within the same Data Model . If Reference has been selected, the Reference Data Class can be selected from a dropdown menu. Terminology : A structured collection of Enumeration Values which have relationships between different data terms. Similarly, if Terminology has been selected, the relevant category can be chosen from a dropdown menu. You can then assign several 'Classifications' by selecting them from the dropdown menu. Once all fields are complete, click 'Submit Data Element' to add the new Data Element to the Data Class . Go back to step '5.1 Create a new Data Element' and repeat the process to add other Data Elements .","title":"5.1.2 Add a new Data Type"},{"location":"user-guides/document-a-dataset/document-a-dataset/#52-copy-a-data-element","text":"To import a Data Element from an existing Data Class , select the 'Copy Data Element(s) from...' option in the first section of the 'New Data Element' form. Select the relevant Data Class by either typing the name in the box or clicking the menu icon to the right of the red cross. This will display the Model Tree from which you can select the relevant Data Model and Data Class . Once selected, click 'Next step' . The 'Data Element Details' section of the form will then appear, with a list of all the Data Elements within the selected Data Class . Select the Data Elements you wish to import by ticking the relevant boxes and these will then appear in a 'Summary of Selected Data Elements' table at the bottom of the form. Once you have checked this table is correct, click 'Submit Data Element' . The selected Data Elements will then be imported into your original Data Class , with the progress illustrated by a green loading bar at the bottom of the form.","title":"5.2 Copy a Data Element"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/","text":"Profiles are a way to store additional information about Data Models or their components. You can read more about profiles in our Properties and profiles tutorial . This user guide walks through the steps for creating and managing dynamic profiles . These are profiles which are defined by a model elsewhere in the system. The first stage is to create a definition model. 1. Creating a profile definition model \u00b6 In this user guide, we'll create a dynamic profile for storing organisation data inventory information. You may decide to create a folder specifically for storing dynamic profile models, but alternatively you may store them within the folder they'll be used. Section 6. 'Admin dashboard for dynamic profiles' describes the admin dashboard which allows these specifications to be easily found at a later date. First, create a Data Model as explained in Section 2. 'Add Data Model' of our 'Create a Data Model' user guide . Add a Label , author and organisation. Select Data Standard from the 'Data Model Type' dropdown and select any relevant classifications. Once completed, click 'Next step' . Next, you must choose to import the default set of 'Profile Specification Data Types' from the dropdown list. These define the supported basic data types for profile fields which you'll be choosing from when adding Data Elements to your profile definition model. Once finished, click 'Submit Data Model' to create this new Data Model . In the next step you will add technical information to assist the server in storing properties against your profile. 2. Apply the Profile Specification Profile \u00b6 From the description page, use the profile dropdown to select 'Add New Profile' . Then choose 'Profile Specification Profile (Data Model)' from the dropdown menu. Click 'Save Changes' and this will bring up the form for editing the profile fields. There are two fields for completion: Metadata Namespace This mandatory field defines the namespace that separates the properties in this profile from others in other profiles. You should try and choose a unique name that is based on a URL and the purpose of this profile model. For more information about namespaces, see this article . Applicable for domains This optional field determines which types of catalogue item can be profiled using this profile. For example, 'DataModel' . You should separate multiple domains with a semi-colon (';'). Leave this field blank to allow this profile to be applicable to any catalogue item. Click 'Save' to finish editing the dynamic profile model details. 3. Add Data Classes and Data Elements \u00b6 Once you've created your profile model, you can start adding fields that will be stored for those catalogue items that use the profile. You should create one or more Data Classes to group your Data Elements . Any description you give to those Data Classes will be visible in the editing interface for that profile. In this example, we'll create classes for 'Basic Information' , 'Data Source Owner' , and 'Physical Location' . Create each Data Class as explained in Section 3. 'Create a Data Class' of our 'Document a Dataset' user guide . Add a Label and a description. You do not need to add Multiplicities to the classes as repeated groups are not currently supported in profile definitions. The structure of the Data Model (red), along with it's Data Classes (blue) is shown below. Within each Data Class you can add Data Elements for the fields you wish to store. Again, create these in the usual way, as explained in Section 5. 'Add Data Elements' of our 'Document a Dataset' user guide . You will need to enter the following details for each element: Label The name of this field in the profile. The 'key' of properties corresponding to this field ( See tutorial ) will be derived from this name unless overridden (See Section 4. 'Customise the fields' ). Description The description of this property. This will be presented to users as they view or edit profile fields, so is a good opportunity to better describe the information you'd like collected, or how you expect this field to be interpreted. Multiplicity The multiplicity will add validation constraints to the field, i.e. 0..1 defines an optional field; 1..1 defines a mandatory field. Data Type For the data type you should choose one of the existing primitive types, as imported in Section 1. 'Creating a profile definition model' . Alternatively, you can create a new Enumeration Data Type and define a list of allowable values. 4. Customise the fields \u00b6 You can further add to the definition of a property by using another profile on the Data Element - the Profile Specification Profile (Data Element) profile. Add this to the element you've just created and you'll be able to enter the following information: Metadata Property Name This will override the key used to store against the property in the database and is accessible through the APIs. You should set this field if your Data Element label has special characters, or if you require the APIs to provide a particular output format. Default Value This indicates the value that will be shown in the interface when a user starts entering data for this profile. Regular expression This field allows you to specify a validation constraint against data entered for the field in question. We use standard Java syntax for these. May be edited after finalisation This functionality is still under construction, but ticking the box will allow this value to be edited on a profile after the containing model has been finalised. This is suitable for fields such as 'Contact email' which may change without affecting the semantics of the model. Once completed, click 'Save' to confirm your changes. 5. Use the dynamic profile \u00b6 Once you've added all required Data Classes and Data Elements , the dynamic profile will be ready to use. It is good practise to try using it with a test model before finalising the specification. This will allow you to verify that the fields are defined correctly. However, once you're happy with the design, you should finalise the model as modifying a profile that has data collected against it may have unexpected consequences. You can add the profile to a Data Model and fill out the form provided, as shown in the example below. If you subsequently wish to revise the profile model, you can create a new version of it. Using the same namespace will ensure that fields are migrated, where possible. Using a new namespace will allow the collection of new fields, with both profiles being maintained separately. 6. Admin dashboard for dynamic profiles \u00b6 Profile specification models are normal models and so it can be tricky to find and maintain them. An admin dashboard has been created to help keep track of those models used as profile specifications. To access your admin dashboard, log in to Mauro Data Mapper and click the white arrow by your user profile to the right of the menu header. From the dropdown menu under 'Admin settings' select 'Dashboard' . Click the 'Profiles' tab and the namespace for each profile used along with the relevant link to the defining model will be displayed. To ensure profiles are well defined before use, we intend to add details of superseded profile models as well as show validation status and other hints.","title":"Dynamic profiles"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#1-creating-a-profile-definition-model","text":"In this user guide, we'll create a dynamic profile for storing organisation data inventory information. You may decide to create a folder specifically for storing dynamic profile models, but alternatively you may store them within the folder they'll be used. Section 6. 'Admin dashboard for dynamic profiles' describes the admin dashboard which allows these specifications to be easily found at a later date. First, create a Data Model as explained in Section 2. 'Add Data Model' of our 'Create a Data Model' user guide . Add a Label , author and organisation. Select Data Standard from the 'Data Model Type' dropdown and select any relevant classifications. Once completed, click 'Next step' . Next, you must choose to import the default set of 'Profile Specification Data Types' from the dropdown list. These define the supported basic data types for profile fields which you'll be choosing from when adding Data Elements to your profile definition model. Once finished, click 'Submit Data Model' to create this new Data Model . In the next step you will add technical information to assist the server in storing properties against your profile.","title":"1. Creating a profile definition model"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#2-apply-the-profile-specification-profile","text":"From the description page, use the profile dropdown to select 'Add New Profile' . Then choose 'Profile Specification Profile (Data Model)' from the dropdown menu. Click 'Save Changes' and this will bring up the form for editing the profile fields. There are two fields for completion: Metadata Namespace This mandatory field defines the namespace that separates the properties in this profile from others in other profiles. You should try and choose a unique name that is based on a URL and the purpose of this profile model. For more information about namespaces, see this article . Applicable for domains This optional field determines which types of catalogue item can be profiled using this profile. For example, 'DataModel' . You should separate multiple domains with a semi-colon (';'). Leave this field blank to allow this profile to be applicable to any catalogue item. Click 'Save' to finish editing the dynamic profile model details.","title":"2. Apply the Profile Specification Profile"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#3-add-data-classes-and-data-elements","text":"Once you've created your profile model, you can start adding fields that will be stored for those catalogue items that use the profile. You should create one or more Data Classes to group your Data Elements . Any description you give to those Data Classes will be visible in the editing interface for that profile. In this example, we'll create classes for 'Basic Information' , 'Data Source Owner' , and 'Physical Location' . Create each Data Class as explained in Section 3. 'Create a Data Class' of our 'Document a Dataset' user guide . Add a Label and a description. You do not need to add Multiplicities to the classes as repeated groups are not currently supported in profile definitions. The structure of the Data Model (red), along with it's Data Classes (blue) is shown below. Within each Data Class you can add Data Elements for the fields you wish to store. Again, create these in the usual way, as explained in Section 5. 'Add Data Elements' of our 'Document a Dataset' user guide . You will need to enter the following details for each element: Label The name of this field in the profile. The 'key' of properties corresponding to this field ( See tutorial ) will be derived from this name unless overridden (See Section 4. 'Customise the fields' ). Description The description of this property. This will be presented to users as they view or edit profile fields, so is a good opportunity to better describe the information you'd like collected, or how you expect this field to be interpreted. Multiplicity The multiplicity will add validation constraints to the field, i.e. 0..1 defines an optional field; 1..1 defines a mandatory field. Data Type For the data type you should choose one of the existing primitive types, as imported in Section 1. 'Creating a profile definition model' . Alternatively, you can create a new Enumeration Data Type and define a list of allowable values.","title":"3. Add Data Classes and Data Elements"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#4-customise-the-fields","text":"You can further add to the definition of a property by using another profile on the Data Element - the Profile Specification Profile (Data Element) profile. Add this to the element you've just created and you'll be able to enter the following information: Metadata Property Name This will override the key used to store against the property in the database and is accessible through the APIs. You should set this field if your Data Element label has special characters, or if you require the APIs to provide a particular output format. Default Value This indicates the value that will be shown in the interface when a user starts entering data for this profile. Regular expression This field allows you to specify a validation constraint against data entered for the field in question. We use standard Java syntax for these. May be edited after finalisation This functionality is still under construction, but ticking the box will allow this value to be edited on a profile after the containing model has been finalised. This is suitable for fields such as 'Contact email' which may change without affecting the semantics of the model. Once completed, click 'Save' to confirm your changes.","title":"4. Customise the fields"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#5-use-the-dynamic-profile","text":"Once you've added all required Data Classes and Data Elements , the dynamic profile will be ready to use. It is good practise to try using it with a test model before finalising the specification. This will allow you to verify that the fields are defined correctly. However, once you're happy with the design, you should finalise the model as modifying a profile that has data collected against it may have unexpected consequences. You can add the profile to a Data Model and fill out the form provided, as shown in the example below. If you subsequently wish to revise the profile model, you can create a new version of it. Using the same namespace will ensure that fields are migrated, where possible. Using a new namespace will allow the collection of new fields, with both profiles being maintained separately.","title":"5. Use the dynamic profile"},{"location":"user-guides/dynamic-profiles/dynamic-profiles/#6-admin-dashboard-for-dynamic-profiles","text":"Profile specification models are normal models and so it can be tricky to find and maintain them. An admin dashboard has been created to help keep track of those models used as profile specifications. To access your admin dashboard, log in to Mauro Data Mapper and click the white arrow by your user profile to the right of the menu header. From the dropdown menu under 'Admin settings' select 'Dashboard' . Click the 'Profiles' tab and the namespace for each profile used along with the relevant link to the defining model will be displayed. To ensure profiles are well defined before use, we intend to add details of superseded profile models as well as show validation status and other hints.","title":"6. Admin dashboard for dynamic profiles"},{"location":"user-guides/exporting-data-models/exporting-data-models/","text":"This user guide will explain how to export a Data Model using a variety of exporters including XML, JSON, XML Schema, Grails and Excel. 1. Export a Data Model \u00b6 Select the Data Model you wish to export in the Model Tree to display it's details panel. In the icon menu at the bottom right of the details panel click the 'Export as JSON, XML...' button. This will display a list of exporters. Select the exporter you wish to use. Once selected, the Data Model will automatically start exporting into the specificed format, with a green loading bar indicating the progress. Once exported, this progress bar will be replaced with a link to download the exported file. Click this link to download the file directly to your local machine. 2. Export multiple Data Models \u00b6 You can also export several Data Models together. To do this, click the 'Export DataModel(s)' icon at the top right of the Model Tree . A 'Data Model(s) Export' form will appear on the right hand side. To select the Data Models you wish to export, click the menu icon to the right of the 'Select Data Model...' box and the Model Tree will be displayed. Select the relevant Data Models and these will automatically appear in the 'Select your Data Model(s):' field. Once you have selected the relevant Data Models , you then need to choose an exporter. Note that only Excel (XLSX) Exporter and Simple Excel (XLSX) Exporter support multiple Data Model exports. Click the 'Select an exporter*' box and a dropdown list of all the available exporters will appear. Select the exporter you want and then click the 'Export Data Model(s)' button. A green loading bar will appear and once exported a green notification box will confirm that the 'Data Model(s) exported successfully' and a link to download the file will appear at the bottom of the form.","title":"Exporting Data Models"},{"location":"user-guides/exporting-data-models/exporting-data-models/#1-export-a-data-model","text":"Select the Data Model you wish to export in the Model Tree to display it's details panel. In the icon menu at the bottom right of the details panel click the 'Export as JSON, XML...' button. This will display a list of exporters. Select the exporter you wish to use. Once selected, the Data Model will automatically start exporting into the specificed format, with a green loading bar indicating the progress. Once exported, this progress bar will be replaced with a link to download the exported file. Click this link to download the file directly to your local machine.","title":"1. Export a Data Model"},{"location":"user-guides/exporting-data-models/exporting-data-models/#2-export-multiple-data-models","text":"You can also export several Data Models together. To do this, click the 'Export DataModel(s)' icon at the top right of the Model Tree . A 'Data Model(s) Export' form will appear on the right hand side. To select the Data Models you wish to export, click the menu icon to the right of the 'Select Data Model...' box and the Model Tree will be displayed. Select the relevant Data Models and these will automatically appear in the 'Select your Data Model(s):' field. Once you have selected the relevant Data Models , you then need to choose an exporter. Note that only Excel (XLSX) Exporter and Simple Excel (XLSX) Exporter support multiple Data Model exports. Click the 'Select an exporter*' box and a dropdown list of all the available exporters will appear. Select the exporter you want and then click the 'Export Data Model(s)' button. A green loading bar will appear and once exported a green notification box will confirm that the 'Data Model(s) exported successfully' and a link to download the file will appear at the bottom of the form.","title":"2. Export multiple Data Models"},{"location":"user-guides/feature-switches/feature-switches/","text":"Introduction \u00b6 This user guide will walk you through the management and purpose of feature switches . Note Only administrators can view and control feature switches in the Mauro user interface. What are feature switches? \u00b6 Feature switches in the Mauro user interface allow an administrator to control which particular feature sets are enabled and visible to users of the Mauro instance running. There are several reasons why feature switches may be used: There are some feature sets in the user interface which are optional or depend on certain plugins or instance configuration to work. Some Mauro instances may not need particular functionality in daily running. There may be features in active development, still experimental or subject to change. In these cases, an administrator is able to switch on or off feature sets that are required or not needed, depending on the requirements of that Mauro instance. Administration \u00b6 Feature switches can be accessed from the 'Configuration' panel in account settings using Mauro API properties . All feature switches are: Categorised under features . Publicly available to any user for reading (but not updating). To add a feature switch and control feature visibility, select 'Configuration' from the user profile dropdown menu. Select the 'Properties' tab and then click '+ Add' . An 'Add Property' form will then appear for you to complete. To select a property, click the 'Select the property to add' box and a dropdown menu will appear. Select any key from the list that starts with 'feature' . Once selected, the 'Key' and 'Category' fields will automatically populate. Tick the checkbox next to 'Publicly visible' if required. Under 'Value' , select either 'Yes' or 'No' from the dropdown menu to enable or disable the feature respectively. Once completed select 'Add property' to save your changes and a green notificaiton box should appear at the bottom right of your screen confirming that the 'Property was saved successfully' . Available features \u00b6 The table below lists all the optional features that can be controlled by feature switches. Warning It is advisable that any feature switches marked as 'In development' should only be used in test environments until fully completed. Feature switch Default value feature.use_subscribed_catalogues Enables the Subscribed Catalogues and Federated Data Models feature set. true feature.use_open_id_connect Enables the user interface for administrators to create/edit/remove OpenID Connect provider details to Mauro, and for the login form to list these external identity providers to redirect and login. As well as enabling this feature switch, you must also install the Mauro OpenID Connect authentication plugin for the Mauro instance too. false feature.use_digital_object_identifiers Enables the management of Digital Object Identifiers (DOIs) using a new profile to store DOI metadata and submit catalogue items to a DOI system. As well as enabling this feature switch, you must also install the Mauro Digital Object Identifiers plugin for the Mauro instance too. false feature.use_versioned_folders Enables the ability to use Versioned Folders . true feature.use_merge_diff_ui Enables a new user interface for the management of merging data models within the Mauro user interface. See the Merging Data Models user guide for more details. true","title":"Feature switches"},{"location":"user-guides/feature-switches/feature-switches/#introduction","text":"This user guide will walk you through the management and purpose of feature switches . Note Only administrators can view and control feature switches in the Mauro user interface.","title":"Introduction"},{"location":"user-guides/feature-switches/feature-switches/#what-are-feature-switches","text":"Feature switches in the Mauro user interface allow an administrator to control which particular feature sets are enabled and visible to users of the Mauro instance running. There are several reasons why feature switches may be used: There are some feature sets in the user interface which are optional or depend on certain plugins or instance configuration to work. Some Mauro instances may not need particular functionality in daily running. There may be features in active development, still experimental or subject to change. In these cases, an administrator is able to switch on or off feature sets that are required or not needed, depending on the requirements of that Mauro instance.","title":"What are feature switches?"},{"location":"user-guides/feature-switches/feature-switches/#administration","text":"Feature switches can be accessed from the 'Configuration' panel in account settings using Mauro API properties . All feature switches are: Categorised under features . Publicly available to any user for reading (but not updating). To add a feature switch and control feature visibility, select 'Configuration' from the user profile dropdown menu. Select the 'Properties' tab and then click '+ Add' . An 'Add Property' form will then appear for you to complete. To select a property, click the 'Select the property to add' box and a dropdown menu will appear. Select any key from the list that starts with 'feature' . Once selected, the 'Key' and 'Category' fields will automatically populate. Tick the checkbox next to 'Publicly visible' if required. Under 'Value' , select either 'Yes' or 'No' from the dropdown menu to enable or disable the feature respectively. Once completed select 'Add property' to save your changes and a green notificaiton box should appear at the bottom right of your screen confirming that the 'Property was saved successfully' .","title":"Administration"},{"location":"user-guides/feature-switches/feature-switches/#available-features","text":"The table below lists all the optional features that can be controlled by feature switches. Warning It is advisable that any feature switches marked as 'In development' should only be used in test environments until fully completed. Feature switch Default value feature.use_subscribed_catalogues Enables the Subscribed Catalogues and Federated Data Models feature set. true feature.use_open_id_connect Enables the user interface for administrators to create/edit/remove OpenID Connect provider details to Mauro, and for the login form to list these external identity providers to redirect and login. As well as enabling this feature switch, you must also install the Mauro OpenID Connect authentication plugin for the Mauro instance too. false feature.use_digital_object_identifiers Enables the management of Digital Object Identifiers (DOIs) using a new profile to store DOI metadata and submit catalogue items to a DOI system. As well as enabling this feature switch, you must also install the Mauro Digital Object Identifiers plugin for the Mauro instance too. false feature.use_versioned_folders Enables the ability to use Versioned Folders . true feature.use_merge_diff_ui Enables a new user interface for the management of merging data models within the Mauro user interface. See the Merging Data Models user guide for more details. true","title":"Available features"},{"location":"user-guides/finalising-data-models/finalising-data-models/","text":"This user guide explains how to finalise catalogue items such as Data Models and Versioned Folders . 1. Overview \u00b6 Finalising a Data Model means that the model is fixed to a specific version and cannot be modified further. Therefore, all the contents of that model are now read only . This ensures that the model represents a final state that will remain as it is forever. However, a finalised Data Model can be modified further by creating a new Version . 2. Criteria \u00b6 The following catalogue items can be Finalised : Data Models Terminologies Code Sets Reference Data Models Versioned Folders Any of the catalogue items above may be Finalised , so long as they are: In their Draft state On their main branch 2.1 Finalising a Versioned Folder \u00b6 When Finalising a Versioned Folder , this means that every model within this folder is also Finalised as well. This is shown in the Model Tree by a document symbol and a version number, with 'Finalised' to the right of the model's name in the details panel. The same version number for the Versioned Folder is automatically assigned to the models within that folder. Version numbers cannot be customised individually on model's within a Versioned Folder . However, a Data Model within a Versioned Folder cannot be individually Finalised as it is version controlled by it's parent folder. Therefore, only when the Versioned Folder is Finalised will the model within it become Finalised too. 3. How to Finalise an item \u00b6 To finalise a catalogue item, first select it in the Model Tree . Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Finalise' from the dropdown menu. A 'Finalise Data Model' dialogue box will then appear where you will have several options for choosing the next version number. Major Represents a major change to the item. For example, restructuring the model. Minor Represents a minor change to the item. For example, editing the description of a model. Patch Represents very minor changes or fixes to the item. For example, correcting spelling mistakes. Custom If the automatic version number above is not sufficient, select this option to manually enter your own. You then have the option to enter a 'Tag name' to attach to this Finalised version. This is useful if you want to apply additional context alongside the version number. For example, 'January 2021 Release' , 'Interim Release' or 'Modelling Milestone' . Once you've completed the form, click 'Finalise Data Model' to confirm your changes and a green notification box should appear at the bottom right of your screen confirming that the 'Data Model finalised successfully' . The item is now in a Finalised state, and the version number chosen will now be attached to the item.","title":"Finalising Data Models"},{"location":"user-guides/finalising-data-models/finalising-data-models/#1-overview","text":"Finalising a Data Model means that the model is fixed to a specific version and cannot be modified further. Therefore, all the contents of that model are now read only . This ensures that the model represents a final state that will remain as it is forever. However, a finalised Data Model can be modified further by creating a new Version .","title":"1. Overview"},{"location":"user-guides/finalising-data-models/finalising-data-models/#2-criteria","text":"The following catalogue items can be Finalised : Data Models Terminologies Code Sets Reference Data Models Versioned Folders Any of the catalogue items above may be Finalised , so long as they are: In their Draft state On their main branch","title":"2. Criteria"},{"location":"user-guides/finalising-data-models/finalising-data-models/#21-finalising-a-versioned-folder","text":"When Finalising a Versioned Folder , this means that every model within this folder is also Finalised as well. This is shown in the Model Tree by a document symbol and a version number, with 'Finalised' to the right of the model's name in the details panel. The same version number for the Versioned Folder is automatically assigned to the models within that folder. Version numbers cannot be customised individually on model's within a Versioned Folder . However, a Data Model within a Versioned Folder cannot be individually Finalised as it is version controlled by it's parent folder. Therefore, only when the Versioned Folder is Finalised will the model within it become Finalised too.","title":"2.1 Finalising a Versioned Folder"},{"location":"user-guides/finalising-data-models/finalising-data-models/#3-how-to-finalise-an-item","text":"To finalise a catalogue item, first select it in the Model Tree . Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Finalise' from the dropdown menu. A 'Finalise Data Model' dialogue box will then appear where you will have several options for choosing the next version number. Major Represents a major change to the item. For example, restructuring the model. Minor Represents a minor change to the item. For example, editing the description of a model. Patch Represents very minor changes or fixes to the item. For example, correcting spelling mistakes. Custom If the automatic version number above is not sufficient, select this option to manually enter your own. You then have the option to enter a 'Tag name' to attach to this Finalised version. This is useful if you want to apply additional context alongside the version number. For example, 'January 2021 Release' , 'Interim Release' or 'Modelling Milestone' . Once you've completed the form, click 'Finalise Data Model' to confirm your changes and a green notification box should appear at the bottom right of your screen confirming that the 'Data Model finalised successfully' . The item is now in a Finalised state, and the version number chosen will now be attached to the item.","title":"3. How to Finalise an item"},{"location":"user-guides/how-to-search/how-to-search/","text":"The search function within Mauro Data Mapper is extremely powerful and allows you to search for datasets or specifications throughout the whole catalogue. This user guide will explain how to effectively search for an item as well as the methods Mauro Data Mapper uses for searching. 1. Types of search \u00b6 There are several different ways to search for an item within Mauro Data Mapper . 1.1 Search Models \u00b6 The first method of searching is to use the search box on the top left of the finder panel above the Model Tree . Here, you can search 'Models' , 'Classifications' and 'Favourites' by clicking the relevant tab and then entering the search term in the search box. As you type your search, the Model Tree will automatically filter, displaying only the relevant results, with the search term highlighted in yellow wherever it appears within the list. When searching Models , the results list will show Data Models and Data Classes which match the search term in any Label , description, comment or property. To refresh the results, click the 'Search' icon or the 'Reload Data Models Tree' icon in the menu to the right of the search box. You can also choose whether you want the results list to feature superseded or old Data Models by clicking the 'Filters' icon and then selecting the relevant options. To remove filtering click the small 'x' on the right of the search box. 1.2 Main search \u00b6 Another method of searching is to use the main search box which can be found under the 'Search' tab in the header. Once again, as you type your search, the results list will automatically filter, with the number of results displayed at the top of the list. You can then click on a result that you're interested in and the details panel will open up in a new tab in your browser. 1.3 Advanced search \u00b6 To conduct a more specific search you can use Advanced search . To the right of the main search box under the 'Search' tab in the header, click 'Advanced' . This will display a variety of different options to filter your search by. Restrict your search \u00b6 Firstly, enter the search term in the main search box and the results list will automtically filter at the bottom of the page. You can then restrict your search to a particular Folder , Data Model or Data Class by clicking the menu icon to the right of the ' Restrict your search to:' box. This will display the Model Tree , from which you can select the relevant item. Domain Types & Search Type \u00b6 You also have the option to filter by 'Domain Types' . This allows you to specify whether you want to search Data Models , Data Classes , Data Elements , Data Types or Enumeration Values by selecting the relevant boxes. You can also define the 'Search Type' . If you only want to search in the title of elements and not in the descriptions , metadata or aliases , then tick the \u2018Title only\u2019 box. Alternatively, select \u2018Exact match\u2019 to search for the exact search term throughout the entire catalogue. Date ranges \u00b6 Advanced search also allows you to specify a range of dates so that you can filter results by when they were 'Last Updated' or 'Created' . Enter the relevant date in the format of dd/mm/yyyy or click the \u2018Calendar\u2019 icon to the right of each date box. This will display a small calendar where you can select the relevant date which will automatically populate the date box. Classifiers \u00b6 In some cases you can also filter your search by Classifiers by clicking the \u2018Classifiers\u2019 box and selecting the relevant options from the dropdown menu. 2. How search works \u00b6 The search functionality in Mauro Data Mapper implements the Simple Query Strings of Hibernate search . This is essentially where a simple syntax is used to parse and split the provided query string into terms based on special operators. The query then analyses each term independently before returning matching results. 2.1 Search syntax \u00b6 The search syntaxes that Mauro Data Mapper supports are: AND using + OR using | NOT using \u2013 To search for different suffixes use * e.g. prefix* To search for an exact phrase use \u201c\u201d e.g. \u201cexample phrase\u201d To add precedence to search terms, use () e.g. (important) example To search for similar terms (fuzzy string searching) use ~2 e.g. smoke~2 will return smokes, smake etc To search for similar phrases use \u201c\u201d and ~2 e.g. \u201ccigarette smoke\u201d~3 For example, if you are interested in information relating to smoking. Searching \u2018smoking\u2019 will return a list of results. However, if you want to be more generic you can type \u2018smoke*\u2019 which will search for smoking, smoker, smoked etc. To look at information relating to smoking and pregnancy you can type \u2018smok*+pregnancy\u2019 which will return results that include both pregnancy and the different variants of smoking. 2.2 Search examples \u00b6 Below are some more examples to further illustrate how to use the search syntaxes effectively: storm~2 - will return results containing storms or sturm war + (peace | harmony) - will return results containing \"war and either peace or harmony\" storm tree - will return results containing the words storm or tree storm and tree - will return results containing exactly the phrase storm and tree 2.3 Label & Metadata Key \u00b6 The Label and Key entries are indexed using a WordDelimiter analyser . This essentially splits up the search term into individual words at the following points: Spaces Hypens Numbers Capital letters Full stops These individual words are referred to as \u2018Keys\u2019 and once the search term has been split up into its various keys, these are then saved and used to conduct the search. Therefore, the results will only match the keys and not the whole search phrase. For example: Datamodel for test - will be searchable by the keys datamodel , for and test Test DataModel - will be searchable by the keys test , data and model , it will not be searchable by the key datamodel because of the capital letter Test DataModel V1.0.1 - will be searchable by the keys test , data , model , v1 , 0 and 1 . It will not be searchable by the keys datamodel or v1.0.1 subject-34567 - will be searchable by the keys subject and 34567","title":"How to search"},{"location":"user-guides/how-to-search/how-to-search/#1-types-of-search","text":"There are several different ways to search for an item within Mauro Data Mapper .","title":"1. Types of search"},{"location":"user-guides/how-to-search/how-to-search/#11-search-models","text":"The first method of searching is to use the search box on the top left of the finder panel above the Model Tree . Here, you can search 'Models' , 'Classifications' and 'Favourites' by clicking the relevant tab and then entering the search term in the search box. As you type your search, the Model Tree will automatically filter, displaying only the relevant results, with the search term highlighted in yellow wherever it appears within the list. When searching Models , the results list will show Data Models and Data Classes which match the search term in any Label , description, comment or property. To refresh the results, click the 'Search' icon or the 'Reload Data Models Tree' icon in the menu to the right of the search box. You can also choose whether you want the results list to feature superseded or old Data Models by clicking the 'Filters' icon and then selecting the relevant options. To remove filtering click the small 'x' on the right of the search box.","title":"1.1 Search Models"},{"location":"user-guides/how-to-search/how-to-search/#12-main-search","text":"Another method of searching is to use the main search box which can be found under the 'Search' tab in the header. Once again, as you type your search, the results list will automatically filter, with the number of results displayed at the top of the list. You can then click on a result that you're interested in and the details panel will open up in a new tab in your browser.","title":"1.2 Main search"},{"location":"user-guides/how-to-search/how-to-search/#13-advanced-search","text":"To conduct a more specific search you can use Advanced search . To the right of the main search box under the 'Search' tab in the header, click 'Advanced' . This will display a variety of different options to filter your search by.","title":"1.3 Advanced search"},{"location":"user-guides/how-to-search/how-to-search/#restrict-your-search","text":"Firstly, enter the search term in the main search box and the results list will automtically filter at the bottom of the page. You can then restrict your search to a particular Folder , Data Model or Data Class by clicking the menu icon to the right of the ' Restrict your search to:' box. This will display the Model Tree , from which you can select the relevant item.","title":"Restrict your search"},{"location":"user-guides/how-to-search/how-to-search/#domain-types-search-type","text":"You also have the option to filter by 'Domain Types' . This allows you to specify whether you want to search Data Models , Data Classes , Data Elements , Data Types or Enumeration Values by selecting the relevant boxes. You can also define the 'Search Type' . If you only want to search in the title of elements and not in the descriptions , metadata or aliases , then tick the \u2018Title only\u2019 box. Alternatively, select \u2018Exact match\u2019 to search for the exact search term throughout the entire catalogue.","title":"Domain Types &amp; Search Type"},{"location":"user-guides/how-to-search/how-to-search/#date-ranges","text":"Advanced search also allows you to specify a range of dates so that you can filter results by when they were 'Last Updated' or 'Created' . Enter the relevant date in the format of dd/mm/yyyy or click the \u2018Calendar\u2019 icon to the right of each date box. This will display a small calendar where you can select the relevant date which will automatically populate the date box.","title":"Date ranges"},{"location":"user-guides/how-to-search/how-to-search/#classifiers","text":"In some cases you can also filter your search by Classifiers by clicking the \u2018Classifiers\u2019 box and selecting the relevant options from the dropdown menu.","title":"Classifiers"},{"location":"user-guides/how-to-search/how-to-search/#2-how-search-works","text":"The search functionality in Mauro Data Mapper implements the Simple Query Strings of Hibernate search . This is essentially where a simple syntax is used to parse and split the provided query string into terms based on special operators. The query then analyses each term independently before returning matching results.","title":"2. How search works"},{"location":"user-guides/how-to-search/how-to-search/#21-search-syntax","text":"The search syntaxes that Mauro Data Mapper supports are: AND using + OR using | NOT using \u2013 To search for different suffixes use * e.g. prefix* To search for an exact phrase use \u201c\u201d e.g. \u201cexample phrase\u201d To add precedence to search terms, use () e.g. (important) example To search for similar terms (fuzzy string searching) use ~2 e.g. smoke~2 will return smokes, smake etc To search for similar phrases use \u201c\u201d and ~2 e.g. \u201ccigarette smoke\u201d~3 For example, if you are interested in information relating to smoking. Searching \u2018smoking\u2019 will return a list of results. However, if you want to be more generic you can type \u2018smoke*\u2019 which will search for smoking, smoker, smoked etc. To look at information relating to smoking and pregnancy you can type \u2018smok*+pregnancy\u2019 which will return results that include both pregnancy and the different variants of smoking.","title":"2.1 Search syntax"},{"location":"user-guides/how-to-search/how-to-search/#22-search-examples","text":"Below are some more examples to further illustrate how to use the search syntaxes effectively: storm~2 - will return results containing storms or sturm war + (peace | harmony) - will return results containing \"war and either peace or harmony\" storm tree - will return results containing the words storm or tree storm and tree - will return results containing exactly the phrase storm and tree","title":"2.2 Search examples"},{"location":"user-guides/how-to-search/how-to-search/#23-label-metadata-key","text":"The Label and Key entries are indexed using a WordDelimiter analyser . This essentially splits up the search term into individual words at the following points: Spaces Hypens Numbers Capital letters Full stops These individual words are referred to as \u2018Keys\u2019 and once the search term has been split up into its various keys, these are then saved and used to conduct the search. Therefore, the results will only match the keys and not the whole search phrase. For example: Datamodel for test - will be searchable by the keys datamodel , for and test Test DataModel - will be searchable by the keys test , data and model , it will not be searchable by the key datamodel because of the capital letter Test DataModel V1.0.1 - will be searchable by the keys test , data , model , v1 , 0 and 1 . It will not be searchable by the keys datamodel or v1.0.1 subject-34567 - will be searchable by the keys subject and 34567","title":"2.3 Label &amp; Metadata Key"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/","text":"This user guide will explain the steps you need to follow to import a health dataset into Mauro Data Mapper using an Excel spreadsheet. To add an existing dataset to Mauro Data Mapper , you can either enter all the information online as explained in the 'Document a Health Dataset user guide' , or you may find it more convenient to import information automatically from an Excel spreadsheet. The importing functionality of Mauro Data Mapper allows you to import several Data Models using the same spreadsheet. 1. Create Data Model import file \u00b6 To ensure all the information is imported correctly, the dataset needs to be entered into a spreadsheet in a specific format. To help with this, you can download a zip file of the standard Data Model import file here . This standard spreadsheet contains two types of worksheets. Firstly, there is the Data Model listing sheet , titled \u2018DataModels\u2019 . This is effectively a contents page which lists the main details of each Data Model you wish to import. There must only ever be one Data Model listing sheet . The next sheet is the Data Model key sheet which is titled \u2018KEY_1\u2019 . This contains all the relevant details of one Data Model listed in the \u2018DataModels\u2019 sheet. Therefore, if you wish to import several Data Models , you will need to add a Data Model key sheet for each additional Data Model and title it \u2018KEY_2\u2019 , \u2018KEY_3\u2019 respectively. 1.1 Data Model listing sheet \u00b6 In the Data Model listing sheet , use one row for each Data Model . Enter the information according to the columns, which are explained below, along with any other properties or metadata which may be relevant. The following columns must be completed: SHEET_KEY The unique name of each Data Model key sheet such as \u2018KEY_1\u2019 , \u2018KEY_2\u2019 etc. Name The unique name or Label of the Data Model . Remember this should be different to all the existing Data Models within Mauro Data Mapper , unless you are updating an existing Data Model . Description Enter a description which explains the contextual details of the dataset within the Data Model . Author Record the name(s) of the authors who are creating and maintaining this Data Model . Organisation Type the name of the organisation responsible for the Data Model , or the underlying data. Type This is the type of Data Model , which can either be a Data Asset or a Data Standard . A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema. Adding properties Any other relevant properties or metadata relating to the Data Model can be included in additional columns to the right of the core columns. Metadata can have the following properties: Namespace This will be used to select the correct profile / property selection and should be entered in row 1 of the spreadsheet. If it is left blank, the default namespace of ox.softeng.metadatacatalogue.plugins.excel will be used. Key This is a relevant property name such as \u2018contact email\u2019 and must be entered in row 2 . If no key is supplied, then the value will not be assigned. Value This is the Value of the given property, for example \u2018 enquiries-mydataset@hub.org \u2019 and should be entered into the relevant row. If multiple rows are being imported and a Namespace and Key column is created, then the property will only be assigned if a Value is supplied. 1.2 Data Model key sheet \u00b6 There should be one Data Model key sheet for each Data Model , and its name should correspond to the relevant 'SHEET_KEY' on the Data Model listing sheet . Any Data Model listed without the correctly named key sheet will not be imported. Therefore, the best practise is to first copy the 'KEY_1' sheet in the standard excel template, rename it and then add the details of the relevant Data Model . Otherwise, formatting issues could occur, resulting in the data importing incorrectly. The following columns must be completed: DataClass Path This is the path from the top level of the Data Model to the Data Class . For a top level Data Class , only the class name should be entered in this field. However, if it is a Nested Data Class , then the name of the parent class along with the child class should be entered and be separated by \u201cI\u201d , for example: A nested class: 'parentIchild' A nested class within a nested class: 'grandparentIparentIchild' Another nested class within a nested class: 'grandparentIparentIanotherChild' DataElement Name If the row is describing a Data Element , instead of a Data Class , then the name of the Data Element should be entered here. If supplied, the remaining information in this row will be used to create the Data Element inside the Data Class provided in the 'DataClass Path' column. However, if left blank, the remaining information in this row will be assigned to the Data Class entered in the 'DataClass Path' column. Description Enter a description which explains any contextual details relating to the Data Element or Data Class . Minimum Multiplicity The minimum number of instances of the Data Class or Data Element within the Data Model . Optional data has a minimum Multiplicity of 0, whereas mandatory data has a minimum Multiplicity of 1. Maximum Multiplicity The maximum number of instances of the Data Class or Data Element within the Data Model . Optional data has a maximum Multiplicity of 1, whereas data which occurs any number of times and therefore has no upper bound has a maximum Multiplicity of * which represents -1 internally. DataType Name This is the name given to the Data Type of the Data Element being described and must be included when entering information for Data Elements . The Data Type describes the range of possible values that the Data Element may take. There are four different Data Types stored within a Data Model : Enumeration : A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value. Primitive : Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference : Data which refers to another Data Class within the same Data Model . Terminology : A structured collection of Enumeration Values which have relationships between different data terms. DataType Description This is a short description of the Data Type . If the same Data Type is used multiple times, then the first description entry will be used so subsequent description fields can be left blank. Reference to DataClass Path If the Data Element is a \u2018Reference\u2019 Data Type and therefore the Data Element is a reference to an instance of another Data Class , then the path to that Data Class should be entered here. This path must match a path provided in the 'DataClass Path' column using the same format with \u2018I\u2019 as the separator. This field cannot be used in conjunction with \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019 . Enumeration Key If the Data Element is an Enumeration Data Type , then the Key and Value of each Enumeration should be included in the \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019 columns respectively, with one Key and Value per row. The Enumeration Key is the text or string that may appear in a column of the dataset. Enumeration Value The Enumeration Value is the data that corresponds to the Key . For example, for a yes/no question on a webform, the Value is either \u2018Yes\u2019 or \u2018No\u2019. Each Value is assigned a Key , which in this case could be \u20181\u2019 and \u20180\u2019 respectively. An Enumeration Value must be provided if an Enumeration Key has been entered. Once all the Enumeration Key and Value pairs have been entered you will need to merge the corresponding cells in the 'DataClass Path' column, and you may wish to merge the other relevant cells in the other columns too for consistency. For example, if you were entering the information for a Data Element that was an Enumeration Data Type then the Data Model key sheet would look similar to the below: 2. Import Data Model \u00b6 Once the Data Model import file has been completed, click the 'Import a Data Model' icon at the top right of the Model Tree . The 'Import a new Data Model' form will then appear on the right. Click the 'Select an importer' box and select 'Excel (XLSX) importer' from the dropdown menu. This will automatically load the rest of the form that you need to complete. You then need to select the relevant folder that you wish to import the Data Model into. You can do this by either typing in the folder name, or clicking the menu icon to the right of the 'Folder location' box. This will display the Model Tree , from which you can then select the relevant folder. You then need to tick the 'Finalised' box to indicate whether the Data Model you are importing is finalised. Although, it is recommended to keep the model as a draft until the gateway presentation of model descriptions has been decided. If the Data Model you are importing is intended to replace an existing Data Model within the current version of Mauro Data Mapper , then tick the 'Import as New Documentation Version' box. This means that the imported model will supersede any Data Models with the same name and will be assigned the latest documentation version of the existing Data Model . If you are importing a new Data Model , then make sure the 'Name' field in the Data Model listing sheet is unique, otherwise this will cause an error when importing. In the 'Source' box, click 'Choose file' and your file explorer will open. Navigate to the relevant Data Model import file and then select 'Import Model(s)' . 3. Round Tripping Data Models \u00b6 Excel files can be safely used to 'Round trip' Data Model descriptions. This is essentially where: A Data Model is exported in the form of an Excel spreadsheet. This exported spreadsheet is edited or updated with new information. The new version of the spreadsheet is re-imported into Mauro Data Mapper and therefore automatically updates the existing Data Model . Note: 'Import as New Documentation Version' should be ticked when re-importing. This method is a quick and easy way to update existing Data Models as well as adding extra information. This can be particularly useful when Data Models have previously been imported using an alternative method, and therefore may only have the basic layout.","title":"Import a Data Model from Excel"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#1-create-data-model-import-file","text":"To ensure all the information is imported correctly, the dataset needs to be entered into a spreadsheet in a specific format. To help with this, you can download a zip file of the standard Data Model import file here . This standard spreadsheet contains two types of worksheets. Firstly, there is the Data Model listing sheet , titled \u2018DataModels\u2019 . This is effectively a contents page which lists the main details of each Data Model you wish to import. There must only ever be one Data Model listing sheet . The next sheet is the Data Model key sheet which is titled \u2018KEY_1\u2019 . This contains all the relevant details of one Data Model listed in the \u2018DataModels\u2019 sheet. Therefore, if you wish to import several Data Models , you will need to add a Data Model key sheet for each additional Data Model and title it \u2018KEY_2\u2019 , \u2018KEY_3\u2019 respectively.","title":"1. Create Data Model import file"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#11-data-model-listing-sheet","text":"In the Data Model listing sheet , use one row for each Data Model . Enter the information according to the columns, which are explained below, along with any other properties or metadata which may be relevant. The following columns must be completed: SHEET_KEY The unique name of each Data Model key sheet such as \u2018KEY_1\u2019 , \u2018KEY_2\u2019 etc. Name The unique name or Label of the Data Model . Remember this should be different to all the existing Data Models within Mauro Data Mapper , unless you are updating an existing Data Model . Description Enter a description which explains the contextual details of the dataset within the Data Model . Author Record the name(s) of the authors who are creating and maintaining this Data Model . Organisation Type the name of the organisation responsible for the Data Model , or the underlying data. Type This is the type of Data Model , which can either be a Data Asset or a Data Standard . A Data Asset is a collection of existing data, such as a database or a completed form. While a Data Standard is a specification template to collect new data, such as a form or schema. Adding properties Any other relevant properties or metadata relating to the Data Model can be included in additional columns to the right of the core columns. Metadata can have the following properties: Namespace This will be used to select the correct profile / property selection and should be entered in row 1 of the spreadsheet. If it is left blank, the default namespace of ox.softeng.metadatacatalogue.plugins.excel will be used. Key This is a relevant property name such as \u2018contact email\u2019 and must be entered in row 2 . If no key is supplied, then the value will not be assigned. Value This is the Value of the given property, for example \u2018 enquiries-mydataset@hub.org \u2019 and should be entered into the relevant row. If multiple rows are being imported and a Namespace and Key column is created, then the property will only be assigned if a Value is supplied.","title":" 1.1 Data Model listing sheet"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#12-data-model-key-sheet","text":"There should be one Data Model key sheet for each Data Model , and its name should correspond to the relevant 'SHEET_KEY' on the Data Model listing sheet . Any Data Model listed without the correctly named key sheet will not be imported. Therefore, the best practise is to first copy the 'KEY_1' sheet in the standard excel template, rename it and then add the details of the relevant Data Model . Otherwise, formatting issues could occur, resulting in the data importing incorrectly. The following columns must be completed: DataClass Path This is the path from the top level of the Data Model to the Data Class . For a top level Data Class , only the class name should be entered in this field. However, if it is a Nested Data Class , then the name of the parent class along with the child class should be entered and be separated by \u201cI\u201d , for example: A nested class: 'parentIchild' A nested class within a nested class: 'grandparentIparentIchild' Another nested class within a nested class: 'grandparentIparentIanotherChild' DataElement Name If the row is describing a Data Element , instead of a Data Class , then the name of the Data Element should be entered here. If supplied, the remaining information in this row will be used to create the Data Element inside the Data Class provided in the 'DataClass Path' column. However, if left blank, the remaining information in this row will be assigned to the Data Class entered in the 'DataClass Path' column. Description Enter a description which explains any contextual details relating to the Data Element or Data Class . Minimum Multiplicity The minimum number of instances of the Data Class or Data Element within the Data Model . Optional data has a minimum Multiplicity of 0, whereas mandatory data has a minimum Multiplicity of 1. Maximum Multiplicity The maximum number of instances of the Data Class or Data Element within the Data Model . Optional data has a maximum Multiplicity of 1, whereas data which occurs any number of times and therefore has no upper bound has a maximum Multiplicity of * which represents -1 internally. DataType Name This is the name given to the Data Type of the Data Element being described and must be included when entering information for Data Elements . The Data Type describes the range of possible values that the Data Element may take. There are four different Data Types stored within a Data Model : Enumeration : A constrained set of possible values. Each Enumeration Type defines a number of Enumeration Values which have a coded key and a human-readable value. Primitive : Data without further details on structure or referencing. Primitive Data Types include \u2018String\u2019 , \u2018Integer\u2019 or \u2018Date\u2019 . Reference : Data which refers to another Data Class within the same Data Model . Terminology : A structured collection of Enumeration Values which have relationships between different data terms. DataType Description This is a short description of the Data Type . If the same Data Type is used multiple times, then the first description entry will be used so subsequent description fields can be left blank. Reference to DataClass Path If the Data Element is a \u2018Reference\u2019 Data Type and therefore the Data Element is a reference to an instance of another Data Class , then the path to that Data Class should be entered here. This path must match a path provided in the 'DataClass Path' column using the same format with \u2018I\u2019 as the separator. This field cannot be used in conjunction with \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019 . Enumeration Key If the Data Element is an Enumeration Data Type , then the Key and Value of each Enumeration should be included in the \u2018Enumeration Key\u2019 and \u2018Enumeration Value\u2019 columns respectively, with one Key and Value per row. The Enumeration Key is the text or string that may appear in a column of the dataset. Enumeration Value The Enumeration Value is the data that corresponds to the Key . For example, for a yes/no question on a webform, the Value is either \u2018Yes\u2019 or \u2018No\u2019. Each Value is assigned a Key , which in this case could be \u20181\u2019 and \u20180\u2019 respectively. An Enumeration Value must be provided if an Enumeration Key has been entered. Once all the Enumeration Key and Value pairs have been entered you will need to merge the corresponding cells in the 'DataClass Path' column, and you may wish to merge the other relevant cells in the other columns too for consistency. For example, if you were entering the information for a Data Element that was an Enumeration Data Type then the Data Model key sheet would look similar to the below:","title":" 1.2 Data Model key sheet"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#2-import-data-model","text":"Once the Data Model import file has been completed, click the 'Import a Data Model' icon at the top right of the Model Tree . The 'Import a new Data Model' form will then appear on the right. Click the 'Select an importer' box and select 'Excel (XLSX) importer' from the dropdown menu. This will automatically load the rest of the form that you need to complete. You then need to select the relevant folder that you wish to import the Data Model into. You can do this by either typing in the folder name, or clicking the menu icon to the right of the 'Folder location' box. This will display the Model Tree , from which you can then select the relevant folder. You then need to tick the 'Finalised' box to indicate whether the Data Model you are importing is finalised. Although, it is recommended to keep the model as a draft until the gateway presentation of model descriptions has been decided. If the Data Model you are importing is intended to replace an existing Data Model within the current version of Mauro Data Mapper , then tick the 'Import as New Documentation Version' box. This means that the imported model will supersede any Data Models with the same name and will be assigned the latest documentation version of the existing Data Model . If you are importing a new Data Model , then make sure the 'Name' field in the Data Model listing sheet is unique, otherwise this will cause an error when importing. In the 'Source' box, click 'Choose file' and your file explorer will open. Navigate to the relevant Data Model import file and then select 'Import Model(s)' .","title":"2. Import Data Model"},{"location":"user-guides/import-data-model-from-excel/import-data-model-from-excel/#3-round-tripping-data-models","text":"Excel files can be safely used to 'Round trip' Data Model descriptions. This is essentially where: A Data Model is exported in the form of an Excel spreadsheet. This exported spreadsheet is edited or updated with new information. The new version of the spreadsheet is re-imported into Mauro Data Mapper and therefore automatically updates the existing Data Model . Note: 'Import as New Documentation Version' should be ticked when re-importing. This method is a quick and easy way to update existing Data Models as well as adding extra information. This can be particularly useful when Data Models have previously been imported using an alternative method, and therefore may only have the basic layout.","title":"3. Round Tripping Data Models"},{"location":"user-guides/merging-data-models/merging-data-models/","text":"This user guide explains how to merge Data Models including comparing and committing changes as well as how to successfully resolve conflicts. 1. Overview \u00b6 When working with multiple branches on the same model, it is likely that these multiple branch versions will have modifications such as additions, deletions and edits. Eventually, it will be necessary to merge all these back into one single draft version of the model. This is why every model has a main branch which acts as a single branching point and is also where models are finalised from. Merging models is quite a complex task, as it is important not to lose any potential changes during the merge process. This is why a special user interface has been designed for you to: Review all changes that have occurred. Assist in resolving conflicts , which are any changes that have occurred in two models that do not automatically align. Commit the changes you wish to make to a model. Merging can be applied to Data Models or Versioned Folders and includes merging changes within the model hierarchy as well. For example, changes made to Data Classes also count as modifications to a Data Model , and so on. 2. Terminology \u00b6 As merging is an advanced user concept, the following terms will be used throughout this user guide: Source The model or folder where changes are coming from . For example, the source would typically be a branch that is not the main branch. Target The model or folder where changes are going to . This is usually the main branch, but is not always the case. Change Any field that has changed between Source and Target . These will be: Addition - something has been added to Source which will then be added to Target Deletion - something has been removed from Source , or has been added to Target Modification - something exists in both Source and Target , but has different values Conflicts When a modification exists which cannot automatically be resolved. This typically happens when a piece of text has been changed and the same sentence has been modified. A user must manually resolve such conflicts to avoid losing important information or context Paths A fully qualified path is the full name of a model field (or a field within the model hierarchy). Paths are used as full unique identifiers to list all the changes within a model or folder Commit Saving all changes gathered together, from both Source and Target , and updating the end Target model or folder 2. Visualise branches \u00b6 It can be difficult to keep track of all branches in the Model Tree , especially if there are multiple branches. Therefore, Mauro allows you to visualise the branches as well as previous versions or forks in a merge graph . To find out how to access this merge graph see section '4. Merge graph' of our 'Branching, versioning and forking Data Models' user guide . 3. How to merge \u00b6 Firstly, select the relevant draft Data Model or Versioned Folder that has more than one branch in the Model Tree . Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Merge...' from the dropdown menu and this will navigate you to the merge interface. 4. The merge interface \u00b6 The interface for merging is split into several parts: 4.1 Merge controls \u00b6 The top of the interface lists: The label of the catalogue item that is being merged The Source branch to get changes from The Target branch to merge changes into. You are able to select from any available branch, typically main is initially selected Note: This is disabled if there are any changes being committed , which locks the target. To change the target branch again, cancel all changes being committed, start again and re-calculate the differences The Commit Changes button. Clicking this will finish the merge operation (see below) The Cancel button. Click this to cancel the merge operation and return to the catalogue without saving any changes 4.2 Changes list \u00b6 The 'Changes' tab lists all the changes made between the chosen Source and Target . All changes use path names to fully explain where they are in relation to the model hierarchy (see below for more details). The changes list displays all the changes that have been made, but each change must be moved to the 'Committing' list in order to actually save the change to the Target . See section '4.5 Comparison view' below. The changes list also allows you to: Filter the list by typing in a name Use all changes taken from the Source branch Use all changes taken from the Target branch 4.3 Committing list \u00b6 The 'Committing' tab lists all the changes that are going to be committed to the Target branch when saved. This is similar to the 'Changes' list, but also displays where the committed changes have come from. For example, from Source , Target , or a mixture of both. Note: After an initial comparison, the 'Committing' list may include some changes automatically. This is because Mauro has determined that these changes have no conflicts and so can be included by default, however you can undo these changes manually. 4.4 Path names \u00b6 Fully qualified paths are used to describe the field changes that have occurred in a model hierarchy. You can hover the mouse over each section of the path name to find out what they mean, but in summary: The first few capital letters represent the domain type . For example 'DM' is for 'Data Model' , 'DC' is for 'Data Class' , 'MD' is for 'Metadata' etc This prefix is followed by the label of that item Anything after a '$' represents the branch name the item is for Anything after a '@' represents the name of the field that has changed These then repeat if hierarchical, split by a '\u00bb' separator 4.5 Comparison view \u00b6 You can click on any item from the 'Changes' or 'Committing' lists to display a comparison view below. There are three types of comparison views that can be shown and this will depend on the change you have selected. If you have selected an addition change, as highlighted by a green '+' symbol, then the comparison view will display what will be added to the Target in its entirety. If you have selected a deletion change, as highlighted by a red '-' symbol, then the comparison view will display what will be removed from the Target in its entirety. If you have selected an edit change, as highlighted by a dark blue pencil symbol, then the comparison view will display the values in both Source and Target , highlighting the differences between the two. Using the comparison view you must then choose how this change should be committed. If the only option that appears at the top of the comparison view is an 'Accept' button, then clicking this will move the change to the Committing list. However, if there are changes to consider, then a range of options will appear at the top of the comparison view: 'Use \"Source\"' Commit the value that is in the Source branch, ignoring the Target copy 'Use \"Target\"' Commit the value that is in the Target branch, ignoring the Source copy 'Open Editor' Open an editor window to manually correct the value to commit to Target . See section '5. Resolving conflicts' below 'Link Scrolls' This will synchronise the scroll bars of the two values to better visualise the changes between them. To switch this off, click this button again 5. Resolving conflicts \u00b6 Sometimes Mauro is unable to determine exactly which change from which branch is required. Or there may be cases where you want some changes from Source and Target included together. To make these decisions, click on the 'Open Editor' button in the top right of the comparison view, this will bring up the Conflict Editor . The Conflict Editor will display: The full path to the field being reviewed The entire value from the Source branch, with differences highlighted (left) The entire value from the Target branch, with differences highlighted (right) The 'Resolved' value (initially the value from Source ) which can be manually edited and is located at the bottom of the Conflict Editor . You can click the differences highlighted in green in the Source and Target views to automatically add them to the Resolved editor, replacing any red highlighted '---' sections. Alternatively, you can manually remove all the red '---' sections from the Resolved editor and adjust any text as you wish. Use the Source and Target views as a guide for how your content should look. Once you are happy with how the resolved text looks, click on the 'Resolve Conflict' button. The text entered in the Resolved editor will now be the committed value for the Target branch. In the 'Committed' list, the field will be tagged with the 'Mixed' suffix to show that the change was manually modified. 6. Commit your changes \u00b6 When you are happy with your 'Committing' list, you can save and commit all changes to the Target branch. Note: Not every listed change has to be pushed to the 'Committing' list, but there must be at least one change to commit. This allows you to perform partial merges, for example to keep track of work in progress . Click on 'Commit Changes' which will display a 'Check in Branch' dialogue box. Here you will find a list of all the changes you are committing for a final review. You can enter an optional comment in the 'Commit Comment' box for this merge change. This will be tracked in the history of the Target catalogue item. You can also select the option to 'Delete Source Branch' once the merge has finished. This can be useful to tidy up remaining branches which are no longer needed once merged back into main . Once you have checked everything, click on the 'Commit' button to save all the selected changes to the Target catalogue item.","title":"Merging Data Models"},{"location":"user-guides/merging-data-models/merging-data-models/#1-overview","text":"When working with multiple branches on the same model, it is likely that these multiple branch versions will have modifications such as additions, deletions and edits. Eventually, it will be necessary to merge all these back into one single draft version of the model. This is why every model has a main branch which acts as a single branching point and is also where models are finalised from. Merging models is quite a complex task, as it is important not to lose any potential changes during the merge process. This is why a special user interface has been designed for you to: Review all changes that have occurred. Assist in resolving conflicts , which are any changes that have occurred in two models that do not automatically align. Commit the changes you wish to make to a model. Merging can be applied to Data Models or Versioned Folders and includes merging changes within the model hierarchy as well. For example, changes made to Data Classes also count as modifications to a Data Model , and so on.","title":"1. Overview"},{"location":"user-guides/merging-data-models/merging-data-models/#2-terminology","text":"As merging is an advanced user concept, the following terms will be used throughout this user guide: Source The model or folder where changes are coming from . For example, the source would typically be a branch that is not the main branch. Target The model or folder where changes are going to . This is usually the main branch, but is not always the case. Change Any field that has changed between Source and Target . These will be: Addition - something has been added to Source which will then be added to Target Deletion - something has been removed from Source , or has been added to Target Modification - something exists in both Source and Target , but has different values Conflicts When a modification exists which cannot automatically be resolved. This typically happens when a piece of text has been changed and the same sentence has been modified. A user must manually resolve such conflicts to avoid losing important information or context Paths A fully qualified path is the full name of a model field (or a field within the model hierarchy). Paths are used as full unique identifiers to list all the changes within a model or folder Commit Saving all changes gathered together, from both Source and Target , and updating the end Target model or folder","title":"2. Terminology"},{"location":"user-guides/merging-data-models/merging-data-models/#2-visualise-branches","text":"It can be difficult to keep track of all branches in the Model Tree , especially if there are multiple branches. Therefore, Mauro allows you to visualise the branches as well as previous versions or forks in a merge graph . To find out how to access this merge graph see section '4. Merge graph' of our 'Branching, versioning and forking Data Models' user guide .","title":"2. Visualise branches"},{"location":"user-guides/merging-data-models/merging-data-models/#3-how-to-merge","text":"Firstly, select the relevant draft Data Model or Versioned Folder that has more than one branch in the Model Tree . Once the item's details panel is displayed on the right, click the three vertical dot menu at the top right of the details panel. Select 'Merge...' from the dropdown menu and this will navigate you to the merge interface.","title":"3. How to merge"},{"location":"user-guides/merging-data-models/merging-data-models/#4-the-merge-interface","text":"The interface for merging is split into several parts:","title":"4. The merge interface"},{"location":"user-guides/merging-data-models/merging-data-models/#41-merge-controls","text":"The top of the interface lists: The label of the catalogue item that is being merged The Source branch to get changes from The Target branch to merge changes into. You are able to select from any available branch, typically main is initially selected Note: This is disabled if there are any changes being committed , which locks the target. To change the target branch again, cancel all changes being committed, start again and re-calculate the differences The Commit Changes button. Clicking this will finish the merge operation (see below) The Cancel button. Click this to cancel the merge operation and return to the catalogue without saving any changes","title":"4.1 Merge controls"},{"location":"user-guides/merging-data-models/merging-data-models/#42-changes-list","text":"The 'Changes' tab lists all the changes made between the chosen Source and Target . All changes use path names to fully explain where they are in relation to the model hierarchy (see below for more details). The changes list displays all the changes that have been made, but each change must be moved to the 'Committing' list in order to actually save the change to the Target . See section '4.5 Comparison view' below. The changes list also allows you to: Filter the list by typing in a name Use all changes taken from the Source branch Use all changes taken from the Target branch","title":"4.2 Changes list"},{"location":"user-guides/merging-data-models/merging-data-models/#43-committing-list","text":"The 'Committing' tab lists all the changes that are going to be committed to the Target branch when saved. This is similar to the 'Changes' list, but also displays where the committed changes have come from. For example, from Source , Target , or a mixture of both. Note: After an initial comparison, the 'Committing' list may include some changes automatically. This is because Mauro has determined that these changes have no conflicts and so can be included by default, however you can undo these changes manually.","title":"4.3 Committing list"},{"location":"user-guides/merging-data-models/merging-data-models/#44-path-names","text":"Fully qualified paths are used to describe the field changes that have occurred in a model hierarchy. You can hover the mouse over each section of the path name to find out what they mean, but in summary: The first few capital letters represent the domain type . For example 'DM' is for 'Data Model' , 'DC' is for 'Data Class' , 'MD' is for 'Metadata' etc This prefix is followed by the label of that item Anything after a '$' represents the branch name the item is for Anything after a '@' represents the name of the field that has changed These then repeat if hierarchical, split by a '\u00bb' separator","title":"4.4 Path names"},{"location":"user-guides/merging-data-models/merging-data-models/#45-comparison-view","text":"You can click on any item from the 'Changes' or 'Committing' lists to display a comparison view below. There are three types of comparison views that can be shown and this will depend on the change you have selected. If you have selected an addition change, as highlighted by a green '+' symbol, then the comparison view will display what will be added to the Target in its entirety. If you have selected a deletion change, as highlighted by a red '-' symbol, then the comparison view will display what will be removed from the Target in its entirety. If you have selected an edit change, as highlighted by a dark blue pencil symbol, then the comparison view will display the values in both Source and Target , highlighting the differences between the two. Using the comparison view you must then choose how this change should be committed. If the only option that appears at the top of the comparison view is an 'Accept' button, then clicking this will move the change to the Committing list. However, if there are changes to consider, then a range of options will appear at the top of the comparison view: 'Use \"Source\"' Commit the value that is in the Source branch, ignoring the Target copy 'Use \"Target\"' Commit the value that is in the Target branch, ignoring the Source copy 'Open Editor' Open an editor window to manually correct the value to commit to Target . See section '5. Resolving conflicts' below 'Link Scrolls' This will synchronise the scroll bars of the two values to better visualise the changes between them. To switch this off, click this button again","title":" 4.5 Comparison view"},{"location":"user-guides/merging-data-models/merging-data-models/#5-resolving-conflicts","text":"Sometimes Mauro is unable to determine exactly which change from which branch is required. Or there may be cases where you want some changes from Source and Target included together. To make these decisions, click on the 'Open Editor' button in the top right of the comparison view, this will bring up the Conflict Editor . The Conflict Editor will display: The full path to the field being reviewed The entire value from the Source branch, with differences highlighted (left) The entire value from the Target branch, with differences highlighted (right) The 'Resolved' value (initially the value from Source ) which can be manually edited and is located at the bottom of the Conflict Editor . You can click the differences highlighted in green in the Source and Target views to automatically add them to the Resolved editor, replacing any red highlighted '---' sections. Alternatively, you can manually remove all the red '---' sections from the Resolved editor and adjust any text as you wish. Use the Source and Target views as a guide for how your content should look. Once you are happy with how the resolved text looks, click on the 'Resolve Conflict' button. The text entered in the Resolved editor will now be the committed value for the Target branch. In the 'Committed' list, the field will be tagged with the 'Mixed' suffix to show that the change was manually modified.","title":"5. Resolving conflicts"},{"location":"user-guides/merging-data-models/merging-data-models/#6-commit-your-changes","text":"When you are happy with your 'Committing' list, you can save and commit all changes to the Target branch. Note: Not every listed change has to be pushed to the 'Committing' list, but there must be at least one change to commit. This allows you to perform partial merges, for example to keep track of work in progress . Click on 'Commit Changes' which will display a 'Check in Branch' dialogue box. Here you will find a list of all the changes you are committing for a final review. You can enter an optional comment in the 'Commit Comment' box for this merge change. This will be tracked in the history of the Target catalogue item. You can also select the option to 'Delete Source Branch' once the merge has finished. This can be useful to tidy up remaining branches which are no longer needed once merged back into main . Once you have checked everything, click on the 'Commit' button to save all the selected changes to the Target catalogue item.","title":"6. Commit your changes"},{"location":"user-guides/organising-data-models/organising-data-models/","text":"This user guide summarises how to organise Data Models including rearranging folders, adding classifiers and selecting favourites. 1. Overview \u00b6 There are three ways to organise Data Models in a Mauro catalogue: Folders Versioned Folders Classifiers 1.1 Folder \u00b6 A folder is a container that can store Data Models and other folders. 1.2 Versioned folders \u00b6 A versioned folder shares the same properties as a folder but also shares additional properties typically used by a Data Model . A versioned folder is able to store models and/or folders, but can also be version controlled like a Data Model . This means that the entire folder can be: Finalised Branched Versioned Forked Merged Finalising a versioned folder is the same as finalising every single, individual model within that folder, but in one action. 1.3 Classifier \u00b6 A Classifier is a container type which acts like a tag on a catalogue item. A Classifier can be attached to one or more catalogue items, such as models, as a way of grouping items together. 2. Create a folder \u00b6 To find out how to create a folder see section '1. Create a new Folder' of our 'Create a Data Model' user guide . These instructions apply to both folders and versioned folders, as both are the same in this case. 3. Create a Classification \u00b6 To create a Classifier make sure the 'Classifications' tab above the Model Tree is selected. Then click the 'Create a new Classification' icon to the right of the 'Search classifications' box. This will bring up a 'Create a new Classifier' dialogue box. Enter a Classifier name and click 'Add Classifier' . 4. Moving folders \u00b6 To move a folder or a versioned folder, left click and hold on the relevant folder in the Model Tree . You can then drag this folder into either another folder, versioned folder or into the top level dropzone that appears. Once the folder has been dragged to the desired location and the top level folder is highlighted, release the left mouse button to confirm the action. There are some restrictions on moving versioned folders. Firstly, a versioned folder cannot be dragged into another versioned folder. Secondly, items cannot be dragged into versioned folder that are finalised. 5. Adding to folders \u00b6 Catalogue items can be created in folders and versioned folders in a number of different ways. 5.1 Create menu \u00b6 Right click on the relevant folder or versioned folder and then hover over the '+ Create' option that appears at the top of the menu. A secondary menu will then appear, from which you can add another 'Folder' , 'Data Model' or 'Code Set' . 5.2 Drag items \u00b6 In a similar way to moving folders, you can also move existing catalogue items into both folders and versioned folders by dragging them. Left click and hold on the item you wish to move in the Model Tree . Drag it to the desired location and once highlighted, release the left mouse button. 5.3 Import \u00b6 You can also import Data Models into folders and versioned folders. To find out how to do this from an Excel spreadsheet, see our 'Import a Data Model from Excel' user guide . 6. Adding to Classifiers \u00b6 To add a catalogue item to a Classifier , select the relevant item in the Model Tree . The description panel will then display on the right hand side of your screen. Under the 'Description' tab, click the 'Edit' button and then select 'Edit' from the dropdown menu. This will bring up an 'Edit Default Profile' form. In the 'Classifications' field, tick the classifiers from the dropdown menu that you would like to associate with this item. Once you've selected all the relevant classifiers, click 'Save' to confirm your changes. 7. Add, remove and view Favourites \u00b6 To easily access catalogue items, you can mark them as Favourites . To do this, right click on the catalogue item in the Model Tree and then select the 'Add to Favourites' option in the menu. This will add a star to the right of the catalogue item name in the Model Tree . To remove a catalogue item from your favourites list, right click on the catalogue item in the Model Tree and then select 'Remove from Favourites' from the menu. To view a list of all the catalogue items that have been bookmarked as favourites, select the 'Favourites' tab above the Model Tree .","title":"Organising Data Models"},{"location":"user-guides/organising-data-models/organising-data-models/#1-overview","text":"There are three ways to organise Data Models in a Mauro catalogue: Folders Versioned Folders Classifiers","title":"1. Overview"},{"location":"user-guides/organising-data-models/organising-data-models/#11-folder","text":"A folder is a container that can store Data Models and other folders.","title":"1.1 Folder"},{"location":"user-guides/organising-data-models/organising-data-models/#12-versioned-folders","text":"A versioned folder shares the same properties as a folder but also shares additional properties typically used by a Data Model . A versioned folder is able to store models and/or folders, but can also be version controlled like a Data Model . This means that the entire folder can be: Finalised Branched Versioned Forked Merged Finalising a versioned folder is the same as finalising every single, individual model within that folder, but in one action.","title":"1.2 Versioned folders"},{"location":"user-guides/organising-data-models/organising-data-models/#13-classifier","text":"A Classifier is a container type which acts like a tag on a catalogue item. A Classifier can be attached to one or more catalogue items, such as models, as a way of grouping items together.","title":"1.3 Classifier"},{"location":"user-guides/organising-data-models/organising-data-models/#2-create-a-folder","text":"To find out how to create a folder see section '1. Create a new Folder' of our 'Create a Data Model' user guide . These instructions apply to both folders and versioned folders, as both are the same in this case.","title":"2. Create a folder"},{"location":"user-guides/organising-data-models/organising-data-models/#3-create-a-classification","text":"To create a Classifier make sure the 'Classifications' tab above the Model Tree is selected. Then click the 'Create a new Classification' icon to the right of the 'Search classifications' box. This will bring up a 'Create a new Classifier' dialogue box. Enter a Classifier name and click 'Add Classifier' .","title":"3. Create a Classification"},{"location":"user-guides/organising-data-models/organising-data-models/#4-moving-folders","text":"To move a folder or a versioned folder, left click and hold on the relevant folder in the Model Tree . You can then drag this folder into either another folder, versioned folder or into the top level dropzone that appears. Once the folder has been dragged to the desired location and the top level folder is highlighted, release the left mouse button to confirm the action. There are some restrictions on moving versioned folders. Firstly, a versioned folder cannot be dragged into another versioned folder. Secondly, items cannot be dragged into versioned folder that are finalised.","title":"4. Moving folders"},{"location":"user-guides/organising-data-models/organising-data-models/#5-adding-to-folders","text":"Catalogue items can be created in folders and versioned folders in a number of different ways.","title":"5. Adding to folders"},{"location":"user-guides/organising-data-models/organising-data-models/#51-create-menu","text":"Right click on the relevant folder or versioned folder and then hover over the '+ Create' option that appears at the top of the menu. A secondary menu will then appear, from which you can add another 'Folder' , 'Data Model' or 'Code Set' .","title":"5.1 Create menu"},{"location":"user-guides/organising-data-models/organising-data-models/#52-drag-items","text":"In a similar way to moving folders, you can also move existing catalogue items into both folders and versioned folders by dragging them. Left click and hold on the item you wish to move in the Model Tree . Drag it to the desired location and once highlighted, release the left mouse button.","title":"5.2 Drag items"},{"location":"user-guides/organising-data-models/organising-data-models/#53-import","text":"You can also import Data Models into folders and versioned folders. To find out how to do this from an Excel spreadsheet, see our 'Import a Data Model from Excel' user guide .","title":"5.3 Import"},{"location":"user-guides/organising-data-models/organising-data-models/#6-adding-to-classifiers","text":"To add a catalogue item to a Classifier , select the relevant item in the Model Tree . The description panel will then display on the right hand side of your screen. Under the 'Description' tab, click the 'Edit' button and then select 'Edit' from the dropdown menu. This will bring up an 'Edit Default Profile' form. In the 'Classifications' field, tick the classifiers from the dropdown menu that you would like to associate with this item. Once you've selected all the relevant classifiers, click 'Save' to confirm your changes.","title":"6. Adding to Classifiers"},{"location":"user-guides/organising-data-models/organising-data-models/#7-add-remove-and-view-favourites","text":"To easily access catalogue items, you can mark them as Favourites . To do this, right click on the catalogue item in the Model Tree and then select the 'Add to Favourites' option in the menu. This will add a star to the right of the catalogue item name in the Model Tree . To remove a catalogue item from your favourites list, right click on the catalogue item in the Model Tree and then select 'Remove from Favourites' from the menu. To view a list of all the catalogue items that have been bookmarked as favourites, select the 'Favourites' tab above the Model Tree .","title":"7. Add, remove and view Favourites"},{"location":"user-guides/permissions/permissions/","text":"This user guide explains the multiple levels of access to catalogue items and adminstration supported in Mauro Data Mapper . 1. Access levels \u00b6 In total there are six different access levels within Mauro. These are divided into Model roles, Container roles and Application roles. Each level along with it's permissions are summarised below in ascending order. Reader \u00b6 Read models Reviewer \u00b6 As Reader (above) plus: Add comments Author \u00b6 As Reviewer (above) plus: Add and edit descriptions Editor \u00b6 As Author (above) plus: Create models Edit model structures Mark models as 'deleted' Container Administrator \u00b6 As Editor (above) plus: Permanently delete models Container Group Administrator \u00b6 As Container Administrator (above) plus: Add container groups Assign groups to containers Add users to container groups Remove users from container groups 2. Users \u00b6 Users can be created in Mauro to control authenticated access to the catalogue. Some or all of the catalogue may be made publicly available, which means that anonymous users may view the contents. However, for any non-public content an authenticated user can only see that catalogue content. When a new Mauro instance is created, a default administrator account will be created to allow you administration access and manage further users. By default this account is: Username: admin@maurodatamapper.com Password: password Warning Please change this password as soon as possible to avoid security issues. To find out how to change your password see section '4. Change your password' of our 'User profile' user guide . An administrator is then able to manage and create further users in Mauro via the administrator dashboard. See section '4. Manage users' of our 'Admin functionality' user guide to find out how to do this. To differentiate user access levels and determine what each user is allowed to action, each user should be assigned to one of these groups: Readers Editors Administrators 3. Groups \u00b6 Groups are used to manage collections of users and their permissions as a whole. Each Mauro instance is created with the following core user groups defined: Readers Users are only able to read the Mauro catalogue content and cannot modify anything. Editors Users can read and modify the Mauro catalogue content, but are not able to perform administrative actions, such as creating new users. Administrators Users are able to perform any action in Mauro, including reading and modifying content and administration tasks. Each user should be placed into one of these groups initially to govern what permissions they have, such as: Read access Write access Delete access, etc A user can belong to more than one group, in which case the permissions for each group are then combined. Further groups can also be created to define logical collections of users together. To find out how to manage groups, see section '6. Manage groups' of our 'Admin functionality' user guide . Groups can then be used to assign access to particular catalogue items for a collection of users as a whole, as explained in the next section. 4. Catalogue items \u00b6 Catalogue items can be restricted to be globally accessible to allow them to be either: Fully public Accessible to authenticated users only Accessible only to certain groups of users These restrictions can be placed on: Folders Data Models Code Sets Terminologies Reference Data Models These permissions also propogate down the catalogue item hierarchy. For instance, if a folder were made 'Publicly available' , then all models and sub-folders within it would automatically become 'Publicly available' as well. 5. Manage User and Group Access \u00b6 Only Editors can manage the user and group access. To do this, select the relevant Data Model or Folder in the Model Tree . Then click the 'User & Group Access' button at the top right of the details panel. This will bring up a 'Restrict user access globally' dialogue box. Here you can select whether you would like the model or folder to be 'Publicly readable' , 'Readable by authenticated users' or 'Restrict user access based on groups' . 5.1 Publicly readable \u00b6 Any catalogue item made publicly readable allows anyone to view it, whether they are an anonymous or registered user. This is useful when building a catalogue for public consumption. Note that although catalogue items may be publicly readable, only authenticated users in the Editors group or higher are able to modify those catalogue items. 5.2 Authenticated users \u00b6 Any catalogue item made readable by authenticated users guarantees that only registered users can view this item. This is useful when building an internal catalogue for an organisation and viewing access must be governed by an administrator to manage users. Again, only users in the Editors group or higher can modifiy these catalogue items. 5.3 Group access \u00b6 Zero or more user groups may be assigned to a catalogue item to control which collection(s) of users may view the catalogue item. This allows a further level of control compared to the 'Authenticated users' option. To add group access to a catalogue item, click the '+Add Group' button on the 'Restrict user access globally dialogue box' . This will add a row to the group list that you will then need to populate. First, you need to select a group name which determines the collection of users to allow. Click the 'Search...' box under 'Group name' and select the relevant group from the dropdown menu. You then need to select which access level you would like to assign to the group. This determines what the collection of users are capable of doing to this catalogue item. Available access levels are listed below, with the permissions of each detailed in '1. Access levels' . Reader Reviewer Author Editor Container Administrator (Folders only) Container Group Administrator (Folders only) Once you've decided on a suitable access level , click the 'Search...' box under 'Access Level' and select the relevant access option. Then click the green tick to the right of the row to save your changes. You can remove group access from a catalogue item by clicking the red bin to the right of the group you wish to remove.","title":"Permissions"},{"location":"user-guides/permissions/permissions/#1-access-levels","text":"In total there are six different access levels within Mauro. These are divided into Model roles, Container roles and Application roles. Each level along with it's permissions are summarised below in ascending order.","title":"1. Access levels"},{"location":"user-guides/permissions/permissions/#reader","text":"Read models","title":"Reader"},{"location":"user-guides/permissions/permissions/#reviewer","text":"As Reader (above) plus: Add comments","title":"Reviewer"},{"location":"user-guides/permissions/permissions/#author","text":"As Reviewer (above) plus: Add and edit descriptions","title":"Author"},{"location":"user-guides/permissions/permissions/#editor","text":"As Author (above) plus: Create models Edit model structures Mark models as 'deleted'","title":"Editor"},{"location":"user-guides/permissions/permissions/#container-administrator","text":"As Editor (above) plus: Permanently delete models","title":"Container Administrator"},{"location":"user-guides/permissions/permissions/#container-group-administrator","text":"As Container Administrator (above) plus: Add container groups Assign groups to containers Add users to container groups Remove users from container groups","title":"Container Group Administrator"},{"location":"user-guides/permissions/permissions/#2-users","text":"Users can be created in Mauro to control authenticated access to the catalogue. Some or all of the catalogue may be made publicly available, which means that anonymous users may view the contents. However, for any non-public content an authenticated user can only see that catalogue content. When a new Mauro instance is created, a default administrator account will be created to allow you administration access and manage further users. By default this account is: Username: admin@maurodatamapper.com Password: password Warning Please change this password as soon as possible to avoid security issues. To find out how to change your password see section '4. Change your password' of our 'User profile' user guide . An administrator is then able to manage and create further users in Mauro via the administrator dashboard. See section '4. Manage users' of our 'Admin functionality' user guide to find out how to do this. To differentiate user access levels and determine what each user is allowed to action, each user should be assigned to one of these groups: Readers Editors Administrators","title":"2. Users"},{"location":"user-guides/permissions/permissions/#3-groups","text":"Groups are used to manage collections of users and their permissions as a whole. Each Mauro instance is created with the following core user groups defined: Readers Users are only able to read the Mauro catalogue content and cannot modify anything. Editors Users can read and modify the Mauro catalogue content, but are not able to perform administrative actions, such as creating new users. Administrators Users are able to perform any action in Mauro, including reading and modifying content and administration tasks. Each user should be placed into one of these groups initially to govern what permissions they have, such as: Read access Write access Delete access, etc A user can belong to more than one group, in which case the permissions for each group are then combined. Further groups can also be created to define logical collections of users together. To find out how to manage groups, see section '6. Manage groups' of our 'Admin functionality' user guide . Groups can then be used to assign access to particular catalogue items for a collection of users as a whole, as explained in the next section.","title":"3. Groups"},{"location":"user-guides/permissions/permissions/#4-catalogue-items","text":"Catalogue items can be restricted to be globally accessible to allow them to be either: Fully public Accessible to authenticated users only Accessible only to certain groups of users These restrictions can be placed on: Folders Data Models Code Sets Terminologies Reference Data Models These permissions also propogate down the catalogue item hierarchy. For instance, if a folder were made 'Publicly available' , then all models and sub-folders within it would automatically become 'Publicly available' as well.","title":"4. Catalogue items"},{"location":"user-guides/permissions/permissions/#5-manage-user-and-group-access","text":"Only Editors can manage the user and group access. To do this, select the relevant Data Model or Folder in the Model Tree . Then click the 'User & Group Access' button at the top right of the details panel. This will bring up a 'Restrict user access globally' dialogue box. Here you can select whether you would like the model or folder to be 'Publicly readable' , 'Readable by authenticated users' or 'Restrict user access based on groups' .","title":"5. Manage User and Group Access"},{"location":"user-guides/permissions/permissions/#51-publicly-readable","text":"Any catalogue item made publicly readable allows anyone to view it, whether they are an anonymous or registered user. This is useful when building a catalogue for public consumption. Note that although catalogue items may be publicly readable, only authenticated users in the Editors group or higher are able to modify those catalogue items.","title":"5.1 Publicly readable"},{"location":"user-guides/permissions/permissions/#52-authenticated-users","text":"Any catalogue item made readable by authenticated users guarantees that only registered users can view this item. This is useful when building an internal catalogue for an organisation and viewing access must be governed by an administrator to manage users. Again, only users in the Editors group or higher can modifiy these catalogue items.","title":"5.2 Authenticated users"},{"location":"user-guides/permissions/permissions/#53-group-access","text":"Zero or more user groups may be assigned to a catalogue item to control which collection(s) of users may view the catalogue item. This allows a further level of control compared to the 'Authenticated users' option. To add group access to a catalogue item, click the '+Add Group' button on the 'Restrict user access globally dialogue box' . This will add a row to the group list that you will then need to populate. First, you need to select a group name which determines the collection of users to allow. Click the 'Search...' box under 'Group name' and select the relevant group from the dropdown menu. You then need to select which access level you would like to assign to the group. This determines what the collection of users are capable of doing to this catalogue item. Available access levels are listed below, with the permissions of each detailed in '1. Access levels' . Reader Reviewer Author Editor Container Administrator (Folders only) Container Group Administrator (Folders only) Once you've decided on a suitable access level , click the 'Search...' box under 'Access Level' and select the relevant access option. Then click the green tick to the right of the row to save your changes. You can remove group access from a catalogue item by clicking the red bin to the right of the group you wish to remove.","title":"5.3 Group access"},{"location":"user-guides/publish-subscribe/publish-subscribe/","text":"Mauro Data Mapper instances are able to provide an Atom feed that publishes all finalised versions of Data Models in the catalogue. This enables other Mauro Data Mapper instances to subscribe to these feeds and view the Federated Data Models available from external catalogues. Furthermore, a Mauro Data Mapper instance has the ability to subscribe to one or more of these Federated Data Models to import them into their own catalogues. This user guide will explain how. 1. Feeds and API Keys \u00b6 All Mauro Data Mapper server instances expose the URL api/feeds/all which returns the Atom syndication data. For another server instance to view all available data in that feed, each server instance must generate an API key and provide it to the required external parties. 1.1 Create or copy an API Key \u00b6 API keys may be set up through the web interface or via the API. To create an API key, you must be logged in with a username and password. On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu. This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys. For each API belonging to the user, the list displays: Name This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys used for different purposes. Key This is the key itself, which is a UUID, unique to this user. Expiry date This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be 'refreshed' before expiry. Refreshable Specifies whether an API key can be refreshed once expired. Status Details whether an API key is 'Active' or 'Disabled' . You can either copy an existing key value or create a new one to provide to an external party. To copy the key value to your clipboard click the 'Copy' button in the relevant 'Key' box. To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete. Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes. For more information about creating and managing API Keys, please see the API Keys page in the REST API documentation 2. Add a Subscribed Catalogue \u00b6 Note: The following can only be carried out by an administrator. Administrators can manage the set of Subscribed Catalogues a Mauro Data Mapper instance connects to. To manage the catalogues, click the white arrow next to your user profile and select 'Subscribed catalogues' from the dropdown menu. This will take you to a list of existing catalogues. Here you can test, edit and delete catalogues by clicking the three vertical dots to the right of each item in the list which will display a dropdown menu. For each existing catalogue, the list displays: Label A unique label to help identify the subscribed catalogue. Description A description to help explain what the subscribed catalogue is for. URL The URL to the server instance to connect to. API Key The generated key value to authenticate against the server instance. Refresh period State how often, in days, this server instance should refresh the subscribed catalogue feed to check for new data. If not provided, a suitable default will be used instead. To add a Subscribed Catalogue , you will need: The URL to the Mauro Data Mapper server instance to connect to. The API key to authenticate on that server instance. Note: The URL only needs to refer to the instance and not the absolute URL to the Atom feed. Click the '+Add' button at the top right of the 'Catalogues' list. This will open an 'Add Subscribed Catalogue' form which you will need to complete. Enter the label, description, URL, API Key and refresh period as described above. Once completed, click 'Add subscription' and a green notification box should appear at the bottom right of your screen confirming that the 'subscribed catalogue saved successfully' . 3. Federated Data Models \u00b6 When at least one subscribed catalogue has been configured, any signed in user may be able to view the Federated Data Models from those catalogues in the Model Tree . The Model Tree will be updated to list two new root nodes: This catalogue Lists all the folders and Data Models contained within this Mauro Data Mapper instance. External catalogues Lists each subscribed catalogue created for this Mauro Data Mapper instance. Expanding each subscribed catalogue will list all readable Federated Data Models that can be individually subscribed to. 3.1 Subscribe to a Federated Data Model \u00b6 You can view the subscription status of a Federated Data Model by selecting the relevant model in the Model Tree which will display it's details panel. To subscribe to a Federated Data Model so that it is included in the current catalogue, click 'Subscribe' and a 'Subscribe to Data Model' dialogue box will appear. Click the menu icon to the right of the box to display the Model Tree . Select the folder you wish to import the Federated Data Model into and click '+Subscribe' . The status of this model will now change to 'Subscribed' in the details panel, meaning that this catalogue will automatically maintain this subscription until the user chooses to unsubscribe. 3.2 Imported Federated Data Models \u00b6 Once subscribed to, each Federated Data Model will be automatically imported into the current Mauro Data Mapper catalogue at the chosen target folder, including the links between model versions. It is possible to manually synchronise the subscribed catalogues. To do this, select the subscribed catalogue in the Model Tree . In the relevant details panel, click the 'Synchronise' button. Each subscribed catalogue will also list version updates when the current Mauro Data Mapper instance refreshes the Atom feed, the frequency of which is determined during configuration. New versions of a Data Model will appear with their updated versions listed for you to also subscribe to, which is a manual operation.","title":"Publish / Subscribe"},{"location":"user-guides/publish-subscribe/publish-subscribe/#1-feeds-and-api-keys","text":"All Mauro Data Mapper server instances expose the URL api/feeds/all which returns the Atom syndication data. For another server instance to view all available data in that feed, each server instance must generate an API key and provide it to the required external parties.","title":"1. Feeds and API Keys"},{"location":"user-guides/publish-subscribe/publish-subscribe/#11-create-or-copy-an-api-key","text":"API keys may be set up through the web interface or via the API. To create an API key, you must be logged in with a username and password. On the top right of the menu header, click the white arrow next to your user profile and select 'API keys' from the dropdown menu. This will take you to a list of existing API keys that belong to you. Here, you can enable or re-enable existing keys by clicking the three vertical dots to the right of each item in the list. This dropdown menu also allows you to delete keys. For each API belonging to the user, the list displays: Name This is a human readable name for each API key, which must be unique for each user. Users can use the name to differentiate between different keys used for different purposes. Key This is the key itself, which is a UUID, unique to this user. Expiry date This is the date from which the API key will no longer be valid. For security purposes, every API key is given an expiry date, but may be 'refreshed' before expiry. Refreshable Specifies whether an API key can be refreshed once expired. Status Details whether an API key is 'Active' or 'Disabled' . You can either copy an existing key value or create a new one to provide to an external party. To copy the key value to your clipboard click the 'Copy' button in the relevant 'Key' box. To create a new API key, click the 'Create Key' button at the top right of the list. This will open a 'Create an API Key' form which you will need to complete. Enter the human-readable name of the API key (as described above), choose a number of days before expiry and select whether the key is to be refreshable on expiry or not. Once completed, click 'Create Key' to confirm your changes. For more information about creating and managing API Keys, please see the API Keys page in the REST API documentation","title":"1.1 Create or copy an API Key"},{"location":"user-guides/publish-subscribe/publish-subscribe/#2-add-a-subscribed-catalogue","text":"Note: The following can only be carried out by an administrator. Administrators can manage the set of Subscribed Catalogues a Mauro Data Mapper instance connects to. To manage the catalogues, click the white arrow next to your user profile and select 'Subscribed catalogues' from the dropdown menu. This will take you to a list of existing catalogues. Here you can test, edit and delete catalogues by clicking the three vertical dots to the right of each item in the list which will display a dropdown menu. For each existing catalogue, the list displays: Label A unique label to help identify the subscribed catalogue. Description A description to help explain what the subscribed catalogue is for. URL The URL to the server instance to connect to. API Key The generated key value to authenticate against the server instance. Refresh period State how often, in days, this server instance should refresh the subscribed catalogue feed to check for new data. If not provided, a suitable default will be used instead. To add a Subscribed Catalogue , you will need: The URL to the Mauro Data Mapper server instance to connect to. The API key to authenticate on that server instance. Note: The URL only needs to refer to the instance and not the absolute URL to the Atom feed. Click the '+Add' button at the top right of the 'Catalogues' list. This will open an 'Add Subscribed Catalogue' form which you will need to complete. Enter the label, description, URL, API Key and refresh period as described above. Once completed, click 'Add subscription' and a green notification box should appear at the bottom right of your screen confirming that the 'subscribed catalogue saved successfully' .","title":"2. Add a Subscribed Catalogue"},{"location":"user-guides/publish-subscribe/publish-subscribe/#3-federated-data-models","text":"When at least one subscribed catalogue has been configured, any signed in user may be able to view the Federated Data Models from those catalogues in the Model Tree . The Model Tree will be updated to list two new root nodes: This catalogue Lists all the folders and Data Models contained within this Mauro Data Mapper instance. External catalogues Lists each subscribed catalogue created for this Mauro Data Mapper instance. Expanding each subscribed catalogue will list all readable Federated Data Models that can be individually subscribed to.","title":"3. Federated Data Models"},{"location":"user-guides/publish-subscribe/publish-subscribe/#31-subscribe-to-a-federated-data-model","text":"You can view the subscription status of a Federated Data Model by selecting the relevant model in the Model Tree which will display it's details panel. To subscribe to a Federated Data Model so that it is included in the current catalogue, click 'Subscribe' and a 'Subscribe to Data Model' dialogue box will appear. Click the menu icon to the right of the box to display the Model Tree . Select the folder you wish to import the Federated Data Model into and click '+Subscribe' . The status of this model will now change to 'Subscribed' in the details panel, meaning that this catalogue will automatically maintain this subscription until the user chooses to unsubscribe.","title":"3.1 Subscribe to a Federated Data Model"},{"location":"user-guides/publish-subscribe/publish-subscribe/#32-imported-federated-data-models","text":"Once subscribed to, each Federated Data Model will be automatically imported into the current Mauro Data Mapper catalogue at the chosen target folder, including the links between model versions. It is possible to manually synchronise the subscribed catalogues. To do this, select the subscribed catalogue in the Model Tree . In the relevant details panel, click the 'Synchronise' button. Each subscribed catalogue will also list version updates when the current Mauro Data Mapper instance refreshes the Atom feed, the frequency of which is determined during configuration. New versions of a Data Model will appear with their updated versions listed for you to also subscribe to, which is a manual operation.","title":"3.2 Imported Federated Data Models"},{"location":"user-guides/user-profile/user-profile/","text":"This user guide will show you how to add and edit information in your user profile. 1. Edit your profile \u00b6 When logged in to Mauro Data Mapper , your user profile is displayed on the top right of the menu header. Click the white arrow and a dropdown menu will appear. The options available in this menu will depend on whether you are an 'Editor' or an 'Administrator' . To edit your profile, select the 'My profile' option from the dropdown menu. This will take you to a page displaying your details, with the compulsory fields indicated by a *. These will already be completed according to the information entered when you were registered. Complete the fields below: First and last name This is used for identification. Organisation This allows other Mauro Data Mapper users to know which organisation you belong to. Role in Organisation This allows other Mauro Data Mapper users to know your role within your organisation. Once you have amended your details, click 'Save profile' and your changes will be updated. 2. Change your profile picture \u00b6 To add or edit an image to your profile picture, navigate to the 'My profile' page and click the pencil icon on your profile picture. A dropdown menu will appear with the option to either 'Change image' or 'Remove image' . To change the image for your profile picture, select 'Change image' and the file explorer on your computer will open. Navigate to the image you would like to use and then click 'Open' . This image will then be imported into your 'My profile' page, with a preview displayed on the right. You can then resize the image by hovering over the corners of the highlighted box until an arrow appears. Click and hold the left mouse button and drag the box until it is the desired size. To drag the entire highlighted box, hover your cursor inside the box until the 4 headed arrow appears. Click and hold the left mouse button until the box is in the desired location. Once you have finished adjusting your profile picture, click 'Update profile image' and a notification will appear on the bottom right of your screen to confirm the update. Then click 'Save profile' to save all changes. To remove a profile picture, click the pencil icon and select 'Remove image' from the dropdown menu. Again, a notification will appear at the bottom right of your screen to confirm the change. Then click 'Save profile' . 3. Update preferences \u00b6 You can also specify some interface preferences that are permanently saved to your user profile. To do this, navigate to the 'Preferences' page on your user profile and choose the options that suit you. Number of records per table By default this is set to 20. However depending on screen size, you may wish to view 5, 10 or 50. Select the option that you prefer. Always expand descriptions The next option allows descriptions to be expanded automatically, regardless of the character count. This is unchecked by default. However, those with larger screens may prefer to always expand descriptions in tables. If so, check this box. Include Superseded Document Models in Data Model Tree The final option determines whether you want the Finder Panel to show superseded document models within the Data Model Tree view. This is unchecked by default as typically these models represent invalid or outdated descriptions. However, feel free to check this box to change this. Once you have selected all your preferences, click 'Save preferences' and these will now be saved for future sessions. 4. Change your password \u00b6 To change your password, go to the 'Password' page under 'Account settings' . Enter your old password in the first box and your new password in the second box. The colour bar underneath will automatically change from red to green when you start typing. The width of the green bar will then change according to the strength of your chosen password. You can use both lower and upper case letters as well as special characters and numbers. If the colour bar changes to dark blue, then the password is too long and won't be accepted. To view or hide your passwords, click the eye icon to the right of each box. Once you have finalised your password, re-enter it into the 'Confirm password' box and then click 'Change password' to confirm the change.","title":"User profile"},{"location":"user-guides/user-profile/user-profile/#1-edit-your-profile","text":"When logged in to Mauro Data Mapper , your user profile is displayed on the top right of the menu header. Click the white arrow and a dropdown menu will appear. The options available in this menu will depend on whether you are an 'Editor' or an 'Administrator' . To edit your profile, select the 'My profile' option from the dropdown menu. This will take you to a page displaying your details, with the compulsory fields indicated by a *. These will already be completed according to the information entered when you were registered. Complete the fields below: First and last name This is used for identification. Organisation This allows other Mauro Data Mapper users to know which organisation you belong to. Role in Organisation This allows other Mauro Data Mapper users to know your role within your organisation. Once you have amended your details, click 'Save profile' and your changes will be updated.","title":"1. Edit your profile"},{"location":"user-guides/user-profile/user-profile/#2-change-your-profile-picture","text":"To add or edit an image to your profile picture, navigate to the 'My profile' page and click the pencil icon on your profile picture. A dropdown menu will appear with the option to either 'Change image' or 'Remove image' . To change the image for your profile picture, select 'Change image' and the file explorer on your computer will open. Navigate to the image you would like to use and then click 'Open' . This image will then be imported into your 'My profile' page, with a preview displayed on the right. You can then resize the image by hovering over the corners of the highlighted box until an arrow appears. Click and hold the left mouse button and drag the box until it is the desired size. To drag the entire highlighted box, hover your cursor inside the box until the 4 headed arrow appears. Click and hold the left mouse button until the box is in the desired location. Once you have finished adjusting your profile picture, click 'Update profile image' and a notification will appear on the bottom right of your screen to confirm the update. Then click 'Save profile' to save all changes. To remove a profile picture, click the pencil icon and select 'Remove image' from the dropdown menu. Again, a notification will appear at the bottom right of your screen to confirm the change. Then click 'Save profile' .","title":"2. Change your profile picture"},{"location":"user-guides/user-profile/user-profile/#3-update-preferences","text":"You can also specify some interface preferences that are permanently saved to your user profile. To do this, navigate to the 'Preferences' page on your user profile and choose the options that suit you. Number of records per table By default this is set to 20. However depending on screen size, you may wish to view 5, 10 or 50. Select the option that you prefer. Always expand descriptions The next option allows descriptions to be expanded automatically, regardless of the character count. This is unchecked by default. However, those with larger screens may prefer to always expand descriptions in tables. If so, check this box. Include Superseded Document Models in Data Model Tree The final option determines whether you want the Finder Panel to show superseded document models within the Data Model Tree view. This is unchecked by default as typically these models represent invalid or outdated descriptions. However, feel free to check this box to change this. Once you have selected all your preferences, click 'Save preferences' and these will now be saved for future sessions.","title":"3. Update preferences"},{"location":"user-guides/user-profile/user-profile/#4-change-your-password","text":"To change your password, go to the 'Password' page under 'Account settings' . Enter your old password in the first box and your new password in the second box. The colour bar underneath will automatically change from red to green when you start typing. The width of the green bar will then change according to the strength of your chosen password. You can use both lower and upper case letters as well as special characters and numbers. If the colour bar changes to dark blue, then the password is too long and won't be accepted. To view or hide your passwords, click the eye icon to the right of each box. Once you have finalised your password, re-enter it into the 'Confirm password' box and then click 'Change password' to confirm the change.","title":"4. Change your password"},{"location":"user-guides/version-data-models/version-data-models/","text":"This user guide will show you how to create working versions of Data Models as well as how to merge completed Data Models into the main branch. This ensures that you can make changes without adversely affecting other users of Mauro Data Mapper . 1. Versioning \u00b6 Data Models can only be versioned if they meet the below criteria: The Data Model must be finalised The Data Model must have a version number If your Data Model satisfies both of these conditions, then you will be able to create a new version. To do this, highlight the relevant Data Model in the Model Tree and click the three vertical dots at the top right of the details panel. Select 'Create a New Version' from the dropdown menu. You will then be presented with three options: New Fork This will create a copy of the Data Model with a new name and a new 'main' branch. Use this option if you are planning on taking this model in a new direction, or under a new authority. New Version This will create a copy of the Data Model under the 'main' branch. Use this option if you want to create the next iteration of this model. New Branch This will create a copy of the Data Model in a new branch, which you can choose the name for. Use this option if you want to make some changes that you subsequently wish to merge back into 'main' . 1.1 New Branch \u00b6 For this user guide, we will select 'New Branch' . A text box will then appear where you can enter a 'Branch name' . This will distinguish your Data Model from the others in your Data Model family. Once completed, click 'Add Branch' . This will create a copy of the Data Model for you to edit. If this is the first branch, then a 'main' branch will also be created. This will be the branch that changes from other branches will be merged into. 2. Merging \u00b6 To merge completed changes of a Data Model into the main branch, click the three vertical dots at the top right of the details panel. Select 'Merge' from the dropdown menu and then click 'Merge Model' . This will take you to a merge screen which is split into three columns. The left column shows the source branch which you are merging from. The middle column displays the items to be merged and the right column shows the items currently on the main branch. The right and left columns will display a tree which represents the Data Model and its changes. There are three types of possible changes which are represented by different colours: Addition - Green This is a change present on the source branch but isn\u2019t currently on the target branch. These types of changes will usually be auto merged and therefore won\u2019t need selecting. Modification - Yellow This is a change to an item which is present on both the source and target branches. These changes aren\u2019t normally auto merged and therefore will require selection by the user. Deletion - Red These changes will remove items from the main branch. These are never auto merged and will require selection by the user. As changes can be made to different items within the Data Model , it is recommended that you check the tree in the left column to ensure that all the changes you wish to merge are selected. 2.1 Property changes \u00b6 If required changes can be selected from the trees in both the left and right columns, a new editor window will appear. This allows you to make custom changes and then commit these changes to the main branch. 2.2 Check In \u00b6 Once all the relevant changes have been selected and are highlighted by a green tick, click 'Commit Changes' . A new dialogue box will appear where you can enter a check in comment. This is not required but can be useful for future auditing. You also have the option to delete the source branch by selecting the tick box to the left of the 'Delete Source Branch' option. To migrate all the changes to the main branch click 'Commit' and a green notification box should appear at the bottom right of your screen, confirming that the commit was successful. If the commit was unsuccessful, then a red notification box will appear at the bottom right of your screen. Correct the errors and try again.","title":"Version data models"},{"location":"user-guides/version-data-models/version-data-models/#1-versioning","text":"Data Models can only be versioned if they meet the below criteria: The Data Model must be finalised The Data Model must have a version number If your Data Model satisfies both of these conditions, then you will be able to create a new version. To do this, highlight the relevant Data Model in the Model Tree and click the three vertical dots at the top right of the details panel. Select 'Create a New Version' from the dropdown menu. You will then be presented with three options: New Fork This will create a copy of the Data Model with a new name and a new 'main' branch. Use this option if you are planning on taking this model in a new direction, or under a new authority. New Version This will create a copy of the Data Model under the 'main' branch. Use this option if you want to create the next iteration of this model. New Branch This will create a copy of the Data Model in a new branch, which you can choose the name for. Use this option if you want to make some changes that you subsequently wish to merge back into 'main' .","title":"1. Versioning"},{"location":"user-guides/version-data-models/version-data-models/#11-new-branch","text":"For this user guide, we will select 'New Branch' . A text box will then appear where you can enter a 'Branch name' . This will distinguish your Data Model from the others in your Data Model family. Once completed, click 'Add Branch' . This will create a copy of the Data Model for you to edit. If this is the first branch, then a 'main' branch will also be created. This will be the branch that changes from other branches will be merged into.","title":"1.1 New Branch"},{"location":"user-guides/version-data-models/version-data-models/#2-merging","text":"To merge completed changes of a Data Model into the main branch, click the three vertical dots at the top right of the details panel. Select 'Merge' from the dropdown menu and then click 'Merge Model' . This will take you to a merge screen which is split into three columns. The left column shows the source branch which you are merging from. The middle column displays the items to be merged and the right column shows the items currently on the main branch. The right and left columns will display a tree which represents the Data Model and its changes. There are three types of possible changes which are represented by different colours: Addition - Green This is a change present on the source branch but isn\u2019t currently on the target branch. These types of changes will usually be auto merged and therefore won\u2019t need selecting. Modification - Yellow This is a change to an item which is present on both the source and target branches. These changes aren\u2019t normally auto merged and therefore will require selection by the user. Deletion - Red These changes will remove items from the main branch. These are never auto merged and will require selection by the user. As changes can be made to different items within the Data Model , it is recommended that you check the tree in the left column to ensure that all the changes you wish to merge are selected.","title":"2. Merging"},{"location":"user-guides/version-data-models/version-data-models/#21-property-changes","text":"If required changes can be selected from the trees in both the left and right columns, a new editor window will appear. This allows you to make custom changes and then commit these changes to the main branch.","title":"2.1 Property changes"},{"location":"user-guides/version-data-models/version-data-models/#22-check-in","text":"Once all the relevant changes have been selected and are highlighted by a green tick, click 'Commit Changes' . A new dialogue box will appear where you can enter a check in comment. This is not required but can be useful for future auditing. You also have the option to delete the source branch by selecting the tick box to the left of the 'Delete Source Branch' option. To migrate all the changes to the main branch click 'Commit' and a green notification box should appear at the bottom right of your screen, confirming that the commit was successful. If the commit was unsuccessful, then a red notification box will appear at the bottom right of your screen. Correct the errors and try again.","title":"2.2 Check In"}]}